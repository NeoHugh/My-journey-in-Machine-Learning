{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pandas.read_csv('./data_ml1/train.csv',encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = data.iloc[:,3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Neo\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\Neo\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\frame.py:3089: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._where(-key, value, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "data[data =='NR']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['14', '14', '14', ..., '15', '15', '15'],\n",
       "       ['1.8', '1.8', '1.8', ..., '1.8', '1.8', '1.8'],\n",
       "       ['0.51', '0.41', '0.39', ..., '0.35', '0.36', '0.32'],\n",
       "       ...,\n",
       "       ['36', '55', '72', ..., '118', '100', '105'],\n",
       "       ['1.9', '2.4', '1.9', ..., '1.5', '2', '2'],\n",
       "       ['0.7', '0.8', '1.8', ..., '1.6', '1.8', '2']], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_data = {}\n",
    "for month in range(12):\n",
    "    sample = numpy.empty([18,480])\n",
    "    for day in range(20):\n",
    "        sample[:,day*24:(day+1)*24]= raw_data[18*(20*month+day):18*(20*month+day+1),:]\n",
    "    month_data[month]=sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([[14.  , 14.  , 14.  , ..., 14.  , 13.  , 13.  ],\n",
       "        [ 1.8 ,  1.8 ,  1.8 , ...,  1.8 ,  1.8 ,  1.8 ],\n",
       "        [ 0.51,  0.41,  0.39, ...,  0.34,  0.41,  0.43],\n",
       "        ...,\n",
       "        [35.  , 79.  ,  2.4 , ..., 48.  , 63.  , 53.  ],\n",
       "        [ 1.4 ,  1.8 ,  1.  , ...,  1.1 ,  1.9 ,  1.9 ],\n",
       "        [ 0.5 ,  0.9 ,  0.6 , ...,  1.2 ,  1.2 ,  1.3 ]]),\n",
       " 1: array([[ 15.  ,  14.  ,  14.  , ...,   8.4 ,   8.  ,   7.6 ],\n",
       "        [  1.8 ,   1.8 ,   1.7 , ...,   1.7 ,   1.7 ,   1.7 ],\n",
       "        [  0.27,   0.26,   0.25, ...,   0.36,   0.35,   0.32],\n",
       "        ...,\n",
       "        [113.  , 109.  , 104.  , ...,  72.  ,  65.  ,  69.  ],\n",
       "        [  2.3 ,   2.2 ,   2.6 , ...,   1.9 ,   2.9 ,   1.5 ],\n",
       "        [  2.5 ,   2.2 ,   2.2 , ...,   0.9 ,   1.6 ,   1.1 ]]),\n",
       " 2: array([[ 18.  ,  18.  ,  18.  , ...,  14.  ,  13.  ,  13.  ],\n",
       "        [  1.8 ,   1.8 ,   1.8 , ...,   1.8 ,   1.8 ,   1.8 ],\n",
       "        [  0.39,   0.36,   0.4 , ...,   0.42,   0.47,   0.49],\n",
       "        ...,\n",
       "        [103.  , 128.  , 115.  , ...,  60.  ,  94.  ,  53.  ],\n",
       "        [  1.7 ,   1.4 ,   1.8 , ...,   4.2 ,   3.5 ,   4.3 ],\n",
       "        [  1.9 ,   0.8 ,   1.5 , ...,   3.1 ,   2.4 ,   2.4 ]]),\n",
       " 3: array([[ 19.  ,  18.  ,  17.  , ...,  24.  ,  24.  ,  23.  ],\n",
       "        [  1.7 ,   1.7 ,   1.7 , ...,   1.8 ,   1.8 ,   1.9 ],\n",
       "        [  0.42,   0.42,   0.42, ...,   0.41,   0.46,   0.42],\n",
       "        ...,\n",
       "        [308.  , 308.  , 320.  , ..., 331.  , 261.  , 273.  ],\n",
       "        [  1.7 ,   2.2 ,   2.  , ...,   1.  ,   1.  ,   0.8 ],\n",
       "        [  1.5 ,   1.5 ,   1.2 , ...,   0.6 ,   1.1 ,   0.9 ]]),\n",
       " 4: array([[1.90e+01, 1.90e+01, 2.00e+01, ..., 2.60e+01, 2.60e+01, 2.50e+01],\n",
       "        [1.80e+00, 1.80e+00, 1.80e+00, ..., 1.60e+00, 1.60e+00, 1.60e+00],\n",
       "        [4.80e-01, 4.70e-01, 4.50e-01, ..., 1.50e-01, 1.50e-01, 1.30e-01],\n",
       "        ...,\n",
       "        [2.90e+02, 6.90e+01, 2.50e+02, ..., 1.74e+02, 1.95e+02, 1.69e+02],\n",
       "        [1.50e+00, 1.90e+00, 1.70e+00, ..., 3.10e+00, 3.10e+00, 2.90e+00],\n",
       "        [4.00e-01, 5.00e-01, 1.00e+00, ..., 2.90e+00, 2.40e+00, 3.10e+00]]),\n",
       " 5: array([[2.60e+01, 2.50e+01, 2.50e+01, ..., 2.70e+01, 2.70e+01, 2.80e+01],\n",
       "        [1.70e+00, 1.70e+00, 1.70e+00, ..., 1.60e+00, 1.60e+00, 1.60e+00],\n",
       "        [3.50e-01, 3.40e-01, 3.40e-01, ..., 2.60e-01, 1.90e-01, 1.60e-01],\n",
       "        ...,\n",
       "        [1.18e+02, 1.22e+02, 1.19e+02, ..., 1.16e+02, 1.59e+02, 1.62e+02],\n",
       "        [1.60e+00, 1.40e+00, 1.30e+00, ..., 1.70e+00, 1.00e+00, 2.40e+00],\n",
       "        [1.50e+00, 1.50e+00, 1.30e+00, ..., 1.30e+00, 1.30e+00, 1.70e+00]]),\n",
       " 6: array([[2.60e+01, 2.50e+01, 2.60e+01, ..., 2.80e+01, 2.80e+01, 2.80e+01],\n",
       "        [1.60e+00, 1.60e+00, 1.60e+00, ..., 1.60e+00, 1.60e+00, 1.70e+00],\n",
       "        [1.40e-01, 1.30e-01, 1.30e-01, ..., 3.10e-01, 3.00e-01, 2.70e-01],\n",
       "        ...,\n",
       "        [1.06e+02, 1.24e+02, 1.17e+02, ..., 1.27e+02, 1.33e+02, 1.72e+02],\n",
       "        [1.60e+00, 1.80e+00, 1.20e+00, ..., 1.60e+00, 1.40e+00, 1.70e+00],\n",
       "        [2.00e+00, 2.20e+00, 1.70e+00, ..., 1.70e+00, 1.30e+00, 1.60e+00]]),\n",
       " 7: array([[2.80e+01, 2.80e+01, 2.80e+01, ..., 2.60e+01, 2.60e+01, 2.60e+01],\n",
       "        [1.60e+00, 1.60e+00, 1.60e+00, ..., 1.70e+00, 1.70e+00, 1.70e+00],\n",
       "        [2.60e-01, 2.00e-01, 1.60e-01, ..., 1.60e-01, 1.40e-01, 1.30e-01],\n",
       "        ...,\n",
       "        [2.04e+02, 1.77e+02, 1.72e+02, ..., 1.68e+02, 1.80e+02, 1.62e+02],\n",
       "        [2.90e+00, 2.80e+00, 2.70e+00, ..., 2.90e+00, 2.80e+00, 2.50e+00],\n",
       "        [3.00e+00, 2.80e+00, 2.70e+00, ..., 3.10e+00, 2.90e+00, 2.50e+00]]),\n",
       " 8: array([[ 25.  ,  25.  ,  25.  , ...,  26.  ,  26.  ,  26.  ],\n",
       "        [  1.7 ,   1.7 ,   1.7 , ...,   1.6 ,   1.6 ,   1.7 ],\n",
       "        [  0.28,   0.27,   0.26, ...,   0.28,   0.24,   0.23],\n",
       "        ...,\n",
       "        [ 98.  , 109.  , 108.  , ..., 163.  ,  71.  ,  55.  ],\n",
       "        [  1.8 ,   1.9 ,   1.1 , ...,   1.2 ,   1.1 ,   0.7 ],\n",
       "        [  1.4 ,   1.9 ,   1.7 , ...,   3.4 ,   1.  ,   0.7 ]]),\n",
       " 9: array([[ 25.  ,  25.  ,  25.  , ...,  23.  ,  22.  ,  22.  ],\n",
       "        [  1.7 ,   1.7 ,   1.7 , ...,   1.8 ,   1.7 ,   1.7 ],\n",
       "        [  0.24,   0.26,   0.27, ...,   0.42,   0.35,   0.26],\n",
       "        ...,\n",
       "        [ 72.  , 100.  ,  68.  , ..., 109.  , 110.  , 107.  ],\n",
       "        [  1.1 ,   1.4 ,   1.1 , ...,   2.2 ,   2.4 ,   2.5 ],\n",
       "        [  1.8 ,   1.2 ,   0.9 , ...,   2.1 ,   2.2 ,   2.3 ]]),\n",
       " 10: array([[ 22.  ,  21.  ,  21.  , ...,  19.  ,  18.  ,  18.  ],\n",
       "        [  1.9 ,   1.9 ,   1.9 , ...,   1.7 ,   1.7 ,   1.7 ],\n",
       "        [  0.79,   0.71,   0.61, ...,   0.36,   0.36,   0.37],\n",
       "        ...,\n",
       "        [100.  , 117.  , 110.  , ..., 117.  , 117.  , 114.  ],\n",
       "        [  1.1 ,   1.9 ,   1.7 , ...,   2.1 ,   2.2 ,   1.9 ],\n",
       "        [  0.7 ,   1.1 ,   1.2 , ...,   1.8 ,   2.1 ,   1.9 ]]),\n",
       " 11: array([[ 23.  ,  23.  ,  23.  , ...,  13.  ,  13.  ,  13.  ],\n",
       "        [  1.6 ,   1.7 ,   1.7 , ...,   1.8 ,   1.8 ,   1.8 ],\n",
       "        [  0.22,   0.2 ,   0.18, ...,   0.51,   0.57,   0.56],\n",
       "        ...,\n",
       "        [ 93.  ,  50.  ,  99.  , ..., 118.  , 100.  , 105.  ],\n",
       "        [  1.8 ,   2.1 ,   3.2 , ...,   1.5 ,   2.  ,   2.  ],\n",
       "        [  1.3 ,   0.9 ,   1.  , ...,   1.6 ,   1.8 ,   2.  ]])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "month_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = numpy.empty([12 * 471, 18 * 9], dtype = float)\n",
    "y = numpy.empty([12 * 471, 1], dtype = float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14.  14.  14.  ...  2.   2.   0.5]\n",
      " [14.  14.  13.  ...  2.   0.5  0.3]\n",
      " [14.  13.  12.  ...  0.5  0.3  0.8]\n",
      " ...\n",
      " [17.  18.  19.  ...  1.1  1.4  1.3]\n",
      " [18.  19.  18.  ...  1.4  1.3  1.6]\n",
      " [19.  18.  17.  ...  1.3  1.6  1.8]]\n",
      "[[30.]\n",
      " [41.]\n",
      " [44.]\n",
      " ...\n",
      " [17.]\n",
      " [24.]\n",
      " [29.]]\n"
     ]
    }
   ],
   "source": [
    "for month in range(12):\n",
    "    for day in range(20):\n",
    "        for hour in range(24):\n",
    "            if day == 19 and hour > 14:\n",
    "                continue\n",
    "            x[month * 471 + day * 24 + hour, :] = month_data[month][:,day * 24 + hour : day * 24 + hour + 9].reshape(1, -1) #vector dim:18*9 (9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9)\n",
    "#9*9\n",
    "            y[month * 471 + day * 24 + hour, 0] = month_data[month][9, day * 24 + hour + 9] #value\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x：0-17（hour1 18-35（hour2 36-53 (hour3  ……\n",
    "#x: 12*471 rows in total\n",
    "mean_x= numpy.mean(x,axis=0)\n",
    "std_x=numpy.std(x,axis=0)\n",
    "for i in range(len(x)):\n",
    "    for j in range(len(x[0])):\n",
    "        if std_x[j]!=0:\n",
    "            x[i][j] = (x[i][j] - mean_x[j])/std_x[j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_y=numpy.mean(y,axis=0)\n",
    "std_y=numpy.std(y,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y)):\n",
    "    y[i]-=mean_y\n",
    "    y[i]/=std_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.shape(mean_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5652"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "av=numpy.mean(y,axis=0)\n",
    "std=numpy.std(y,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5652, 1)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.shape(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight=numpy.random.rand(162,1)\n",
    "#print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.35825331 -1.35883937 -1.359222   ...  0.26650729  0.2656797\n",
      "  -1.14082131]\n",
      " [-1.35825331 -1.35883937 -1.51819928 ...  0.26650729 -1.13963133\n",
      "  -1.32832904]\n",
      " [-1.35825331 -1.51789368 -1.67717656 ... -1.13923451 -1.32700613\n",
      "  -0.85955971]\n",
      " ...\n",
      " [-0.88092053 -0.72262212 -0.56433559 ... -0.57693779 -0.29644471\n",
      "  -0.39079039]\n",
      " [-0.7218096  -0.56356781 -0.72331287 ... -0.29578943 -0.39013211\n",
      "  -0.1095288 ]\n",
      " [-0.56269867 -0.72262212 -0.88229015 ... -0.38950555 -0.10906991\n",
      "   0.07797893]] (5652, 162)\n",
      "[[ 0.51922102]\n",
      " [ 1.18134123]\n",
      " [ 1.36191947]\n",
      " ...\n",
      " [-0.26328469]\n",
      " [ 0.15806453]\n",
      " [ 0.45902827]] (5652, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x,numpy.shape(x))\n",
    "print(y,numpy.shape(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.8 为train, 0.2 为test\n",
    "from math import floor \n",
    "train_x=x[:floor(len(x)*0.8),:]\n",
    "test_x=x[floor(len(x)*0.8):,:]\n",
    "train_y=y[:floor(len(y)*0.8),:]\n",
    "test_y=y[floor(len(y)*0.8):,:]\n",
    "#print(train_x)\n",
    "#print(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4521, 162)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.shape(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162, 1)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.shape(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4521, 162)\n",
      "(162, 1)\n",
      "(4521, 1)\n"
     ]
    }
   ],
   "source": [
    "print(numpy.shape(train_x))\n",
    "print(numpy.shape(weight))\n",
    "print(numpy.shape(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 0 th iteration gives loss of 2213.929802015846\n",
      "The 1 th iteration gives loss of 986.1969550161099\n",
      "The 2 th iteration gives loss of 454.0067917233801\n",
      "The 3 th iteration gives loss of 223.19623195383252\n",
      "The 4 th iteration gives loss of 122.9750445689453\n",
      "The 5 th iteration gives loss of 79.3400828549911\n",
      "The 6 th iteration gives loss of 60.225883158879604\n",
      "The 7 th iteration gives loss of 51.738510460836004\n",
      "The 8 th iteration gives loss of 47.85772316875278\n",
      "The 9 th iteration gives loss of 45.97493824016939\n",
      "The 10 th iteration gives loss of 44.95983888420414\n",
      "The 11 th iteration gives loss of 44.32263888954187\n",
      "The 12 th iteration gives loss of 43.851093147002096\n",
      "The 13 th iteration gives loss of 43.453214908996834\n",
      "The 14 th iteration gives loss of 43.0891254361393\n",
      "The 15 th iteration gives loss of 42.741524559067905\n",
      "The 16 th iteration gives loss of 42.402895618669376\n",
      "The 17 th iteration gives loss of 42.0699613846858\n",
      "The 18 th iteration gives loss of 41.741281811243695\n",
      "The 19 th iteration gives loss of 41.416213145141114\n",
      "The 20 th iteration gives loss of 41.0944569060639\n",
      "The 21 th iteration gives loss of 40.7758644573692\n",
      "The 22 th iteration gives loss of 40.46035232387925\n",
      "The 23 th iteration gives loss of 40.147865496209626\n",
      "The 24 th iteration gives loss of 39.838361527624286\n",
      "The 25 th iteration gives loss of 39.53180364029677\n",
      "The 26 th iteration gives loss of 39.22815773537119\n",
      "The 27 th iteration gives loss of 38.92739109457159\n",
      "The 28 th iteration gives loss of 38.629471814794215\n",
      "The 29 th iteration gives loss of 38.334368560337424\n",
      "The 30 th iteration gives loss of 38.042050452800474\n",
      "The 31 th iteration gives loss of 37.75248702066996\n",
      "The 32 th iteration gives loss of 37.46564817480507\n",
      "The 33 th iteration gives loss of 37.18150419518006\n",
      "The 34 th iteration gives loss of 36.90002572253895\n",
      "The 35 th iteration gives loss of 36.621183752213646\n",
      "The 36 th iteration gives loss of 36.344949628913255\n",
      "The 37 th iteration gives loss of 36.07129504196724\n",
      "The 38 th iteration gives loss of 35.800192020799116\n",
      "The 39 th iteration gives loss of 35.53161293053175\n",
      "The 40 th iteration gives loss of 35.26553046768249\n",
      "The 41 th iteration gives loss of 35.001917655928544\n",
      "The 42 th iteration gives loss of 34.74074784193423\n",
      "The 43 th iteration gives loss of 34.48199469123595\n",
      "The 44 th iteration gives loss of 34.225632184182395\n",
      "The 45 th iteration gives loss of 33.971634611929034\n",
      "The 46 th iteration gives loss of 33.71997657248544\n",
      "The 47 th iteration gives loss of 33.470632966814875\n",
      "The 48 th iteration gives loss of 33.22357899498543\n",
      "The 49 th iteration gives loss of 32.97879015237185\n",
      "The 50 th iteration gives loss of 32.73624222590703\n",
      "The 51 th iteration gives loss of 32.49591129038393\n",
      "The 52 th iteration gives loss of 32.25777370480474\n",
      "The 53 th iteration gives loss of 32.021806108779224\n",
      "The 54 th iteration gives loss of 31.787985418969832\n",
      "The 55 th iteration gives loss of 31.556288825584005\n",
      "The 56 th iteration gives loss of 31.32669378891219\n",
      "The 57 th iteration gives loss of 31.099178035911553\n",
      "The 58 th iteration gives loss of 30.873719556834583\n",
      "The 59 th iteration gives loss of 30.65029660190186\n",
      "The 60 th iteration gives loss of 30.428887678018953\n",
      "The 61 th iteration gives loss of 30.20947154553552\n",
      "The 62 th iteration gives loss of 29.992027215047866\n",
      "The 63 th iteration gives loss of 29.776533944242974\n",
      "The 64 th iteration gives loss of 29.56297123478392\n",
      "The 65 th iteration gives loss of 29.351318829236238\n",
      "The 66 th iteration gives loss of 29.14155670803453\n",
      "The 67 th iteration gives loss of 28.933665086488826\n",
      "The 68 th iteration gives loss of 28.727624411830064\n",
      "The 69 th iteration gives loss of 28.523415360294422\n",
      "The 70 th iteration gives loss of 28.321018834245503\n",
      "The 71 th iteration gives loss of 28.12041595933466\n",
      "The 72 th iteration gives loss of 27.92158808169788\n",
      "The 73 th iteration gives loss of 27.724516765189776\n",
      "The 74 th iteration gives loss of 27.529183788653466\n",
      "The 75 th iteration gives loss of 27.335571143226325\n",
      "The 76 th iteration gives loss of 27.143661029680693\n",
      "The 77 th iteration gives loss of 26.95343585579948\n",
      "The 78 th iteration gives loss of 26.764878233786007\n",
      "The 79 th iteration gives loss of 26.577970977707594\n",
      "The 80 th iteration gives loss of 26.392697100972484\n",
      "The 81 th iteration gives loss of 26.20903981383972\n",
      "The 82 th iteration gives loss of 26.02698252096142\n",
      "The 83 th iteration gives loss of 25.846508818957115\n",
      "The 84 th iteration gives loss of 25.66760249401976\n",
      "The 85 th iteration gives loss of 25.490247519552685\n",
      "The 86 th iteration gives loss of 25.314428053837684\n",
      "The 87 th iteration gives loss of 25.140128437732898\n",
      "The 88 th iteration gives loss of 24.96733319240129\n",
      "The 89 th iteration gives loss of 24.79602701706814\n",
      "The 90 th iteration gives loss of 24.62619478680799\n",
      "The 91 th iteration gives loss of 24.45782155036016\n",
      "The 92 th iteration gives loss of 24.2908925279728\n",
      "The 93 th iteration gives loss of 24.125393109274793\n",
      "The 94 th iteration gives loss of 23.961308851175417\n",
      "The 95 th iteration gives loss of 23.798625475791184\n",
      "The 96 th iteration gives loss of 23.637328868399543\n",
      "The 97 th iteration gives loss of 23.477405075419203\n",
      "The 98 th iteration gives loss of 23.31884030241658\n",
      "The 99 th iteration gives loss of 23.16162091213811\n",
      "The 100 th iteration gives loss of 23.00573342256802\n",
      "The 101 th iteration gives loss of 22.851164505011216\n",
      "The 102 th iteration gives loss of 22.69790098220111\n",
      "The 103 th iteration gives loss of 22.54592982643177\n",
      "The 104 th iteration gives loss of 22.39523815771444\n",
      "The 105 th iteration gives loss of 22.245813241957624\n",
      "The 106 th iteration gives loss of 22.09764248917089\n",
      "The 107 th iteration gives loss of 21.95071345169183\n",
      "The 108 th iteration gives loss of 21.80501382243597\n",
      "The 109 th iteration gives loss of 21.66053143316918\n",
      "The 110 th iteration gives loss of 21.517254252802434\n",
      "The 111 th iteration gives loss of 21.37517038570852\n",
      "The 112 th iteration gives loss of 21.23426807006056\n",
      "The 113 th iteration gives loss of 21.09453567619189\n",
      "The 114 th iteration gives loss of 20.955961704977106\n",
      "The 115 th iteration gives loss of 20.818534786233847\n",
      "The 116 th iteration gives loss of 20.6822436771453\n",
      "The 117 th iteration gives loss of 20.547077260702995\n",
      "The 118 th iteration gives loss of 20.413024544169527\n",
      "The 119 th iteration gives loss of 20.280074657561272\n",
      "The 120 th iteration gives loss of 20.148216852150306\n",
      "The 121 th iteration gives loss of 20.01744049898575\n",
      "The 122 th iteration gives loss of 19.887735087434354\n",
      "The 123 th iteration gives loss of 19.75909022373919\n",
      "The 124 th iteration gives loss of 19.631495629597552\n",
      "The 125 th iteration gives loss of 19.504941140756607\n",
      "The 126 th iteration gives loss of 19.37941670562728\n",
      "The 127 th iteration gives loss of 19.254912383915936\n",
      "The 128 th iteration gives loss of 19.13141834527345\n",
      "The 129 th iteration gives loss of 19.008924867961728\n",
      "The 130 th iteration gives loss of 18.887422337537235\n",
      "The 131 th iteration gives loss of 18.766901245551335\n",
      "The 132 th iteration gives loss of 18.64735218826741\n",
      "The 133 th iteration gives loss of 18.52876586539428\n",
      "The 134 th iteration gives loss of 18.411133078835793\n",
      "The 135 th iteration gives loss of 18.294444731456398\n",
      "The 136 th iteration gives loss of 18.178691825862707\n",
      "The 137 th iteration gives loss of 18.063865463200248\n",
      "The 138 th iteration gives loss of 17.94995684196591\n",
      "The 139 th iteration gives loss of 17.836957256835255\n",
      "The 140 th iteration gives loss of 17.724858097505095\n",
      "The 141 th iteration gives loss of 17.61365084755056\n",
      "The 142 th iteration gives loss of 17.503327083296902\n",
      "The 143 th iteration gives loss of 17.393878472705524\n",
      "The 144 th iteration gives loss of 17.285296774274308\n",
      "The 145 th iteration gives loss of 17.177573835951982\n",
      "The 146 th iteration gives loss of 17.07070159406609\n",
      "The 147 th iteration gives loss of 16.964672072264804\n",
      "The 148 th iteration gives loss of 16.859477380472125\n",
      "The 149 th iteration gives loss of 16.755109713856253\n",
      "The 150 th iteration gives loss of 16.65156135181128\n",
      "The 151 th iteration gives loss of 16.548824656951474\n",
      "The 152 th iteration gives loss of 16.44689207411876\n",
      "The 153 th iteration gives loss of 16.345756129402453\n",
      "The 154 th iteration gives loss of 16.245409429171396\n",
      "The 155 th iteration gives loss of 16.145844659118715\n",
      "The 156 th iteration gives loss of 16.047054583318282\n",
      "The 157 th iteration gives loss of 15.949032043293354\n",
      "The 158 th iteration gives loss of 15.851769957096923\n",
      "The 159 th iteration gives loss of 15.755261318403683\n",
      "The 160 th iteration gives loss of 15.65949919561348\n",
      "The 161 th iteration gives loss of 15.56447673096615\n",
      "The 162 th iteration gives loss of 15.470187139667525\n",
      "The 163 th iteration gives loss of 15.376623709026381\n",
      "The 164 th iteration gives loss of 15.283779797602348\n",
      "The 165 th iteration gives loss of 15.191648834364583\n",
      "The 166 th iteration gives loss of 15.100224317861038\n",
      "The 167 th iteration gives loss of 15.009499815398115\n",
      "The 168 th iteration gives loss of 14.919468962230841\n",
      "The 169 th iteration gives loss of 14.830125460763028\n",
      "The 170 th iteration gives loss of 14.741463079757603\n",
      "The 171 th iteration gives loss of 14.653475653556884\n",
      "The 172 th iteration gives loss of 14.566157081312559\n",
      "The 173 th iteration gives loss of 14.479501326225536\n",
      "The 174 th iteration gives loss of 14.393502414795028\n",
      "The 175 th iteration gives loss of 14.308154436077507\n",
      "The 176 th iteration gives loss of 14.223451540954414\n",
      "The 177 th iteration gives loss of 14.139387941409488\n",
      "The 178 th iteration gives loss of 14.055957909815014\n",
      "The 179 th iteration gives loss of 13.97315577822692\n",
      "The 180 th iteration gives loss of 13.890975937688808\n",
      "The 181 th iteration gives loss of 13.809412837544679\n",
      "The 182 th iteration gives loss of 13.72846098476016\n",
      "The 183 th iteration gives loss of 13.648114943252388\n",
      "The 184 th iteration gives loss of 13.568369333228091\n",
      "The 185 th iteration gives loss of 13.48921883053006\n",
      "The 186 th iteration gives loss of 13.410658165991789\n",
      "The 187 th iteration gives loss of 13.33268212479993\n",
      "The 188 th iteration gives loss of 13.255285545865085\n",
      "The 189 th iteration gives loss of 13.178463321200013\n",
      "The 190 th iteration gives loss of 13.102210395305994\n",
      "The 191 th iteration gives loss of 13.026521764566581\n",
      "The 192 th iteration gives loss of 12.951392476648818\n",
      "The 193 th iteration gives loss of 12.876817629912257\n",
      "The 194 th iteration gives loss of 12.802792372824845\n",
      "The 195 th iteration gives loss of 12.729311903386574\n",
      "The 196 th iteration gives loss of 12.65637146855981\n",
      "The 197 th iteration gives loss of 12.583966363707074\n",
      "The 198 th iteration gives loss of 12.512091932035489\n",
      "The 199 th iteration gives loss of 12.440743564048383\n",
      "The 200 th iteration gives loss of 12.369916697003424\n",
      "The 201 th iteration gives loss of 12.299606814377766\n",
      "The 202 th iteration gives loss of 12.229809445339459\n",
      "The 203 th iteration gives loss of 12.160520164225632\n",
      "The 204 th iteration gives loss of 12.091734590027201\n",
      "The 205 th iteration gives loss of 12.02344838587957\n",
      "The 206 th iteration gives loss of 11.95565725856009\n",
      "The 207 th iteration gives loss of 11.8883569579913\n",
      "The 208 th iteration gives loss of 11.821543276750617\n",
      "The 209 th iteration gives loss of 11.7552120495858\n",
      "The 210 th iteration gives loss of 11.689359152936694\n",
      "The 211 th iteration gives loss of 11.623980504462454\n",
      "The 212 th iteration gives loss of 11.559072062574938\n",
      "The 213 th iteration gives loss of 11.494629825977709\n",
      "The 214 th iteration gives loss of 11.430649833210612\n",
      "The 215 th iteration gives loss of 11.367128162200084\n",
      "The 216 th iteration gives loss of 11.304060929814955\n",
      "The 217 th iteration gives loss of 11.241444291427612\n",
      "The 218 th iteration gives loss of 11.179274440480556\n",
      "The 219 th iteration gives loss of 11.117547608058404\n",
      "The 220 th iteration gives loss of 11.056260062464927\n",
      "The 221 th iteration gives loss of 10.995408108805476\n",
      "The 222 th iteration gives loss of 10.934988088574297\n",
      "The 223 th iteration gives loss of 10.874996379247065\n",
      "The 224 th iteration gives loss of 10.815429393878272\n",
      "The 225 th iteration gives loss of 10.756283580703593\n",
      "The 226 th iteration gives loss of 10.69755542274712\n",
      "The 227 th iteration gives loss of 10.639241437433256\n",
      "The 228 th iteration gives loss of 10.581338176203525\n",
      "The 229 th iteration gives loss of 10.523842224137868\n",
      "The 230 th iteration gives loss of 10.466750199580696\n",
      "The 231 th iteration gives loss of 10.41005875377133\n",
      "The 232 th iteration gives loss of 10.353764570479068\n",
      "The 233 th iteration gives loss of 10.297864365642573\n",
      "The 234 th iteration gives loss of 10.242354887013814\n",
      "The 235 th iteration gives loss of 10.187232913806005\n",
      "The 236 th iteration gives loss of 10.132495256346152\n",
      "The 237 th iteration gives loss of 10.078138755731649\n",
      "The 238 th iteration gives loss of 10.024160283490954\n",
      "The 239 th iteration gives loss of 9.97055674124847\n",
      "The 240 th iteration gives loss of 9.917325060393518\n",
      "The 241 th iteration gives loss of 9.86446220175311\n",
      "The 242 th iteration gives loss of 9.811965155268945\n",
      "The 243 th iteration gives loss of 9.759830939678105\n",
      "The 244 th iteration gives loss of 9.708056602197573\n",
      "The 245 th iteration gives loss of 9.65663921821269\n",
      "The 246 th iteration gives loss of 9.60557589096929\n",
      "The 247 th iteration gives loss of 9.554863751269524\n",
      "The 248 th iteration gives loss of 9.504499957171372\n",
      "The 249 th iteration gives loss of 9.454481693691674\n",
      "The 250 th iteration gives loss of 9.404806172512874\n",
      "The 251 th iteration gives loss of 9.355470631693144\n",
      "The 252 th iteration gives loss of 9.306472335380063\n",
      "The 253 th iteration gives loss of 9.25780857352767\n",
      "The 254 th iteration gives loss of 9.209476661616936\n",
      "The 255 th iteration gives loss of 9.161473940379544\n",
      "The 256 th iteration gives loss of 9.113797775525025\n",
      "The 257 th iteration gives loss of 9.066445557471086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 258 th iteration gives loss of 9.019414701077176\n",
      "The 259 th iteration gives loss of 8.972702645381249\n",
      "The 260 th iteration gives loss of 8.926306853339629\n",
      "The 261 th iteration gives loss of 8.880224811570015\n",
      "The 262 th iteration gives loss of 8.834454030097483\n",
      "The 263 th iteration gives loss of 8.788992042103528\n",
      "The 264 th iteration gives loss of 8.743836403678133\n",
      "The 265 th iteration gives loss of 8.698984693574696\n",
      "The 266 th iteration gives loss of 8.654434512967928\n",
      "The 267 th iteration gives loss of 8.61018348521459\n",
      "The 268 th iteration gives loss of 8.566229255617015\n",
      "The 269 th iteration gives loss of 8.522569491189579\n",
      "The 270 th iteration gives loss of 8.479201880427738\n",
      "The 271 th iteration gives loss of 8.436124133079911\n",
      "The 272 th iteration gives loss of 8.393333979922051\n",
      "The 273 th iteration gives loss of 8.350829172534803\n",
      "The 274 th iteration gives loss of 8.308607483083355\n",
      "The 275 th iteration gives loss of 8.266666704099881\n",
      "The 276 th iteration gives loss of 8.22500464826843\n",
      "The 277 th iteration gives loss of 8.183619148212564\n",
      "The 278 th iteration gives loss of 8.14250805628519\n",
      "The 279 th iteration gives loss of 8.101669244361192\n",
      "The 280 th iteration gives loss of 8.06110060363219\n",
      "The 281 th iteration gives loss of 8.02080004440388\n",
      "The 282 th iteration gives loss of 7.98076549589575\n",
      "The 283 th iteration gives loss of 7.940994906042973\n",
      "The 284 th iteration gives loss of 7.901486241300888\n",
      "The 285 th iteration gives loss of 7.862237486451427\n",
      "The 286 th iteration gives loss of 7.8232466444121425\n",
      "The 287 th iteration gives loss of 7.784511736047126\n",
      "The 288 th iteration gives loss of 7.746030799980456\n",
      "The 289 th iteration gives loss of 7.707801892411579\n",
      "The 290 th iteration gives loss of 7.669823086932869\n",
      "The 291 th iteration gives loss of 7.6320924743494505\n",
      "The 292 th iteration gives loss of 7.594608162500917\n",
      "The 293 th iteration gives loss of 7.557368276085241\n",
      "The 294 th iteration gives loss of 7.520370956484749\n",
      "The 295 th iteration gives loss of 7.483614361593955\n",
      "The 296 th iteration gives loss of 7.447096665649549\n",
      "The 297 th iteration gives loss of 7.410816059062246\n",
      "The 298 th iteration gives loss of 7.374770748250631\n",
      "The 299 th iteration gives loss of 7.338958955476903\n",
      "The 300 th iteration gives loss of 7.303378918684497\n",
      "The 301 th iteration gives loss of 7.268028891337577\n",
      "The 302 th iteration gives loss of 7.232907142262417\n",
      "The 303 th iteration gives loss of 7.198011955490509\n",
      "The 304 th iteration gives loss of 7.1633416301036\n",
      "The 305 th iteration gives loss of 7.1288944800804215\n",
      "The 306 th iteration gives loss of 7.0946688341451845\n",
      "The 307 th iteration gives loss of 7.060663035617762\n",
      "The 308 th iteration gives loss of 7.026875442265776\n",
      "The 309 th iteration gives loss of 6.993304426158095\n",
      "The 310 th iteration gives loss of 6.959948373520242\n",
      "The 311 th iteration gives loss of 6.926805684591279\n",
      "The 312 th iteration gives loss of 6.893874773482499\n",
      "The 313 th iteration gives loss of 6.861154068037501\n",
      "The 314 th iteration gives loss of 6.82864200969409\n",
      "The 315 th iteration gives loss of 6.796337053347606\n",
      "The 316 th iteration gives loss of 6.76423766721579\n",
      "The 317 th iteration gives loss of 6.732342332705309\n",
      "The 318 th iteration gives loss of 6.700649544279596\n",
      "The 319 th iteration gives loss of 6.669157809328432\n",
      "The 320 th iteration gives loss of 6.637865648038752\n",
      "The 321 th iteration gives loss of 6.606771593267156\n",
      "The 322 th iteration gives loss of 6.575874190413591\n",
      "The 323 th iteration gives loss of 6.545171997296706\n",
      "The 324 th iteration gives loss of 6.514663584030469\n",
      "The 325 th iteration gives loss of 6.484347532902187\n",
      "The 326 th iteration gives loss of 6.454222438251923\n",
      "The 327 th iteration gives loss of 6.424286906353267\n",
      "The 328 th iteration gives loss of 6.394539555295474\n",
      "The 329 th iteration gives loss of 6.3649790148667895\n",
      "The 330 th iteration gives loss of 6.335603926439278\n",
      "The 331 th iteration gives loss of 6.306412942854768\n",
      "The 332 th iteration gives loss of 6.277404728312145\n",
      "The 333 th iteration gives loss of 6.248577958255995\n",
      "The 334 th iteration gives loss of 6.219931319266241\n",
      "The 335 th iteration gives loss of 6.191463508949294\n",
      "The 336 th iteration gives loss of 6.163173235830176\n",
      "The 337 th iteration gives loss of 6.135059219246055\n",
      "The 338 th iteration gives loss of 6.107120189240738\n",
      "The 339 th iteration gives loss of 6.079354886460532\n",
      "The 340 th iteration gives loss of 6.051762062051166\n",
      "The 341 th iteration gives loss of 6.024340477555854\n",
      "The 342 th iteration gives loss of 5.997088904814473\n",
      "The 343 th iteration gives loss of 5.970006125863922\n",
      "The 344 th iteration gives loss of 5.9430909328394605\n",
      "The 345 th iteration gives loss of 5.916342127877357\n",
      "The 346 th iteration gives loss of 5.889758523018298\n",
      "The 347 th iteration gives loss of 5.863338940112104\n",
      "The 348 th iteration gives loss of 5.837082210723427\n",
      "The 349 th iteration gives loss of 5.810987176038434\n",
      "The 350 th iteration gives loss of 5.785052686772511\n",
      "The 351 th iteration gives loss of 5.759277603079129\n",
      "The 352 th iteration gives loss of 5.733660794459474\n",
      "The 353 th iteration gives loss of 5.708201139673224\n",
      "The 354 th iteration gives loss of 5.682897526650275\n",
      "The 355 th iteration gives loss of 5.657748852403416\n",
      "The 356 th iteration gives loss of 5.632754022941898\n",
      "The 357 th iteration gives loss of 5.607911953186049\n",
      "The 358 th iteration gives loss of 5.58322156688275\n",
      "The 359 th iteration gives loss of 5.558681796521789\n",
      "The 360 th iteration gives loss of 5.53429158325332\n",
      "The 361 th iteration gives loss of 5.5100498768058745\n",
      "The 362 th iteration gives loss of 5.485955635405626\n",
      "The 363 th iteration gives loss of 5.462007825696278\n",
      "The 364 th iteration gives loss of 5.438205422659876\n",
      "The 365 th iteration gives loss of 5.414547409538562\n",
      "The 366 th iteration gives loss of 5.391032777757024\n",
      "The 367 th iteration gives loss of 5.367660526845945\n",
      "The 368 th iteration gives loss of 5.344429664366136\n",
      "The 369 th iteration gives loss of 5.321339205833506\n",
      "The 370 th iteration gives loss of 5.298388174644938\n",
      "The 371 th iteration gives loss of 5.275575602004862\n",
      "The 372 th iteration gives loss of 5.252900526852596\n",
      "The 373 th iteration gives loss of 5.23036199579056\n",
      "The 374 th iteration gives loss of 5.207959063013161\n",
      "The 375 th iteration gives loss of 5.185690790236486\n",
      "The 376 th iteration gives loss of 5.163556246628796\n",
      "The 377 th iteration gives loss of 5.141554508741574\n",
      "The 378 th iteration gives loss of 5.119684660441512\n",
      "The 379 th iteration gives loss of 5.097945792843158\n",
      "The 380 th iteration gives loss of 5.07633700424219\n",
      "The 381 th iteration gives loss of 5.054857400049432\n",
      "The 382 th iteration gives loss of 5.033506092725735\n",
      "The 383 th iteration gives loss of 5.012282201717281\n",
      "The 384 th iteration gives loss of 4.9911848533916965\n",
      "The 385 th iteration gives loss of 4.9702131809749535\n",
      "The 386 th iteration gives loss of 4.9493663244886665\n",
      "The 387 th iteration gives loss of 4.9286434306883065\n",
      "The 388 th iteration gives loss of 4.908043653001913\n",
      "The 389 th iteration gives loss of 4.8875661514695095\n",
      "The 390 th iteration gives loss of 4.867210092683082\n",
      "The 391 th iteration gives loss of 4.8469746497273425\n",
      "The 392 th iteration gives loss of 4.826859002120921\n",
      "The 393 th iteration gives loss of 4.80686233575828\n",
      "The 394 th iteration gives loss of 4.786983842852236\n",
      "The 395 th iteration gives loss of 4.767222721876988\n",
      "The 396 th iteration gives loss of 4.7475781775119\n",
      "The 397 th iteration gives loss of 4.728049420585732\n",
      "The 398 th iteration gives loss of 4.708635668021439\n",
      "The 399 th iteration gives loss of 4.689336142781659\n",
      "The 400 th iteration gives loss of 4.670150073814675\n",
      "The 401 th iteration gives loss of 4.65107669600093\n",
      "The 402 th iteration gives loss of 4.63211525010019\n",
      "The 403 th iteration gives loss of 4.61326498269913\n",
      "The 404 th iteration gives loss of 4.594525146159492\n",
      "The 405 th iteration gives loss of 4.575894998566857\n",
      "The 406 th iteration gives loss of 4.557373803679853\n",
      "The 407 th iteration gives loss of 4.538960830879876\n",
      "The 408 th iteration gives loss of 4.520655355121466\n",
      "The 409 th iteration gives loss of 4.502456656882965\n",
      "The 410 th iteration gives loss of 4.484364022117865\n",
      "The 411 th iteration gives loss of 4.466376742206609\n",
      "The 412 th iteration gives loss of 4.448494113908788\n",
      "The 413 th iteration gives loss of 4.43071543931599\n",
      "The 414 th iteration gives loss of 4.413040025804952\n",
      "The 415 th iteration gives loss of 4.39546718599135\n",
      "The 416 th iteration gives loss of 4.377996237683907\n",
      "The 417 th iteration gives loss of 4.360626503839106\n",
      "The 418 th iteration gives loss of 4.343357312516228\n",
      "The 419 th iteration gives loss of 4.326187996832935\n",
      "The 420 th iteration gives loss of 4.309117894921338\n",
      "The 421 th iteration gives loss of 4.2921463498843115\n",
      "The 422 th iteration gives loss of 4.275272709752529\n",
      "The 423 th iteration gives loss of 4.258496327441645\n",
      "The 424 th iteration gives loss of 4.24181656071014\n",
      "The 425 th iteration gives loss of 4.225232772117455\n",
      "The 426 th iteration gives loss of 4.208744328982563\n",
      "The 427 th iteration gives loss of 4.192350603343039\n",
      "The 428 th iteration gives loss of 4.176050971914367\n",
      "The 429 th iteration gives loss of 4.159844816049847\n",
      "The 430 th iteration gives loss of 4.1437315217007855\n",
      "The 431 th iteration gives loss of 4.127710479377074\n",
      "The 432 th iteration gives loss of 4.11178108410823\n",
      "The 433 th iteration gives loss of 4.095942735404789\n",
      "The 434 th iteration gives loss of 4.080194837220048\n",
      "The 435 th iteration gives loss of 4.06453679791224\n",
      "The 436 th iteration gives loss of 4.048968030207025\n",
      "The 437 th iteration gives loss of 4.03348795116044\n",
      "The 438 th iteration gives loss of 4.01809598212212\n",
      "The 439 th iteration gives loss of 4.00279154869889\n",
      "The 440 th iteration gives loss of 3.987574080718806\n",
      "The 441 th iteration gives loss of 3.9724430121954546\n",
      "The 442 th iteration gives loss of 3.957397781292615\n",
      "The 443 th iteration gives loss of 3.9424378302893124\n",
      "The 444 th iteration gives loss of 3.9275626055451913\n",
      "The 445 th iteration gives loss of 3.912771557466166\n",
      "The 446 th iteration gives loss of 3.8980641404705554\n",
      "The 447 th iteration gives loss of 3.883439812955379\n",
      "The 448 th iteration gives loss of 3.8688980372630493\n",
      "The 449 th iteration gives loss of 3.8544382796484244\n",
      "The 450 th iteration gives loss of 3.840060010246185\n",
      "The 451 th iteration gives loss of 3.8257627030383756\n",
      "The 452 th iteration gives loss of 3.811545835822515\n",
      "The 453 th iteration gives loss of 3.7974088901797205\n",
      "The 454 th iteration gives loss of 3.7833513514434727\n",
      "The 455 th iteration gives loss of 3.7693727086683224\n",
      "The 456 th iteration gives loss of 3.755472454599179\n",
      "The 457 th iteration gives loss of 3.7416500856407895\n",
      "The 458 th iteration gives loss of 3.7279051018274525\n",
      "The 459 th iteration gives loss of 3.7142370067931734\n",
      "The 460 th iteration gives loss of 3.700645307741921\n",
      "The 461 th iteration gives loss of 3.6871295154183152\n",
      "The 462 th iteration gives loss of 3.6736891440785038\n",
      "The 463 th iteration gives loss of 3.6603237114614195\n",
      "The 464 th iteration gives loss of 3.647032738760133\n",
      "The 465 th iteration gives loss of 3.633815750593727\n",
      "The 466 th iteration gives loss of 3.6206722749791527\n",
      "The 467 th iteration gives loss of 3.60760184330362\n",
      "The 468 th iteration gives loss of 3.5946039902970526\n",
      "The 469 th iteration gives loss of 3.5816782540049066\n",
      "The 470 th iteration gives loss of 3.568824175761228\n",
      "The 471 th iteration gives loss of 3.556041300161911\n",
      "The 472 th iteration gives loss of 3.5433291750382834\n",
      "The 473 th iteration gives loss of 3.53068735143091\n",
      "The 474 th iteration gives loss of 3.518115383563633\n",
      "The 475 th iteration gives loss of 3.505612828817832\n",
      "The 476 th iteration gives loss of 3.493179247706997\n",
      "The 477 th iteration gives loss of 3.480814203851463\n",
      "The 478 th iteration gives loss of 3.4685172639534447\n",
      "The 479 th iteration gives loss of 3.456287997772216\n",
      "The 480 th iteration gives loss of 3.4441259780996623\n",
      "The 481 th iteration gives loss of 3.4320307807358783\n",
      "The 482 th iteration gives loss of 3.4200019844651677\n",
      "The 483 th iteration gives loss of 3.408039171032152\n",
      "The 484 th iteration gives loss of 3.3961419251181155\n",
      "The 485 th iteration gives loss of 3.3843098343176004\n",
      "The 486 th iteration gives loss of 3.3725424891152667\n",
      "The 487 th iteration gives loss of 3.36083948286278\n",
      "The 488 th iteration gives loss of 3.3492004117561325\n",
      "The 489 th iteration gives loss of 3.3376248748130775\n",
      "The 490 th iteration gives loss of 3.3261124738506873\n",
      "The 491 th iteration gives loss of 3.314662813463277\n",
      "The 492 th iteration gives loss of 3.303275501000427\n",
      "The 493 th iteration gives loss of 3.2919501465452288\n",
      "The 494 th iteration gives loss of 3.2806863628926974\n",
      "The 495 th iteration gives loss of 3.269483765528488\n",
      "The 496 th iteration gives loss of 3.258341972607682\n",
      "The 497 th iteration gives loss of 3.2472606049338273\n",
      "The 498 th iteration gives loss of 3.23623928593815\n",
      "The 499 th iteration gives loss of 3.225277641658991\n",
      "The 500 th iteration gives loss of 3.2143753007213967\n",
      "The 501 th iteration gives loss of 3.20353189431682\n",
      "The 502 th iteration gives loss of 3.1927470561832174\n",
      "The 503 th iteration gives loss of 3.182020422585093\n",
      "The 504 th iteration gives loss of 3.171351632293829\n",
      "The 505 th iteration gives loss of 3.160740326568234\n",
      "The 506 th iteration gives loss of 3.1501861491351693\n",
      "The 507 th iteration gives loss of 3.139688746170409\n",
      "The 508 th iteration gives loss of 3.1292477662796747\n",
      "The 509 th iteration gives loss of 3.118862860479785\n",
      "The 510 th iteration gives loss of 3.10853368218005\n",
      "The 511 th iteration gives loss of 3.098259887163778\n",
      "The 512 th iteration gives loss of 3.088041133569979\n",
      "The 513 th iteration gives loss of 3.077877081875149\n",
      "The 514 th iteration gives loss of 3.0677673948753363\n",
      "The 515 th iteration gives loss of 3.0577117376683622\n",
      "The 516 th iteration gives loss of 3.047709777636024\n",
      "The 517 th iteration gives loss of 3.0377611844266794\n",
      "The 518 th iteration gives loss of 3.027865629937842\n",
      "The 519 th iteration gives loss of 3.0180227882989676\n",
      "The 520 th iteration gives loss of 3.0082323358544345\n",
      "The 521 th iteration gives loss of 2.99849395114661\n",
      "The 522 th iteration gives loss of 2.988807314899064\n",
      "The 523 th iteration gives loss of 2.979172110000047\n",
      "The 524 th iteration gives loss of 2.9695880214859147\n",
      "The 525 th iteration gives loss of 2.9600547365248917\n",
      "The 526 th iteration gives loss of 2.9505719444008385\n",
      "The 527 th iteration gives loss of 2.9411393364972755\n",
      "The 528 th iteration gives loss of 2.9317566062813776\n",
      "The 529 th iteration gives loss of 2.922423449288321\n",
      "The 530 th iteration gives loss of 2.9131395631056245\n",
      "The 531 th iteration gives loss of 2.9039046473576406\n",
      "The 532 th iteration gives loss of 2.8947184036902405\n",
      "The 533 th iteration gives loss of 2.8855805357555475\n",
      "The 534 th iteration gives loss of 2.876490749196881\n",
      "The 535 th iteration gives loss of 2.8674487516337974\n",
      "The 536 th iteration gives loss of 2.8584542526472334\n",
      "The 537 th iteration gives loss of 2.849506963764851\n",
      "The 538 th iteration gives loss of 2.8406065984464046\n",
      "The 539 th iteration gives loss of 2.8317528720693237\n",
      "The 540 th iteration gives loss of 2.822945501914401\n",
      "The 541 th iteration gives loss of 2.8141842071515324\n",
      "The 542 th iteration gives loss of 2.805468708825703\n",
      "The 543 th iteration gives loss of 2.796798729842966\n",
      "The 544 th iteration gives loss of 2.788173994956641\n",
      "The 545 th iteration gives loss of 2.779594230753567\n",
      "The 546 th iteration gives loss of 2.7710591656404855\n",
      "The 547 th iteration gives loss of 2.762568529830552\n",
      "The 548 th iteration gives loss of 2.7541220553299914\n",
      "The 549 th iteration gives loss of 2.745719475924797\n",
      "The 550 th iteration gives loss of 2.737360527167541\n",
      "The 551 th iteration gives loss of 2.7290449463644393\n",
      "The 552 th iteration gives loss of 2.720772472562301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 553 th iteration gives loss of 2.7125428465357957\n",
      "The 554 th iteration gives loss of 2.7043558107746652\n",
      "The 555 th iteration gives loss of 2.6962111094711583\n",
      "The 556 th iteration gives loss of 2.6881084885075257\n",
      "The 557 th iteration gives loss of 2.6800476954436245\n",
      "The 558 th iteration gives loss of 2.6720284795045943\n",
      "The 559 th iteration gives loss of 2.6640505915686843\n",
      "The 560 th iteration gives loss of 2.6561137841551496\n",
      "The 561 th iteration gives loss of 2.648217811412302\n",
      "The 562 th iteration gives loss of 2.6403624291055934\n",
      "The 563 th iteration gives loss of 2.632547394605786\n",
      "The 564 th iteration gives loss of 2.624772466877338\n",
      "The 565 th iteration gives loss of 2.6170374064667654\n",
      "The 566 th iteration gives loss of 2.6093419754910965\n",
      "The 567 th iteration gives loss of 2.6016859376265766\n",
      "The 568 th iteration gives loss of 2.5940690580972507\n",
      "The 569 th iteration gives loss of 2.5864911036637825\n",
      "The 570 th iteration gives loss of 2.578951842612384\n",
      "The 571 th iteration gives loss of 2.571451044743697\n",
      "The 572 th iteration gives loss of 2.5639884813619127\n",
      "The 573 th iteration gives loss of 2.556563925263879\n",
      "The 574 th iteration gives loss of 2.5491771507283385\n",
      "The 575 th iteration gives loss of 2.5418279335053167\n",
      "The 576 th iteration gives loss of 2.534516050805411\n",
      "The 577 th iteration gives loss of 2.527241281289406\n",
      "The 578 th iteration gives loss of 2.520003405057801\n",
      "The 579 th iteration gives loss of 2.512802203640488\n",
      "The 580 th iteration gives loss of 2.5056374599864957\n",
      "The 581 th iteration gives loss of 2.4985089584538605\n",
      "The 582 th iteration gives loss of 2.4914164847994664\n",
      "The 583 th iteration gives loss of 2.4843598261691295\n",
      "The 584 th iteration gives loss of 2.477338771087659\n",
      "The 585 th iteration gives loss of 2.4703531094489555\n",
      "The 586 th iteration gives loss of 2.4634026325063276\n",
      "The 587 th iteration gives loss of 2.4564871328627844\n",
      "The 588 th iteration gives loss of 2.4496064044614316\n",
      "The 589 th iteration gives loss of 2.442760242575922\n",
      "The 590 th iteration gives loss of 2.435948443801039\n",
      "The 591 th iteration gives loss of 2.4291708060433206\n",
      "The 592 th iteration gives loss of 2.422427128511739\n",
      "The 593 th iteration gives loss of 2.415717211708479\n",
      "The 594 th iteration gives loss of 2.409040857419798\n",
      "The 595 th iteration gives loss of 2.402397868706979\n",
      "The 596 th iteration gives loss of 2.39578804989728\n",
      "The 597 th iteration gives loss of 2.3892112065749638\n",
      "The 598 th iteration gives loss of 2.382667145572578\n",
      "The 599 th iteration gives loss of 2.376155674961977\n",
      "The 600 th iteration gives loss of 2.3696766040457677\n",
      "The 601 th iteration gives loss of 2.363229743348537\n",
      "The 602 th iteration gives loss of 2.3568149046083455\n",
      "The 603 th iteration gives loss of 2.350431900768131\n",
      "The 604 th iteration gives loss of 2.344080545967345\n",
      "The 605 th iteration gives loss of 2.337760655533542\n",
      "The 606 th iteration gives loss of 2.3314720459739915\n",
      "The 607 th iteration gives loss of 2.3252145349675453\n",
      "The 608 th iteration gives loss of 2.3189879413563714\n",
      "The 609 th iteration gives loss of 2.312792085137823\n",
      "The 610 th iteration gives loss of 2.3066267874564628\n",
      "The 611 th iteration gives loss of 2.300491870595978\n",
      "The 612 th iteration gives loss of 2.2943871579712845\n",
      "The 613 th iteration gives loss of 2.2883124741206515\n",
      "The 614 th iteration gives loss of 2.282267644697899\n",
      "The 615 th iteration gives loss of 2.276252496464649\n",
      "The 616 th iteration gives loss of 2.2702668572826252\n",
      "The 617 th iteration gives loss of 2.2643105561060253\n",
      "The 618 th iteration gives loss of 2.2583834229739788\n",
      "The 619 th iteration gives loss of 2.25248528900298\n",
      "The 620 th iteration gives loss of 2.2466159863794717\n",
      "The 621 th iteration gives loss of 2.2407753483524706\n",
      "The 622 th iteration gives loss of 2.234963209226199\n",
      "The 623 th iteration gives loss of 2.2291794043528195\n",
      "The 624 th iteration gives loss of 2.223423770125177\n",
      "The 625 th iteration gives loss of 2.2176961439696736\n",
      "The 626 th iteration gives loss of 2.2119963643390874\n",
      "The 627 th iteration gives loss of 2.206324270705648\n",
      "The 628 th iteration gives loss of 2.200679703553853\n",
      "The 629 th iteration gives loss of 2.195062504373675\n",
      "The 630 th iteration gives loss of 2.189472515653539\n",
      "The 631 th iteration gives loss of 2.1839095808735616\n",
      "The 632 th iteration gives loss of 2.178373544498743\n",
      "The 633 th iteration gives loss of 2.1728642519721775\n",
      "The 634 th iteration gives loss of 2.167381549708393\n",
      "The 635 th iteration gives loss of 2.161925285086745\n",
      "The 636 th iteration gives loss of 2.1564953064447825\n",
      "The 637 th iteration gives loss of 2.151091463071728\n",
      "The 638 th iteration gives loss of 2.1457136052019745\n",
      "The 639 th iteration gives loss of 2.1403615840086694\n",
      "The 640 th iteration gives loss of 2.1350352515973023\n",
      "The 641 th iteration gives loss of 2.1297344609993734\n",
      "The 642 th iteration gives loss of 2.124459066166122\n",
      "The 643 th iteration gives loss of 2.1192089219621795\n",
      "The 644 th iteration gives loss of 2.1139838841595404\n",
      "The 645 th iteration gives loss of 2.108783809431229\n",
      "The 646 th iteration gives loss of 2.1036085553453256\n",
      "The 647 th iteration gives loss of 2.0984579803588583\n",
      "The 648 th iteration gives loss of 2.093331943811727\n",
      "The 649 th iteration gives loss of 2.0882303059208622\n",
      "The 650 th iteration gives loss of 2.0831529277741683\n",
      "The 651 th iteration gives loss of 2.0780996713247593\n",
      "The 652 th iteration gives loss of 2.0730703993849855\n",
      "The 653 th iteration gives loss of 2.0680649756207874\n",
      "The 654 th iteration gives loss of 2.0630832645458574\n",
      "The 655 th iteration gives loss of 2.058125131515927\n",
      "The 656 th iteration gives loss of 2.0531904427231433\n",
      "The 657 th iteration gives loss of 2.048279065190425\n",
      "The 658 th iteration gives loss of 2.0433908667658818\n",
      "The 659 th iteration gives loss of 2.038525716117255\n",
      "The 660 th iteration gives loss of 2.0336834827264694\n",
      "The 661 th iteration gives loss of 2.0288640368841495\n",
      "The 662 th iteration gives loss of 2.0240672496841583\n",
      "The 663 th iteration gives loss of 2.0192929930183086\n",
      "The 664 th iteration gives loss of 2.014541139570929\n",
      "The 665 th iteration gives loss of 2.009811562813634\n",
      "The 666 th iteration gives loss of 2.0051041370000413\n",
      "The 667 th iteration gives loss of 2.000418737160556\n",
      "The 668 th iteration gives loss of 1.9957552390971827\n",
      "The 669 th iteration gives loss of 1.9911135193783709\n",
      "The 670 th iteration gives loss of 1.9864934553339255\n",
      "The 671 th iteration gives loss of 1.9818949250499402\n",
      "The 672 th iteration gives loss of 1.9773178073637647\n",
      "The 673 th iteration gives loss of 1.972761981858991\n",
      "The 674 th iteration gives loss of 1.9682273288604946\n",
      "The 675 th iteration gives loss of 1.9637137294295386\n",
      "The 676 th iteration gives loss of 1.9592210653588662\n",
      "The 677 th iteration gives loss of 1.9547492191678895\n",
      "The 678 th iteration gives loss of 1.9502980740978009\n",
      "The 679 th iteration gives loss of 1.94586751410683\n",
      "The 680 th iteration gives loss of 1.9414574238655231\n",
      "The 681 th iteration gives loss of 1.9370676887520146\n",
      "The 682 th iteration gives loss of 1.932698194847333\n",
      "The 683 th iteration gives loss of 1.9283488289307724\n",
      "The 684 th iteration gives loss of 1.9240194784752633\n",
      "The 685 th iteration gives loss of 1.9197100316428384\n",
      "The 686 th iteration gives loss of 1.9154203772800642\n",
      "The 687 th iteration gives loss of 1.911150404913519\n",
      "The 688 th iteration gives loss of 1.906900004745308\n",
      "The 689 th iteration gives loss of 1.9026690676486908\n",
      "The 690 th iteration gives loss of 1.8984574851635645\n",
      "The 691 th iteration gives loss of 1.8942651494921514\n",
      "The 692 th iteration gives loss of 1.8900919534946439\n",
      "The 693 th iteration gives loss of 1.8859377906848371\n",
      "The 694 th iteration gives loss of 1.881802555225913\n",
      "The 695 th iteration gives loss of 1.8776861419261197\n",
      "The 696 th iteration gives loss of 1.873588446234561\n",
      "The 697 th iteration gives loss of 1.8695093642370353\n",
      "The 698 th iteration gives loss of 1.8654487926518315\n",
      "The 699 th iteration gives loss of 1.861406628825601\n",
      "The 700 th iteration gives loss of 1.8573827707292592\n",
      "The 701 th iteration gives loss of 1.8533771169538846\n",
      "The 702 th iteration gives loss of 1.8493895667067113\n",
      "The 703 th iteration gives loss of 1.845420019807041\n",
      "The 704 th iteration gives loss of 1.8414683766823645\n",
      "The 705 th iteration gives loss of 1.8375345383642658\n",
      "The 706 th iteration gives loss of 1.833618406484586\n",
      "The 707 th iteration gives loss of 1.829719883271452\n",
      "The 708 th iteration gives loss of 1.8258388715454223\n",
      "The 709 th iteration gives loss of 1.8219752747157103\n",
      "The 710 th iteration gives loss of 1.818128996776186\n",
      "The 711 th iteration gives loss of 1.8142999423017407\n",
      "The 712 th iteration gives loss of 1.810488016444453\n",
      "The 713 th iteration gives loss of 1.8066931249298148\n",
      "The 714 th iteration gives loss of 1.8029151740530749\n",
      "The 715 th iteration gives loss of 1.799154070675507\n",
      "The 716 th iteration gives loss of 1.7954097222207572\n",
      "The 717 th iteration gives loss of 1.7916820366711987\n",
      "The 718 th iteration gives loss of 1.7879709225643265\n",
      "The 719 th iteration gives loss of 1.7842762889891466\n",
      "The 720 th iteration gives loss of 1.7805980455826527\n",
      "The 721 th iteration gives loss of 1.7769361025262373\n",
      "The 722 th iteration gives loss of 1.773290370542235\n",
      "The 723 th iteration gives loss of 1.769660760890375\n",
      "The 724 th iteration gives loss of 1.7660471853643445\n",
      "The 725 th iteration gives loss of 1.762449556288346\n",
      "The 726 th iteration gives loss of 1.7588677865136766\n",
      "The 727 th iteration gives loss of 1.7553017894153744\n",
      "The 728 th iteration gives loss of 1.7517514788887838\n",
      "The 729 th iteration gives loss of 1.7482167693462423\n",
      "The 730 th iteration gives loss of 1.7446975757137357\n",
      "The 731 th iteration gives loss of 1.7411938134276586\n",
      "The 732 th iteration gives loss of 1.7377053984314512\n",
      "The 733 th iteration gives loss of 1.7342322471724352\n",
      "The 734 th iteration gives loss of 1.730774276598517\n",
      "The 735 th iteration gives loss of 1.727331404155\n",
      "The 736 th iteration gives loss of 1.7239035477814106\n",
      "The 737 th iteration gives loss of 1.72049062590829\n",
      "The 738 th iteration gives loss of 1.7170925574541367\n",
      "The 739 th iteration gives loss of 1.7137092618221954\n",
      "The 740 th iteration gives loss of 1.7103406588973886\n",
      "The 741 th iteration gives loss of 1.70698666904325\n",
      "The 742 th iteration gives loss of 1.7036472130989044\n",
      "The 743 th iteration gives loss of 1.7003222123759139\n",
      "The 744 th iteration gives loss of 1.697011588655409\n",
      "The 745 th iteration gives loss of 1.693715264184981\n",
      "The 746 th iteration gives loss of 1.6904331616757573\n",
      "The 747 th iteration gives loss of 1.6871652042994592\n",
      "The 748 th iteration gives loss of 1.6839113156854353\n",
      "The 749 th iteration gives loss of 1.6806714199177708\n",
      "The 750 th iteration gives loss of 1.6774454415324158\n",
      "The 751 th iteration gives loss of 1.674233305514222\n",
      "The 752 th iteration gives loss of 1.67103493729423\n",
      "The 753 th iteration gives loss of 1.6678502627467102\n",
      "The 754 th iteration gives loss of 1.6646792081864104\n",
      "The 755 th iteration gives loss of 1.66152170036574\n",
      "The 756 th iteration gives loss of 1.658377666472024\n",
      "The 757 th iteration gives loss of 1.6552470341247043\n",
      "The 758 th iteration gives loss of 1.6521297313726135\n",
      "The 759 th iteration gives loss of 1.6490256866912667\n",
      "The 760 th iteration gives loss of 1.6459348289801865\n",
      "The 761 th iteration gives loss of 1.6428570875601471\n",
      "The 762 th iteration gives loss of 1.6397923921705726\n",
      "The 763 th iteration gives loss of 1.6367406729668672\n",
      "The 764 th iteration gives loss of 1.6337018605177747\n",
      "The 765 th iteration gives loss of 1.6306758858027937\n",
      "The 766 th iteration gives loss of 1.6276626802095533\n",
      "The 767 th iteration gives loss of 1.6246621755313373\n",
      "The 768 th iteration gives loss of 1.6216743039643136\n",
      "The 769 th iteration gives loss of 1.6186989981052347\n",
      "The 770 th iteration gives loss of 1.6157361909487293\n",
      "The 771 th iteration gives loss of 1.6127858158849422\n",
      "The 772 th iteration gives loss of 1.6098478066969177\n",
      "The 773 th iteration gives loss of 1.6069220975582017\n",
      "The 774 th iteration gives loss of 1.6040086230303665\n",
      "The 775 th iteration gives loss of 1.6011073180605657\n",
      "The 776 th iteration gives loss of 1.5982181179791317\n",
      "The 777 th iteration gives loss of 1.5953409584971234\n",
      "The 778 th iteration gives loss of 1.592475775703993\n",
      "The 779 th iteration gives loss of 1.5896225060651643\n",
      "The 780 th iteration gives loss of 1.5867810864197158\n",
      "The 781 th iteration gives loss of 1.5839514539779735\n",
      "The 782 th iteration gives loss of 1.5811335463192933\n",
      "The 783 th iteration gives loss of 1.5783273013895995\n",
      "The 784 th iteration gives loss of 1.5755326574992272\n",
      "The 785 th iteration gives loss of 1.5727495533205575\n",
      "The 786 th iteration gives loss of 1.5699779278857944\n",
      "The 787 th iteration gives loss of 1.5672177205846805\n",
      "The 788 th iteration gives loss of 1.5644688711623134\n",
      "The 789 th iteration gives loss of 1.56173131971686\n",
      "The 790 th iteration gives loss of 1.5590050066973768\n",
      "The 791 th iteration gives loss of 1.556289872901653\n",
      "The 792 th iteration gives loss of 1.5535858594739935\n",
      "The 793 th iteration gives loss of 1.5508929079030462\n",
      "The 794 th iteration gives loss of 1.5482109600197351\n",
      "The 795 th iteration gives loss of 1.5455399579950149\n",
      "The 796 th iteration gives loss of 1.54287984433784\n",
      "The 797 th iteration gives loss of 1.5402305618929852\n",
      "The 798 th iteration gives loss of 1.5375920538390375\n",
      "The 799 th iteration gives loss of 1.5349642636862664\n",
      "The 800 th iteration gives loss of 1.532347135274562\n",
      "The 801 th iteration gives loss of 1.5297406127713962\n",
      "The 802 th iteration gives loss of 1.5271446406698121\n",
      "The 803 th iteration gives loss of 1.524559163786333\n",
      "The 804 th iteration gives loss of 1.521984127259011\n",
      "The 805 th iteration gives loss of 1.519419476545442\n",
      "The 806 th iteration gives loss of 1.5168651574207113\n",
      "The 807 th iteration gives loss of 1.5143211159755012\n",
      "The 808 th iteration gives loss of 1.5117872986140817\n",
      "The 809 th iteration gives loss of 1.5092636520524114\n",
      "The 810 th iteration gives loss of 1.5067501233161393\n",
      "The 811 th iteration gives loss of 1.5042466597387891\n",
      "The 812 th iteration gives loss of 1.501753208959724\n",
      "The 813 th iteration gives loss of 1.4992697189224\n",
      "The 814 th iteration gives loss of 1.49679613787233\n",
      "The 815 th iteration gives loss of 1.4943324143553691\n",
      "The 816 th iteration gives loss of 1.4918784972157326\n",
      "The 817 th iteration gives loss of 1.489434335594231\n",
      "The 818 th iteration gives loss of 1.4869998789263692\n",
      "The 819 th iteration gives loss of 1.4845750769406383\n",
      "The 820 th iteration gives loss of 1.4821598796565822\n",
      "The 821 th iteration gives loss of 1.4797542373830777\n",
      "The 822 th iteration gives loss of 1.477358100716526\n",
      "The 823 th iteration gives loss of 1.4749714205390838\n",
      "The 824 th iteration gives loss of 1.4725941480169327\n",
      "The 825 th iteration gives loss of 1.470226234598455\n",
      "The 826 th iteration gives loss of 1.4678676320125743\n",
      "The 827 th iteration gives loss of 1.4655182922669494\n",
      "The 828 th iteration gives loss of 1.463178167646373\n",
      "The 829 th iteration gives loss of 1.460847210710985\n",
      "The 830 th iteration gives loss of 1.4585253742945465\n",
      "The 831 th iteration gives loss of 1.4562126115028646\n",
      "The 832 th iteration gives loss of 1.4539088757120209\n",
      "The 833 th iteration gives loss of 1.4516141205667894\n",
      "The 834 th iteration gives loss of 1.449328299978913\n",
      "The 835 th iteration gives loss of 1.4470513681255521\n",
      "The 836 th iteration gives loss of 1.4447832794475488\n",
      "The 837 th iteration gives loss of 1.442523988647936\n",
      "The 838 th iteration gives loss of 1.4402734506902142\n",
      "The 839 th iteration gives loss of 1.4380316207968187\n",
      "The 840 th iteration gives loss of 1.4357984544475468\n",
      "The 841 th iteration gives loss of 1.4335739073778941\n",
      "The 842 th iteration gives loss of 1.4313579355776287\n",
      "The 843 th iteration gives loss of 1.4291504952890803\n",
      "The 844 th iteration gives loss of 1.4269515430057211\n",
      "The 845 th iteration gives loss of 1.4247610354705849\n",
      "The 846 th iteration gives loss of 1.4225789296746982\n",
      "The 847 th iteration gives loss of 1.4204051828556246\n",
      "The 848 th iteration gives loss of 1.418239752495963\n",
      "The 849 th iteration gives loss of 1.4160825963217958\n",
      "The 850 th iteration gives loss of 1.4139336723012297\n",
      "The 851 th iteration gives loss of 1.411792938642958\n",
      "The 852 th iteration gives loss of 1.4096603537947339\n",
      "The 853 th iteration gives loss of 1.4075358764419592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 854 th iteration gives loss of 1.4054194655061727\n",
      "The 855 th iteration gives loss of 1.4033110801437123\n",
      "The 856 th iteration gives loss of 1.401210679744164\n",
      "The 857 th iteration gives loss of 1.3991182239290731\n",
      "The 858 th iteration gives loss of 1.3970336725504153\n",
      "The 859 th iteration gives loss of 1.394956985689218\n",
      "The 860 th iteration gives loss of 1.392888123654272\n",
      "The 861 th iteration gives loss of 1.3908270469806028\n",
      "The 862 th iteration gives loss of 1.3887737164281635\n",
      "The 863 th iteration gives loss of 1.386728092980496\n",
      "The 864 th iteration gives loss of 1.384690137843321\n",
      "The 865 th iteration gives loss of 1.3826598124431995\n",
      "The 866 th iteration gives loss of 1.380637078426235\n",
      "The 867 th iteration gives loss of 1.3786218976566724\n",
      "The 868 th iteration gives loss of 1.376614232215616\n",
      "The 869 th iteration gives loss of 1.3746140443997261\n",
      "The 870 th iteration gives loss of 1.372621296719884\n",
      "The 871 th iteration gives loss of 1.3706359518999027\n",
      "The 872 th iteration gives loss of 1.3686579728752437\n",
      "The 873 th iteration gives loss of 1.3666873227917324\n",
      "The 874 th iteration gives loss of 1.3647239650042882\n",
      "The 875 th iteration gives loss of 1.3627678630756417\n",
      "The 876 th iteration gives loss of 1.3608189807750908\n",
      "The 877 th iteration gives loss of 1.3588772820772725\n",
      "The 878 th iteration gives loss of 1.3569427311608513\n",
      "The 879 th iteration gives loss of 1.3550152924073955\n",
      "The 880 th iteration gives loss of 1.3530949304000588\n",
      "The 881 th iteration gives loss of 1.3511816099223959\n",
      "The 882 th iteration gives loss of 1.3492752959571486\n",
      "The 883 th iteration gives loss of 1.3473759536850571\n",
      "The 884 th iteration gives loss of 1.3454835484836427\n",
      "The 885 th iteration gives loss of 1.3435980459260346\n",
      "The 886 th iteration gives loss of 1.341719411779807\n",
      "The 887 th iteration gives loss of 1.3398476120057543\n",
      "The 888 th iteration gives loss of 1.337982612756816\n",
      "The 889 th iteration gives loss of 1.3361243803767895\n",
      "The 890 th iteration gives loss of 1.3342728813993199\n",
      "The 891 th iteration gives loss of 1.3324280825466581\n",
      "The 892 th iteration gives loss of 1.3305899507285748\n",
      "The 893 th iteration gives loss of 1.328758453041222\n",
      "The 894 th iteration gives loss of 1.3269335567660019\n",
      "The 895 th iteration gives loss of 1.3251152293684352\n",
      "The 896 th iteration gives loss of 1.3233034384971194\n",
      "The 897 th iteration gives loss of 1.321498151982564\n",
      "The 898 th iteration gives loss of 1.319699337836146\n",
      "The 899 th iteration gives loss of 1.3179069642489696\n",
      "The 900 th iteration gives loss of 1.3161209995908298\n",
      "The 901 th iteration gives loss of 1.3143414124091186\n",
      "The 902 th iteration gives loss of 1.3125681714278108\n",
      "The 903 th iteration gives loss of 1.3108012455462923\n",
      "The 904 th iteration gives loss of 1.3090406038384637\n",
      "The 905 th iteration gives loss of 1.307286215551524\n",
      "The 906 th iteration gives loss of 1.305538050105071\n",
      "The 907 th iteration gives loss of 1.3037960770900134\n",
      "The 908 th iteration gives loss of 1.3020602662675373\n",
      "The 909 th iteration gives loss of 1.3003305875681042\n",
      "The 910 th iteration gives loss of 1.298607011090419\n",
      "The 911 th iteration gives loss of 1.2968895071004285\n",
      "The 912 th iteration gives loss of 1.295178046030352\n",
      "The 913 th iteration gives loss of 1.2934725984776896\n",
      "The 914 th iteration gives loss of 1.2917731352041226\n",
      "The 915 th iteration gives loss of 1.2900796271346922\n",
      "The 916 th iteration gives loss of 1.2883920453567355\n",
      "The 917 th iteration gives loss of 1.2867103611189232\n",
      "The 918 th iteration gives loss of 1.2850345458302987\n",
      "The 919 th iteration gives loss of 1.2833645710593318\n",
      "The 920 th iteration gives loss of 1.281700408532965\n",
      "The 921 th iteration gives loss of 1.2800420301356654\n",
      "The 922 th iteration gives loss of 1.278389407908476\n",
      "The 923 th iteration gives loss of 1.2767425140481479\n",
      "The 924 th iteration gives loss of 1.2751013209060602\n",
      "The 925 th iteration gives loss of 1.273465800987473\n",
      "The 926 th iteration gives loss of 1.2718359269505235\n",
      "The 927 th iteration gives loss of 1.270211671605293\n",
      "The 928 th iteration gives loss of 1.268593007912951\n",
      "The 929 th iteration gives loss of 1.2669799089848732\n",
      "The 930 th iteration gives loss of 1.2653723480816776\n",
      "The 931 th iteration gives loss of 1.2637702986124053\n",
      "The 932 th iteration gives loss of 1.262173734133573\n",
      "The 933 th iteration gives loss of 1.2605826283483552\n",
      "The 934 th iteration gives loss of 1.2589969551057092\n",
      "The 935 th iteration gives loss of 1.257416688399477\n",
      "The 936 th iteration gives loss of 1.2558418023675215\n",
      "The 937 th iteration gives loss of 1.2542722712908947\n",
      "The 938 th iteration gives loss of 1.2527080695930113\n",
      "The 939 th iteration gives loss of 1.251149171838738\n",
      "The 940 th iteration gives loss of 1.2495955527336213\n",
      "The 941 th iteration gives loss of 1.248047187123019\n",
      "The 942 th iteration gives loss of 1.2465040499912672\n",
      "The 943 th iteration gives loss of 1.2449661164608714\n",
      "The 944 th iteration gives loss of 1.24343336179168\n",
      "The 945 th iteration gives loss of 1.2419057613800824\n",
      "The 946 th iteration gives loss of 1.240383290758215\n",
      "The 947 th iteration gives loss of 1.238865925593085\n",
      "The 948 th iteration gives loss of 1.2373536416858653\n",
      "The 949 th iteration gives loss of 1.2358464149710573\n",
      "The 950 th iteration gives loss of 1.2343442215156926\n",
      "The 951 th iteration gives loss of 1.232847037518537\n",
      "The 952 th iteration gives loss of 1.231354839309392\n",
      "The 953 th iteration gives loss of 1.2298676033482485\n",
      "The 954 th iteration gives loss of 1.2283853062245087\n",
      "The 955 th iteration gives loss of 1.2269079246562833\n",
      "The 956 th iteration gives loss of 1.2254354354895662\n",
      "The 957 th iteration gives loss of 1.223967815697547\n",
      "The 958 th iteration gives loss of 1.2225050423797887\n",
      "The 959 th iteration gives loss of 1.2210470927615344\n",
      "The 960 th iteration gives loss of 1.2195939441929855\n",
      "The 961 th iteration gives loss of 1.2181455741484806\n",
      "The 962 th iteration gives loss of 1.2167019602258395\n",
      "The 963 th iteration gives loss of 1.215263080145599\n",
      "The 964 th iteration gives loss of 1.2138289117503045\n",
      "The 965 th iteration gives loss of 1.2123994330037982\n",
      "The 966 th iteration gives loss of 1.2109746219904864\n",
      "The 967 th iteration gives loss of 1.2095544569146697\n",
      "The 968 th iteration gives loss of 1.2081389160997735\n",
      "The 969 th iteration gives loss of 1.2067279779877016\n",
      "The 970 th iteration gives loss of 1.205321621138108\n",
      "The 971 th iteration gives loss of 1.2039198242277374\n",
      "The 972 th iteration gives loss of 1.202522566049705\n",
      "The 973 th iteration gives loss of 1.2011298255128258\n",
      "The 974 th iteration gives loss of 1.1997415816409436\n",
      "The 975 th iteration gives loss of 1.1983578135722317\n",
      "The 976 th iteration gives loss of 1.196978500558569\n",
      "The 977 th iteration gives loss of 1.1956036219648114\n",
      "The 978 th iteration gives loss of 1.1942331572681861\n",
      "The 979 th iteration gives loss of 1.1928670860575867\n",
      "The 980 th iteration gives loss of 1.1915053880329445\n",
      "The 981 th iteration gives loss of 1.1901480430045859\n",
      "The 982 th iteration gives loss of 1.1887950308925384\n",
      "The 983 th iteration gives loss of 1.1874463317259873\n",
      "The 984 th iteration gives loss of 1.1861019256425187\n",
      "The 985 th iteration gives loss of 1.1847617928875191\n",
      "The 986 th iteration gives loss of 1.183425913813631\n",
      "The 987 th iteration gives loss of 1.182094268880013\n",
      "The 988 th iteration gives loss of 1.1807668386517762\n",
      "The 989 th iteration gives loss of 1.1794436037993237\n",
      "The 990 th iteration gives loss of 1.1781245450978137\n",
      "The 991 th iteration gives loss of 1.1768096434264648\n",
      "The 992 th iteration gives loss of 1.1754988797679866\n",
      "The 993 th iteration gives loss of 1.1741922352079628\n",
      "The 994 th iteration gives loss of 1.1728896909342428\n",
      "The 995 th iteration gives loss of 1.1715912282363936\n",
      "The 996 th iteration gives loss of 1.1702968285050552\n",
      "The 997 th iteration gives loss of 1.1690064732313636\n",
      "The 998 th iteration gives loss of 1.1677201440063596\n",
      "The 999 th iteration gives loss of 1.1664378225204604\n",
      "The 1000 th iteration gives loss of 1.1651594905627753\n",
      "The 1001 th iteration gives loss of 1.163885130020614\n",
      "The 1002 th iteration gives loss of 1.1626147228789148\n",
      "The 1003 th iteration gives loss of 1.161348251219638\n",
      "The 1004 th iteration gives loss of 1.1600856972211833\n",
      "The 1005 th iteration gives loss of 1.1588270431579093\n",
      "The 1006 th iteration gives loss of 1.157572271399513\n",
      "The 1007 th iteration gives loss of 1.1563213644104833\n",
      "The 1008 th iteration gives loss of 1.1550743047495524\n",
      "The 1009 th iteration gives loss of 1.1538310750691796\n",
      "The 1010 th iteration gives loss of 1.1525916581149374\n",
      "The 1011 th iteration gives loss of 1.1513560367250526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 1012 th iteration gives loss of 1.150124193829792\n",
      "The 1013 th iteration gives loss of 1.148896112451012\n",
      "The 1014 th iteration gives loss of 1.1476717757015251\n",
      "The 1015 th iteration gives loss of 1.1464511667846573\n",
      "The 1016 th iteration gives loss of 1.1452342689936745\n",
      "The 1017 th iteration gives loss of 1.1440210657113128\n",
      "The 1018 th iteration gives loss of 1.1428115404091912\n",
      "The 1019 th iteration gives loss of 1.1416056766473364\n",
      "The 1020 th iteration gives loss of 1.1404034580736677\n",
      "The 1021 th iteration gives loss of 1.1392048684234852\n",
      "The 1022 th iteration gives loss of 1.1380098915190004\n",
      "The 1023 th iteration gives loss of 1.136818511268724\n",
      "The 1024 th iteration gives loss of 1.1356307116670858\n",
      "The 1025 th iteration gives loss of 1.1344464767938909\n",
      "The 1026 th iteration gives loss of 1.1332657908138113\n",
      "The 1027 th iteration gives loss of 1.1320886379759156\n",
      "The 1028 th iteration gives loss of 1.130915002613169\n",
      "The 1029 th iteration gives loss of 1.1297448691419576\n",
      "The 1030 th iteration gives loss of 1.1285782220616138\n",
      "The 1031 th iteration gives loss of 1.12741504595389\n",
      "The 1032 th iteration gives loss of 1.126255325482582\n",
      "The 1033 th iteration gives loss of 1.1250990453929366\n",
      "The 1034 th iteration gives loss of 1.1239461905112655\n",
      "The 1035 th iteration gives loss of 1.1227967457444337\n",
      "The 1036 th iteration gives loss of 1.1216506960794195\n",
      "The 1037 th iteration gives loss of 1.1205080265828546\n",
      "The 1038 th iteration gives loss of 1.119368722400515\n",
      "The 1039 th iteration gives loss of 1.118232768756965\n",
      "The 1040 th iteration gives loss of 1.1171001509549974\n",
      "The 1041 th iteration gives loss of 1.1159708543752291\n",
      "The 1042 th iteration gives loss of 1.114844864475674\n",
      "The 1043 th iteration gives loss of 1.1137221667912647\n",
      "The 1044 th iteration gives loss of 1.1126027469334014\n",
      "The 1045 th iteration gives loss of 1.1114865905895688\n",
      "The 1046 th iteration gives loss of 1.1103736835228408\n",
      "The 1047 th iteration gives loss of 1.1092640115714612\n",
      "The 1048 th iteration gives loss of 1.1081575606484262\n",
      "The 1049 th iteration gives loss of 1.107054316741033\n",
      "The 1050 th iteration gives loss of 1.10595426591046\n",
      "The 1051 th iteration gives loss of 1.1048573942913829\n",
      "The 1052 th iteration gives loss of 1.103763688091479\n",
      "The 1053 th iteration gives loss of 1.1026731335910773\n",
      "The 1054 th iteration gives loss of 1.1015857171426826\n",
      "The 1055 th iteration gives loss of 1.100501425170601\n",
      "The 1056 th iteration gives loss of 1.0994202441704963\n",
      "The 1057 th iteration gives loss of 1.0983421607090724\n",
      "The 1058 th iteration gives loss of 1.0972671614235108\n",
      "The 1059 th iteration gives loss of 1.0961952330211702\n",
      "The 1060 th iteration gives loss of 1.0951263622792282\n",
      "The 1061 th iteration gives loss of 1.0940605360441207\n",
      "The 1062 th iteration gives loss of 1.0929977412313234\n",
      "The 1063 th iteration gives loss of 1.091937964824825\n",
      "The 1064 th iteration gives loss of 1.09088119387682\n",
      "The 1065 th iteration gives loss of 1.0898274155072833\n",
      "The 1066 th iteration gives loss of 1.088776616903557\n",
      "The 1067 th iteration gives loss of 1.0877287853200048\n",
      "The 1068 th iteration gives loss of 1.0866839080776491\n",
      "The 1069 th iteration gives loss of 1.0856419725636928\n",
      "The 1070 th iteration gives loss of 1.084602966231287\n",
      "The 1071 th iteration gives loss of 1.0835668765990016\n",
      "The 1072 th iteration gives loss of 1.0825336912505545\n",
      "The 1073 th iteration gives loss of 1.0815033978344246\n",
      "The 1074 th iteration gives loss of 1.080475984063443\n",
      "The 1075 th iteration gives loss of 1.0794514377144495\n",
      "The 1076 th iteration gives loss of 1.078429746627971\n",
      "The 1077 th iteration gives loss of 1.0774108987077429\n",
      "The 1078 th iteration gives loss of 1.0763948819204876\n",
      "The 1079 th iteration gives loss of 1.0753816842954596\n",
      "The 1080 th iteration gives loss of 1.0743712939240977\n",
      "The 1081 th iteration gives loss of 1.073363698959758\n",
      "The 1082 th iteration gives loss of 1.0723588876172239\n",
      "The 1083 th iteration gives loss of 1.0713568481724827\n",
      "The 1084 th iteration gives loss of 1.0703575689623006\n",
      "The 1085 th iteration gives loss of 1.0693610383839052\n",
      "The 1086 th iteration gives loss of 1.0683672448946182\n",
      "The 1087 th iteration gives loss of 1.0673761770115686\n",
      "The 1088 th iteration gives loss of 1.0663878233112847\n",
      "The 1089 th iteration gives loss of 1.0654021724293905\n",
      "The 1090 th iteration gives loss of 1.0644192130603074\n",
      "The 1091 th iteration gives loss of 1.063438933956818\n",
      "The 1092 th iteration gives loss of 1.0624613239298712\n",
      "The 1093 th iteration gives loss of 1.0614863718480985\n",
      "The 1094 th iteration gives loss of 1.0605140666376436\n",
      "The 1095 th iteration gives loss of 1.059544397281691\n",
      "The 1096 th iteration gives loss of 1.0585773528202822\n",
      "The 1097 th iteration gives loss of 1.0576129223498827\n",
      "The 1098 th iteration gives loss of 1.0566510950230634\n",
      "The 1099 th iteration gives loss of 1.0556918600483098\n",
      "The 1100 th iteration gives loss of 1.0547352066895688\n",
      "The 1101 th iteration gives loss of 1.053781124265963\n",
      "The 1102 th iteration gives loss of 1.0528296021515524\n",
      "The 1103 th iteration gives loss of 1.0518806297749417\n",
      "The 1104 th iteration gives loss of 1.0509341966189811\n",
      "The 1105 th iteration gives loss of 1.0499902922205078\n",
      "The 1106 th iteration gives loss of 1.0490489061699884\n",
      "The 1107 th iteration gives loss of 1.0481100281112903\n",
      "The 1108 th iteration gives loss of 1.047173647741271\n",
      "The 1109 th iteration gives loss of 1.046239754809559\n",
      "The 1110 th iteration gives loss of 1.0453083391182367\n",
      "The 1111 th iteration gives loss of 1.0443793905215513\n",
      "The 1112 th iteration gives loss of 1.0434528989255898\n",
      "The 1113 th iteration gives loss of 1.0425288542880176\n",
      "The 1114 th iteration gives loss of 1.0416072466177704\n",
      "The 1115 th iteration gives loss of 1.0406880659747935\n",
      "The 1116 th iteration gives loss of 1.0397713024697195\n",
      "The 1117 th iteration gives loss of 1.038856946263587\n",
      "The 1118 th iteration gives loss of 1.0379449875675644\n",
      "The 1119 th iteration gives loss of 1.0370354166427116\n",
      "The 1120 th iteration gives loss of 1.036128223799613\n",
      "The 1121 th iteration gives loss of 1.0352233993981497\n",
      "The 1122 th iteration gives loss of 1.0343209338472372\n",
      "The 1123 th iteration gives loss of 1.033420817604505\n",
      "The 1124 th iteration gives loss of 1.0325230411760458\n",
      "The 1125 th iteration gives loss of 1.0316275951161817\n",
      "The 1126 th iteration gives loss of 1.0307344700271066\n",
      "The 1127 th iteration gives loss of 1.029843656558676\n",
      "The 1128 th iteration gives loss of 1.0289551454081496\n",
      "The 1129 th iteration gives loss of 1.0280689273198829\n",
      "The 1130 th iteration gives loss of 1.0271849930851091\n",
      "The 1131 th iteration gives loss of 1.0263033335416183\n",
      "The 1132 th iteration gives loss of 1.0254239395735465\n",
      "The 1133 th iteration gives loss of 1.024546802111106\n",
      "The 1134 th iteration gives loss of 1.0236719121303175\n",
      "The 1135 th iteration gives loss of 1.02279926065274\n",
      "The 1136 th iteration gives loss of 1.0219288387452525\n",
      "The 1137 th iteration gives loss of 1.0210606375197602\n",
      "The 1138 th iteration gives loss of 1.0201946481329844\n",
      "The 1139 th iteration gives loss of 1.0193308617861687\n",
      "The 1140 th iteration gives loss of 1.0184692697248512\n",
      "The 1141 th iteration gives loss of 1.0176098632386301\n",
      "The 1142 th iteration gives loss of 1.016752633660883\n",
      "The 1143 th iteration gives loss of 1.0158975723685797\n",
      "The 1144 th iteration gives loss of 1.0150446707819383\n",
      "The 1145 th iteration gives loss of 1.014193920364326\n",
      "The 1146 th iteration gives loss of 1.0133453126218512\n",
      "The 1147 th iteration gives loss of 1.0124988391032852\n",
      "The 1148 th iteration gives loss of 1.0116544913997212\n",
      "The 1149 th iteration gives loss of 1.0108122611443624\n",
      "The 1150 th iteration gives loss of 1.0099721400123018\n",
      "The 1151 th iteration gives loss of 1.0091341197202706\n",
      "The 1152 th iteration gives loss of 1.0082981920264407\n",
      "The 1153 th iteration gives loss of 1.0074643487301562\n",
      "The 1154 th iteration gives loss of 1.0066325816716892\n",
      "The 1155 th iteration gives loss of 1.0058028827320633\n",
      "The 1156 th iteration gives loss of 1.0049752438328123\n",
      "The 1157 th iteration gives loss of 1.0041496569357264\n",
      "The 1158 th iteration gives loss of 1.003326114042643\n",
      "The 1159 th iteration gives loss of 1.002504607195243\n",
      "The 1160 th iteration gives loss of 1.0016851284748265\n",
      "The 1161 th iteration gives loss of 1.000867670002049\n",
      "The 1162 th iteration gives loss of 1.000052223936737\n",
      "The 1163 th iteration gives loss of 0.9992387824776763\n",
      "The 1164 th iteration gives loss of 0.9984273378624179\n",
      "The 1165 th iteration gives loss of 0.9976178823669596\n",
      "The 1166 th iteration gives loss of 0.9968104083056765\n",
      "The 1167 th iteration gives loss of 0.99600490803099\n",
      "The 1168 th iteration gives loss of 0.9952013739332428\n",
      "The 1169 th iteration gives loss of 0.9943997984403804\n",
      "The 1170 th iteration gives loss of 0.9936001740178787\n",
      "The 1171 th iteration gives loss of 0.9928024931684751\n",
      "The 1172 th iteration gives loss of 0.9920067484318646\n",
      "The 1173 th iteration gives loss of 0.9912129323846824\n",
      "The 1174 th iteration gives loss of 0.9904210376401774\n",
      "The 1175 th iteration gives loss of 0.9896310568480027\n",
      "The 1176 th iteration gives loss of 0.9888429826940787\n",
      "The 1177 th iteration gives loss of 0.9880568079003643\n",
      "The 1178 th iteration gives loss of 0.9872725252246216\n",
      "The 1179 th iteration gives loss of 0.9864901274602682\n",
      "The 1180 th iteration gives loss of 0.9857096074361833\n",
      "The 1181 th iteration gives loss of 0.9849309580164646\n",
      "The 1182 th iteration gives loss of 0.9841541721002669\n",
      "The 1183 th iteration gives loss of 0.9833792426216003\n",
      "The 1184 th iteration gives loss of 0.9826061625491455\n",
      "The 1185 th iteration gives loss of 0.9818349248860503\n",
      "The 1186 th iteration gives loss of 0.9810655226697594\n",
      "The 1187 th iteration gives loss of 0.9802979489718029\n",
      "The 1188 th iteration gives loss of 0.97953219689761\n",
      "The 1189 th iteration gives loss of 0.9787682595863314\n",
      "The 1190 th iteration gives loss of 0.9780061302106563\n",
      "The 1191 th iteration gives loss of 0.977245801976633\n",
      "The 1192 th iteration gives loss of 0.9764872681234724\n",
      "The 1193 th iteration gives loss of 0.9757305219233662\n",
      "The 1194 th iteration gives loss of 0.9749755566812852\n",
      "The 1195 th iteration gives loss of 0.9742223657348628\n",
      "The 1196 th iteration gives loss of 0.9734709424541449\n",
      "The 1197 th iteration gives loss of 0.9727212802414634\n",
      "The 1198 th iteration gives loss of 0.9719733725312067\n",
      "The 1199 th iteration gives loss of 0.9712272127897026\n",
      "The 1200 th iteration gives loss of 0.9704827945150235\n",
      "The 1201 th iteration gives loss of 0.9697401112367625\n",
      "The 1202 th iteration gives loss of 0.9689991565159434\n",
      "The 1203 th iteration gives loss of 0.9682599239447639\n",
      "The 1204 th iteration gives loss of 0.9675224071465051\n",
      "The 1205 th iteration gives loss of 0.9667865997753232\n",
      "The 1206 th iteration gives loss of 0.9660524955160508\n",
      "The 1207 th iteration gives loss of 0.9653200880840704\n",
      "The 1208 th iteration gives loss of 0.9645893712251541\n",
      "The 1209 th iteration gives loss of 0.9638603387152547\n",
      "The 1210 th iteration gives loss of 0.9631329843603773\n",
      "The 1211 th iteration gives loss of 0.9624073019964015\n",
      "The 1212 th iteration gives loss of 0.961683285488907\n",
      "The 1213 th iteration gives loss of 0.960960928733053\n",
      "The 1214 th iteration gives loss of 0.960240225653359\n",
      "The 1215 th iteration gives loss of 0.9595211702035661\n",
      "The 1216 th iteration gives loss of 0.9588037563665053\n",
      "The 1217 th iteration gives loss of 0.9580879781539113\n",
      "The 1218 th iteration gives loss of 0.9573738296062614\n",
      "The 1219 th iteration gives loss of 0.9566613047926541\n",
      "The 1220 th iteration gives loss of 0.9559503978106111\n",
      "The 1221 th iteration gives loss of 0.9552411027858958\n",
      "The 1222 th iteration gives loss of 0.954533413872464\n",
      "The 1223 th iteration gives loss of 0.9538273252522386\n",
      "The 1224 th iteration gives loss of 0.953122831134939\n",
      "The 1225 th iteration gives loss of 0.9524199257579735\n",
      "The 1226 th iteration gives loss of 0.9517186033862685\n",
      "The 1227 th iteration gives loss of 0.951018858312137\n",
      "The 1228 th iteration gives loss of 0.9503206848550949\n",
      "The 1229 th iteration gives loss of 0.9496240773617386\n",
      "The 1230 th iteration gives loss of 0.9489290302056459\n",
      "The 1231 th iteration gives loss of 0.948235537787081\n",
      "The 1232 th iteration gives loss of 0.9475435945330125\n",
      "The 1233 th iteration gives loss of 0.9468531948968809\n",
      "The 1234 th iteration gives loss of 0.9461643333584958\n",
      "The 1235 th iteration gives loss of 0.9454770044238431\n",
      "The 1236 th iteration gives loss of 0.9447912026249836\n",
      "The 1237 th iteration gives loss of 0.9441069225199252\n",
      "The 1238 th iteration gives loss of 0.9434241586924211\n",
      "The 1239 th iteration gives loss of 0.9427429057518802\n",
      "The 1240 th iteration gives loss of 0.9420631583332388\n",
      "The 1241 th iteration gives loss of 0.9413849110967659\n",
      "The 1242 th iteration gives loss of 0.9407081587279753\n",
      "The 1243 th iteration gives loss of 0.9400328959374635\n",
      "The 1244 th iteration gives loss of 0.9393591174607994\n",
      "The 1245 th iteration gives loss of 0.9386868180583665\n",
      "The 1246 th iteration gives loss of 0.9380159925152255\n",
      "The 1247 th iteration gives loss of 0.9373466356410075\n",
      "The 1248 th iteration gives loss of 0.9366787422697541\n",
      "The 1249 th iteration gives loss of 0.9360123072598012\n",
      "The 1250 th iteration gives loss of 0.9353473254936583\n",
      "The 1251 th iteration gives loss of 0.9346837918778356\n",
      "The 1252 th iteration gives loss of 0.9340217013427641\n",
      "The 1253 th iteration gives loss of 0.9333610488426117\n",
      "The 1254 th iteration gives loss of 0.93270182935524\n",
      "The 1255 th iteration gives loss of 0.9320440378819934\n",
      "The 1256 th iteration gives loss of 0.9313876694476155\n",
      "The 1257 th iteration gives loss of 0.9307327191000965\n",
      "The 1258 th iteration gives loss of 0.9300791819105851\n",
      "The 1259 th iteration gives loss of 0.9294270529732223\n",
      "The 1260 th iteration gives loss of 0.9287763274050619\n",
      "The 1261 th iteration gives loss of 0.9281270003459103\n",
      "The 1262 th iteration gives loss of 0.9274790669582255\n",
      "The 1263 th iteration gives loss of 0.9268325224269583\n",
      "The 1264 th iteration gives loss of 0.9261873619595081\n",
      "The 1265 th iteration gives loss of 0.925543580785529\n",
      "The 1266 th iteration gives loss of 0.9249011741568349\n",
      "The 1267 th iteration gives loss of 0.9242601373472945\n",
      "The 1268 th iteration gives loss of 0.9236204656526943\n",
      "The 1269 th iteration gives loss of 0.922982154390618\n",
      "The 1270 th iteration gives loss of 0.9223451989003784\n",
      "The 1271 th iteration gives loss of 0.9217095945428162\n",
      "The 1272 th iteration gives loss of 0.9210753367002616\n",
      "The 1273 th iteration gives loss of 0.9204424207763724\n",
      "The 1274 th iteration gives loss of 0.9198108421960475\n",
      "The 1275 th iteration gives loss of 0.9191805964052945\n",
      "The 1276 th iteration gives loss of 0.9185516788711331\n",
      "The 1277 th iteration gives loss of 0.9179240850814803\n",
      "The 1278 th iteration gives loss of 0.9172978105450053\n",
      "The 1279 th iteration gives loss of 0.9166728507910864\n",
      "The 1280 th iteration gives loss of 0.9160492013696196\n",
      "The 1281 th iteration gives loss of 0.915426857850989\n",
      "The 1282 th iteration gives loss of 0.9148058158258902\n",
      "The 1283 th iteration gives loss of 0.9141860709052623\n",
      "The 1284 th iteration gives loss of 0.9135676187201737\n",
      "The 1285 th iteration gives loss of 0.9129504549217119\n",
      "The 1286 th iteration gives loss of 0.9123345751808499\n",
      "The 1287 th iteration gives loss of 0.9117199751883933\n",
      "The 1288 th iteration gives loss of 0.9111066506548293\n",
      "The 1289 th iteration gives loss of 0.9104945973102458\n",
      "The 1290 th iteration gives loss of 0.9098838109042306\n",
      "The 1291 th iteration gives loss of 0.9092742872057596\n",
      "The 1292 th iteration gives loss of 0.908666022003056\n",
      "The 1293 th iteration gives loss of 0.9080590111035698\n",
      "The 1294 th iteration gives loss of 0.9074532503338046\n",
      "The 1295 th iteration gives loss of 0.9068487355392653\n",
      "The 1296 th iteration gives loss of 0.9062454625842935\n",
      "The 1297 th iteration gives loss of 0.9056434273520485\n",
      "The 1298 th iteration gives loss of 0.9050426257443517\n",
      "The 1299 th iteration gives loss of 0.9044430536816149\n",
      "The 1300 th iteration gives loss of 0.9038447071027299\n",
      "The 1301 th iteration gives loss of 0.903247581964932\n",
      "The 1302 th iteration gives loss of 0.9026516742438017\n",
      "The 1303 th iteration gives loss of 0.9020569799330642\n",
      "The 1304 th iteration gives loss of 0.9014634950445785\n",
      "The 1305 th iteration gives loss of 0.9008712156081462\n",
      "The 1306 th iteration gives loss of 0.9002801376715286\n",
      "The 1307 th iteration gives loss of 0.8996902573002548\n",
      "The 1308 th iteration gives loss of 0.8991015705776036\n",
      "The 1309 th iteration gives loss of 0.8985140736044236\n",
      "The 1310 th iteration gives loss of 0.8979277624991567\n",
      "The 1311 th iteration gives loss of 0.8973426333976121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 1312 th iteration gives loss of 0.8967586824529784\n",
      "The 1313 th iteration gives loss of 0.8961759058357035\n",
      "The 1314 th iteration gives loss of 0.8955942997334041\n",
      "The 1315 th iteration gives loss of 0.8950138603507137\n",
      "The 1316 th iteration gives loss of 0.8944345839092883\n",
      "The 1317 th iteration gives loss of 0.8938564666476833\n",
      "The 1318 th iteration gives loss of 0.8932795048212289\n",
      "The 1319 th iteration gives loss of 0.89270369470198\n",
      "The 1320 th iteration gives loss of 0.8921290325786453\n",
      "The 1321 th iteration gives loss of 0.8915555147564492\n",
      "The 1322 th iteration gives loss of 0.8909831375570567\n",
      "The 1323 th iteration gives loss of 0.8904118973185186\n",
      "The 1324 th iteration gives loss of 0.8898417903951396\n",
      "The 1325 th iteration gives loss of 0.8892728131574748\n",
      "The 1326 th iteration gives loss of 0.8887049619921359\n",
      "The 1327 th iteration gives loss of 0.8881382333017772\n",
      "The 1328 th iteration gives loss of 0.8875726235050174\n",
      "The 1329 th iteration gives loss of 0.8870081290363053\n",
      "The 1330 th iteration gives loss of 0.8864447463458712\n",
      "The 1331 th iteration gives loss of 0.8858824718996563\n",
      "The 1332 th iteration gives loss of 0.8853213021791725\n",
      "The 1333 th iteration gives loss of 0.8847612336815132\n",
      "The 1334 th iteration gives loss of 0.8842022629191973\n",
      "The 1335 th iteration gives loss of 0.8836443864200931\n",
      "The 1336 th iteration gives loss of 0.8830876007273799\n",
      "The 1337 th iteration gives loss of 0.8825319023994602\n",
      "The 1338 th iteration gives loss of 0.8819772880098196\n",
      "The 1339 th iteration gives loss of 0.8814237541470289\n",
      "The 1340 th iteration gives loss of 0.8808712974146353\n",
      "The 1341 th iteration gives loss of 0.8803199144310471\n",
      "The 1342 th iteration gives loss of 0.8797696018295054\n",
      "The 1343 th iteration gives loss of 0.8792203562579917\n",
      "The 1344 th iteration gives loss of 0.8786721743791565\n",
      "The 1345 th iteration gives loss of 0.8781250528702322\n",
      "The 1346 th iteration gives loss of 0.877578988422944\n",
      "The 1347 th iteration gives loss of 0.8770339777434971\n",
      "The 1348 th iteration gives loss of 0.8764900175524014\n",
      "The 1349 th iteration gives loss of 0.8759471045845\n",
      "The 1350 th iteration gives loss of 0.8754052355887969\n",
      "The 1351 th iteration gives loss of 0.8748644073284937\n",
      "The 1352 th iteration gives loss of 0.8743246165808176\n",
      "The 1353 th iteration gives loss of 0.8737858601369819\n",
      "The 1354 th iteration gives loss of 0.8732481348021457\n",
      "The 1355 th iteration gives loss of 0.872711437395292\n",
      "The 1356 th iteration gives loss of 0.8721757647492009\n",
      "The 1357 th iteration gives loss of 0.8716411137103586\n",
      "The 1358 th iteration gives loss of 0.8711074811388508\n",
      "The 1359 th iteration gives loss of 0.870574863908371\n",
      "The 1360 th iteration gives loss of 0.8700432589060695\n",
      "The 1361 th iteration gives loss of 0.8695126630325279\n",
      "The 1362 th iteration gives loss of 0.8689830732016809\n",
      "The 1363 th iteration gives loss of 0.8684544863407878\n",
      "The 1364 th iteration gives loss of 0.8679268993902648\n",
      "The 1365 th iteration gives loss of 0.8674003093037203\n",
      "The 1366 th iteration gives loss of 0.8668747130478145\n",
      "The 1367 th iteration gives loss of 0.8663501076022565\n",
      "The 1368 th iteration gives loss of 0.8658264899596624\n",
      "The 1369 th iteration gives loss of 0.865303857125544\n",
      "The 1370 th iteration gives loss of 0.8647822061182395\n",
      "The 1371 th iteration gives loss of 0.8642615339688191\n",
      "The 1372 th iteration gives loss of 0.8637418377210493\n",
      "The 1373 th iteration gives loss of 0.8632231144313035\n",
      "The 1374 th iteration gives loss of 0.862705361168496\n",
      "The 1375 th iteration gives loss of 0.8621885750140607\n",
      "The 1376 th iteration gives loss of 0.8616727530618312\n",
      "The 1377 th iteration gives loss of 0.861157892418028\n",
      "The 1378 th iteration gives loss of 0.8606439902011114\n",
      "The 1379 th iteration gives loss of 0.8601310435418484\n",
      "The 1380 th iteration gives loss of 0.85961904958313\n",
      "The 1381 th iteration gives loss of 0.8591080054799822\n",
      "The 1382 th iteration gives loss of 0.8585979083994479\n",
      "The 1383 th iteration gives loss of 0.8580887555205888\n",
      "The 1384 th iteration gives loss of 0.8575805440343828\n",
      "The 1385 th iteration gives loss of 0.8570732711436523\n",
      "The 1386 th iteration gives loss of 0.8565669340630367\n",
      "The 1387 th iteration gives loss of 0.8560615300189343\n",
      "The 1388 th iteration gives loss of 0.8555570562493858\n",
      "The 1389 th iteration gives loss of 0.8550535100041\n",
      "The 1390 th iteration gives loss of 0.8545508885443176\n",
      "The 1391 th iteration gives loss of 0.8540491891428194\n",
      "The 1392 th iteration gives loss of 0.8535484090837787\n",
      "The 1393 th iteration gives loss of 0.8530485456628193\n",
      "The 1394 th iteration gives loss of 0.8525495961868531\n",
      "The 1395 th iteration gives loss of 0.8520515579740747\n",
      "The 1396 th iteration gives loss of 0.8515544283539066\n",
      "The 1397 th iteration gives loss of 0.8510582046669086\n",
      "The 1398 th iteration gives loss of 0.8505628842647848\n",
      "The 1399 th iteration gives loss of 0.8500684645102181\n",
      "The 1400 th iteration gives loss of 0.8495749427769355\n",
      "The 1401 th iteration gives loss of 0.8490823164495889\n",
      "The 1402 th iteration gives loss of 0.8485905829237063\n",
      "The 1403 th iteration gives loss of 0.8480997396056129\n",
      "The 1404 th iteration gives loss of 0.8476097839124461\n",
      "The 1405 th iteration gives loss of 0.8471207132720345\n",
      "The 1406 th iteration gives loss of 0.8466325251228523\n",
      "The 1407 th iteration gives loss of 0.8461452169140179\n",
      "The 1408 th iteration gives loss of 0.8456587861051805\n",
      "The 1409 th iteration gives loss of 0.8451732301664809\n",
      "The 1410 th iteration gives loss of 0.8446885465785353\n",
      "The 1411 th iteration gives loss of 0.8442047328322986\n",
      "The 1412 th iteration gives loss of 0.843721786429128\n",
      "The 1413 th iteration gives loss of 0.8432397048806325\n",
      "The 1414 th iteration gives loss of 0.8427584857086998\n",
      "The 1415 th iteration gives loss of 0.8422781264453373\n",
      "The 1416 th iteration gives loss of 0.8417986246327441\n",
      "The 1417 th iteration gives loss of 0.8413199778231758\n",
      "The 1418 th iteration gives loss of 0.8408421835789314\n",
      "The 1419 th iteration gives loss of 0.8403652394723033\n",
      "The 1420 th iteration gives loss of 0.839889143085481\n",
      "The 1421 th iteration gives loss of 0.8394138920105874\n",
      "The 1422 th iteration gives loss of 0.8389394838495213\n",
      "The 1423 th iteration gives loss of 0.8384659162140049\n",
      "The 1424 th iteration gives loss of 0.8379931867254855\n",
      "The 1425 th iteration gives loss of 0.837521293015095\n",
      "The 1426 th iteration gives loss of 0.8370502327235992\n",
      "The 1427 th iteration gives loss of 0.8365800035013539\n",
      "The 1428 th iteration gives loss of 0.8361106030082666\n",
      "The 1429 th iteration gives loss of 0.8356420289137194\n",
      "The 1430 th iteration gives loss of 0.8351742788965705\n",
      "The 1431 th iteration gives loss of 0.8347073506450328\n",
      "The 1432 th iteration gives loss of 0.8342412418567191\n",
      "The 1433 th iteration gives loss of 0.833775950238497\n",
      "The 1434 th iteration gives loss of 0.8333114735065418\n",
      "The 1435 th iteration gives loss of 0.8328478093862384\n",
      "The 1436 th iteration gives loss of 0.8323849556120818\n",
      "The 1437 th iteration gives loss of 0.8319229099277452\n",
      "The 1438 th iteration gives loss of 0.8314616700859554\n",
      "The 1439 th iteration gives loss of 0.8310012338484736\n",
      "The 1440 th iteration gives loss of 0.8305415989860644\n",
      "The 1441 th iteration gives loss of 0.8300827632783977\n",
      "The 1442 th iteration gives loss of 0.8296247245140554\n",
      "The 1443 th iteration gives loss of 0.8291674804904821\n",
      "The 1444 th iteration gives loss of 0.8287110290139243\n",
      "The 1445 th iteration gives loss of 0.8282553678993929\n",
      "The 1446 th iteration gives loss of 0.8278004949706089\n",
      "The 1447 th iteration gives loss of 0.8273464080599986\n",
      "The 1448 th iteration gives loss of 0.826893105008597\n",
      "The 1449 th iteration gives loss of 0.8264405836660492\n",
      "The 1450 th iteration gives loss of 0.825988841890542\n",
      "The 1451 th iteration gives loss of 0.8255378775487707\n",
      "The 1452 th iteration gives loss of 0.8250876885159003\n",
      "The 1453 th iteration gives loss of 0.8246382726755133\n",
      "The 1454 th iteration gives loss of 0.8241896279195705\n",
      "The 1455 th iteration gives loss of 0.8237417521483843\n",
      "The 1456 th iteration gives loss of 0.8232946432705781\n",
      "The 1457 th iteration gives loss of 0.8228482992029962\n",
      "The 1458 th iteration gives loss of 0.8224027178707495\n",
      "The 1459 th iteration gives loss of 0.821957897207082\n",
      "The 1460 th iteration gives loss of 0.8215138351534025\n",
      "The 1461 th iteration gives loss of 0.821070529659211\n",
      "The 1462 th iteration gives loss of 0.8206279786820542\n",
      "The 1463 th iteration gives loss of 0.8201861801875203\n",
      "The 1464 th iteration gives loss of 0.8197451321491567\n",
      "The 1465 th iteration gives loss of 0.8193048325484619\n",
      "The 1466 th iteration gives loss of 0.8188652793748167\n",
      "The 1467 th iteration gives loss of 0.8184264706254819\n",
      "The 1468 th iteration gives loss of 0.8179884043055408\n",
      "The 1469 th iteration gives loss of 0.8175510784278616\n",
      "The 1470 th iteration gives loss of 0.8171144910130468\n",
      "The 1471 th iteration gives loss of 0.8166786400894027\n",
      "The 1472 th iteration gives loss of 0.8162435236929512\n",
      "The 1473 th iteration gives loss of 0.8158091398672911\n",
      "The 1474 th iteration gives loss of 0.8153754866636869\n",
      "The 1475 th iteration gives loss of 0.8149425621408704\n",
      "The 1476 th iteration gives loss of 0.8145103643651728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 1477 th iteration gives loss of 0.8140788914103736\n",
      "The 1478 th iteration gives loss of 0.813648141357706\n",
      "The 1479 th iteration gives loss of 0.8132181122958562\n",
      "The 1480 th iteration gives loss of 0.8127888023208154\n",
      "The 1481 th iteration gives loss of 0.8123602095359782\n",
      "The 1482 th iteration gives loss of 0.8119323320520138\n",
      "The 1483 th iteration gives loss of 0.8115051679868834\n",
      "The 1484 th iteration gives loss of 0.8110787154657418\n",
      "The 1485 th iteration gives loss of 0.81065297262099\n",
      "The 1486 th iteration gives loss of 0.8102279375921689\n",
      "The 1487 th iteration gives loss of 0.8098036085259649\n",
      "The 1488 th iteration gives loss of 0.809379983576143\n",
      "The 1489 th iteration gives loss of 0.8089570609035265\n",
      "The 1490 th iteration gives loss of 0.8085348386759867\n",
      "The 1491 th iteration gives loss of 0.8081133150683764\n",
      "The 1492 th iteration gives loss of 0.8076924882625004\n",
      "The 1493 th iteration gives loss of 0.8072723564471036\n",
      "The 1494 th iteration gives loss of 0.8068529178178114\n",
      "The 1495 th iteration gives loss of 0.8064341705771072\n",
      "The 1496 th iteration gives loss of 0.8060161129343271\n",
      "The 1497 th iteration gives loss of 0.8055987431055314\n",
      "The 1498 th iteration gives loss of 0.8051820593136265\n",
      "The 1499 th iteration gives loss of 0.8047660597882001\n",
      "The 1500 th iteration gives loss of 0.8043507427655224\n",
      "The 1501 th iteration gives loss of 0.8039361064885737\n",
      "The 1502 th iteration gives loss of 0.8035221492069191\n",
      "The 1503 th iteration gives loss of 0.8031088691767382\n",
      "The 1504 th iteration gives loss of 0.8026962646607855\n",
      "The 1505 th iteration gives loss of 0.8022843339283496\n",
      "The 1506 th iteration gives loss of 0.801873075255208\n",
      "The 1507 th iteration gives loss of 0.8014624869236355\n",
      "The 1508 th iteration gives loss of 0.8010525672223305\n",
      "The 1509 th iteration gives loss of 0.8006433144464066\n",
      "The 1510 th iteration gives loss of 0.8002347268973661\n",
      "The 1511 th iteration gives loss of 0.7998268028830536\n",
      "The 1512 th iteration gives loss of 0.7994195407176264\n",
      "The 1513 th iteration gives loss of 0.7990129387215659\n",
      "The 1514 th iteration gives loss of 0.7986069952215529\n",
      "The 1515 th iteration gives loss of 0.7982017085505634\n",
      "The 1516 th iteration gives loss of 0.7977970770477387\n",
      "The 1517 th iteration gives loss of 0.7973930990583741\n",
      "The 1518 th iteration gives loss of 0.796989772933963\n",
      "The 1519 th iteration gives loss of 0.796587097032028\n",
      "The 1520 th iteration gives loss of 0.7961850697162591\n",
      "The 1521 th iteration gives loss of 0.7957836893563505\n",
      "The 1522 th iteration gives loss of 0.7953829543280277\n",
      "The 1523 th iteration gives loss of 0.7949828630130228\n",
      "The 1524 th iteration gives loss of 0.7945834137990306\n",
      "The 1525 th iteration gives loss of 0.7941846050796944\n",
      "The 1526 th iteration gives loss of 0.793786435254545\n",
      "The 1527 th iteration gives loss of 0.7933889027290407\n",
      "The 1528 th iteration gives loss of 0.7929920059144352\n",
      "The 1529 th iteration gives loss of 0.7925957432278745\n",
      "The 1530 th iteration gives loss of 0.7922001130922405\n",
      "The 1531 th iteration gives loss of 0.7918051139362521\n",
      "The 1532 th iteration gives loss of 0.7914107441943051\n",
      "The 1533 th iteration gives loss of 0.7910170023065801\n",
      "The 1534 th iteration gives loss of 0.7906238867189334\n",
      "The 1535 th iteration gives loss of 0.7902313958828566\n",
      "The 1536 th iteration gives loss of 0.7898395282554883\n",
      "The 1537 th iteration gives loss of 0.7894482822995909\n",
      "The 1538 th iteration gives loss of 0.7890576564835299\n",
      "The 1539 th iteration gives loss of 0.7886676492811769\n",
      "The 1540 th iteration gives loss of 0.7882782591719967\n",
      "The 1541 th iteration gives loss of 0.7878894846409201\n",
      "The 1542 th iteration gives loss of 0.7875013241783695\n",
      "The 1543 th iteration gives loss of 0.7871137762802223\n",
      "The 1544 th iteration gives loss of 0.7867268394477811\n",
      "The 1545 th iteration gives loss of 0.7863405121877887\n",
      "The 1546 th iteration gives loss of 0.7859547930122898\n",
      "The 1547 th iteration gives loss of 0.785569680438777\n",
      "The 1548 th iteration gives loss of 0.78518517299\n",
      "The 1549 th iteration gives loss of 0.7848012691940285\n",
      "The 1550 th iteration gives loss of 0.7844179675842406\n",
      "The 1551 th iteration gives loss of 0.7840352666992616\n",
      "The 1552 th iteration gives loss of 0.7836531650829008\n",
      "The 1553 th iteration gives loss of 0.7832716612842293\n",
      "The 1554 th iteration gives loss of 0.7828907538574421\n",
      "The 1555 th iteration gives loss of 0.7825104413619697\n",
      "The 1556 th iteration gives loss of 0.7821307223622974\n",
      "The 1557 th iteration gives loss of 0.781751595428085\n",
      "The 1558 th iteration gives loss of 0.78137305913403\n",
      "The 1559 th iteration gives loss of 0.7809951120599007\n",
      "The 1560 th iteration gives loss of 0.7806177527905144\n",
      "The 1561 th iteration gives loss of 0.780240979915706\n",
      "The 1562 th iteration gives loss of 0.7798647920303137\n",
      "The 1563 th iteration gives loss of 0.7794891877341029\n",
      "The 1564 th iteration gives loss of 0.7791141656318274\n",
      "The 1565 th iteration gives loss of 0.7787397243331516\n",
      "The 1566 th iteration gives loss of 0.778365862452609\n",
      "The 1567 th iteration gives loss of 0.7779925786096494\n",
      "The 1568 th iteration gives loss of 0.7776198714285704\n",
      "The 1569 th iteration gives loss of 0.7772477395385018\n",
      "The 1570 th iteration gives loss of 0.7768761815733676\n",
      "The 1571 th iteration gives loss of 0.7765051961718794\n",
      "The 1572 th iteration gives loss of 0.7761347819775455\n",
      "The 1573 th iteration gives loss of 0.7757649376385989\n",
      "The 1574 th iteration gives loss of 0.7753956618079911\n",
      "The 1575 th iteration gives loss of 0.7750269531433825\n",
      "The 1576 th iteration gives loss of 0.7746588103070945\n",
      "The 1577 th iteration gives loss of 0.7742912319661254\n",
      "The 1578 th iteration gives loss of 0.7739242167921256\n",
      "The 1579 th iteration gives loss of 0.7735577634613218\n",
      "The 1580 th iteration gives loss of 0.773191870654543\n",
      "The 1581 th iteration gives loss of 0.7728265370572089\n",
      "The 1582 th iteration gives loss of 0.7724617613592909\n",
      "The 1583 th iteration gives loss of 0.7720975422552773\n",
      "The 1584 th iteration gives loss of 0.7717338784441676\n",
      "The 1585 th iteration gives loss of 0.7713707686294562\n",
      "The 1586 th iteration gives loss of 0.7710082115191099\n",
      "The 1587 th iteration gives loss of 0.7706462058255374\n",
      "The 1588 th iteration gives loss of 0.7702847502655902\n",
      "The 1589 th iteration gives loss of 0.769923843560499\n",
      "The 1590 th iteration gives loss of 0.7695634844359174\n",
      "The 1591 th iteration gives loss of 0.769203671621867\n",
      "The 1592 th iteration gives loss of 0.7688444038526699\n",
      "The 1593 th iteration gives loss of 0.7684856798670213\n",
      "The 1594 th iteration gives loss of 0.7681274984079217\n",
      "The 1595 th iteration gives loss of 0.7677698582226576\n",
      "The 1596 th iteration gives loss of 0.7674127580627671\n",
      "The 1597 th iteration gives loss of 0.7670561966840598\n",
      "The 1598 th iteration gives loss of 0.7667001728465845\n",
      "The 1599 th iteration gives loss of 0.7663446853145752\n",
      "The 1600 th iteration gives loss of 0.7659897328564784\n",
      "The 1601 th iteration gives loss of 0.7656353142449104\n",
      "The 1602 th iteration gives loss of 0.7652814282566449\n",
      "The 1603 th iteration gives loss of 0.7649280736726033\n",
      "The 1604 th iteration gives loss of 0.7645752492777896\n",
      "The 1605 th iteration gives loss of 0.7642229538613422\n",
      "The 1606 th iteration gives loss of 0.7638711862164893\n",
      "The 1607 th iteration gives loss of 0.763519945140469\n",
      "The 1608 th iteration gives loss of 0.7631692294346234\n",
      "The 1609 th iteration gives loss of 0.7628190379042962\n",
      "The 1610 th iteration gives loss of 0.7624693693588297\n",
      "The 1611 th iteration gives loss of 0.7621202226115772\n",
      "The 1612 th iteration gives loss of 0.7617715964798409\n",
      "The 1613 th iteration gives loss of 0.761423489784915\n",
      "The 1614 th iteration gives loss of 0.7610759013519768\n",
      "The 1615 th iteration gives loss of 0.7607288300101828\n",
      "The 1616 th iteration gives loss of 0.7603822745925553\n",
      "The 1617 th iteration gives loss of 0.7600362339359961\n",
      "The 1618 th iteration gives loss of 0.7596907068812948\n",
      "The 1619 th iteration gives loss of 0.7593456922730868\n",
      "The 1620 th iteration gives loss of 0.7590011889598242\n",
      "The 1621 th iteration gives loss of 0.7586571957938079\n",
      "The 1622 th iteration gives loss of 0.7583137116311102\n",
      "The 1623 th iteration gives loss of 0.7579707353315959\n",
      "The 1624 th iteration gives loss of 0.757628265758879\n",
      "The 1625 th iteration gives loss of 0.7572863017803528\n",
      "The 1626 th iteration gives loss of 0.7569448422671079\n",
      "The 1627 th iteration gives loss of 0.7566038860939842\n",
      "The 1628 th iteration gives loss of 0.7562634321394983\n",
      "The 1629 th iteration gives loss of 0.7559234792858591\n",
      "The 1630 th iteration gives loss of 0.7555840264189144\n",
      "The 1631 th iteration gives loss of 0.7552450724282023\n",
      "The 1632 th iteration gives loss of 0.7549066162068896\n",
      "The 1633 th iteration gives loss of 0.7545686566517416\n",
      "The 1634 th iteration gives loss of 0.754231192663117\n",
      "The 1635 th iteration gives loss of 0.7538942231449944\n",
      "The 1636 th iteration gives loss of 0.7535577470049082\n",
      "The 1637 th iteration gives loss of 0.7532217631539291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 1638 th iteration gives loss of 0.7528862705066836\n",
      "The 1639 th iteration gives loss of 0.7525512679813268\n",
      "The 1640 th iteration gives loss of 0.7522167544995043\n",
      "The 1641 th iteration gives loss of 0.7518827289863708\n",
      "The 1642 th iteration gives loss of 0.7515491903705619\n",
      "The 1643 th iteration gives loss of 0.7512161375841637\n",
      "The 1644 th iteration gives loss of 0.7508835695626802\n",
      "The 1645 th iteration gives loss of 0.750551485245108\n",
      "The 1646 th iteration gives loss of 0.7502198835738275\n",
      "The 1647 th iteration gives loss of 0.7498887634946189\n",
      "The 1648 th iteration gives loss of 0.7495581239566496\n",
      "The 1649 th iteration gives loss of 0.7492279639124718\n",
      "The 1650 th iteration gives loss of 0.7488982823179775\n",
      "The 1651 th iteration gives loss of 0.7485690781324302\n",
      "The 1652 th iteration gives loss of 0.7482403503183629\n",
      "The 1653 th iteration gives loss of 0.7479120978416992\n",
      "The 1654 th iteration gives loss of 0.7475843196716138\n",
      "The 1655 th iteration gives loss of 0.7472570147805692\n",
      "The 1656 th iteration gives loss of 0.7469301821443352\n",
      "The 1657 th iteration gives loss of 0.7466038207418636\n",
      "The 1658 th iteration gives loss of 0.7462779295554389\n",
      "The 1659 th iteration gives loss of 0.7459525075705129\n",
      "The 1660 th iteration gives loss of 0.7456275537757783\n",
      "The 1661 th iteration gives loss of 0.7453030671631035\n",
      "The 1662 th iteration gives loss of 0.7449790467275662\n",
      "The 1663 th iteration gives loss of 0.7446554914674188\n",
      "The 1664 th iteration gives loss of 0.7443324003840568\n",
      "The 1665 th iteration gives loss of 0.7440097724820587\n",
      "The 1666 th iteration gives loss of 0.743687606769076\n",
      "The 1667 th iteration gives loss of 0.7433659022559165\n",
      "The 1668 th iteration gives loss of 0.7430446579564852\n",
      "The 1669 th iteration gives loss of 0.7427238728877964\n",
      "The 1670 th iteration gives loss of 0.7424035460699211\n",
      "The 1671 th iteration gives loss of 0.7420836765259953\n",
      "The 1672 th iteration gives loss of 0.7417642632822268\n",
      "The 1673 th iteration gives loss of 0.7414453053678398\n",
      "The 1674 th iteration gives loss of 0.7411268018150962\n",
      "The 1675 th iteration gives loss of 0.7408087516592767\n",
      "The 1676 th iteration gives loss of 0.7404911539386521\n",
      "The 1677 th iteration gives loss of 0.7401740076945158\n",
      "The 1678 th iteration gives loss of 0.739857311971062\n",
      "The 1679 th iteration gives loss of 0.7395410658155298\n",
      "The 1680 th iteration gives loss of 0.7392252682780466\n",
      "The 1681 th iteration gives loss of 0.738909918411698\n",
      "The 1682 th iteration gives loss of 0.7385950152724987\n",
      "The 1683 th iteration gives loss of 0.7382805579193982\n",
      "The 1684 th iteration gives loss of 0.737966545414189\n",
      "The 1685 th iteration gives loss of 0.7376529768216071\n",
      "The 1686 th iteration gives loss of 0.7373398512092478\n",
      "The 1687 th iteration gives loss of 0.7370271676475266\n",
      "The 1688 th iteration gives loss of 0.7367149252097767\n",
      "The 1689 th iteration gives loss of 0.7364031229721273\n",
      "The 1690 th iteration gives loss of 0.7360917600135519\n",
      "The 1691 th iteration gives loss of 0.7357808354158343\n",
      "The 1692 th iteration gives loss of 0.73547034826356\n",
      "The 1693 th iteration gives loss of 0.7351602976441083\n",
      "The 1694 th iteration gives loss of 0.7348506826476248\n",
      "The 1695 th iteration gives loss of 0.734541502367064\n",
      "The 1696 th iteration gives loss of 0.7342327558980967\n",
      "The 1697 th iteration gives loss of 0.7339244423391331\n",
      "The 1698 th iteration gives loss of 0.733616560791353\n",
      "The 1699 th iteration gives loss of 0.7333091103586304\n",
      "The 1700 th iteration gives loss of 0.7330020901475637\n",
      "The 1701 th iteration gives loss of 0.7326954992674483\n",
      "The 1702 th iteration gives loss of 0.7323893368302528\n",
      "The 1703 th iteration gives loss of 0.7320836019506323\n",
      "The 1704 th iteration gives loss of 0.7317782937459165\n",
      "The 1705 th iteration gives loss of 0.7314734113360819\n",
      "The 1706 th iteration gives loss of 0.7311689538437461\n",
      "The 1707 th iteration gives loss of 0.730864920394166\n",
      "The 1708 th iteration gives loss of 0.7305613101152109\n",
      "The 1709 th iteration gives loss of 0.7302581221373489\n",
      "The 1710 th iteration gives loss of 0.7299553555936718\n",
      "The 1711 th iteration gives loss of 0.7296530096198613\n",
      "The 1712 th iteration gives loss of 0.7293510833541607\n",
      "The 1713 th iteration gives loss of 0.729049575937382\n",
      "The 1714 th iteration gives loss of 0.7287484865128998\n",
      "The 1715 th iteration gives loss of 0.7284478142266226\n",
      "The 1716 th iteration gives loss of 0.7281475582270206\n",
      "The 1717 th iteration gives loss of 0.7278477176650834\n",
      "The 1718 th iteration gives loss of 0.7275482916942585\n",
      "The 1719 th iteration gives loss of 0.7272492794705848\n",
      "The 1720 th iteration gives loss of 0.72695068015255\n",
      "The 1721 th iteration gives loss of 0.7266524929011077\n",
      "The 1722 th iteration gives loss of 0.7263547168797367\n",
      "The 1723 th iteration gives loss of 0.7260573512543353\n",
      "The 1724 th iteration gives loss of 0.7257603951932741\n",
      "The 1725 th iteration gives loss of 0.7254638478673463\n",
      "The 1726 th iteration gives loss of 0.7251677084497989\n",
      "The 1727 th iteration gives loss of 0.724871976116307\n",
      "The 1728 th iteration gives loss of 0.7245766500449408\n",
      "The 1729 th iteration gives loss of 0.724281729416184\n",
      "The 1730 th iteration gives loss of 0.7239872134129058\n",
      "The 1731 th iteration gives loss of 0.723693101220371\n",
      "The 1732 th iteration gives loss of 0.7233993920262224\n",
      "The 1733 th iteration gives loss of 0.7231060850204332\n",
      "The 1734 th iteration gives loss of 0.7228131793953649\n",
      "The 1735 th iteration gives loss of 0.7225206743457081\n",
      "The 1736 th iteration gives loss of 0.7222285690685191\n",
      "The 1737 th iteration gives loss of 0.7219368627631424\n",
      "The 1738 th iteration gives loss of 0.721645554631236\n",
      "The 1739 th iteration gives loss of 0.721354643876808\n",
      "The 1740 th iteration gives loss of 0.721064129706108\n",
      "The 1741 th iteration gives loss of 0.72077401132773\n",
      "The 1742 th iteration gives loss of 0.720484287952513\n",
      "The 1743 th iteration gives loss of 0.7201949587935703\n",
      "The 1744 th iteration gives loss of 0.7199060230662848\n",
      "The 1745 th iteration gives loss of 0.719617479988308\n",
      "The 1746 th iteration gives loss of 0.7193293287794824\n",
      "The 1747 th iteration gives loss of 0.7190415686619192\n",
      "The 1748 th iteration gives loss of 0.7187541988599637\n",
      "The 1749 th iteration gives loss of 0.718467218600173\n",
      "The 1750 th iteration gives loss of 0.7181806271112803\n",
      "The 1751 th iteration gives loss of 0.717894423624244\n",
      "The 1752 th iteration gives loss of 0.7176086073722073\n",
      "The 1753 th iteration gives loss of 0.7173231775905015\n",
      "The 1754 th iteration gives loss of 0.7170381335166082\n",
      "The 1755 th iteration gives loss of 0.7167534743902039\n",
      "The 1756 th iteration gives loss of 0.7164691994530806\n",
      "The 1757 th iteration gives loss of 0.7161853079492039\n",
      "The 1758 th iteration gives loss of 0.7159017991246792\n",
      "The 1759 th iteration gives loss of 0.7156186722277033\n",
      "The 1760 th iteration gives loss of 0.7153359265086429\n",
      "The 1761 th iteration gives loss of 0.71505356121996\n",
      "The 1762 th iteration gives loss of 0.7147715756161904\n",
      "The 1763 th iteration gives loss of 0.714489968954\n",
      "The 1764 th iteration gives loss of 0.7142087404921392\n",
      "The 1765 th iteration gives loss of 0.7139278894914247\n",
      "The 1766 th iteration gives loss of 0.7136474152147343\n",
      "The 1767 th iteration gives loss of 0.7133673169270346\n",
      "The 1768 th iteration gives loss of 0.7130875938953248\n",
      "The 1769 th iteration gives loss of 0.7128082453886775\n",
      "The 1770 th iteration gives loss of 0.7125292706781515\n",
      "The 1771 th iteration gives loss of 0.7122506690368754\n",
      "The 1772 th iteration gives loss of 0.711972439740005\n",
      "The 1773 th iteration gives loss of 0.7116945820646718\n",
      "The 1774 th iteration gives loss of 0.7114170952900749\n",
      "The 1775 th iteration gives loss of 0.7111399786973474\n",
      "The 1776 th iteration gives loss of 0.7108632315696537\n",
      "The 1777 th iteration gives loss of 0.7105868531920934\n",
      "The 1778 th iteration gives loss of 0.7103108428518188\n",
      "The 1779 th iteration gives loss of 0.7100351998378677\n",
      "The 1780 th iteration gives loss of 0.7097599234412852\n",
      "The 1781 th iteration gives loss of 0.7094850129550672\n",
      "The 1782 th iteration gives loss of 0.7092104676741308\n",
      "The 1783 th iteration gives loss of 0.7089362868953396\n",
      "The 1784 th iteration gives loss of 0.7086624699174972\n",
      "The 1785 th iteration gives loss of 0.7083890160412993\n",
      "The 1786 th iteration gives loss of 0.7081159245693947\n",
      "The 1787 th iteration gives loss of 0.707843194806312\n",
      "The 1788 th iteration gives loss of 0.7075708260584785\n",
      "The 1789 th iteration gives loss of 0.7072988176342353\n",
      "The 1790 th iteration gives loss of 0.7070271688437909\n",
      "The 1791 th iteration gives loss of 0.7067558789992039\n",
      "The 1792 th iteration gives loss of 0.706484947414481\n",
      "The 1793 th iteration gives loss of 0.7062143734054012\n",
      "The 1794 th iteration gives loss of 0.7059441562896396\n",
      "The 1795 th iteration gives loss of 0.7056742953867545\n",
      "The 1796 th iteration gives loss of 0.7054047900180745\n",
      "The 1797 th iteration gives loss of 0.7051356395068221\n",
      "The 1798 th iteration gives loss of 0.7048668431780145\n",
      "The 1799 th iteration gives loss of 0.7045984003584971\n",
      "The 1800 th iteration gives loss of 0.7043303103769182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 1801 th iteration gives loss of 0.7040625725637607\n",
      "The 1802 th iteration gives loss of 0.7037951862512765\n",
      "The 1803 th iteration gives loss of 0.7035281507735247\n",
      "The 1804 th iteration gives loss of 0.7032614654663576\n",
      "The 1805 th iteration gives loss of 0.702995129667358\n",
      "The 1806 th iteration gives loss of 0.702729142715948\n",
      "The 1807 th iteration gives loss of 0.702463503953282\n",
      "The 1808 th iteration gives loss of 0.7021982127222857\n",
      "The 1809 th iteration gives loss of 0.7019332683676014\n",
      "The 1810 th iteration gives loss of 0.7016686702356397\n",
      "The 1811 th iteration gives loss of 0.7014044176745691\n",
      "The 1812 th iteration gives loss of 0.7011405100342746\n",
      "The 1813 th iteration gives loss of 0.7008769466663282\n",
      "The 1814 th iteration gives loss of 0.7006137269240962\n",
      "The 1815 th iteration gives loss of 0.7003508501625925\n",
      "The 1816 th iteration gives loss of 0.700088315738559\n",
      "The 1817 th iteration gives loss of 0.6998261230104434\n",
      "The 1818 th iteration gives loss of 0.6995642713383879\n",
      "The 1819 th iteration gives loss of 0.6993027600842089\n",
      "The 1820 th iteration gives loss of 0.6990415886113976\n",
      "The 1821 th iteration gives loss of 0.6987807562851471\n",
      "The 1822 th iteration gives loss of 0.6985202624722839\n",
      "The 1823 th iteration gives loss of 0.6982601065413219\n",
      "The 1824 th iteration gives loss of 0.6980002878624182\n",
      "The 1825 th iteration gives loss of 0.6977408058073675\n",
      "The 1826 th iteration gives loss of 0.697481659749653\n",
      "The 1827 th iteration gives loss of 0.6972228490643153\n",
      "The 1828 th iteration gives loss of 0.6969643731281157\n",
      "The 1829 th iteration gives loss of 0.6967062313193689\n",
      "The 1830 th iteration gives loss of 0.6964484230180475\n",
      "The 1831 th iteration gives loss of 0.6961909476057097\n",
      "The 1832 th iteration gives loss of 0.6959338044655407\n",
      "The 1833 th iteration gives loss of 0.6956769929823171\n",
      "The 1834 th iteration gives loss of 0.6954205125424231\n",
      "The 1835 th iteration gives loss of 0.695164362533807\n",
      "The 1836 th iteration gives loss of 0.6949085423460187\n",
      "The 1837 th iteration gives loss of 0.6946530513701641\n",
      "The 1838 th iteration gives loss of 0.6943978889989434\n",
      "The 1839 th iteration gives loss of 0.6941430546266206\n",
      "The 1840 th iteration gives loss of 0.6938885476489852\n",
      "The 1841 th iteration gives loss of 0.6936343674634399\n",
      "The 1842 th iteration gives loss of 0.6933805134688705\n",
      "The 1843 th iteration gives loss of 0.6931269850657686\n",
      "The 1844 th iteration gives loss of 0.6928737816560854\n",
      "The 1845 th iteration gives loss of 0.6926209026433695\n",
      "The 1846 th iteration gives loss of 0.6923683474326703\n",
      "The 1847 th iteration gives loss of 0.6921161154305544\n",
      "The 1848 th iteration gives loss of 0.691864206045103\n",
      "The 1849 th iteration gives loss of 0.691612618685909\n",
      "The 1850 th iteration gives loss of 0.6913613527640805\n",
      "The 1851 th iteration gives loss of 0.6911104076921818\n",
      "The 1852 th iteration gives loss of 0.6908597828843153\n",
      "The 1853 th iteration gives loss of 0.6906094777560566\n",
      "The 1854 th iteration gives loss of 0.6903594917244449\n",
      "The 1855 th iteration gives loss of 0.6901098242080114\n",
      "The 1856 th iteration gives loss of 0.689860474626751\n",
      "The 1857 th iteration gives loss of 0.6896114424021351\n",
      "The 1858 th iteration gives loss of 0.689362726957068\n",
      "The 1859 th iteration gives loss of 0.6891143277159507\n",
      "The 1860 th iteration gives loss of 0.6888662441045731\n",
      "The 1861 th iteration gives loss of 0.6886184755502264\n",
      "The 1862 th iteration gives loss of 0.6883710214816353\n",
      "The 1863 th iteration gives loss of 0.6881238813289053\n",
      "The 1864 th iteration gives loss of 0.6878770545236336\n",
      "The 1865 th iteration gives loss of 0.6876305404987959\n",
      "The 1866 th iteration gives loss of 0.6873843386888047\n",
      "The 1867 th iteration gives loss of 0.6871384485294902\n",
      "The 1868 th iteration gives loss of 0.6868928694580815\n",
      "The 1869 th iteration gives loss of 0.68664760091322\n",
      "The 1870 th iteration gives loss of 0.6864026423349255\n",
      "The 1871 th iteration gives loss of 0.6861579931646311\n",
      "The 1872 th iteration gives loss of 0.6859136528451389\n",
      "The 1873 th iteration gives loss of 0.6856696208206635\n",
      "The 1874 th iteration gives loss of 0.6854258965367602\n",
      "The 1875 th iteration gives loss of 0.6851824794403703\n",
      "The 1876 th iteration gives loss of 0.6849393689798324\n",
      "The 1877 th iteration gives loss of 0.6846965646047998\n",
      "The 1878 th iteration gives loss of 0.6844540657663272\n",
      "The 1879 th iteration gives loss of 0.6842118719168016\n",
      "The 1880 th iteration gives loss of 0.6839699825099311\n",
      "The 1881 th iteration gives loss of 0.6837283970008398\n",
      "The 1882 th iteration gives loss of 0.6834871148459192\n",
      "The 1883 th iteration gives loss of 0.683246135502946\n",
      "The 1884 th iteration gives loss of 0.6830054584309873\n",
      "The 1885 th iteration gives loss of 0.6827650830904711\n",
      "The 1886 th iteration gives loss of 0.6825250089430993\n",
      "The 1887 th iteration gives loss of 0.6822852354519584\n",
      "The 1888 th iteration gives loss of 0.6820457620813785\n",
      "The 1889 th iteration gives loss of 0.6818065882970156\n",
      "The 1890 th iteration gives loss of 0.68156771356587\n",
      "The 1891 th iteration gives loss of 0.6813291373561742\n",
      "The 1892 th iteration gives loss of 0.6810908591375019\n",
      "The 1893 th iteration gives loss of 0.6808528783806929\n",
      "The 1894 th iteration gives loss of 0.6806151945578662\n",
      "The 1895 th iteration gives loss of 0.6803778071424368\n",
      "The 1896 th iteration gives loss of 0.6801407156090983\n",
      "The 1897 th iteration gives loss of 0.6799039194337928\n",
      "The 1898 th iteration gives loss of 0.6796674180937274\n",
      "The 1899 th iteration gives loss of 0.6794312110673985\n",
      "The 1900 th iteration gives loss of 0.6791952978345249\n",
      "The 1901 th iteration gives loss of 0.678959677876129\n",
      "The 1902 th iteration gives loss of 0.6787243506744225\n",
      "The 1903 th iteration gives loss of 0.6784893157129003\n",
      "The 1904 th iteration gives loss of 0.6782545724762603\n",
      "The 1905 th iteration gives loss of 0.6780201204504966\n",
      "The 1906 th iteration gives loss of 0.6777859591227807\n",
      "The 1907 th iteration gives loss of 0.677552087981525\n",
      "The 1908 th iteration gives loss of 0.677318506516367\n",
      "The 1909 th iteration gives loss of 0.6770852142181818\n",
      "The 1910 th iteration gives loss of 0.6768522105790317\n",
      "The 1911 th iteration gives loss of 0.6766194950921963\n",
      "The 1912 th iteration gives loss of 0.6763870672521488\n",
      "The 1913 th iteration gives loss of 0.6761549265546143\n",
      "The 1914 th iteration gives loss of 0.6759230724964512\n",
      "The 1915 th iteration gives loss of 0.6756915045757554\n",
      "The 1916 th iteration gives loss of 0.6754602222917897\n",
      "The 1917 th iteration gives loss of 0.6752292251450106\n",
      "The 1918 th iteration gives loss of 0.6749985126370718\n",
      "The 1919 th iteration gives loss of 0.6747680842707735\n",
      "The 1920 th iteration gives loss of 0.6745379395501128\n",
      "The 1921 th iteration gives loss of 0.6743080779802386\n",
      "The 1922 th iteration gives loss of 0.6740784990674928\n",
      "The 1923 th iteration gives loss of 0.6738492023193406\n",
      "The 1924 th iteration gives loss of 0.6736201872444393\n",
      "The 1925 th iteration gives loss of 0.673391453352578\n",
      "The 1926 th iteration gives loss of 0.6731630001547074\n",
      "The 1927 th iteration gives loss of 0.6729348271629173\n",
      "The 1928 th iteration gives loss of 0.6727069338904624\n",
      "The 1929 th iteration gives loss of 0.6724793198516894\n",
      "The 1930 th iteration gives loss of 0.6722519845621209\n",
      "The 1931 th iteration gives loss of 0.6720249275383673\n",
      "The 1932 th iteration gives loss of 0.671798148298237\n",
      "The 1933 th iteration gives loss of 0.6715716463605852\n",
      "The 1934 th iteration gives loss of 0.6713454212454392\n",
      "The 1935 th iteration gives loss of 0.6711194724739181\n",
      "The 1936 th iteration gives loss of 0.6708937995682492\n",
      "The 1937 th iteration gives loss of 0.6706684020517569\n",
      "The 1938 th iteration gives loss of 0.6704432794489218\n",
      "The 1939 th iteration gives loss of 0.6702184312852747\n",
      "The 1940 th iteration gives loss of 0.6699938570874627\n",
      "The 1941 th iteration gives loss of 0.6697695563832134\n",
      "The 1942 th iteration gives loss of 0.6695455287013629\n",
      "The 1943 th iteration gives loss of 0.6693217735718319\n",
      "The 1944 th iteration gives loss of 0.6690982905256071\n",
      "The 1945 th iteration gives loss of 0.6688750790947551\n",
      "The 1946 th iteration gives loss of 0.6686521388124413\n",
      "The 1947 th iteration gives loss of 0.6684294692128678\n",
      "The 1948 th iteration gives loss of 0.6682070698313441\n",
      "The 1949 th iteration gives loss of 0.6679849402042243\n",
      "The 1950 th iteration gives loss of 0.6677630798688994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 1951 th iteration gives loss of 0.6675414883638576\n",
      "The 1952 th iteration gives loss of 0.66732016522863\n",
      "The 1953 th iteration gives loss of 0.6670991100037894\n",
      "The 1954 th iteration gives loss of 0.6668783222309722\n",
      "The 1955 th iteration gives loss of 0.6666578014528417\n",
      "The 1956 th iteration gives loss of 0.6664375472130935\n",
      "The 1957 th iteration gives loss of 0.6662175590564937\n",
      "The 1958 th iteration gives loss of 0.6659978365288\n",
      "The 1959 th iteration gives loss of 0.6657783791768415\n",
      "The 1960 th iteration gives loss of 0.665559186548443\n",
      "The 1961 th iteration gives loss of 0.6653402581924783\n",
      "The 1962 th iteration gives loss of 0.6651215936588185\n",
      "The 1963 th iteration gives loss of 0.6649031924983793\n",
      "The 1964 th iteration gives loss of 0.6646850542630389\n",
      "The 1965 th iteration gives loss of 0.6644671785057467\n",
      "The 1966 th iteration gives loss of 0.6642495647804192\n",
      "The 1967 th iteration gives loss of 0.6640322126419755\n",
      "The 1968 th iteration gives loss of 0.6638151216463765\n",
      "The 1969 th iteration gives loss of 0.663598291350547\n",
      "The 1970 th iteration gives loss of 0.6633817213123948\n",
      "The 1971 th iteration gives loss of 0.6631654110908359\n",
      "The 1972 th iteration gives loss of 0.6629493602458029\n",
      "The 1973 th iteration gives loss of 0.6627335683381473\n",
      "The 1974 th iteration gives loss of 0.6625180349297703\n",
      "The 1975 th iteration gives loss of 0.6623027595834728\n",
      "The 1976 th iteration gives loss of 0.6620877418631086\n",
      "The 1977 th iteration gives loss of 0.6618729813334757\n",
      "The 1978 th iteration gives loss of 0.6616584775603279\n",
      "The 1979 th iteration gives loss of 0.6614442301104015\n",
      "The 1980 th iteration gives loss of 0.6612302385513706\n",
      "The 1981 th iteration gives loss of 0.6610165024519024\n",
      "The 1982 th iteration gives loss of 0.6608030213815769\n",
      "The 1983 th iteration gives loss of 0.660589794910985\n",
      "The 1984 th iteration gives loss of 0.6603768226116367\n",
      "The 1985 th iteration gives loss of 0.6601641040559534\n",
      "The 1986 th iteration gives loss of 0.6599516388173796\n",
      "The 1987 th iteration gives loss of 0.6597394264702516\n",
      "The 1988 th iteration gives loss of 0.6595274665898302\n",
      "The 1989 th iteration gives loss of 0.6593157587523472\n",
      "The 1990 th iteration gives loss of 0.659104302534952\n",
      "The 1991 th iteration gives loss of 0.6588930975157216\n",
      "The 1992 th iteration gives loss of 0.6586821432736816\n",
      "The 1993 th iteration gives loss of 0.6584714393887452\n",
      "The 1994 th iteration gives loss of 0.6582609854417553\n",
      "The 1995 th iteration gives loss of 0.6580507810145023\n",
      "The 1996 th iteration gives loss of 0.6578408256896521\n",
      "The 1997 th iteration gives loss of 0.6576311190507941\n",
      "The 1998 th iteration gives loss of 0.657421660682464\n",
      "The 1999 th iteration gives loss of 0.6572124501700555\n",
      "The 2000 th iteration gives loss of 0.6570034870998883\n",
      "The 2001 th iteration gives loss of 0.6567947710591824\n",
      "The 2002 th iteration gives loss of 0.6565863016360428\n",
      "The 2003 th iteration gives loss of 0.65637807841949\n",
      "The 2004 th iteration gives loss of 0.6561701009994126\n",
      "The 2005 th iteration gives loss of 0.6559623689666151\n",
      "The 2006 th iteration gives loss of 0.6557548819127716\n",
      "The 2007 th iteration gives loss of 0.655547639430461\n",
      "The 2008 th iteration gives loss of 0.6553406411131039\n",
      "The 2009 th iteration gives loss of 0.6551338865550334\n",
      "The 2010 th iteration gives loss of 0.6549273753514476\n",
      "The 2011 th iteration gives loss of 0.6547211070984168\n",
      "The 2012 th iteration gives loss of 0.6545150813928967\n",
      "The 2013 th iteration gives loss of 0.6543092978326845\n",
      "The 2014 th iteration gives loss of 0.6541037560164518\n",
      "The 2015 th iteration gives loss of 0.6538984555437568\n",
      "The 2016 th iteration gives loss of 0.6536933960149927\n",
      "The 2017 th iteration gives loss of 0.6534885770314081\n",
      "The 2018 th iteration gives loss of 0.6532839981951111\n",
      "The 2019 th iteration gives loss of 0.6530796591090797\n",
      "The 2020 th iteration gives loss of 0.6528755593771186\n",
      "The 2021 th iteration gives loss of 0.6526716986038827\n",
      "The 2022 th iteration gives loss of 0.652468076394887\n",
      "The 2023 th iteration gives loss of 0.6522646923564587\n",
      "The 2024 th iteration gives loss of 0.6520615460958038\n",
      "The 2025 th iteration gives loss of 0.6518586372209295\n",
      "The 2026 th iteration gives loss of 0.6516559653406908\n",
      "The 2027 th iteration gives loss of 0.6514535300647771\n",
      "The 2028 th iteration gives loss of 0.6512513310037038\n",
      "The 2029 th iteration gives loss of 0.6510493677688122\n",
      "The 2030 th iteration gives loss of 0.6508476399722648\n",
      "The 2031 th iteration gives loss of 0.6506461472270604\n",
      "The 2032 th iteration gives loss of 0.6504448891469962\n",
      "The 2033 th iteration gives loss of 0.6502438653467074\n",
      "The 2034 th iteration gives loss of 0.6500430754416188\n",
      "The 2035 th iteration gives loss of 0.6498425190479785\n",
      "The 2036 th iteration gives loss of 0.6496421957828583\n",
      "The 2037 th iteration gives loss of 0.6494421052641025\n",
      "The 2038 th iteration gives loss of 0.649242247110405\n",
      "The 2039 th iteration gives loss of 0.6490426209412189\n",
      "The 2040 th iteration gives loss of 0.6488432263768239\n",
      "The 2041 th iteration gives loss of 0.6486440630382962\n",
      "The 2042 th iteration gives loss of 0.6484451305474714\n",
      "The 2043 th iteration gives loss of 0.6482464285270262\n",
      "The 2044 th iteration gives loss of 0.6480479566003979\n",
      "The 2045 th iteration gives loss of 0.6478497143918337\n",
      "The 2046 th iteration gives loss of 0.6476517015263463\n",
      "The 2047 th iteration gives loss of 0.6474539176297213\n",
      "The 2048 th iteration gives loss of 0.6472563623285605\n",
      "The 2049 th iteration gives loss of 0.6470590352502292\n",
      "The 2050 th iteration gives loss of 0.6468619360228447\n",
      "The 2051 th iteration gives loss of 0.6466650642753208\n",
      "The 2052 th iteration gives loss of 0.646468419637366\n",
      "The 2053 th iteration gives loss of 0.6462720017393997\n",
      "The 2054 th iteration gives loss of 0.64607581021267\n",
      "The 2055 th iteration gives loss of 0.6458798446891243\n",
      "The 2056 th iteration gives loss of 0.6456841048015308\n",
      "The 2057 th iteration gives loss of 0.6454885901834064\n",
      "The 2058 th iteration gives loss of 0.6452933004690102\n",
      "The 2059 th iteration gives loss of 0.6450982352933461\n",
      "The 2060 th iteration gives loss of 0.6449033942921997\n",
      "The 2061 th iteration gives loss of 0.6447087771021067\n",
      "The 2062 th iteration gives loss of 0.6445143833603155\n",
      "The 2063 th iteration gives loss of 0.6443202127048763\n",
      "The 2064 th iteration gives loss of 0.6441262647745482\n",
      "The 2065 th iteration gives loss of 0.6439325392088168\n",
      "The 2066 th iteration gives loss of 0.6437390356479678\n",
      "The 2067 th iteration gives loss of 0.6435457537329661\n",
      "The 2068 th iteration gives loss of 0.6433526931055464\n",
      "The 2069 th iteration gives loss of 0.6431598534081611\n",
      "The 2070 th iteration gives loss of 0.6429672342840174\n",
      "The 2071 th iteration gives loss of 0.6427748353770036\n",
      "The 2072 th iteration gives loss of 0.6425826563317949\n",
      "The 2073 th iteration gives loss of 0.64239069679376\n",
      "The 2074 th iteration gives loss of 0.6421989564089954\n",
      "The 2075 th iteration gives loss of 0.6420074348243242\n",
      "The 2076 th iteration gives loss of 0.6418161316872775\n",
      "The 2077 th iteration gives loss of 0.6416250466461101\n",
      "The 2078 th iteration gives loss of 0.6414341793498123\n",
      "The 2079 th iteration gives loss of 0.6412435294480474\n",
      "The 2080 th iteration gives loss of 0.6410530965912217\n",
      "The 2081 th iteration gives loss of 0.6408628804304481\n",
      "The 2082 th iteration gives loss of 0.640672880617521\n",
      "The 2083 th iteration gives loss of 0.6404830968049628\n",
      "The 2084 th iteration gives loss of 0.6402935286459966\n",
      "The 2085 th iteration gives loss of 0.640104175794557\n",
      "The 2086 th iteration gives loss of 0.6399150379052473\n",
      "The 2087 th iteration gives loss of 0.6397261146334008\n",
      "The 2088 th iteration gives loss of 0.6395374056350122\n",
      "The 2089 th iteration gives loss of 0.639348910566809\n",
      "The 2090 th iteration gives loss of 0.639160629086179\n",
      "The 2091 th iteration gives loss of 0.6389725608512126\n",
      "The 2092 th iteration gives loss of 0.6387847055206715\n",
      "The 2093 th iteration gives loss of 0.6385970627540364\n",
      "The 2094 th iteration gives loss of 0.638409632211439\n",
      "The 2095 th iteration gives loss of 0.638222413553704\n",
      "The 2096 th iteration gives loss of 0.6380354064423457\n",
      "The 2097 th iteration gives loss of 0.6378486105395306\n",
      "The 2098 th iteration gives loss of 0.6376620255081339\n",
      "The 2099 th iteration gives loss of 0.6374756510116741\n",
      "The 2100 th iteration gives loss of 0.6372894867143752\n",
      "The 2101 th iteration gives loss of 0.6371035322810983\n",
      "The 2102 th iteration gives loss of 0.6369177873773935\n",
      "The 2103 th iteration gives loss of 0.6367322516694529\n",
      "The 2104 th iteration gives loss of 0.6365469248241761\n",
      "The 2105 th iteration gives loss of 0.6363618065090785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2106 th iteration gives loss of 0.6361768963923741\n",
      "The 2107 th iteration gives loss of 0.6359921941429009\n",
      "The 2108 th iteration gives loss of 0.6358076994301921\n",
      "The 2109 th iteration gives loss of 0.6356234119244139\n",
      "The 2110 th iteration gives loss of 0.6354393312963881\n",
      "The 2111 th iteration gives loss of 0.6352554572175843\n",
      "The 2112 th iteration gives loss of 0.635071789360136\n",
      "The 2113 th iteration gives loss of 0.63488832739681\n",
      "The 2114 th iteration gives loss of 0.6347050710010264\n",
      "The 2115 th iteration gives loss of 0.6345220198468507\n",
      "The 2116 th iteration gives loss of 0.6343391736089925\n",
      "The 2117 th iteration gives loss of 0.6341565319628132\n",
      "The 2118 th iteration gives loss of 0.6339740945842705\n",
      "The 2119 th iteration gives loss of 0.6337918611500114\n",
      "The 2120 th iteration gives loss of 0.6336098313372949\n",
      "The 2121 th iteration gives loss of 0.6334280048240157\n",
      "The 2122 th iteration gives loss of 0.6332463812887121\n",
      "The 2123 th iteration gives loss of 0.6330649604105024\n",
      "The 2124 th iteration gives loss of 0.6328837418692264\n",
      "The 2125 th iteration gives loss of 0.6327027253452627\n",
      "The 2126 th iteration gives loss of 0.6325219105196658\n",
      "The 2127 th iteration gives loss of 0.6323412970740918\n",
      "The 2128 th iteration gives loss of 0.6321608846908413\n",
      "The 2129 th iteration gives loss of 0.6319806730528034\n",
      "The 2130 th iteration gives loss of 0.6318006618435141\n",
      "The 2131 th iteration gives loss of 0.6316208507471297\n",
      "The 2132 th iteration gives loss of 0.6314412394483822\n",
      "The 2133 th iteration gives loss of 0.6312618276326569\n",
      "The 2134 th iteration gives loss of 0.6310826149859311\n",
      "The 2135 th iteration gives loss of 0.6309036011948052\n",
      "The 2136 th iteration gives loss of 0.6307247859464666\n",
      "The 2137 th iteration gives loss of 0.6305461689287525\n",
      "The 2138 th iteration gives loss of 0.6303677498300676\n",
      "The 2139 th iteration gives loss of 0.6301895283394242\n",
      "The 2140 th iteration gives loss of 0.6300115041464435\n",
      "The 2141 th iteration gives loss of 0.6298336769413629\n",
      "The 2142 th iteration gives loss of 0.6296560464149868\n",
      "The 2143 th iteration gives loss of 0.6294786122587517\n",
      "The 2144 th iteration gives loss of 0.6293013741646581\n",
      "The 2145 th iteration gives loss of 0.6291243318253253\n",
      "The 2146 th iteration gives loss of 0.6289474849339495\n",
      "The 2147 th iteration gives loss of 0.628770833184335\n",
      "The 2148 th iteration gives loss of 0.6285943762708539\n",
      "The 2149 th iteration gives loss of 0.6284181138884699\n",
      "The 2150 th iteration gives loss of 0.6282420457327664\n",
      "The 2151 th iteration gives loss of 0.628066171499859\n",
      "The 2152 th iteration gives loss of 0.6278904908864842\n",
      "The 2153 th iteration gives loss of 0.6277150035899555\n",
      "The 2154 th iteration gives loss of 0.6275397093081554\n",
      "The 2155 th iteration gives loss of 0.6273646077395406\n",
      "The 2156 th iteration gives loss of 0.6271896985831633\n",
      "The 2157 th iteration gives loss of 0.6270149815386669\n",
      "The 2158 th iteration gives loss of 0.6268404563062088\n",
      "The 2159 th iteration gives loss of 0.6266661225865721\n",
      "The 2160 th iteration gives loss of 0.6264919800810987\n",
      "The 2161 th iteration gives loss of 0.6263180284916787\n",
      "The 2162 th iteration gives loss of 0.6261442675208044\n",
      "The 2163 th iteration gives loss of 0.6259706968714991\n",
      "The 2164 th iteration gives loss of 0.625797316247404\n",
      "The 2165 th iteration gives loss of 0.625624125352677\n",
      "The 2166 th iteration gives loss of 0.6254511238920374\n",
      "The 2167 th iteration gives loss of 0.6252783115707858\n",
      "The 2168 th iteration gives loss of 0.6251056880947605\n",
      "The 2169 th iteration gives loss of 0.624933253170404\n",
      "The 2170 th iteration gives loss of 0.6247610065046729\n",
      "The 2171 th iteration gives loss of 0.6245889478050948\n",
      "The 2172 th iteration gives loss of 0.6244170767797278\n",
      "The 2173 th iteration gives loss of 0.6242453931372266\n",
      "The 2174 th iteration gives loss of 0.6240738965867481\n",
      "The 2175 th iteration gives loss of 0.6239025868380373\n",
      "The 2176 th iteration gives loss of 0.6237314636013721\n",
      "The 2177 th iteration gives loss of 0.6235605265875692\n",
      "The 2178 th iteration gives loss of 0.6233897755079878\n",
      "The 2179 th iteration gives loss of 0.6232192100745552\n",
      "The 2180 th iteration gives loss of 0.6230488299997342\n",
      "The 2181 th iteration gives loss of 0.6228786349964933\n",
      "The 2182 th iteration gives loss of 0.6227086247783798\n",
      "The 2183 th iteration gives loss of 0.6225387990594856\n",
      "The 2184 th iteration gives loss of 0.6223691575543994\n",
      "The 2185 th iteration gives loss of 0.622199699978262\n",
      "The 2186 th iteration gives loss of 0.6220304260467664\n",
      "The 2187 th iteration gives loss of 0.6218613354761099\n",
      "The 2188 th iteration gives loss of 0.6216924279830681\n",
      "The 2189 th iteration gives loss of 0.6215237032848849\n",
      "The 2190 th iteration gives loss of 0.6213551610993728\n",
      "The 2191 th iteration gives loss of 0.6211868011448539\n",
      "The 2192 th iteration gives loss of 0.6210186231402042\n",
      "The 2193 th iteration gives loss of 0.620850626804771\n",
      "The 2194 th iteration gives loss of 0.6206828118584797\n",
      "The 2195 th iteration gives loss of 0.6205151780217514\n",
      "The 2196 th iteration gives loss of 0.6203477250155374\n",
      "The 2197 th iteration gives loss of 0.6201804525612901\n",
      "The 2198 th iteration gives loss of 0.6200133603809875\n",
      "The 2199 th iteration gives loss of 0.6198464481971591\n",
      "The 2200 th iteration gives loss of 0.6196797157327969\n",
      "The 2201 th iteration gives loss of 0.6195131627114292\n",
      "The 2202 th iteration gives loss of 0.619346788857113\n",
      "The 2203 th iteration gives loss of 0.619180593894391\n",
      "The 2204 th iteration gives loss of 0.6190145775483351\n",
      "The 2205 th iteration gives loss of 0.6188487395445007\n",
      "The 2206 th iteration gives loss of 0.6186830796089838\n",
      "The 2207 th iteration gives loss of 0.618517597468375\n",
      "The 2208 th iteration gives loss of 0.618352292849753\n",
      "The 2209 th iteration gives loss of 0.6181871654807346\n",
      "The 2210 th iteration gives loss of 0.6180222150893965\n",
      "The 2211 th iteration gives loss of 0.6178574414043462\n",
      "The 2212 th iteration gives loss of 0.6176928441546966\n",
      "The 2213 th iteration gives loss of 0.617528423070034\n",
      "The 2214 th iteration gives loss of 0.6173641778804576\n",
      "The 2215 th iteration gives loss of 0.6172001083165615\n",
      "The 2216 th iteration gives loss of 0.6170362141094445\n",
      "The 2217 th iteration gives loss of 0.6168724949906786\n",
      "The 2218 th iteration gives loss of 0.6167089506923501\n",
      "The 2219 th iteration gives loss of 0.616545580947013\n",
      "The 2220 th iteration gives loss of 0.6163823854877476\n",
      "The 2221 th iteration gives loss of 0.6162193640480657\n",
      "The 2222 th iteration gives loss of 0.616056516362029\n",
      "The 2223 th iteration gives loss of 0.6158938421641661\n",
      "The 2224 th iteration gives loss of 0.6157313411894548\n",
      "The 2225 th iteration gives loss of 0.6155690131734267\n",
      "The 2226 th iteration gives loss of 0.6154068578520098\n",
      "The 2227 th iteration gives loss of 0.6152448749617004\n",
      "The 2228 th iteration gives loss of 0.61508306423943\n",
      "The 2229 th iteration gives loss of 0.6149214254225907\n",
      "The 2230 th iteration gives loss of 0.6147599582491088\n",
      "The 2231 th iteration gives loss of 0.6145986624573498\n",
      "The 2232 th iteration gives loss of 0.6144375377861651\n",
      "The 2233 th iteration gives loss of 0.6142765839748721\n",
      "The 2234 th iteration gives loss of 0.6141158007632548\n",
      "The 2235 th iteration gives loss of 0.6139551878916267\n",
      "The 2236 th iteration gives loss of 0.6137947451007018\n",
      "The 2237 th iteration gives loss of 0.6136344721317024\n",
      "The 2238 th iteration gives loss of 0.6134743687263046\n",
      "The 2239 th iteration gives loss of 0.6133144346266645\n",
      "The 2240 th iteration gives loss of 0.6131546695753928\n",
      "The 2241 th iteration gives loss of 0.6129950733155998\n",
      "The 2242 th iteration gives loss of 0.6128356455908207\n",
      "The 2243 th iteration gives loss of 0.6126763861450448\n",
      "The 2244 th iteration gives loss of 0.612517294722777\n",
      "The 2245 th iteration gives loss of 0.6123583710689375\n",
      "The 2246 th iteration gives loss of 0.6121996149289294\n",
      "The 2247 th iteration gives loss of 0.612041026048607\n",
      "The 2248 th iteration gives loss of 0.6118826041742872\n",
      "The 2249 th iteration gives loss of 0.6117243490527525\n",
      "The 2250 th iteration gives loss of 0.6115662604312185\n",
      "The 2251 th iteration gives loss of 0.6114083380573674\n",
      "The 2252 th iteration gives loss of 0.6112505816793328\n",
      "The 2253 th iteration gives loss of 0.6110929910457029\n",
      "The 2254 th iteration gives loss of 0.6109355659055398\n",
      "The 2255 th iteration gives loss of 0.6107783060083201\n",
      "The 2256 th iteration gives loss of 0.610621211103968\n",
      "The 2257 th iteration gives loss of 0.6104642809428935\n",
      "The 2258 th iteration gives loss of 0.6103075152759535\n",
      "The 2259 th iteration gives loss of 0.6101509138543822\n",
      "The 2260 th iteration gives loss of 0.6099944764299416\n",
      "The 2261 th iteration gives loss of 0.6098382027547969\n",
      "The 2262 th iteration gives loss of 0.6096820925815567\n",
      "The 2263 th iteration gives loss of 0.6095261456632938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2264 th iteration gives loss of 0.6093703617534952\n",
      "The 2265 th iteration gives loss of 0.6092147406060929\n",
      "The 2266 th iteration gives loss of 0.6090592819754789\n",
      "The 2267 th iteration gives loss of 0.6089039856164674\n",
      "The 2268 th iteration gives loss of 0.6087488512843259\n",
      "The 2269 th iteration gives loss of 0.6085938787347128\n",
      "The 2270 th iteration gives loss of 0.6084390677237651\n",
      "The 2271 th iteration gives loss of 0.6082844180080391\n",
      "The 2272 th iteration gives loss of 0.6081299293445331\n",
      "The 2273 th iteration gives loss of 0.6079756014906689\n",
      "The 2274 th iteration gives loss of 0.6078214342042799\n",
      "The 2275 th iteration gives loss of 0.6076674272436815\n",
      "The 2276 th iteration gives loss of 0.6075135803675639\n",
      "The 2277 th iteration gives loss of 0.6073598933350712\n",
      "The 2278 th iteration gives loss of 0.607206365905768\n",
      "The 2279 th iteration gives loss of 0.6070529978396649\n",
      "The 2280 th iteration gives loss of 0.6068997888971449\n",
      "The 2281 th iteration gives loss of 0.6067467388390848\n",
      "The 2282 th iteration gives loss of 0.6065938474267315\n",
      "The 2283 th iteration gives loss of 0.6064411144217737\n",
      "The 2284 th iteration gives loss of 0.606288539586315\n",
      "The 2285 th iteration gives loss of 0.6061361226829047\n",
      "The 2286 th iteration gives loss of 0.605983863474471\n",
      "The 2287 th iteration gives loss of 0.60583176172437\n",
      "The 2288 th iteration gives loss of 0.6056798171963907\n",
      "The 2289 th iteration gives loss of 0.605528029654742\n",
      "The 2290 th iteration gives loss of 0.6053763988640241\n",
      "The 2291 th iteration gives loss of 0.6052249245892777\n",
      "The 2292 th iteration gives loss of 0.6050736065959256\n",
      "The 2293 th iteration gives loss of 0.6049224446498387\n",
      "The 2294 th iteration gives loss of 0.6047714385172729\n",
      "The 2295 th iteration gives loss of 0.6046205879649065\n",
      "The 2296 th iteration gives loss of 0.6044698927598158\n",
      "The 2297 th iteration gives loss of 0.6043193526694959\n",
      "The 2298 th iteration gives loss of 0.604168967461865\n",
      "The 2299 th iteration gives loss of 0.6040187369052006\n",
      "The 2300 th iteration gives loss of 0.6038686607682465\n",
      "The 2301 th iteration gives loss of 0.6037187388200952\n",
      "The 2302 th iteration gives loss of 0.6035689708302929\n",
      "The 2303 th iteration gives loss of 0.6034193565687507\n",
      "The 2304 th iteration gives loss of 0.6032698958057865\n",
      "The 2305 th iteration gives loss of 0.6031205883121568\n",
      "The 2306 th iteration gives loss of 0.6029714338589622\n",
      "The 2307 th iteration gives loss of 0.6028224322177478\n",
      "The 2308 th iteration gives loss of 0.6026735831604556\n",
      "The 2309 th iteration gives loss of 0.6025248864593697\n",
      "The 2310 th iteration gives loss of 0.6023763418872421\n",
      "The 2311 th iteration gives loss of 0.6022279492171781\n",
      "The 2312 th iteration gives loss of 0.6020797082226979\n",
      "The 2313 th iteration gives loss of 0.6019316186776958\n",
      "The 2314 th iteration gives loss of 0.6017836803564767\n",
      "The 2315 th iteration gives loss of 0.6016358930337489\n",
      "The 2316 th iteration gives loss of 0.6014882564845747\n",
      "The 2317 th iteration gives loss of 0.6013407704844358\n",
      "The 2318 th iteration gives loss of 0.6011934348092004\n",
      "The 2319 th iteration gives loss of 0.6010462492351046\n",
      "The 2320 th iteration gives loss of 0.6008992135388149\n",
      "The 2321 th iteration gives loss of 0.6007523274973298\n",
      "The 2322 th iteration gives loss of 0.6006055908880845\n",
      "The 2323 th iteration gives loss of 0.6004590034888636\n",
      "The 2324 th iteration gives loss of 0.6003125650778642\n",
      "The 2325 th iteration gives loss of 0.6001662754336443\n",
      "The 2326 th iteration gives loss of 0.600020134335151\n",
      "The 2327 th iteration gives loss of 0.5998741415617315\n",
      "The 2328 th iteration gives loss of 0.5997282968930827\n",
      "The 2329 th iteration gives loss of 0.5995826001093035\n",
      "The 2330 th iteration gives loss of 0.5994370509908621\n",
      "The 2331 th iteration gives loss of 0.5992916493186229\n",
      "The 2332 th iteration gives loss of 0.599146394873799\n",
      "The 2333 th iteration gives loss of 0.5990012874380044\n",
      "The 2334 th iteration gives loss of 0.5988563267932135\n",
      "The 2335 th iteration gives loss of 0.598711512721808\n",
      "The 2336 th iteration gives loss of 0.5985668450064923\n",
      "The 2337 th iteration gives loss of 0.5984223234303847\n",
      "The 2338 th iteration gives loss of 0.5982779477769633\n",
      "The 2339 th iteration gives loss of 0.5981337178300681\n",
      "The 2340 th iteration gives loss of 0.5979896333739251\n",
      "The 2341 th iteration gives loss of 0.5978456941931355\n",
      "The 2342 th iteration gives loss of 0.5977019000726608\n",
      "The 2343 th iteration gives loss of 0.597558250797803\n",
      "The 2344 th iteration gives loss of 0.5974147461543053\n",
      "The 2345 th iteration gives loss of 0.5972713859281763\n",
      "The 2346 th iteration gives loss of 0.5971281699058995\n",
      "The 2347 th iteration gives loss of 0.5969850978742521\n",
      "The 2348 th iteration gives loss of 0.5968421696203847\n",
      "The 2349 th iteration gives loss of 0.596699384931832\n",
      "The 2350 th iteration gives loss of 0.5965567435964886\n",
      "The 2351 th iteration gives loss of 0.5964142454025951\n",
      "The 2352 th iteration gives loss of 0.5962718901387731\n",
      "The 2353 th iteration gives loss of 0.5961296775939934\n",
      "The 2354 th iteration gives loss of 0.5959876075575844\n",
      "The 2355 th iteration gives loss of 0.5958456798192472\n",
      "The 2356 th iteration gives loss of 0.5957038941690167\n",
      "The 2357 th iteration gives loss of 0.5955622503973165\n",
      "The 2358 th iteration gives loss of 0.5954207482949054\n",
      "The 2359 th iteration gives loss of 0.5952793876529213\n",
      "The 2360 th iteration gives loss of 0.5951381682628114\n",
      "The 2361 th iteration gives loss of 0.5949970899164448\n",
      "The 2362 th iteration gives loss of 0.5948561524059632\n",
      "The 2363 th iteration gives loss of 0.5947153555239443\n",
      "The 2364 th iteration gives loss of 0.5945746990632647\n",
      "The 2365 th iteration gives loss of 0.5944341828171633\n",
      "The 2366 th iteration gives loss of 0.5942938065792382\n",
      "The 2367 th iteration gives loss of 0.5941535701434183\n",
      "The 2368 th iteration gives loss of 0.5940134733040228\n",
      "The 2369 th iteration gives loss of 0.5938735158556777\n",
      "The 2370 th iteration gives loss of 0.593733697593385\n",
      "The 2371 th iteration gives loss of 0.5935940183124626\n",
      "The 2372 th iteration gives loss of 0.5934544778085937\n",
      "The 2373 th iteration gives loss of 0.5933150758778318\n",
      "The 2374 th iteration gives loss of 0.5931758123165211\n",
      "The 2375 th iteration gives loss of 0.5930366869213974\n",
      "The 2376 th iteration gives loss of 0.5928976994895163\n",
      "The 2377 th iteration gives loss of 0.59275884981828\n",
      "The 2378 th iteration gives loss of 0.5926201377054245\n",
      "The 2379 th iteration gives loss of 0.5924815629490634\n",
      "The 2380 th iteration gives loss of 0.5923431253476006\n",
      "The 2381 th iteration gives loss of 0.5922048246998218\n",
      "The 2382 th iteration gives loss of 0.5920666608048251\n",
      "The 2383 th iteration gives loss of 0.5919286334620392\n",
      "The 2384 th iteration gives loss of 0.5917907424712642\n",
      "The 2385 th iteration gives loss of 0.5916529876326305\n",
      "The 2386 th iteration gives loss of 0.5915153687465673\n",
      "The 2387 th iteration gives loss of 0.5913778856138799\n",
      "The 2388 th iteration gives loss of 0.5912405380356891\n",
      "The 2389 th iteration gives loss of 0.5911033258134571\n",
      "The 2390 th iteration gives loss of 0.5909662487489756\n",
      "The 2391 th iteration gives loss of 0.5908293066443825\n",
      "The 2392 th iteration gives loss of 0.5906924993021297\n",
      "The 2393 th iteration gives loss of 0.590555826525009\n",
      "The 2394 th iteration gives loss of 0.5904192881161343\n",
      "The 2395 th iteration gives loss of 0.5902828838789539\n",
      "The 2396 th iteration gives loss of 0.5901466136172442\n",
      "The 2397 th iteration gives loss of 0.5900104771351414\n",
      "The 2398 th iteration gives loss of 0.5898744742370475\n",
      "The 2399 th iteration gives loss of 0.5897386047277604\n",
      "The 2400 th iteration gives loss of 0.5896028684123338\n",
      "The 2401 th iteration gives loss of 0.5894672650961937\n",
      "The 2402 th iteration gives loss of 0.5893317945850947\n",
      "The 2403 th iteration gives loss of 0.5891964566850924\n",
      "The 2404 th iteration gives loss of 0.5890612512025797\n",
      "The 2405 th iteration gives loss of 0.5889261779442713\n",
      "The 2406 th iteration gives loss of 0.5887912367172088\n",
      "The 2407 th iteration gives loss of 0.5886564273287278\n",
      "The 2408 th iteration gives loss of 0.5885217495865216\n",
      "The 2409 th iteration gives loss of 0.5883872032985884\n",
      "The 2410 th iteration gives loss of 0.5882527882732356\n",
      "The 2411 th iteration gives loss of 0.5881185043191057\n",
      "The 2412 th iteration gives loss of 0.5879843512451697\n",
      "The 2413 th iteration gives loss of 0.5878503288606804\n",
      "The 2414 th iteration gives loss of 0.587716436975234\n",
      "The 2415 th iteration gives loss of 0.5875826753987536\n",
      "The 2416 th iteration gives loss of 0.5874490439414556\n",
      "The 2417 th iteration gives loss of 0.5873155424138625\n",
      "The 2418 th iteration gives loss of 0.5871821706268535\n",
      "The 2419 th iteration gives loss of 0.5870489283915771\n",
      "The 2420 th iteration gives loss of 0.586915815519527\n",
      "The 2421 th iteration gives loss of 0.586782831822475\n",
      "The 2422 th iteration gives loss of 0.5866499771125515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2423 th iteration gives loss of 0.5865172512021652\n",
      "The 2424 th iteration gives loss of 0.5863846539040365\n",
      "The 2425 th iteration gives loss of 0.5862521850312182\n",
      "The 2426 th iteration gives loss of 0.5861198443970491\n",
      "The 2427 th iteration gives loss of 0.585987631815179\n",
      "The 2428 th iteration gives loss of 0.5858555470995864\n",
      "The 2429 th iteration gives loss of 0.585723590064535\n",
      "The 2430 th iteration gives loss of 0.5855917605246163\n",
      "The 2431 th iteration gives loss of 0.5854600582946994\n",
      "The 2432 th iteration gives loss of 0.5853284831899866\n",
      "The 2433 th iteration gives loss of 0.5851970350259693\n",
      "The 2434 th iteration gives loss of 0.5850657136184643\n",
      "The 2435 th iteration gives loss of 0.584934518783563\n",
      "The 2436 th iteration gives loss of 0.5848034503376627\n",
      "The 2437 th iteration gives loss of 0.5846725080974899\n",
      "The 2438 th iteration gives loss of 0.5845416918800613\n",
      "The 2439 th iteration gives loss of 0.5844110015026801\n",
      "The 2440 th iteration gives loss of 0.5842804367829618\n",
      "The 2441 th iteration gives loss of 0.5841499975388438\n",
      "The 2442 th iteration gives loss of 0.5840196835885196\n",
      "The 2443 th iteration gives loss of 0.5838894947505048\n",
      "The 2444 th iteration gives loss of 0.5837594308436211\n",
      "The 2445 th iteration gives loss of 0.5836294916869782\n",
      "The 2446 th iteration gives loss of 0.5834996770999887\n",
      "The 2447 th iteration gives loss of 0.5833699869023405\n",
      "The 2448 th iteration gives loss of 0.5832404209140537\n",
      "The 2449 th iteration gives loss of 0.5831109789554173\n",
      "The 2450 th iteration gives loss of 0.5829816608470212\n",
      "The 2451 th iteration gives loss of 0.5828524664097553\n",
      "The 2452 th iteration gives loss of 0.5827233954647962\n",
      "The 2453 th iteration gives loss of 0.5825944478336248\n",
      "The 2454 th iteration gives loss of 0.5824656233379947\n",
      "The 2455 th iteration gives loss of 0.5823369217999641\n",
      "The 2456 th iteration gives loss of 0.5822083430418905\n",
      "The 2457 th iteration gives loss of 0.5820798868863918\n",
      "The 2458 th iteration gives loss of 0.5819515531564283\n",
      "The 2459 th iteration gives loss of 0.5818233416752003\n",
      "The 2460 th iteration gives loss of 0.5816952522662175\n",
      "The 2461 th iteration gives loss of 0.5815672847532781\n",
      "The 2462 th iteration gives loss of 0.5814394389604791\n",
      "The 2463 th iteration gives loss of 0.5813117147121722\n",
      "The 2464 th iteration gives loss of 0.5811841118330348\n",
      "The 2465 th iteration gives loss of 0.5810566301480098\n",
      "The 2466 th iteration gives loss of 0.5809292694823079\n",
      "The 2467 th iteration gives loss of 0.5808020296614687\n",
      "The 2468 th iteration gives loss of 0.5806749105112659\n",
      "The 2469 th iteration gives loss of 0.580547911857816\n",
      "The 2470 th iteration gives loss of 0.5804210335274629\n",
      "The 2471 th iteration gives loss of 0.5802942753468672\n",
      "The 2472 th iteration gives loss of 0.5801676371429701\n",
      "The 2473 th iteration gives loss of 0.5800411187429767\n",
      "The 2474 th iteration gives loss of 0.5799147199743782\n",
      "The 2475 th iteration gives loss of 0.5797884406649649\n",
      "The 2476 th iteration gives loss of 0.5796622806427864\n",
      "The 2477 th iteration gives loss of 0.5795362397361699\n",
      "The 2478 th iteration gives loss of 0.5794103177737417\n",
      "The 2479 th iteration gives loss of 0.5792845145843831\n",
      "The 2480 th iteration gives loss of 0.5791588299972937\n",
      "The 2481 th iteration gives loss of 0.5790332638419102\n",
      "The 2482 th iteration gives loss of 0.578907815947945\n",
      "The 2483 th iteration gives loss of 0.5787824861454116\n",
      "The 2484 th iteration gives loss of 0.5786572742645817\n",
      "The 2485 th iteration gives loss of 0.578532180136017\n",
      "The 2486 th iteration gives loss of 0.5784072035905348\n",
      "The 2487 th iteration gives loss of 0.5782823444592498\n",
      "The 2488 th iteration gives loss of 0.5781576025735294\n",
      "The 2489 th iteration gives loss of 0.5780329777650212\n",
      "The 2490 th iteration gives loss of 0.5779084698656627\n",
      "The 2491 th iteration gives loss of 0.577784078707629\n",
      "The 2492 th iteration gives loss of 0.5776598041233763\n",
      "The 2493 th iteration gives loss of 0.5775356459456683\n",
      "The 2494 th iteration gives loss of 0.5774116040074925\n",
      "The 2495 th iteration gives loss of 0.5772876781421379\n",
      "The 2496 th iteration gives loss of 0.5771638681831484\n",
      "The 2497 th iteration gives loss of 0.5770401739643118\n",
      "The 2498 th iteration gives loss of 0.5769165953197386\n",
      "The 2499 th iteration gives loss of 0.5767931320837828\n",
      "The 2500 th iteration gives loss of 0.5766697840910432\n",
      "The 2501 th iteration gives loss of 0.5765465511764227\n",
      "The 2502 th iteration gives loss of 0.5764234331750586\n",
      "The 2503 th iteration gives loss of 0.5763004299223643\n",
      "The 2504 th iteration gives loss of 0.5761775412540369\n",
      "The 2505 th iteration gives loss of 0.5760547670060212\n",
      "The 2506 th iteration gives loss of 0.5759321070145371\n",
      "The 2507 th iteration gives loss of 0.5758095611160233\n",
      "The 2508 th iteration gives loss of 0.575687129147255\n",
      "The 2509 th iteration gives loss of 0.5755648109452272\n",
      "The 2510 th iteration gives loss of 0.5754426063471844\n",
      "The 2511 th iteration gives loss of 0.5753205151906797\n",
      "The 2512 th iteration gives loss of 0.5751985373134656\n",
      "The 2513 th iteration gives loss of 0.5750766725536174\n",
      "The 2514 th iteration gives loss of 0.5749549207494323\n",
      "The 2515 th iteration gives loss of 0.5748332817394752\n",
      "The 2516 th iteration gives loss of 0.5747117553625722\n",
      "The 2517 th iteration gives loss of 0.5745903414578248\n",
      "The 2518 th iteration gives loss of 0.5744690398645484\n",
      "The 2519 th iteration gives loss of 0.5743478504223768\n",
      "The 2520 th iteration gives loss of 0.574226772971148\n",
      "The 2521 th iteration gives loss of 0.5741058073509565\n",
      "The 2522 th iteration gives loss of 0.5739849534022116\n",
      "The 2523 th iteration gives loss of 0.5738642109655262\n",
      "The 2524 th iteration gives loss of 0.573743579881795\n",
      "The 2525 th iteration gives loss of 0.5736230599921266\n",
      "The 2526 th iteration gives loss of 0.5735026511379389\n",
      "The 2527 th iteration gives loss of 0.5733823531608663\n",
      "The 2528 th iteration gives loss of 0.5732621659028214\n",
      "The 2529 th iteration gives loss of 0.5731420892059249\n",
      "The 2530 th iteration gives loss of 0.5730221229126078\n",
      "The 2531 th iteration gives loss of 0.5729022668655414\n",
      "The 2532 th iteration gives loss of 0.5727825209076058\n",
      "The 2533 th iteration gives loss of 0.5726628848819629\n",
      "The 2534 th iteration gives loss of 0.572543358632041\n",
      "The 2535 th iteration gives loss of 0.5724239420014806\n",
      "The 2536 th iteration gives loss of 0.5723046348341944\n",
      "The 2537 th iteration gives loss of 0.5721854369743467\n",
      "The 2538 th iteration gives loss of 0.5720663482663476\n",
      "The 2539 th iteration gives loss of 0.5719473685548522\n",
      "The 2540 th iteration gives loss of 0.5718284976847495\n",
      "The 2541 th iteration gives loss of 0.5717097355012138\n",
      "The 2542 th iteration gives loss of 0.5715910818496243\n",
      "The 2543 th iteration gives loss of 0.5714725365756373\n",
      "The 2544 th iteration gives loss of 0.5713540995251263\n",
      "The 2545 th iteration gives loss of 0.5712357705442327\n",
      "The 2546 th iteration gives loss of 0.5711175494793566\n",
      "The 2547 th iteration gives loss of 0.5709994361770965\n",
      "The 2548 th iteration gives loss of 0.5708814304843358\n",
      "The 2549 th iteration gives loss of 0.570763532248173\n",
      "The 2550 th iteration gives loss of 0.570645741315972\n",
      "The 2551 th iteration gives loss of 0.5705280575353411\n",
      "The 2552 th iteration gives loss of 0.5704104807541137\n",
      "The 2553 th iteration gives loss of 0.5702930108203582\n",
      "The 2554 th iteration gives loss of 0.5701756475824171\n",
      "The 2555 th iteration gives loss of 0.5700583908888469\n",
      "The 2556 th iteration gives loss of 0.5699412405884576\n",
      "The 2557 th iteration gives loss of 0.5698241965302882\n",
      "The 2558 th iteration gives loss of 0.5697072585636316\n",
      "The 2559 th iteration gives loss of 0.5695904265379931\n",
      "The 2560 th iteration gives loss of 0.5694737003031666\n",
      "The 2561 th iteration gives loss of 0.5693570797091205\n",
      "The 2562 th iteration gives loss of 0.5692405646061222\n",
      "The 2563 th iteration gives loss of 0.5691241548446222\n",
      "The 2564 th iteration gives loss of 0.5690078502753356\n",
      "The 2565 th iteration gives loss of 0.5688916507492312\n",
      "The 2566 th iteration gives loss of 0.5687755561174735\n",
      "The 2567 th iteration gives loss of 0.5686595662315111\n",
      "The 2568 th iteration gives loss of 0.5685436809429663\n",
      "The 2569 th iteration gives loss of 0.5684279001037545\n",
      "The 2570 th iteration gives loss of 0.5683122235659823\n",
      "The 2571 th iteration gives loss of 0.5681966511820377\n",
      "The 2572 th iteration gives loss of 0.5680811828044806\n",
      "The 2573 th iteration gives loss of 0.5679658182861566\n",
      "The 2574 th iteration gives loss of 0.5678505574801125\n",
      "The 2575 th iteration gives loss of 0.5677354002396496\n",
      "The 2576 th iteration gives loss of 0.5676203464182841\n",
      "The 2577 th iteration gives loss of 0.5675053958697619\n",
      "The 2578 th iteration gives loss of 0.5673905484480815\n",
      "The 2579 th iteration gives loss of 0.5672758040074399\n",
      "The 2580 th iteration gives loss of 0.5671611624023049\n",
      "The 2581 th iteration gives loss of 0.5670466234873299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2582 th iteration gives loss of 0.5669321871174435\n",
      "The 2583 th iteration gives loss of 0.5668178531477565\n",
      "The 2584 th iteration gives loss of 0.5667036214336311\n",
      "The 2585 th iteration gives loss of 0.566589491830664\n",
      "The 2586 th iteration gives loss of 0.5664754641946665\n",
      "The 2587 th iteration gives loss of 0.5663615383816962\n",
      "The 2588 th iteration gives loss of 0.5662477142480012\n",
      "The 2589 th iteration gives loss of 0.5661339916500832\n",
      "The 2590 th iteration gives loss of 0.5660203704446767\n",
      "The 2591 th iteration gives loss of 0.5659068504887198\n",
      "The 2592 th iteration gives loss of 0.5657934316393874\n",
      "The 2593 th iteration gives loss of 0.565680113754085\n",
      "The 2594 th iteration gives loss of 0.5655668966904118\n",
      "The 2595 th iteration gives loss of 0.5654537803062372\n",
      "The 2596 th iteration gives loss of 0.5653407644596151\n",
      "The 2597 th iteration gives loss of 0.5652278490088435\n",
      "The 2598 th iteration gives loss of 0.5651150338124624\n",
      "The 2599 th iteration gives loss of 0.5650023187291598\n",
      "The 2600 th iteration gives loss of 0.5648897036179275\n",
      "The 2601 th iteration gives loss of 0.5647771883379444\n",
      "The 2602 th iteration gives loss of 0.5646647727485903\n",
      "The 2603 th iteration gives loss of 0.5645524567095055\n",
      "The 2604 th iteration gives loss of 0.5644402400805307\n",
      "The 2605 th iteration gives loss of 0.5643281227217314\n",
      "The 2606 th iteration gives loss of 0.564216104493364\n",
      "The 2607 th iteration gives loss of 0.56410418525596\n",
      "The 2608 th iteration gives loss of 0.5639923648702274\n",
      "The 2609 th iteration gives loss of 0.5638806431971045\n",
      "The 2610 th iteration gives loss of 0.5637690200977485\n",
      "The 2611 th iteration gives loss of 0.5636574954335112\n",
      "The 2612 th iteration gives loss of 0.5635460690660175\n",
      "The 2613 th iteration gives loss of 0.5634347408570525\n",
      "The 2614 th iteration gives loss of 0.563323510668622\n",
      "The 2615 th iteration gives loss of 0.5632123783630029\n",
      "The 2616 th iteration gives loss of 0.5631013438026273\n",
      "The 2617 th iteration gives loss of 0.5629904068501661\n",
      "The 2618 th iteration gives loss of 0.5628795673685038\n",
      "The 2619 th iteration gives loss of 0.5627688252207457\n",
      "The 2620 th iteration gives loss of 0.562658180270195\n",
      "The 2621 th iteration gives loss of 0.5625476323803825\n",
      "The 2622 th iteration gives loss of 0.5624371814150435\n",
      "The 2623 th iteration gives loss of 0.5623268272381352\n",
      "The 2624 th iteration gives loss of 0.5622165697138132\n",
      "The 2625 th iteration gives loss of 0.5621064087064711\n",
      "The 2626 th iteration gives loss of 0.5619963440806824\n",
      "The 2627 th iteration gives loss of 0.5618863757012466\n",
      "The 2628 th iteration gives loss of 0.5617765034331826\n",
      "The 2629 th iteration gives loss of 0.5616667271417096\n",
      "The 2630 th iteration gives loss of 0.561557046692255\n",
      "The 2631 th iteration gives loss of 0.5614474619504609\n",
      "The 2632 th iteration gives loss of 0.5613379727821766\n",
      "The 2633 th iteration gives loss of 0.5612285790534591\n",
      "The 2634 th iteration gives loss of 0.5611192806305844\n",
      "The 2635 th iteration gives loss of 0.5610100773800347\n",
      "The 2636 th iteration gives loss of 0.5609009691684808\n",
      "The 2637 th iteration gives loss of 0.5607919558628169\n",
      "The 2638 th iteration gives loss of 0.5606830373301501\n",
      "The 2639 th iteration gives loss of 0.5605742134377856\n",
      "The 2640 th iteration gives loss of 0.560465484053224\n",
      "The 2641 th iteration gives loss of 0.5603568490441929\n",
      "The 2642 th iteration gives loss of 0.5602483082786208\n",
      "The 2643 th iteration gives loss of 0.5601398616246255\n",
      "The 2644 th iteration gives loss of 0.5600315089505619\n",
      "The 2645 th iteration gives loss of 0.5599232501249518\n",
      "The 2646 th iteration gives loss of 0.5598150850165603\n",
      "The 2647 th iteration gives loss of 0.5597070134943031\n",
      "The 2648 th iteration gives loss of 0.5595990354273556\n",
      "The 2649 th iteration gives loss of 0.5594911506850707\n",
      "The 2650 th iteration gives loss of 0.5593833591369964\n",
      "The 2651 th iteration gives loss of 0.559275660652905\n",
      "The 2652 th iteration gives loss of 0.5591680551027495\n",
      "The 2653 th iteration gives loss of 0.5590605423567034\n",
      "The 2654 th iteration gives loss of 0.5589531222851178\n",
      "The 2655 th iteration gives loss of 0.5588457947585742\n",
      "The 2656 th iteration gives loss of 0.5587385596478374\n",
      "The 2657 th iteration gives loss of 0.5586314168238745\n",
      "The 2658 th iteration gives loss of 0.5585243661578413\n",
      "The 2659 th iteration gives loss of 0.5584174075211227\n",
      "The 2660 th iteration gives loss of 0.5583105407852802\n",
      "The 2661 th iteration gives loss of 0.5582037658220801\n",
      "The 2662 th iteration gives loss of 0.5580970825034894\n",
      "The 2663 th iteration gives loss of 0.5579904907016547\n",
      "The 2664 th iteration gives loss of 0.5578839902889579\n",
      "The 2665 th iteration gives loss of 0.5577775811379497\n",
      "The 2666 th iteration gives loss of 0.5576712631213743\n",
      "The 2667 th iteration gives loss of 0.5575650361122189\n",
      "The 2668 th iteration gives loss of 0.5574588999835847\n",
      "The 2669 th iteration gives loss of 0.5573528546088513\n",
      "The 2670 th iteration gives loss of 0.557246899861556\n",
      "The 2671 th iteration gives loss of 0.5571410356154277\n",
      "The 2672 th iteration gives loss of 0.5570352617443863\n",
      "The 2673 th iteration gives loss of 0.55692957812259\n",
      "The 2674 th iteration gives loss of 0.556823984624341\n",
      "The 2675 th iteration gives loss of 0.5567184811241517\n",
      "The 2676 th iteration gives loss of 0.5566130674967558\n",
      "The 2677 th iteration gives loss of 0.5565077436170319\n",
      "The 2678 th iteration gives loss of 0.5564025093600757\n",
      "The 2679 th iteration gives loss of 0.556297364601189\n",
      "The 2680 th iteration gives loss of 0.5561923092158486\n",
      "The 2681 th iteration gives loss of 0.5560873430797202\n",
      "The 2682 th iteration gives loss of 0.5559824660686778\n",
      "The 2683 th iteration gives loss of 0.5558776780587816\n",
      "The 2684 th iteration gives loss of 0.5557729789262802\n",
      "The 2685 th iteration gives loss of 0.5556683685476069\n",
      "The 2686 th iteration gives loss of 0.5555638467993953\n",
      "The 2687 th iteration gives loss of 0.5554594135584526\n",
      "The 2688 th iteration gives loss of 0.5553550687018064\n",
      "The 2689 th iteration gives loss of 0.5552508121066523\n",
      "The 2690 th iteration gives loss of 0.5551466436503667\n",
      "The 2691 th iteration gives loss of 0.5550425632105331\n",
      "The 2692 th iteration gives loss of 0.554938570664928\n",
      "The 2693 th iteration gives loss of 0.5548346658914943\n",
      "The 2694 th iteration gives loss of 0.5547308487683819\n",
      "The 2695 th iteration gives loss of 0.5546271191738978\n",
      "The 2696 th iteration gives loss of 0.5545234769865894\n",
      "The 2697 th iteration gives loss of 0.5544199220851338\n",
      "The 2698 th iteration gives loss of 0.5543164543484363\n",
      "The 2699 th iteration gives loss of 0.5542130736555834\n",
      "The 2700 th iteration gives loss of 0.5541097798858139\n",
      "The 2701 th iteration gives loss of 0.5540065729186014\n",
      "The 2702 th iteration gives loss of 0.5539034526335502\n",
      "The 2703 th iteration gives loss of 0.5538004189104966\n",
      "The 2704 th iteration gives loss of 0.5536974716294463\n",
      "The 2705 th iteration gives loss of 0.5535946106705755\n",
      "The 2706 th iteration gives loss of 0.5534918359142722\n",
      "The 2707 th iteration gives loss of 0.5533891472410635\n",
      "The 2708 th iteration gives loss of 0.5532865445317139\n",
      "The 2709 th iteration gives loss of 0.5531840276671439\n",
      "The 2710 th iteration gives loss of 0.5530815965284457\n",
      "The 2711 th iteration gives loss of 0.5529792509969136\n",
      "The 2712 th iteration gives loss of 0.5528769909540086\n",
      "The 2713 th iteration gives loss of 0.5527748162814021\n",
      "The 2714 th iteration gives loss of 0.5526727268609033\n",
      "The 2715 th iteration gives loss of 0.5525707225745479\n",
      "The 2716 th iteration gives loss of 0.5524688033045057\n",
      "The 2717 th iteration gives loss of 0.552366968933175\n",
      "The 2718 th iteration gives loss of 0.5522652193431045\n",
      "The 2719 th iteration gives loss of 0.5521635544170315\n",
      "The 2720 th iteration gives loss of 0.5520619740378474\n",
      "The 2721 th iteration gives loss of 0.5519604780886838\n",
      "The 2722 th iteration gives loss of 0.5518590664527863\n",
      "The 2723 th iteration gives loss of 0.5517577390136322\n",
      "The 2724 th iteration gives loss of 0.5516564956548365\n",
      "The 2725 th iteration gives loss of 0.5515553362602191\n",
      "The 2726 th iteration gives loss of 0.5514542607137539\n",
      "The 2727 th iteration gives loss of 0.5513532688996186\n",
      "The 2728 th iteration gives loss of 0.5512523607021279\n",
      "The 2729 th iteration gives loss of 0.5511515360058417\n",
      "The 2730 th iteration gives loss of 0.5510507946954308\n",
      "The 2731 th iteration gives loss of 0.5509501366557721\n",
      "The 2732 th iteration gives loss of 0.5508495617719161\n",
      "The 2733 th iteration gives loss of 0.550749069929084\n",
      "The 2734 th iteration gives loss of 0.5506486610126836\n",
      "The 2735 th iteration gives loss of 0.5505483349082726\n",
      "The 2736 th iteration gives loss of 0.5504480915016139\n",
      "The 2737 th iteration gives loss of 0.5503479306786221\n",
      "The 2738 th iteration gives loss of 0.5502478523254034\n",
      "The 2739 th iteration gives loss of 0.5501478563282315\n",
      "The 2740 th iteration gives loss of 0.5500479425735411\n",
      "The 2741 th iteration gives loss of 0.549948110947971\n",
      "The 2742 th iteration gives loss of 0.5498483613382946\n",
      "The 2743 th iteration gives loss of 0.549748693631483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2744 th iteration gives loss of 0.5496491077146708\n",
      "The 2745 th iteration gives loss of 0.5495496034751799\n",
      "The 2746 th iteration gives loss of 0.5494501808004719\n",
      "The 2747 th iteration gives loss of 0.549350839578222\n",
      "The 2748 th iteration gives loss of 0.5492515796962535\n",
      "The 2749 th iteration gives loss of 0.5491524010425409\n",
      "The 2750 th iteration gives loss of 0.5490533035052818\n",
      "The 2751 th iteration gives loss of 0.5489542869727845\n",
      "The 2752 th iteration gives loss of 0.548855351333587\n",
      "The 2753 th iteration gives loss of 0.5487564964763326\n",
      "The 2754 th iteration gives loss of 0.5486577222898836\n",
      "The 2755 th iteration gives loss of 0.5485590286632651\n",
      "The 2756 th iteration gives loss of 0.5484604154856522\n",
      "The 2757 th iteration gives loss of 0.54836188264639\n",
      "The 2758 th iteration gives loss of 0.5482634300350245\n",
      "The 2759 th iteration gives loss of 0.548165057541238\n",
      "The 2760 th iteration gives loss of 0.5480667650548754\n",
      "The 2761 th iteration gives loss of 0.5479685524659873\n",
      "The 2762 th iteration gives loss of 0.5478704196647509\n",
      "The 2763 th iteration gives loss of 0.5477723665415436\n",
      "The 2764 th iteration gives loss of 0.5476743929868594\n",
      "The 2765 th iteration gives loss of 0.5475764988914368\n",
      "The 2766 th iteration gives loss of 0.5474786841461049\n",
      "The 2767 th iteration gives loss of 0.5473809486418947\n",
      "The 2768 th iteration gives loss of 0.5472832922700152\n",
      "The 2769 th iteration gives loss of 0.5471857149218021\n",
      "The 2770 th iteration gives loss of 0.5470882164887911\n",
      "The 2771 th iteration gives loss of 0.5469907968626674\n",
      "The 2772 th iteration gives loss of 0.5468934559352896\n",
      "The 2773 th iteration gives loss of 0.5467961935986599\n",
      "The 2774 th iteration gives loss of 0.5466990097449739\n",
      "The 2775 th iteration gives loss of 0.5466019042665597\n",
      "The 2776 th iteration gives loss of 0.5465048770559466\n",
      "The 2777 th iteration gives loss of 0.5464079280057882\n",
      "The 2778 th iteration gives loss of 0.5463110570089273\n",
      "The 2779 th iteration gives loss of 0.5462142639583677\n",
      "The 2780 th iteration gives loss of 0.5461175487472534\n",
      "The 2781 th iteration gives loss of 0.5460209112689076\n",
      "The 2782 th iteration gives loss of 0.5459243514168206\n",
      "The 2783 th iteration gives loss of 0.5458278690846489\n",
      "The 2784 th iteration gives loss of 0.5457314641661655\n",
      "The 2785 th iteration gives loss of 0.5456351365553622\n",
      "The 2786 th iteration gives loss of 0.5455388861463684\n",
      "The 2787 th iteration gives loss of 0.5454427128334641\n",
      "The 2788 th iteration gives loss of 0.5453466165110964\n",
      "The 2789 th iteration gives loss of 0.5452505970738725\n",
      "The 2790 th iteration gives loss of 0.5451546544165572\n",
      "The 2791 th iteration gives loss of 0.545058788434107\n",
      "The 2792 th iteration gives loss of 0.5449629990215843\n",
      "The 2793 th iteration gives loss of 0.5448672860742456\n",
      "The 2794 th iteration gives loss of 0.54477164948748\n",
      "The 2795 th iteration gives loss of 0.5446760891568633\n",
      "The 2796 th iteration gives loss of 0.544580604978111\n",
      "The 2797 th iteration gives loss of 0.5444851968471175\n",
      "The 2798 th iteration gives loss of 0.5443898646599044\n",
      "The 2799 th iteration gives loss of 0.5442946083126787\n",
      "The 2800 th iteration gives loss of 0.5441994277017794\n",
      "The 2801 th iteration gives loss of 0.5441043227237068\n",
      "The 2802 th iteration gives loss of 0.5440092932751475\n",
      "The 2803 th iteration gives loss of 0.5439143392529215\n",
      "The 2804 th iteration gives loss of 0.5438194605539922\n",
      "The 2805 th iteration gives loss of 0.5437246570754959\n",
      "The 2806 th iteration gives loss of 0.5436299287147282\n",
      "The 2807 th iteration gives loss of 0.5435352753691202\n",
      "The 2808 th iteration gives loss of 0.5434406969362897\n",
      "The 2809 th iteration gives loss of 0.5433461933139699\n",
      "The 2810 th iteration gives loss of 0.5432517644000755\n",
      "The 2811 th iteration gives loss of 0.5431574100926774\n",
      "The 2812 th iteration gives loss of 0.5430631302900033\n",
      "The 2813 th iteration gives loss of 0.5429689248903891\n",
      "The 2814 th iteration gives loss of 0.5428747937923892\n",
      "The 2815 th iteration gives loss of 0.5427807368946616\n",
      "The 2816 th iteration gives loss of 0.5426867540960415\n",
      "The 2817 th iteration gives loss of 0.5425928452955162\n",
      "The 2818 th iteration gives loss of 0.5424990103922173\n",
      "The 2819 th iteration gives loss of 0.5424052492854462\n",
      "The 2820 th iteration gives loss of 0.5423115618746222\n",
      "The 2821 th iteration gives loss of 0.5422179480593414\n",
      "The 2822 th iteration gives loss of 0.5421244077393625\n",
      "The 2823 th iteration gives loss of 0.542030940814561\n",
      "The 2824 th iteration gives loss of 0.5419375471849852\n",
      "The 2825 th iteration gives loss of 0.5418442267508337\n",
      "The 2826 th iteration gives loss of 0.541750979412463\n",
      "The 2827 th iteration gives loss of 0.5416578050703491\n",
      "The 2828 th iteration gives loss of 0.541564703625151\n",
      "The 2829 th iteration gives loss of 0.5414716749776763\n",
      "The 2830 th iteration gives loss of 0.5413787190288473\n",
      "The 2831 th iteration gives loss of 0.541285835679776\n",
      "The 2832 th iteration gives loss of 0.5411930248317055\n",
      "The 2833 th iteration gives loss of 0.5411002863860315\n",
      "The 2834 th iteration gives loss of 0.5410076202442916\n",
      "The 2835 th iteration gives loss of 0.5409150263081648\n",
      "The 2836 th iteration gives loss of 0.540822504479506\n",
      "The 2837 th iteration gives loss of 0.540730054660299\n",
      "The 2838 th iteration gives loss of 0.5406376767526653\n",
      "The 2839 th iteration gives loss of 0.5405453706588949\n",
      "The 2840 th iteration gives loss of 0.5404531362814197\n",
      "The 2841 th iteration gives loss of 0.5403609735228135\n",
      "The 2842 th iteration gives loss of 0.5402688822858028\n",
      "The 2843 th iteration gives loss of 0.5401768624732238\n",
      "The 2844 th iteration gives loss of 0.5400849139881185\n",
      "The 2845 th iteration gives loss of 0.5399930367336572\n",
      "The 2846 th iteration gives loss of 0.5399012306131127\n",
      "The 2847 th iteration gives loss of 0.539809495529962\n",
      "The 2848 th iteration gives loss of 0.539717831387794\n",
      "The 2849 th iteration gives loss of 0.5396262380903577\n",
      "The 2850 th iteration gives loss of 0.5395347155415307\n",
      "The 2851 th iteration gives loss of 0.5394432636453423\n",
      "The 2852 th iteration gives loss of 0.5393518823059666\n",
      "The 2853 th iteration gives loss of 0.5392605714277362\n",
      "The 2854 th iteration gives loss of 0.5391693309150996\n",
      "The 2855 th iteration gives loss of 0.5390781606726607\n",
      "The 2856 th iteration gives loss of 0.5389870606051739\n",
      "The 2857 th iteration gives loss of 0.5388960306175448\n",
      "The 2858 th iteration gives loss of 0.5388050706147954\n",
      "The 2859 th iteration gives loss of 0.5387141805021072\n",
      "The 2860 th iteration gives loss of 0.5386233601847842\n",
      "The 2861 th iteration gives loss of 0.538532609568311\n",
      "The 2862 th iteration gives loss of 0.538441928558293\n",
      "The 2863 th iteration gives loss of 0.5383513170604638\n",
      "The 2864 th iteration gives loss of 0.538260774980716\n",
      "The 2865 th iteration gives loss of 0.5381703022250812\n",
      "The 2866 th iteration gives loss of 0.538079898699733\n",
      "The 2867 th iteration gives loss of 0.5379895643109733\n",
      "The 2868 th iteration gives loss of 0.5378992989652568\n",
      "The 2869 th iteration gives loss of 0.5378091025691649\n",
      "The 2870 th iteration gives loss of 0.5377189750294512\n",
      "The 2871 th iteration gives loss of 0.537628916252976\n",
      "The 2872 th iteration gives loss of 0.5375389261467542\n",
      "The 2873 th iteration gives loss of 0.5374490046179271\n",
      "The 2874 th iteration gives loss of 0.5373591515738025\n",
      "The 2875 th iteration gives loss of 0.5372693669217851\n",
      "The 2876 th iteration gives loss of 0.5371796505694674\n",
      "The 2877 th iteration gives loss of 0.5370900024245435\n",
      "The 2878 th iteration gives loss of 0.5370004223948559\n",
      "The 2879 th iteration gives loss of 0.5369109103883962\n",
      "The 2880 th iteration gives loss of 0.5368214663132674\n",
      "The 2881 th iteration gives loss of 0.5367320900777349\n",
      "The 2882 th iteration gives loss of 0.5366427815902123\n",
      "The 2883 th iteration gives loss of 0.5365535407592222\n",
      "The 2884 th iteration gives loss of 0.5364643674934291\n",
      "The 2885 th iteration gives loss of 0.536375261701637\n",
      "The 2886 th iteration gives loss of 0.5362862232928144\n",
      "The 2887 th iteration gives loss of 0.5361972521759969\n",
      "The 2888 th iteration gives loss of 0.5361083482604215\n",
      "The 2889 th iteration gives loss of 0.5360195114554449\n",
      "The 2890 th iteration gives loss of 0.535930741670566\n",
      "The 2891 th iteration gives loss of 0.5358420388153862\n",
      "The 2892 th iteration gives loss of 0.5357534027996761\n",
      "The 2893 th iteration gives loss of 0.5356648335333166\n",
      "The 2894 th iteration gives loss of 0.5355763309263337\n",
      "The 2895 th iteration gives loss of 0.5354878948889047\n",
      "The 2896 th iteration gives loss of 0.5353995253313146\n",
      "The 2897 th iteration gives loss of 0.5353112221640017\n",
      "The 2898 th iteration gives loss of 0.5352229852975253\n",
      "The 2899 th iteration gives loss of 0.5351348146425837\n",
      "The 2900 th iteration gives loss of 0.5350467101100067\n",
      "The 2901 th iteration gives loss of 0.5349586716107526\n",
      "The 2902 th iteration gives loss of 0.5348706990559491\n",
      "The 2903 th iteration gives loss of 0.5347827923568077\n",
      "The 2904 th iteration gives loss of 0.534694951424689\n",
      "The 2905 th iteration gives loss of 0.5346071761710914\n",
      "The 2906 th iteration gives loss of 0.5345194665076409\n",
      "The 2907 th iteration gives loss of 0.5344318223461058\n",
      "The 2908 th iteration gives loss of 0.5343442435983659\n",
      "The 2909 th iteration gives loss of 0.5342567301764531\n",
      "The 2910 th iteration gives loss of 0.5341692819925284\n",
      "The 2911 th iteration gives loss of 0.5340818989588657\n",
      "The 2912 th iteration gives loss of 0.5339945809878941\n",
      "The 2913 th iteration gives loss of 0.5339073279921468\n",
      "The 2914 th iteration gives loss of 0.533820139884303\n",
      "The 2915 th iteration gives loss of 0.5337330165771869\n",
      "The 2916 th iteration gives loss of 0.5336459579837349\n",
      "The 2917 th iteration gives loss of 0.5335589640170014\n",
      "The 2918 th iteration gives loss of 0.5334720345902068\n",
      "The 2919 th iteration gives loss of 0.53338516961665\n",
      "The 2920 th iteration gives loss of 0.533298369009807\n",
      "The 2921 th iteration gives loss of 0.5332116326832722\n",
      "The 2922 th iteration gives loss of 0.5331249605507412\n",
      "The 2923 th iteration gives loss of 0.5330383525260758\n",
      "The 2924 th iteration gives loss of 0.5329518085232315\n",
      "The 2925 th iteration gives loss of 0.532865328456325\n",
      "The 2926 th iteration gives loss of 0.532778912239577\n",
      "The 2927 th iteration gives loss of 0.5326925597873384\n",
      "The 2928 th iteration gives loss of 0.5326062710141086\n",
      "The 2929 th iteration gives loss of 0.5325200458344787\n",
      "The 2930 th iteration gives loss of 0.5324338841632031\n",
      "The 2931 th iteration gives loss of 0.532347785915138\n",
      "The 2932 th iteration gives loss of 0.5322617510052917\n",
      "The 2933 th iteration gives loss of 0.5321757793487732\n",
      "The 2934 th iteration gives loss of 0.5320898708608435\n",
      "The 2935 th iteration gives loss of 0.5320040254568482\n",
      "The 2936 th iteration gives loss of 0.5319182430522923\n",
      "The 2937 th iteration gives loss of 0.5318325235628081\n",
      "The 2938 th iteration gives loss of 0.5317468669041411\n",
      "The 2939 th iteration gives loss of 0.5316612729921631\n",
      "The 2940 th iteration gives loss of 0.5315757417428862\n",
      "The 2941 th iteration gives loss of 0.5314902730724312\n",
      "The 2942 th iteration gives loss of 0.5314048668970457\n",
      "The 2943 th iteration gives loss of 0.5313195231331086\n",
      "The 2944 th iteration gives loss of 0.5312342416971162\n",
      "The 2945 th iteration gives loss of 0.5311490225056895\n",
      "The 2946 th iteration gives loss of 0.5310638654755677\n",
      "The 2947 th iteration gives loss of 0.5309787705236472\n",
      "The 2948 th iteration gives loss of 0.5308937375669025\n",
      "The 2949 th iteration gives loss of 0.5308087665224496\n",
      "The 2950 th iteration gives loss of 0.5307238573075487\n",
      "The 2951 th iteration gives loss of 0.530639009839558\n",
      "The 2952 th iteration gives loss of 0.5305542240359524\n",
      "The 2953 th iteration gives loss of 0.5304694998143581\n",
      "The 2954 th iteration gives loss of 0.5303848370924986\n",
      "The 2955 th iteration gives loss of 0.5303002357882451\n",
      "The 2956 th iteration gives loss of 0.5302156958195572\n",
      "The 2957 th iteration gives loss of 0.5301312171045373\n",
      "The 2958 th iteration gives loss of 0.5300467995614114\n",
      "The 2959 th iteration gives loss of 0.5299624431085141\n",
      "The 2960 th iteration gives loss of 0.5298781476643206\n",
      "The 2961 th iteration gives loss of 0.5297939131474148\n",
      "The 2962 th iteration gives loss of 0.5297097394765063\n",
      "The 2963 th iteration gives loss of 0.5296256265703779\n",
      "The 2964 th iteration gives loss of 0.5295415743480498\n",
      "The 2965 th iteration gives loss of 0.529457582728537\n",
      "The 2966 th iteration gives loss of 0.529373651631046\n",
      "The 2967 th iteration gives loss of 0.5292897809748682\n",
      "The 2968 th iteration gives loss of 0.5292059706794711\n",
      "The 2969 th iteration gives loss of 0.5291222206643611\n",
      "The 2970 th iteration gives loss of 0.5290385308492142\n",
      "The 2971 th iteration gives loss of 0.5289549011538137\n",
      "The 2972 th iteration gives loss of 0.5288713314980995\n",
      "The 2973 th iteration gives loss of 0.5287878218020579\n",
      "The 2974 th iteration gives loss of 0.5287043719858378\n",
      "The 2975 th iteration gives loss of 0.5286209819697074\n",
      "The 2976 th iteration gives loss of 0.5285376516740342\n",
      "The 2977 th iteration gives loss of 0.528454381019319\n",
      "The 2978 th iteration gives loss of 0.5283711699262016\n",
      "The 2979 th iteration gives loss of 0.5282880183153947\n",
      "The 2980 th iteration gives loss of 0.5282049261077421\n",
      "The 2981 th iteration gives loss of 0.5281218932242241\n",
      "The 2982 th iteration gives loss of 0.5280389195859286\n",
      "The 2983 th iteration gives loss of 0.5279560051140462\n",
      "The 2984 th iteration gives loss of 0.5278731497298956\n",
      "The 2985 th iteration gives loss of 0.5277903533549156\n",
      "The 2986 th iteration gives loss of 0.5277076159106731\n",
      "The 2987 th iteration gives loss of 0.5276249373188128\n",
      "The 2988 th iteration gives loss of 0.5275423175011346\n",
      "The 2989 th iteration gives loss of 0.5274597563795361\n",
      "The 2990 th iteration gives loss of 0.5273772538760411\n",
      "The 2991 th iteration gives loss of 0.527294809912765\n",
      "The 2992 th iteration gives loss of 0.5272124244119685\n",
      "The 2993 th iteration gives loss of 0.5271300972959998\n",
      "The 2994 th iteration gives loss of 0.5270478284873455\n",
      "The 2995 th iteration gives loss of 0.5269656179086084\n",
      "The 2996 th iteration gives loss of 0.5268834654824747\n",
      "The 2997 th iteration gives loss of 0.526801371131783\n",
      "The 2998 th iteration gives loss of 0.5267193347794513\n",
      "The 2999 th iteration gives loss of 0.5266373563485403\n",
      "The 3000 th iteration gives loss of 0.5265554357622214\n",
      "The 3001 th iteration gives loss of 0.5264735729437496\n",
      "The 3002 th iteration gives loss of 0.5263917678165423\n",
      "The 3003 th iteration gives loss of 0.5263100203040888\n",
      "The 3004 th iteration gives loss of 0.5262283303299993\n",
      "The 3005 th iteration gives loss of 0.5261466978180185\n",
      "The 3006 th iteration gives loss of 0.5260651226919829\n",
      "The 3007 th iteration gives loss of 0.5259836048758408\n",
      "The 3008 th iteration gives loss of 0.5259021442936731\n",
      "The 3009 th iteration gives loss of 0.5258207408696726\n",
      "The 3010 th iteration gives loss of 0.5257393945281122\n",
      "The 3011 th iteration gives loss of 0.5256581051933908\n",
      "The 3012 th iteration gives loss of 0.5255768727900464\n",
      "The 3013 th iteration gives loss of 0.5254956972426972\n",
      "The 3014 th iteration gives loss of 0.5254145784760823\n",
      "The 3015 th iteration gives loss of 0.525333516415053\n",
      "The 3016 th iteration gives loss of 0.5252525109845825\n",
      "The 3017 th iteration gives loss of 0.5251715621097198\n",
      "The 3018 th iteration gives loss of 0.5250906697156711\n",
      "The 3019 th iteration gives loss of 0.5250098337277334\n",
      "The 3020 th iteration gives loss of 0.5249290540712825\n",
      "The 3021 th iteration gives loss of 0.5248483306718708\n",
      "The 3022 th iteration gives loss of 0.5247676634550918\n",
      "The 3023 th iteration gives loss of 0.5246870523467044\n",
      "The 3024 th iteration gives loss of 0.5246064972725395\n",
      "The 3025 th iteration gives loss of 0.5245259981585593\n",
      "The 3026 th iteration gives loss of 0.5244455549308219\n",
      "The 3027 th iteration gives loss of 0.5243651675155067\n",
      "The 3028 th iteration gives loss of 0.5242848358388813\n",
      "The 3029 th iteration gives loss of 0.5242045598273671\n",
      "The 3030 th iteration gives loss of 0.5241243394074352\n",
      "The 3031 th iteration gives loss of 0.5240441745056983\n",
      "The 3032 th iteration gives loss of 0.5239640650488894\n",
      "The 3033 th iteration gives loss of 0.5238840109638229\n",
      "The 3034 th iteration gives loss of 0.5238040121774243\n",
      "The 3035 th iteration gives loss of 0.5237240686167379\n",
      "The 3036 th iteration gives loss of 0.5236441802089181\n",
      "The 3037 th iteration gives loss of 0.5235643468812294\n",
      "The 3038 th iteration gives loss of 0.5234845685610167\n",
      "The 3039 th iteration gives loss of 0.5234048451757525\n",
      "The 3040 th iteration gives loss of 0.5233251766530347\n",
      "The 3041 th iteration gives loss of 0.523245562920526\n",
      "The 3042 th iteration gives loss of 0.5231660039060118\n",
      "The 3043 th iteration gives loss of 0.5230864995374216\n",
      "The 3044 th iteration gives loss of 0.5230070497427276\n",
      "The 3045 th iteration gives loss of 0.5229276544500558\n",
      "The 3046 th iteration gives loss of 0.5228483135876218\n",
      "The 3047 th iteration gives loss of 0.5227690270837501\n",
      "The 3048 th iteration gives loss of 0.5226897948668546\n",
      "The 3049 th iteration gives loss of 0.5226106168654806\n",
      "The 3050 th iteration gives loss of 0.5225314930082652\n",
      "The 3051 th iteration gives loss of 0.5224524232239596\n",
      "The 3052 th iteration gives loss of 0.5223734074414074\n",
      "The 3053 th iteration gives loss of 0.5222944455895474\n",
      "The 3054 th iteration gives loss of 0.5222155375974541\n",
      "The 3055 th iteration gives loss of 0.5221366833942903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 3056 th iteration gives loss of 0.5220578829093167\n",
      "The 3057 th iteration gives loss of 0.5219791360719099\n",
      "The 3058 th iteration gives loss of 0.5219004428115611\n",
      "The 3059 th iteration gives loss of 0.5218218030578144\n",
      "The 3060 th iteration gives loss of 0.5217432167403789\n",
      "The 3061 th iteration gives loss of 0.5216646837890381\n",
      "The 3062 th iteration gives loss of 0.5215862041336898\n",
      "The 3063 th iteration gives loss of 0.5215077777042931\n",
      "The 3064 th iteration gives loss of 0.5214294044309882\n",
      "The 3065 th iteration gives loss of 0.5213510842439604\n",
      "The 3066 th iteration gives loss of 0.5212728170735097\n",
      "The 3067 th iteration gives loss of 0.5211946028500464\n",
      "The 3068 th iteration gives loss of 0.521116441504056\n",
      "The 3069 th iteration gives loss of 0.5210383329661685\n",
      "The 3070 th iteration gives loss of 0.5209602771671072\n",
      "The 3071 th iteration gives loss of 0.5208822740376666\n",
      "The 3072 th iteration gives loss of 0.520804323508771\n",
      "The 3073 th iteration gives loss of 0.5207264255114404\n",
      "The 3074 th iteration gives loss of 0.5206485799767895\n",
      "The 3075 th iteration gives loss of 0.5205707868360357\n",
      "The 3076 th iteration gives loss of 0.5204930460205185\n",
      "The 3077 th iteration gives loss of 0.5204153574616316\n",
      "The 3078 th iteration gives loss of 0.5203377210909199\n",
      "The 3079 th iteration gives loss of 0.5202601368400213\n",
      "The 3080 th iteration gives loss of 0.5201826046406276\n",
      "The 3081 th iteration gives loss of 0.5201051244245846\n",
      "The 3082 th iteration gives loss of 0.5200276961238093\n",
      "The 3083 th iteration gives loss of 0.5199503196703321\n",
      "The 3084 th iteration gives loss of 0.5198729949962934\n",
      "The 3085 th iteration gives loss of 0.5197957220339029\n",
      "The 3086 th iteration gives loss of 0.5197185007154778\n",
      "The 3087 th iteration gives loss of 0.5196413309734641\n",
      "The 3088 th iteration gives loss of 0.5195642127403718\n",
      "The 3089 th iteration gives loss of 0.5194871459488474\n",
      "The 3090 th iteration gives loss of 0.5194101305315763\n",
      "The 3091 th iteration gives loss of 0.5193331664213992\n",
      "The 3092 th iteration gives loss of 0.5192562535512416\n",
      "The 3093 th iteration gives loss of 0.519179391854123\n",
      "The 3094 th iteration gives loss of 0.5191025812631549\n",
      "The 3095 th iteration gives loss of 0.5190258217115566\n",
      "The 3096 th iteration gives loss of 0.5189491131326373\n",
      "The 3097 th iteration gives loss of 0.5188724554598144\n",
      "The 3098 th iteration gives loss of 0.5187958486265829\n",
      "The 3099 th iteration gives loss of 0.5187192925665562\n",
      "The 3100 th iteration gives loss of 0.5186427872134695\n",
      "The 3101 th iteration gives loss of 0.518566332501081\n",
      "The 3102 th iteration gives loss of 0.5184899283633214\n",
      "The 3103 th iteration gives loss of 0.5184135747341679\n",
      "The 3104 th iteration gives loss of 0.5183372715477268\n",
      "The 3105 th iteration gives loss of 0.5182610187381711\n",
      "The 3106 th iteration gives loss of 0.5181848162398033\n",
      "The 3107 th iteration gives loss of 0.5181086639870129\n",
      "The 3108 th iteration gives loss of 0.5180325619142656\n",
      "The 3109 th iteration gives loss of 0.5179565099561565\n",
      "The 3110 th iteration gives loss of 0.5178805080473176\n",
      "The 3111 th iteration gives loss of 0.517804556122564\n",
      "The 3112 th iteration gives loss of 0.5177286541167267\n",
      "The 3113 th iteration gives loss of 0.5176528019647835\n",
      "The 3114 th iteration gives loss of 0.5175769996017833\n",
      "The 3115 th iteration gives loss of 0.5175012469628785\n",
      "The 3116 th iteration gives loss of 0.5174255439833042\n",
      "The 3117 th iteration gives loss of 0.517349890598403\n",
      "The 3118 th iteration gives loss of 0.5172742867436172\n",
      "The 3119 th iteration gives loss of 0.5171987323544852\n",
      "The 3120 th iteration gives loss of 0.5171232273666075\n",
      "The 3121 th iteration gives loss of 0.5170477717157163\n",
      "The 3122 th iteration gives loss of 0.5169723653376337\n",
      "The 3123 th iteration gives loss of 0.5168970081682469\n",
      "The 3124 th iteration gives loss of 0.5168217001435781\n",
      "The 3125 th iteration gives loss of 0.5167464411997046\n",
      "The 3126 th iteration gives loss of 0.516671231272831\n",
      "The 3127 th iteration gives loss of 0.5165960702992277\n",
      "The 3128 th iteration gives loss of 0.5165209582152815\n",
      "The 3129 th iteration gives loss of 0.5164458949574534\n",
      "The 3130 th iteration gives loss of 0.5163708804623035\n",
      "The 3131 th iteration gives loss of 0.5162959146664929\n",
      "The 3132 th iteration gives loss of 0.5162209975067721\n",
      "The 3133 th iteration gives loss of 0.5161461289199749\n",
      "The 3134 th iteration gives loss of 0.5160713088430424\n",
      "The 3135 th iteration gives loss of 0.5159965372129867\n",
      "The 3136 th iteration gives loss of 0.5159218139669349\n",
      "The 3137 th iteration gives loss of 0.5158471390421033\n",
      "The 3138 th iteration gives loss of 0.5157725123758027\n",
      "The 3139 th iteration gives loss of 0.5156979339053936\n",
      "The 3140 th iteration gives loss of 0.5156234035683984\n",
      "The 3141 th iteration gives loss of 0.5155489213023678\n",
      "The 3142 th iteration gives loss of 0.5154744870449894\n",
      "The 3143 th iteration gives loss of 0.5154001007340057\n",
      "The 3144 th iteration gives loss of 0.5153257623072884\n",
      "The 3145 th iteration gives loss of 0.515251471702771\n",
      "The 3146 th iteration gives loss of 0.5151772288584818\n",
      "The 3147 th iteration gives loss of 0.5151030337125405\n",
      "The 3148 th iteration gives loss of 0.5150288862031632\n",
      "The 3149 th iteration gives loss of 0.514954786268683\n",
      "The 3150 th iteration gives loss of 0.5148807338474666\n",
      "The 3151 th iteration gives loss of 0.5148067288780085\n",
      "The 3152 th iteration gives loss of 0.514732771298892\n",
      "The 3153 th iteration gives loss of 0.5146588610487742\n",
      "The 3154 th iteration gives loss of 0.5145849980663966\n",
      "The 3155 th iteration gives loss of 0.5145111822906304\n",
      "The 3156 th iteration gives loss of 0.5144374136604037\n",
      "The 3157 th iteration gives loss of 0.514363692114727\n",
      "The 3158 th iteration gives loss of 0.5142900175927329\n",
      "The 3159 th iteration gives loss of 0.5142163900336001\n",
      "The 3160 th iteration gives loss of 0.5141428093766384\n",
      "The 3161 th iteration gives loss of 0.51406927556122\n",
      "The 3162 th iteration gives loss of 0.5139957885268189\n",
      "The 3163 th iteration gives loss of 0.5139223482129824\n",
      "The 3164 th iteration gives loss of 0.5138489545593712\n",
      "The 3165 th iteration gives loss of 0.5137756075057152\n",
      "The 3166 th iteration gives loss of 0.5137023069918301\n",
      "The 3167 th iteration gives loss of 0.5136290529576225\n",
      "The 3168 th iteration gives loss of 0.5135558453431132\n",
      "The 3169 th iteration gives loss of 0.5134826840883653\n",
      "The 3170 th iteration gives loss of 0.513409569133566\n",
      "The 3171 th iteration gives loss of 0.5133365004189604\n",
      "The 3172 th iteration gives loss of 0.5132634778849118\n",
      "The 3173 th iteration gives loss of 0.5131905014718596\n",
      "The 3174 th iteration gives loss of 0.5131175711203025\n",
      "The 3175 th iteration gives loss of 0.5130446867708727\n",
      "The 3176 th iteration gives loss of 0.5129718483642651\n",
      "The 3177 th iteration gives loss of 0.5128990558412502\n",
      "The 3178 th iteration gives loss of 0.5128263091427081\n",
      "The 3179 th iteration gives loss of 0.5127536082095894\n",
      "The 3180 th iteration gives loss of 0.5126809529829359\n",
      "The 3181 th iteration gives loss of 0.5126083434038773\n",
      "The 3182 th iteration gives loss of 0.5125357794136397\n",
      "The 3183 th iteration gives loss of 0.5124632609535056\n",
      "The 3184 th iteration gives loss of 0.5123907879648834\n",
      "The 3185 th iteration gives loss of 0.512318360389217\n",
      "The 3186 th iteration gives loss of 0.5122459781680772\n",
      "The 3187 th iteration gives loss of 0.5121736412431169\n",
      "The 3188 th iteration gives loss of 0.512101349556049\n",
      "The 3189 th iteration gives loss of 0.5120291030486931\n",
      "The 3190 th iteration gives loss of 0.511956901662943\n",
      "The 3191 th iteration gives loss of 0.5118847453407972\n",
      "The 3192 th iteration gives loss of 0.5118126340243028\n",
      "The 3193 th iteration gives loss of 0.511740567655623\n",
      "The 3194 th iteration gives loss of 0.511668546176995\n",
      "The 3195 th iteration gives loss of 0.5115965695307493\n",
      "The 3196 th iteration gives loss of 0.5115246376592726\n",
      "The 3197 th iteration gives loss of 0.5114527505050451\n",
      "The 3198 th iteration gives loss of 0.5113809080106708\n",
      "The 3199 th iteration gives loss of 0.5113091101187908\n",
      "The 3200 th iteration gives loss of 0.5112373567721484\n",
      "The 3201 th iteration gives loss of 0.5111656479135593\n",
      "The 3202 th iteration gives loss of 0.5110939834859445\n",
      "The 3203 th iteration gives loss of 0.5110223634322947\n",
      "The 3204 th iteration gives loss of 0.5109507876956715\n",
      "The 3205 th iteration gives loss of 0.510879256219258\n",
      "The 3206 th iteration gives loss of 0.5108077689462607\n",
      "The 3207 th iteration gives loss of 0.5107363258200301\n",
      "The 3208 th iteration gives loss of 0.5106649267839644\n",
      "The 3209 th iteration gives loss of 0.5105935717815403\n",
      "The 3210 th iteration gives loss of 0.5105222607563462\n",
      "The 3211 th iteration gives loss of 0.5104509936520324\n",
      "The 3212 th iteration gives loss of 0.5103797704123115\n",
      "The 3213 th iteration gives loss of 0.5103085909810334\n",
      "The 3214 th iteration gives loss of 0.5102374553020933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 3215 th iteration gives loss of 0.5101663633194501\n",
      "The 3216 th iteration gives loss of 0.510095314977182\n",
      "The 3217 th iteration gives loss of 0.5100243102194421\n",
      "The 3218 th iteration gives loss of 0.5099533489904383\n",
      "The 3219 th iteration gives loss of 0.5098824312344866\n",
      "The 3220 th iteration gives loss of 0.5098115568959625\n",
      "The 3221 th iteration gives loss of 0.509740725919356\n",
      "The 3222 th iteration gives loss of 0.5096699382492217\n",
      "The 3223 th iteration gives loss of 0.5095991938301754\n",
      "The 3224 th iteration gives loss of 0.5095284926069196\n",
      "The 3225 th iteration gives loss of 0.5094578345242773\n",
      "The 3226 th iteration gives loss of 0.509387219527099\n",
      "The 3227 th iteration gives loss of 0.5093166475603483\n",
      "The 3228 th iteration gives loss of 0.5092461185690542\n",
      "The 3229 th iteration gives loss of 0.5091756324983329\n",
      "The 3230 th iteration gives loss of 0.5091051892933697\n",
      "The 3231 th iteration gives loss of 0.5090347888994445\n",
      "The 3232 th iteration gives loss of 0.5089644312619199\n",
      "The 3233 th iteration gives loss of 0.5088941163262264\n",
      "The 3234 th iteration gives loss of 0.5088238440378656\n",
      "The 3235 th iteration gives loss of 0.5087536143424359\n",
      "The 3236 th iteration gives loss of 0.508683427185615\n",
      "The 3237 th iteration gives loss of 0.5086132825131405\n",
      "The 3238 th iteration gives loss of 0.5085431802708597\n",
      "The 3239 th iteration gives loss of 0.5084731204046693\n",
      "The 3240 th iteration gives loss of 0.5084031028605391\n",
      "The 3241 th iteration gives loss of 0.5083331275845665\n",
      "The 3242 th iteration gives loss of 0.5082631945228742\n",
      "The 3243 th iteration gives loss of 0.5081933036216881\n",
      "The 3244 th iteration gives loss of 0.5081234548273285\n",
      "The 3245 th iteration gives loss of 0.5080536480861474\n",
      "The 3246 th iteration gives loss of 0.5079838833446202\n",
      "The 3247 th iteration gives loss of 0.5079141605492736\n",
      "The 3248 th iteration gives loss of 0.5078444796467225\n",
      "The 3249 th iteration gives loss of 0.5077748405836727\n",
      "The 3250 th iteration gives loss of 0.5077052433068725\n",
      "The 3251 th iteration gives loss of 0.5076356877631575\n",
      "The 3252 th iteration gives loss of 0.5075661738994818\n",
      "The 3253 th iteration gives loss of 0.5074967016628308\n",
      "The 3254 th iteration gives loss of 0.507427271000283\n",
      "The 3255 th iteration gives loss of 0.507357881858992\n",
      "The 3256 th iteration gives loss of 0.5072885341861911\n",
      "The 3257 th iteration gives loss of 0.5072192279291861\n",
      "The 3258 th iteration gives loss of 0.5071499630353767\n",
      "The 3259 th iteration gives loss of 0.5070807394522061\n",
      "The 3260 th iteration gives loss of 0.5070115571272159\n",
      "The 3261 th iteration gives loss of 0.5069424160080239\n",
      "The 3262 th iteration gives loss of 0.5068733160423069\n",
      "The 3263 th iteration gives loss of 0.5068042571778466\n",
      "The 3264 th iteration gives loss of 0.5067352393624847\n",
      "The 3265 th iteration gives loss of 0.5066662625441404\n",
      "The 3266 th iteration gives loss of 0.5065973266708077\n",
      "The 3267 th iteration gives loss of 0.5065284316905466\n",
      "The 3268 th iteration gives loss of 0.5064595775515277\n",
      "The 3269 th iteration gives loss of 0.5063907642019362\n",
      "The 3270 th iteration gives loss of 0.5063219915900902\n",
      "The 3271 th iteration gives loss of 0.5062532596643629\n",
      "The 3272 th iteration gives loss of 0.5061845683731879\n",
      "The 3273 th iteration gives loss of 0.5061159176650897\n",
      "The 3274 th iteration gives loss of 0.5060473074886633\n",
      "The 3275 th iteration gives loss of 0.5059787377925936\n",
      "The 3276 th iteration gives loss of 0.5059102085256094\n",
      "The 3277 th iteration gives loss of 0.5058417196365267\n",
      "The 3278 th iteration gives loss of 0.5057732710742706\n",
      "The 3279 th iteration gives loss of 0.5057048627877848\n",
      "The 3280 th iteration gives loss of 0.5056364947261096\n",
      "The 3281 th iteration gives loss of 0.5055681668383707\n",
      "The 3282 th iteration gives loss of 0.5054998790737527\n",
      "The 3283 th iteration gives loss of 0.5054316313815348\n",
      "The 3284 th iteration gives loss of 0.5053634237110318\n",
      "The 3285 th iteration gives loss of 0.5052952560116774\n",
      "The 3286 th iteration gives loss of 0.5052271282329387\n",
      "The 3287 th iteration gives loss of 0.5051590403243876\n",
      "The 3288 th iteration gives loss of 0.5050909922356474\n",
      "The 3289 th iteration gives loss of 0.5050229839164345\n",
      "The 3290 th iteration gives loss of 0.504955015316506\n",
      "The 3291 th iteration gives loss of 0.5048870863857504\n",
      "The 3292 th iteration gives loss of 0.5048191970740646\n",
      "The 3293 th iteration gives loss of 0.5047513473314398\n",
      "The 3294 th iteration gives loss of 0.5046835371079549\n",
      "The 3295 th iteration gives loss of 0.5046157663537603\n",
      "The 3296 th iteration gives loss of 0.504548035019049\n",
      "The 3297 th iteration gives loss of 0.5044803430541297\n",
      "The 3298 th iteration gives loss of 0.5044126904093588\n",
      "The 3299 th iteration gives loss of 0.5043450770351522\n",
      "The 3300 th iteration gives loss of 0.5042775028820251\n",
      "The 3301 th iteration gives loss of 0.5042099679005417\n",
      "The 3302 th iteration gives loss of 0.5041424720413475\n",
      "The 3303 th iteration gives loss of 0.5040750152551726\n",
      "The 3304 th iteration gives loss of 0.5040075974928095\n",
      "The 3305 th iteration gives loss of 0.5039402187051117\n",
      "The 3306 th iteration gives loss of 0.5038728788429979\n",
      "The 3307 th iteration gives loss of 0.5038055778574938\n",
      "The 3308 th iteration gives loss of 0.5037383156996686\n",
      "The 3309 th iteration gives loss of 0.5036710923206543\n",
      "The 3310 th iteration gives loss of 0.5036039076716844\n",
      "The 3311 th iteration gives loss of 0.5035367617040372\n",
      "The 3312 th iteration gives loss of 0.5034696543690743\n",
      "The 3313 th iteration gives loss of 0.5034025856182174\n",
      "The 3314 th iteration gives loss of 0.5033355554029778\n",
      "The 3315 th iteration gives loss of 0.503268563674898\n",
      "The 3316 th iteration gives loss of 0.5032016103856587\n",
      "The 3317 th iteration gives loss of 0.5031346954869372\n",
      "The 3318 th iteration gives loss of 0.5030678189305353\n",
      "The 3319 th iteration gives loss of 0.5030009806682838\n",
      "The 3320 th iteration gives loss of 0.5029341806521124\n",
      "The 3321 th iteration gives loss of 0.5028674188340009\n",
      "The 3322 th iteration gives loss of 0.502800695166019\n",
      "The 3323 th iteration gives loss of 0.502734009600283\n",
      "The 3324 th iteration gives loss of 0.5026673620890079\n",
      "The 3325 th iteration gives loss of 0.5026007525844589\n",
      "The 3326 th iteration gives loss of 0.5025341810389653\n",
      "The 3327 th iteration gives loss of 0.5024676474049192\n",
      "The 3328 th iteration gives loss of 0.5024011516348262\n",
      "The 3329 th iteration gives loss of 0.5023346936812001\n",
      "The 3330 th iteration gives loss of 0.5022682734966724\n",
      "The 3331 th iteration gives loss of 0.5022018910339113\n",
      "The 3332 th iteration gives loss of 0.5021355462456749\n",
      "The 3333 th iteration gives loss of 0.5020692390847774\n",
      "The 3334 th iteration gives loss of 0.5020029695041065\n",
      "The 3335 th iteration gives loss of 0.5019367374566109\n",
      "The 3336 th iteration gives loss of 0.5018705428953265\n",
      "The 3337 th iteration gives loss of 0.5018043857733304\n",
      "The 3338 th iteration gives loss of 0.5017382660437889\n",
      "The 3339 th iteration gives loss of 0.501672183659923\n",
      "The 3340 th iteration gives loss of 0.5016061385750399\n",
      "The 3341 th iteration gives loss of 0.5015401307424883\n",
      "The 3342 th iteration gives loss of 0.5014741601156992\n",
      "The 3343 th iteration gives loss of 0.5014082266481746\n",
      "The 3344 th iteration gives loss of 0.5013423302934842\n",
      "The 3345 th iteration gives loss of 0.5012764710052477\n",
      "The 3346 th iteration gives loss of 0.5012106487371709\n",
      "The 3347 th iteration gives loss of 0.5011448634430178\n",
      "The 3348 th iteration gives loss of 0.5010791150766136\n",
      "The 3349 th iteration gives loss of 0.5010134035918834\n",
      "The 3350 th iteration gives loss of 0.5009477289427595\n",
      "The 3351 th iteration gives loss of 0.5008820910833116\n",
      "The 3352 th iteration gives loss of 0.5008164899676076\n",
      "The 3353 th iteration gives loss of 0.5007509255498269\n",
      "The 3354 th iteration gives loss of 0.5006853977842097\n",
      "The 3355 th iteration gives loss of 0.5006199066250502\n",
      "The 3356 th iteration gives loss of 0.5005544520266909\n",
      "The 3357 th iteration gives loss of 0.5004890339436033\n",
      "The 3358 th iteration gives loss of 0.5004236523302485\n",
      "The 3359 th iteration gives loss of 0.5003583071412187\n",
      "The 3360 th iteration gives loss of 0.5002929983311255\n",
      "The 3361 th iteration gives loss of 0.5002277258546565\n",
      "The 3362 th iteration gives loss of 0.5001624896665928\n",
      "The 3363 th iteration gives loss of 0.5000972897217435\n",
      "The 3364 th iteration gives loss of 0.5000321259750079\n",
      "The 3365 th iteration gives loss of 0.4999669983813288\n",
      "The 3366 th iteration gives loss of 0.4999019068957444\n",
      "The 3367 th iteration gives loss of 0.49983685147333945\n",
      "The 3368 th iteration gives loss of 0.4997718320692557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 3369 th iteration gives loss of 0.49970684863871573\n",
      "The 3370 th iteration gives loss of 0.4996419011369945\n",
      "The 3371 th iteration gives loss of 0.49957698951945184\n",
      "The 3372 th iteration gives loss of 0.4995121137414757\n",
      "The 3373 th iteration gives loss of 0.49944727375856424\n",
      "The 3374 th iteration gives loss of 0.4993824695262345\n",
      "The 3375 th iteration gives loss of 0.49931770100012074\n",
      "The 3376 th iteration gives loss of 0.49925296813584646\n",
      "The 3377 th iteration gives loss of 0.4991882708891813\n",
      "The 3378 th iteration gives loss of 0.499123609215914\n",
      "The 3379 th iteration gives loss of 0.4990589830718948\n",
      "The 3380 th iteration gives loss of 0.49899439241305027\n",
      "The 3381 th iteration gives loss of 0.49892983719536954\n",
      "The 3382 th iteration gives loss of 0.4988653173749076\n",
      "The 3383 th iteration gives loss of 0.4988008329077654\n",
      "The 3384 th iteration gives loss of 0.498736383750137\n",
      "The 3385 th iteration gives loss of 0.49867196985825774\n",
      "The 3386 th iteration gives loss of 0.49860759118844117\n",
      "The 3387 th iteration gives loss of 0.49854324769702707\n",
      "The 3388 th iteration gives loss of 0.49847893934047505\n",
      "The 3389 th iteration gives loss of 0.4984146660752735\n",
      "The 3390 th iteration gives loss of 0.498350427857959\n",
      "The 3391 th iteration gives loss of 0.4982862246451802\n",
      "The 3392 th iteration gives loss of 0.4982220563936086\n",
      "The 3393 th iteration gives loss of 0.49815792305998263\n",
      "The 3394 th iteration gives loss of 0.49809382460112184\n",
      "The 3395 th iteration gives loss of 0.4980297609738784\n",
      "The 3396 th iteration gives loss of 0.4979657321352001\n",
      "The 3397 th iteration gives loss of 0.4979017380420653\n",
      "The 3398 th iteration gives loss of 0.49783777865155293\n",
      "The 3399 th iteration gives loss of 0.49777385392075985\n",
      "The 3400 th iteration gives loss of 0.49770996380688487\n",
      "The 3401 th iteration gives loss of 0.49764610826715167\n",
      "The 3402 th iteration gives loss of 0.49758228725886905\n",
      "The 3403 th iteration gives loss of 0.4975185007394198\n",
      "The 3404 th iteration gives loss of 0.49745474866621814\n",
      "The 3405 th iteration gives loss of 0.4973910309967596\n",
      "The 3406 th iteration gives loss of 0.49732734768858067\n",
      "The 3407 th iteration gives loss of 0.49726369869930614\n",
      "The 3408 th iteration gives loss of 0.4972000839865992\n",
      "The 3409 th iteration gives loss of 0.4971365035082056\n",
      "The 3410 th iteration gives loss of 0.49707295722190614\n",
      "The 3411 th iteration gives loss of 0.49700944508556744\n",
      "The 3412 th iteration gives loss of 0.49694596705710836\n",
      "The 3413 th iteration gives loss of 0.496882523094494\n",
      "The 3414 th iteration gives loss of 0.49681911315576877\n",
      "The 3415 th iteration gives loss of 0.4967557371990399\n",
      "The 3416 th iteration gives loss of 0.49669239518246433\n",
      "The 3417 th iteration gives loss of 0.49662908706423897\n",
      "The 3418 th iteration gives loss of 0.4965658128026801\n",
      "The 3419 th iteration gives loss of 0.4965025723560899\n",
      "The 3420 th iteration gives loss of 0.4964393656828953\n",
      "The 3421 th iteration gives loss of 0.49637619274154354\n",
      "The 3422 th iteration gives loss of 0.49631305349057375\n",
      "The 3423 th iteration gives loss of 0.4962499478885527\n",
      "The 3424 th iteration gives loss of 0.496186875894124\n",
      "The 3425 th iteration gives loss of 0.4961238374659853\n",
      "The 3426 th iteration gives loss of 0.49606083256289335\n",
      "The 3427 th iteration gives loss of 0.495997861143669\n",
      "The 3428 th iteration gives loss of 0.49593492316718585\n",
      "The 3429 th iteration gives loss of 0.4958720185923881\n",
      "The 3430 th iteration gives loss of 0.4958091473782783\n",
      "The 3431 th iteration gives loss of 0.49574630948390896\n",
      "The 3432 th iteration gives loss of 0.49568350486836865\n",
      "The 3433 th iteration gives loss of 0.49562073349086616\n",
      "The 3434 th iteration gives loss of 0.4955579953106148\n",
      "The 3435 th iteration gives loss of 0.4954952902869268\n",
      "The 3436 th iteration gives loss of 0.4954326183791245\n",
      "The 3437 th iteration gives loss of 0.4953699795466511\n",
      "The 3438 th iteration gives loss of 0.4953073737489368\n",
      "The 3439 th iteration gives loss of 0.4952448009455279\n",
      "The 3440 th iteration gives loss of 0.4951822610960129\n",
      "The 3441 th iteration gives loss of 0.4951197541600283\n",
      "The 3442 th iteration gives loss of 0.49505728009727634\n",
      "The 3443 th iteration gives loss of 0.4949948388675112\n",
      "The 3444 th iteration gives loss of 0.4949324304305464\n",
      "The 3445 th iteration gives loss of 0.49487005474627904\n",
      "The 3446 th iteration gives loss of 0.4948077117746221\n",
      "The 3447 th iteration gives loss of 0.49474540147556945\n",
      "The 3448 th iteration gives loss of 0.4946831238091738\n",
      "The 3449 th iteration gives loss of 0.4946208787355286\n",
      "The 3450 th iteration gives loss of 0.49455866621482036\n",
      "The 3451 th iteration gives loss of 0.49449648620725106\n",
      "The 3452 th iteration gives loss of 0.4944343386730968\n",
      "The 3453 th iteration gives loss of 0.4943722235727089\n",
      "The 3454 th iteration gives loss of 0.4943101408664746\n",
      "The 3455 th iteration gives loss of 0.49424809051482854\n",
      "The 3456 th iteration gives loss of 0.4941860724782887\n",
      "The 3457 th iteration gives loss of 0.49412408671742114\n",
      "The 3458 th iteration gives loss of 0.49406213319285314\n",
      "The 3459 th iteration gives loss of 0.49400021186524884\n",
      "The 3460 th iteration gives loss of 0.4939383226953495\n",
      "The 3461 th iteration gives loss of 0.4938764656439443\n",
      "The 3462 th iteration gives loss of 0.4938146406718611\n",
      "The 3463 th iteration gives loss of 0.49375284774002925\n",
      "The 3464 th iteration gives loss of 0.4936910868093984\n",
      "The 3465 th iteration gives loss of 0.49362935784099676\n",
      "The 3466 th iteration gives loss of 0.4935676607958692\n",
      "The 3467 th iteration gives loss of 0.4935059956351567\n",
      "The 3468 th iteration gives loss of 0.49344436232006\n",
      "The 3469 th iteration gives loss of 0.4933827608117855\n",
      "The 3470 th iteration gives loss of 0.4933211910716603\n",
      "The 3471 th iteration gives loss of 0.4932596530610224\n",
      "The 3472 th iteration gives loss of 0.4931981467412843\n",
      "The 3473 th iteration gives loss of 0.49313667207390044\n",
      "The 3474 th iteration gives loss of 0.49307522902039014\n",
      "The 3475 th iteration gives loss of 0.49301381754233903\n",
      "The 3476 th iteration gives loss of 0.4929524376013575\n",
      "The 3477 th iteration gives loss of 0.4928910891591436\n",
      "The 3478 th iteration gives loss of 0.4928297721774245\n",
      "The 3479 th iteration gives loss of 0.4927684866180087\n",
      "The 3480 th iteration gives loss of 0.49270723244273873\n",
      "The 3481 th iteration gives loss of 0.4926460096135238\n",
      "The 3482 th iteration gives loss of 0.49258481809230675\n",
      "The 3483 th iteration gives loss of 0.4925236578411225\n",
      "The 3484 th iteration gives loss of 0.49246252882203473\n",
      "The 3485 th iteration gives loss of 0.4924014309971602\n",
      "The 3486 th iteration gives loss of 0.492340364328685\n",
      "The 3487 th iteration gives loss of 0.4922793287788336\n",
      "The 3488 th iteration gives loss of 0.4922183243098896\n",
      "The 3489 th iteration gives loss of 0.4921573508841875\n",
      "The 3490 th iteration gives loss of 0.49209640846414493\n",
      "The 3491 th iteration gives loss of 0.4920354970122029\n",
      "The 3492 th iteration gives loss of 0.4919746164908514\n",
      "The 3493 th iteration gives loss of 0.49191376686266874\n",
      "The 3494 th iteration gives loss of 0.4918529480902446\n",
      "The 3495 th iteration gives loss of 0.49179216013624694\n",
      "The 3496 th iteration gives loss of 0.49173140296341356\n",
      "The 3497 th iteration gives loss of 0.49167067653449653\n",
      "The 3498 th iteration gives loss of 0.49160998081234414\n",
      "The 3499 th iteration gives loss of 0.4915493157598044\n",
      "The 3500 th iteration gives loss of 0.4914886813398243\n",
      "The 3501 th iteration gives loss of 0.4914280775154023\n",
      "The 3502 th iteration gives loss of 0.4913675042495553\n",
      "The 3503 th iteration gives loss of 0.49130696150538433\n",
      "The 3504 th iteration gives loss of 0.49124644924604005\n",
      "The 3505 th iteration gives loss of 0.4911859674347275\n",
      "The 3506 th iteration gives loss of 0.49112551603466903\n",
      "The 3507 th iteration gives loss of 0.4910650950091969\n",
      "The 3508 th iteration gives loss of 0.49100470432166005\n",
      "The 3509 th iteration gives loss of 0.49094434393544384\n",
      "The 3510 th iteration gives loss of 0.4908840138140523\n",
      "The 3511 th iteration gives loss of 0.4908237139209682\n",
      "The 3512 th iteration gives loss of 0.49076344421979246\n",
      "The 3513 th iteration gives loss of 0.4907032046741059\n",
      "The 3514 th iteration gives loss of 0.4906429952475955\n",
      "The 3515 th iteration gives loss of 0.490582815903991\n",
      "The 3516 th iteration gives loss of 0.49052266660706034\n",
      "The 3517 th iteration gives loss of 0.49046254732063976\n",
      "The 3518 th iteration gives loss of 0.4904024580085982\n",
      "The 3519 th iteration gives loss of 0.49034239863487195\n",
      "The 3520 th iteration gives loss of 0.49028236916344503\n",
      "The 3521 th iteration gives loss of 0.49022236955835724\n",
      "The 3522 th iteration gives loss of 0.4901623997836981\n",
      "The 3523 th iteration gives loss of 0.49010245980359807\n",
      "The 3524 th iteration gives loss of 0.4900425495822616\n",
      "The 3525 th iteration gives loss of 0.48998266908390764\n",
      "The 3526 th iteration gives loss of 0.4899228182728444\n",
      "The 3527 th iteration gives loss of 0.48986299711342013\n",
      "The 3528 th iteration gives loss of 0.48980320557001583\n",
      "The 3529 th iteration gives loss of 0.4897434436070973\n",
      "The 3530 th iteration gives loss of 0.48968371118914367\n",
      "The 3531 th iteration gives loss of 0.48962400828071545\n",
      "The 3532 th iteration gives loss of 0.48956433484640316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 3533 th iteration gives loss of 0.489504690850864\n",
      "The 3534 th iteration gives loss of 0.48944507625880224\n",
      "The 3535 th iteration gives loss of 0.4893854910349776\n",
      "The 3536 th iteration gives loss of 0.48932593514417466\n",
      "The 3537 th iteration gives loss of 0.48926640855125625\n",
      "The 3538 th iteration gives loss of 0.4892069112211217\n",
      "The 3539 th iteration gives loss of 0.4891474431187128\n",
      "The 3540 th iteration gives loss of 0.4890880042090823\n",
      "The 3541 th iteration gives loss of 0.48902859445722574\n",
      "The 3542 th iteration gives loss of 0.48896921382828773\n",
      "The 3543 th iteration gives loss of 0.48890986228740224\n",
      "The 3544 th iteration gives loss of 0.48885053979978016\n",
      "The 3545 th iteration gives loss of 0.4887912463306772\n",
      "The 3546 th iteration gives loss of 0.4887319818454082\n",
      "The 3547 th iteration gives loss of 0.48867274630931845\n",
      "The 3548 th iteration gives loss of 0.4886135396878109\n",
      "The 3549 th iteration gives loss of 0.4885543619463427\n",
      "The 3550 th iteration gives loss of 0.48849521305041177\n",
      "The 3551 th iteration gives loss of 0.4884360929655717\n",
      "The 3552 th iteration gives loss of 0.48837700165743336\n",
      "The 3553 th iteration gives loss of 0.4883179390916435\n",
      "The 3554 th iteration gives loss of 0.48825890523391086\n",
      "The 3555 th iteration gives loss of 0.488199900049974\n",
      "The 3556 th iteration gives loss of 0.48814092350563665\n",
      "The 3557 th iteration gives loss of 0.48808197556674926\n",
      "The 3558 th iteration gives loss of 0.48802305619921305\n",
      "The 3559 th iteration gives loss of 0.48796416536896264\n",
      "The 3560 th iteration gives loss of 0.4879053030420051\n",
      "The 3561 th iteration gives loss of 0.4878464691843818\n",
      "The 3562 th iteration gives loss of 0.4877876637621845\n",
      "The 3563 th iteration gives loss of 0.4877288867415533\n",
      "The 3564 th iteration gives loss of 0.48767013808868004\n",
      "The 3565 th iteration gives loss of 0.48761141776981304\n",
      "The 3566 th iteration gives loss of 0.4875527257512247\n",
      "The 3567 th iteration gives loss of 0.4874940619992436\n",
      "The 3568 th iteration gives loss of 0.4874354264802667\n",
      "The 3569 th iteration gives loss of 0.4873768191607362\n",
      "The 3570 th iteration gives loss of 0.4873182400071104\n",
      "The 3571 th iteration gives loss of 0.4872596889859337\n",
      "The 3572 th iteration gives loss of 0.4872011660637752\n",
      "The 3573 th iteration gives loss of 0.48714267120725\n",
      "The 3574 th iteration gives loss of 0.4870842043830353\n",
      "The 3575 th iteration gives loss of 0.48702576555786425\n",
      "The 3576 th iteration gives loss of 0.4869673546984976\n",
      "The 3577 th iteration gives loss of 0.48690897177174214\n",
      "The 3578 th iteration gives loss of 0.4868506167444665\n",
      "The 3579 th iteration gives loss of 0.4867922895835928\n",
      "The 3580 th iteration gives loss of 0.4867339902560415\n",
      "The 3581 th iteration gives loss of 0.486675718728851\n",
      "The 3582 th iteration gives loss of 0.4866174749690726\n",
      "The 3583 th iteration gives loss of 0.4865592589437924\n",
      "The 3584 th iteration gives loss of 0.4865010706201531\n",
      "The 3585 th iteration gives loss of 0.4864429099653647\n",
      "The 3586 th iteration gives loss of 0.48638477694665466\n",
      "The 3587 th iteration gives loss of 0.48632667153131603\n",
      "The 3588 th iteration gives loss of 0.4862685936866891\n",
      "The 3589 th iteration gives loss of 0.4862105433801408\n",
      "The 3590 th iteration gives loss of 0.48615252057911085\n",
      "The 3591 th iteration gives loss of 0.4860945252510743\n",
      "The 3592 th iteration gives loss of 0.4860365573635332\n",
      "The 3593 th iteration gives loss of 0.4859786168840746\n",
      "The 3594 th iteration gives loss of 0.4859207037803218\n",
      "The 3595 th iteration gives loss of 0.4858628180199014\n",
      "The 3596 th iteration gives loss of 0.48580495957055575\n",
      "The 3597 th iteration gives loss of 0.48574712840001044\n",
      "The 3598 th iteration gives loss of 0.4856893244760878\n",
      "The 3599 th iteration gives loss of 0.4856315477666182\n",
      "The 3600 th iteration gives loss of 0.48557379823950647\n",
      "The 3601 th iteration gives loss of 0.4855160758626684\n",
      "The 3602 th iteration gives loss of 0.4854583806041004\n",
      "The 3603 th iteration gives loss of 0.485400712431839\n",
      "The 3604 th iteration gives loss of 0.48534307131394516\n",
      "The 3605 th iteration gives loss of 0.4852854572185412\n",
      "The 3606 th iteration gives loss of 0.48522787011380725\n",
      "The 3607 th iteration gives loss of 0.48517030996794636\n",
      "The 3608 th iteration gives loss of 0.4851127767492195\n",
      "The 3609 th iteration gives loss of 0.4850552704259176\n",
      "The 3610 th iteration gives loss of 0.48499779096641094\n",
      "The 3611 th iteration gives loss of 0.484940338339064\n",
      "The 3612 th iteration gives loss of 0.484882912512331\n",
      "The 3613 th iteration gives loss of 0.48482551345470465\n",
      "The 3614 th iteration gives loss of 0.4847681411346882\n",
      "The 3615 th iteration gives loss of 0.4847107955208944\n",
      "The 3616 th iteration gives loss of 0.48465347658191976\n",
      "The 3617 th iteration gives loss of 0.48459618428642065\n",
      "The 3618 th iteration gives loss of 0.48453891860311726\n",
      "The 3619 th iteration gives loss of 0.48448167950076576\n",
      "The 3620 th iteration gives loss of 0.4844244669481441\n",
      "The 3621 th iteration gives loss of 0.48436728091412184\n",
      "The 3622 th iteration gives loss of 0.4843101213675837\n",
      "The 3623 th iteration gives loss of 0.4842529882774486\n",
      "The 3624 th iteration gives loss of 0.48419588161269667\n",
      "The 3625 th iteration gives loss of 0.48413880134234905\n",
      "The 3626 th iteration gives loss of 0.48408174743546367\n",
      "The 3627 th iteration gives loss of 0.48402471986117024\n",
      "The 3628 th iteration gives loss of 0.4839677185886095\n",
      "The 3629 th iteration gives loss of 0.4839107435869808\n",
      "The 3630 th iteration gives loss of 0.48385379482551827\n",
      "The 3631 th iteration gives loss of 0.4837968722735247\n",
      "The 3632 th iteration gives loss of 0.4837399759003114\n",
      "The 3633 th iteration gives loss of 0.4836831056752766\n",
      "The 3634 th iteration gives loss of 0.48362626156781\n",
      "The 3635 th iteration gives loss of 0.4835694435473785\n",
      "The 3636 th iteration gives loss of 0.483512651583506\n",
      "The 3637 th iteration gives loss of 0.4834558856457303\n",
      "The 3638 th iteration gives loss of 0.48339914570363407\n",
      "The 3639 th iteration gives loss of 0.48334243172686314\n",
      "The 3640 th iteration gives loss of 0.4832857436850936\n",
      "The 3641 th iteration gives loss of 0.48322908154804495\n",
      "The 3642 th iteration gives loss of 0.48317244528547737\n",
      "The 3643 th iteration gives loss of 0.483115834867215\n",
      "The 3644 th iteration gives loss of 0.4830592502631129\n",
      "The 3645 th iteration gives loss of 0.48300269144303887\n",
      "The 3646 th iteration gives loss of 0.4829461583769582\n",
      "The 3647 th iteration gives loss of 0.48288965103483106\n",
      "The 3648 th iteration gives loss of 0.4828331693866968\n",
      "The 3649 th iteration gives loss of 0.48277671340261524\n",
      "The 3650 th iteration gives loss of 0.48272028305269465\n",
      "The 3651 th iteration gives loss of 0.48266387830709667\n",
      "The 3652 th iteration gives loss of 0.48260749913600764\n",
      "The 3653 th iteration gives loss of 0.48255114550967426\n",
      "The 3654 th iteration gives loss of 0.4824948173983568\n",
      "The 3655 th iteration gives loss of 0.4824385147723994\n",
      "The 3656 th iteration gives loss of 0.48238223760214594\n",
      "The 3657 th iteration gives loss of 0.48232598585801845\n",
      "The 3658 th iteration gives loss of 0.48226975951046336\n",
      "The 3659 th iteration gives loss of 0.4822135585299738\n",
      "The 3660 th iteration gives loss of 0.4821573828870857\n",
      "The 3661 th iteration gives loss of 0.4821012325523593\n",
      "The 3662 th iteration gives loss of 0.4820451074964296\n",
      "The 3663 th iteration gives loss of 0.4819890076899405\n",
      "The 3664 th iteration gives loss of 0.48193293310363006\n",
      "The 3665 th iteration gives loss of 0.4818768837081926\n",
      "The 3666 th iteration gives loss of 0.4818208594744443\n",
      "The 3667 th iteration gives loss of 0.4817648603732049\n",
      "The 3668 th iteration gives loss of 0.48170888637534476\n",
      "The 3669 th iteration gives loss of 0.4816529374517768\n",
      "The 3670 th iteration gives loss of 0.48159701357343976\n",
      "The 3671 th iteration gives loss of 0.48154111471134253\n",
      "The 3672 th iteration gives loss of 0.4814852408365094\n",
      "The 3673 th iteration gives loss of 0.4814293919200276\n",
      "The 3674 th iteration gives loss of 0.48137356793300407\n",
      "The 3675 th iteration gives loss of 0.4813177688465966\n",
      "The 3676 th iteration gives loss of 0.48126199463201536\n",
      "The 3677 th iteration gives loss of 0.48120624526049227\n",
      "The 3678 th iteration gives loss of 0.4811505207033129\n",
      "The 3679 th iteration gives loss of 0.48109482093180045\n",
      "The 3680 th iteration gives loss of 0.4810391459173207\n",
      "The 3681 th iteration gives loss of 0.48098349563127407\n",
      "The 3682 th iteration gives loss of 0.4809278700451119\n",
      "The 3683 th iteration gives loss of 0.48087226913032216\n",
      "The 3684 th iteration gives loss of 0.48081669285841594\n",
      "The 3685 th iteration gives loss of 0.4807611412009675\n",
      "The 3686 th iteration gives loss of 0.48070561412959223\n",
      "The 3687 th iteration gives loss of 0.4806501116159408\n",
      "The 3688 th iteration gives loss of 0.4805946336317051\n",
      "The 3689 th iteration gives loss of 0.4805391801485918\n",
      "The 3690 th iteration gives loss of 0.4804837511383913\n",
      "The 3691 th iteration gives loss of 0.4804283465729133\n",
      "The 3692 th iteration gives loss of 0.48037296642400257\n",
      "The 3693 th iteration gives loss of 0.48031761066354905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 3694 th iteration gives loss of 0.4802622792634929\n",
      "The 3695 th iteration gives loss of 0.48020697219577974\n",
      "The 3696 th iteration gives loss of 0.4801516894324475\n",
      "The 3697 th iteration gives loss of 0.480096430945541\n",
      "The 3698 th iteration gives loss of 0.4800411967071382\n",
      "The 3699 th iteration gives loss of 0.4799859866893763\n",
      "The 3700 th iteration gives loss of 0.4799308008644426\n",
      "The 3701 th iteration gives loss of 0.4798756392045132\n",
      "The 3702 th iteration gives loss of 0.47982050168185764\n",
      "The 3703 th iteration gives loss of 0.4797653882687661\n",
      "The 3704 th iteration gives loss of 0.4797102989375576\n",
      "The 3705 th iteration gives loss of 0.47965523366060825\n",
      "The 3706 th iteration gives loss of 0.4796001924103217\n",
      "The 3707 th iteration gives loss of 0.4795451751591667\n",
      "The 3708 th iteration gives loss of 0.47949018187956916\n",
      "The 3709 th iteration gives loss of 0.479435212544116\n",
      "The 3710 th iteration gives loss of 0.47938026712534965\n",
      "The 3711 th iteration gives loss of 0.4793253455958721\n",
      "The 3712 th iteration gives loss of 0.47927044792831386\n",
      "The 3713 th iteration gives loss of 0.4792155740953756\n",
      "The 3714 th iteration gives loss of 0.47916072406976357\n",
      "The 3715 th iteration gives loss of 0.4791058978242396\n",
      "The 3716 th iteration gives loss of 0.4790510953316092\n",
      "The 3717 th iteration gives loss of 0.4789963165647202\n",
      "The 3718 th iteration gives loss of 0.47894156149641387\n",
      "The 3719 th iteration gives loss of 0.47888683009963107\n",
      "The 3720 th iteration gives loss of 0.47883212234731337\n",
      "The 3721 th iteration gives loss of 0.4787774382124583\n",
      "The 3722 th iteration gives loss of 0.47872277766809823\n",
      "The 3723 th iteration gives loss of 0.47866814068727365\n",
      "The 3724 th iteration gives loss of 0.4786135272431374\n",
      "The 3725 th iteration gives loss of 0.47855893730880755\n",
      "The 3726 th iteration gives loss of 0.4785043708574687\n",
      "The 3727 th iteration gives loss of 0.47844982786233226\n",
      "The 3728 th iteration gives loss of 0.4783953082966679\n",
      "The 3729 th iteration gives loss of 0.4783408121337859\n",
      "The 3730 th iteration gives loss of 0.4782863393470119\n",
      "The 3731 th iteration gives loss of 0.47823188990970955\n",
      "The 3732 th iteration gives loss of 0.47817746379529313\n",
      "The 3733 th iteration gives loss of 0.4781230609772205\n",
      "The 3734 th iteration gives loss of 0.4780686814289744\n",
      "The 3735 th iteration gives loss of 0.47801432512408637\n",
      "The 3736 th iteration gives loss of 0.47795999203610945\n",
      "The 3737 th iteration gives loss of 0.4779056821386449\n",
      "The 3738 th iteration gives loss of 0.47785139540532884\n",
      "The 3739 th iteration gives loss of 0.4777971318098467\n",
      "The 3740 th iteration gives loss of 0.4777428913258965\n",
      "The 3741 th iteration gives loss of 0.4776886739272391\n",
      "The 3742 th iteration gives loss of 0.4776344795876597\n",
      "The 3743 th iteration gives loss of 0.47758030828098996\n",
      "The 3744 th iteration gives loss of 0.4775261599810798\n",
      "The 3745 th iteration gives loss of 0.47747203466182603\n",
      "The 3746 th iteration gives loss of 0.4774179322971803\n",
      "The 3747 th iteration gives loss of 0.47736385286110405\n",
      "The 3748 th iteration gives loss of 0.4773097963276127\n",
      "The 3749 th iteration gives loss of 0.47725576267075354\n",
      "The 3750 th iteration gives loss of 0.47720175186461333\n",
      "The 3751 th iteration gives loss of 0.47714776388331576\n",
      "The 3752 th iteration gives loss of 0.47709379870100344\n",
      "The 3753 th iteration gives loss of 0.47703985629188456\n",
      "The 3754 th iteration gives loss of 0.47698593663019045\n",
      "The 3755 th iteration gives loss of 0.47693203969017944\n",
      "The 3756 th iteration gives loss of 0.47687816544616474\n",
      "The 3757 th iteration gives loss of 0.4768243138724902\n",
      "The 3758 th iteration gives loss of 0.47677048494352675\n",
      "The 3759 th iteration gives loss of 0.4767166786337023\n",
      "The 3760 th iteration gives loss of 0.4766628949174577\n",
      "The 3761 th iteration gives loss of 0.47660913376927555\n",
      "The 3762 th iteration gives loss of 0.4765553951636836\n",
      "The 3763 th iteration gives loss of 0.47650167907523727\n",
      "The 3764 th iteration gives loss of 0.47644798547853495\n",
      "The 3765 th iteration gives loss of 0.47639431434821644\n",
      "The 3766 th iteration gives loss of 0.47634066565894007\n",
      "The 3767 th iteration gives loss of 0.4762870393854248\n",
      "The 3768 th iteration gives loss of 0.47623343550239056\n",
      "The 3769 th iteration gives loss of 0.47617985398462404\n",
      "The 3770 th iteration gives loss of 0.4761262948069352\n",
      "The 3771 th iteration gives loss of 0.4760727579441682\n",
      "The 3772 th iteration gives loss of 0.4760192433712055\n",
      "The 3773 th iteration gives loss of 0.4759657510629789\n",
      "The 3774 th iteration gives loss of 0.4759122809944288\n",
      "The 3775 th iteration gives loss of 0.47585883314054284\n",
      "The 3776 th iteration gives loss of 0.4758054074763643\n",
      "The 3777 th iteration gives loss of 0.47575200397694567\n",
      "The 3778 th iteration gives loss of 0.47569862261737894\n",
      "The 3779 th iteration gives loss of 0.47564526337279994\n",
      "The 3780 th iteration gives loss of 0.4755919262183817\n",
      "The 3781 th iteration gives loss of 0.47553861112932544\n",
      "The 3782 th iteration gives loss of 0.4754853180808568\n",
      "The 3783 th iteration gives loss of 0.4754320470482522\n",
      "The 3784 th iteration gives loss of 0.47537879800683047\n",
      "The 3785 th iteration gives loss of 0.47532557093193595\n",
      "The 3786 th iteration gives loss of 0.47527236579893184\n",
      "The 3787 th iteration gives loss of 0.4752191825832499\n",
      "The 3788 th iteration gives loss of 0.47516602126032215\n",
      "The 3789 th iteration gives loss of 0.47511288180564\n",
      "The 3790 th iteration gives loss of 0.4750597641947176\n",
      "The 3791 th iteration gives loss of 0.47500666840312256\n",
      "The 3792 th iteration gives loss of 0.47495359440642193\n",
      "The 3793 th iteration gives loss of 0.47490054218023636\n",
      "The 3794 th iteration gives loss of 0.4748475117002443\n",
      "The 3795 th iteration gives loss of 0.4747945029421195\n",
      "The 3796 th iteration gives loss of 0.4747415158815921\n",
      "The 3797 th iteration gives loss of 0.474688550494432\n",
      "The 3798 th iteration gives loss of 0.47463560675642047\n",
      "The 3799 th iteration gives loss of 0.47458268464339215\n",
      "The 3800 th iteration gives loss of 0.4745297841312054\n",
      "The 3801 th iteration gives loss of 0.4744769051957711\n",
      "The 3802 th iteration gives loss of 0.4744240478130025\n",
      "The 3803 th iteration gives loss of 0.47437121195889087\n",
      "The 3804 th iteration gives loss of 0.47431839760941347\n",
      "The 3805 th iteration gives loss of 0.474265604740611\n",
      "The 3806 th iteration gives loss of 0.47421283332855874\n",
      "The 3807 th iteration gives loss of 0.47416008334934856\n",
      "The 3808 th iteration gives loss of 0.4741073547791204\n",
      "The 3809 th iteration gives loss of 0.4740546475940442\n",
      "The 3810 th iteration gives loss of 0.47400196177032594\n",
      "The 3811 th iteration gives loss of 0.4739492972841981\n",
      "The 3812 th iteration gives loss of 0.4738966541119438\n",
      "The 3813 th iteration gives loss of 0.4738440322298623\n",
      "The 3814 th iteration gives loss of 0.47379143161427695\n",
      "The 3815 th iteration gives loss of 0.4737388522415986\n",
      "The 3816 th iteration gives loss of 0.47368629408818635\n",
      "The 3817 th iteration gives loss of 0.4736337571305081\n",
      "The 3818 th iteration gives loss of 0.47358124134503055\n",
      "The 3819 th iteration gives loss of 0.4735287467082551\n",
      "The 3820 th iteration gives loss of 0.47347627319672486\n",
      "The 3821 th iteration gives loss of 0.4734238207870162\n",
      "The 3822 th iteration gives loss of 0.4733713894557249\n",
      "The 3823 th iteration gives loss of 0.4733189791795065\n",
      "The 3824 th iteration gives loss of 0.473266589935017\n",
      "The 3825 th iteration gives loss of 0.473214221698963\n",
      "The 3826 th iteration gives loss of 0.473161874448083\n",
      "The 3827 th iteration gives loss of 0.47310954815915673\n",
      "The 3828 th iteration gives loss of 0.47305724280897615\n",
      "The 3829 th iteration gives loss of 0.47300495837439316\n",
      "The 3830 th iteration gives loss of 0.4729526948322702\n",
      "The 3831 th iteration gives loss of 0.4729004521595115\n",
      "The 3832 th iteration gives loss of 0.4728482303330486\n",
      "The 3833 th iteration gives loss of 0.47279602932984377\n",
      "The 3834 th iteration gives loss of 0.47274384912692113\n",
      "The 3835 th iteration gives loss of 0.4726916897012892\n",
      "The 3836 th iteration gives loss of 0.4726395510300254\n",
      "The 3837 th iteration gives loss of 0.47258743309022694\n",
      "The 3838 th iteration gives loss of 0.47253533585902235\n",
      "The 3839 th iteration gives loss of 0.47248325931357343\n",
      "The 3840 th iteration gives loss of 0.47243120343109096\n",
      "The 3841 th iteration gives loss of 0.472379168188789\n",
      "The 3842 th iteration gives loss of 0.47232715356392524\n",
      "The 3843 th iteration gives loss of 0.47227515953379895\n",
      "The 3844 th iteration gives loss of 0.4722231860757276\n",
      "The 3845 th iteration gives loss of 0.47217123316707527\n",
      "The 3846 th iteration gives loss of 0.4721193007852357\n",
      "The 3847 th iteration gives loss of 0.4720673889076101\n",
      "The 3848 th iteration gives loss of 0.47201549751168304\n",
      "The 3849 th iteration gives loss of 0.4719636265748956\n",
      "The 3850 th iteration gives loss of 0.4719117760747954\n",
      "The 3851 th iteration gives loss of 0.4718599459889339\n",
      "The 3852 th iteration gives loss of 0.4718081362948886\n",
      "The 3853 th iteration gives loss of 0.47175634697025204\n",
      "The 3854 th iteration gives loss of 0.4717045779926786\n",
      "The 3855 th iteration gives loss of 0.47165282933985136\n",
      "The 3856 th iteration gives loss of 0.4716011009894778\n",
      "The 3857 th iteration gives loss of 0.4715493929192761\n",
      "The 3858 th iteration gives loss of 0.4714977051070449\n",
      "The 3859 th iteration gives loss of 0.47144603753056347\n",
      "The 3860 th iteration gives loss of 0.471394390167681\n",
      "The 3861 th iteration gives loss of 0.47134276299626454\n",
      "The 3862 th iteration gives loss of 0.47129115599418814\n",
      "The 3863 th iteration gives loss of 0.47123956913939363\n",
      "The 3864 th iteration gives loss of 0.471188002409834\n",
      "The 3865 th iteration gives loss of 0.47113645578349855\n",
      "The 3866 th iteration gives loss of 0.4710849292384268\n",
      "The 3867 th iteration gives loss of 0.47103342275263754\n",
      "The 3868 th iteration gives loss of 0.4709819363042412\n",
      "The 3869 th iteration gives loss of 0.47093046987134024\n",
      "The 3870 th iteration gives loss of 0.4708790234320738\n",
      "The 3871 th iteration gives loss of 0.47082759696462395\n",
      "The 3872 th iteration gives loss of 0.47077619044721103\n",
      "The 3873 th iteration gives loss of 0.4707248038580511\n",
      "The 3874 th iteration gives loss of 0.4706734371754237\n",
      "The 3875 th iteration gives loss of 0.4706220903776169\n",
      "The 3876 th iteration gives loss of 0.47057076344297877\n",
      "The 3877 th iteration gives loss of 0.47051945634986403\n",
      "The 3878 th iteration gives loss of 0.470468169076647\n",
      "The 3879 th iteration gives loss of 0.47041690160176414\n",
      "The 3880 th iteration gives loss of 0.4703656539036634\n",
      "The 3881 th iteration gives loss of 0.4703144259608317\n",
      "The 3882 th iteration gives loss of 0.4702632177517672\n",
      "The 3883 th iteration gives loss of 0.4702120292550409\n",
      "The 3884 th iteration gives loss of 0.470160860449217\n",
      "The 3885 th iteration gives loss of 0.4701097113128864\n",
      "The 3886 th iteration gives loss of 0.4700585818246841\n",
      "The 3887 th iteration gives loss of 0.47000747196328146\n",
      "The 3888 th iteration gives loss of 0.46995638170737913\n",
      "The 3889 th iteration gives loss of 0.46990531103569894\n",
      "The 3890 th iteration gives loss of 0.4698542599269916\n",
      "The 3891 th iteration gives loss of 0.4698032283600389\n",
      "The 3892 th iteration gives loss of 0.4697522163136601\n",
      "The 3893 th iteration gives loss of 0.4697012237667067\n",
      "The 3894 th iteration gives loss of 0.4696502506980397\n",
      "The 3895 th iteration gives loss of 0.4695992970865737\n",
      "The 3896 th iteration gives loss of 0.46954836291124036\n",
      "The 3897 th iteration gives loss of 0.4694974481509927\n",
      "The 3898 th iteration gives loss of 0.4694465527848474\n",
      "The 3899 th iteration gives loss of 0.4693956767918243\n",
      "The 3900 th iteration gives loss of 0.469344820150954\n",
      "The 3901 th iteration gives loss of 0.4692939828413433\n",
      "The 3902 th iteration gives loss of 0.46924316484209216\n",
      "The 3903 th iteration gives loss of 0.46919236613235427\n",
      "The 3904 th iteration gives loss of 0.46914158669128514\n",
      "The 3905 th iteration gives loss of 0.46909082649809575\n",
      "The 3906 th iteration gives loss of 0.4690400855320122\n",
      "The 3907 th iteration gives loss of 0.46898936377230405\n",
      "The 3908 th iteration gives loss of 0.468938661198234\n",
      "The 3909 th iteration gives loss of 0.4688879777891542\n",
      "The 3910 th iteration gives loss of 0.4688373135243946\n",
      "The 3911 th iteration gives loss of 0.4687866683833319\n",
      "The 3912 th iteration gives loss of 0.4687360423453682\n",
      "The 3913 th iteration gives loss of 0.4686854353899556\n",
      "The 3914 th iteration gives loss of 0.4686348474965432\n",
      "The 3915 th iteration gives loss of 0.46858427864462654\n",
      "The 3916 th iteration gives loss of 0.4685337288137191\n",
      "The 3917 th iteration gives loss of 0.4684831979833871\n",
      "The 3918 th iteration gives loss of 0.46843268613319133\n",
      "The 3919 th iteration gives loss of 0.4683821932427633\n",
      "The 3920 th iteration gives loss of 0.468331719291719\n",
      "The 3921 th iteration gives loss of 0.4682812642597458\n",
      "The 3922 th iteration gives loss of 0.46823082812652533\n",
      "The 3923 th iteration gives loss of 0.4681804108717784\n",
      "The 3924 th iteration gives loss of 0.46813001247525854\n",
      "The 3925 th iteration gives loss of 0.4680796329167471\n",
      "The 3926 th iteration gives loss of 0.4680292721760478\n",
      "The 3927 th iteration gives loss of 0.46797893023301895\n",
      "The 3928 th iteration gives loss of 0.4679286070675086\n",
      "The 3929 th iteration gives loss of 0.4678783026594128\n",
      "The 3930 th iteration gives loss of 0.4678280169886537\n",
      "The 3931 th iteration gives loss of 0.46777775003518585\n",
      "The 3932 th iteration gives loss of 0.4677275017789921\n",
      "The 3933 th iteration gives loss of 0.46767727220007077\n",
      "The 3934 th iteration gives loss of 0.4676270612784679\n",
      "The 3935 th iteration gives loss of 0.46757686899424705\n",
      "The 3936 th iteration gives loss of 0.4675266953274894\n",
      "The 3937 th iteration gives loss of 0.467476540258316\n",
      "The 3938 th iteration gives loss of 0.46742640376688716\n",
      "The 3939 th iteration gives loss of 0.46737628583336954\n",
      "The 3940 th iteration gives loss of 0.4673261864379783\n",
      "The 3941 th iteration gives loss of 0.4672761055609279\n",
      "The 3942 th iteration gives loss of 0.46722604318248356\n",
      "The 3943 th iteration gives loss of 0.4671759992829463\n",
      "The 3944 th iteration gives loss of 0.46712597384262955\n",
      "The 3945 th iteration gives loss of 0.4670759668418473\n",
      "The 3946 th iteration gives loss of 0.4670259782610209\n",
      "The 3947 th iteration gives loss of 0.4669760080805054\n",
      "The 3948 th iteration gives loss of 0.46692605628074796\n",
      "The 3949 th iteration gives loss of 0.46687612284220276\n",
      "The 3950 th iteration gives loss of 0.4668262077453486\n",
      "The 3951 th iteration gives loss of 0.4667763109706862\n",
      "The 3952 th iteration gives loss of 0.4667264324987658\n",
      "The 3953 th iteration gives loss of 0.46667657231014587\n",
      "The 3954 th iteration gives loss of 0.4666267303854139\n",
      "The 3955 th iteration gives loss of 0.4665769067051885\n",
      "The 3956 th iteration gives loss of 0.46652710125012486\n",
      "The 3957 th iteration gives loss of 0.4664773140008809\n",
      "The 3958 th iteration gives loss of 0.46642754493816974\n",
      "The 3959 th iteration gives loss of 0.4663777940427143\n",
      "The 3960 th iteration gives loss of 0.4663280612952805\n",
      "The 3961 th iteration gives loss of 0.46627834667663565\n",
      "The 3962 th iteration gives loss of 0.4662286501676026\n",
      "The 3963 th iteration gives loss of 0.4661789717490018\n",
      "The 3964 th iteration gives loss of 0.46612931140170716\n",
      "The 3965 th iteration gives loss of 0.4660796691066022\n",
      "The 3966 th iteration gives loss of 0.46603004484461413\n",
      "The 3967 th iteration gives loss of 0.4659804385966712\n",
      "The 3968 th iteration gives loss of 0.4659308503437615\n",
      "The 3969 th iteration gives loss of 0.4658812800668736\n",
      "The 3970 th iteration gives loss of 0.4658317277470298\n",
      "The 3971 th iteration gives loss of 0.4657821933652916\n",
      "The 3972 th iteration gives loss of 0.4657326769027149\n",
      "The 3973 th iteration gives loss of 0.46568317834042505\n",
      "The 3974 th iteration gives loss of 0.4656336976595561\n",
      "The 3975 th iteration gives loss of 0.4655842348412478\n",
      "The 3976 th iteration gives loss of 0.46553478986669095\n",
      "The 3977 th iteration gives loss of 0.46548536271710356\n",
      "The 3978 th iteration gives loss of 0.46543595337371635\n",
      "The 3979 th iteration gives loss of 0.465386561817785\n",
      "The 3980 th iteration gives loss of 0.46533718803061647\n",
      "The 3981 th iteration gives loss of 0.4652878319935149\n",
      "The 3982 th iteration gives loss of 0.4652384936878324\n",
      "The 3983 th iteration gives loss of 0.46518917309491964\n",
      "The 3984 th iteration gives loss of 0.4651398701961894\n",
      "The 3985 th iteration gives loss of 0.4650905849730633\n",
      "The 3986 th iteration gives loss of 0.4650413174069794\n",
      "The 3987 th iteration gives loss of 0.4649920674794126\n",
      "The 3988 th iteration gives loss of 0.46494283517186596\n",
      "The 3989 th iteration gives loss of 0.4648936204658572\n",
      "The 3990 th iteration gives loss of 0.4648444233429454\n",
      "The 3991 th iteration gives loss of 0.4647952437846993\n",
      "The 3992 th iteration gives loss of 0.4647460817727338\n",
      "The 3993 th iteration gives loss of 0.4646969372886698\n",
      "The 3994 th iteration gives loss of 0.464647810314175\n",
      "The 3995 th iteration gives loss of 0.46459870083091565\n",
      "The 3996 th iteration gives loss of 0.46454960882059443\n",
      "The 3997 th iteration gives loss of 0.464500534264977\n",
      "The 3998 th iteration gives loss of 0.46445147714578017\n",
      "The 3999 th iteration gives loss of 0.464402437444814\n",
      "The 4000 th iteration gives loss of 0.4643534151438723\n",
      "The 4001 th iteration gives loss of 0.46430441022480473\n",
      "The 4002 th iteration gives loss of 0.46425542266945957\n",
      "The 4003 th iteration gives loss of 0.4642064524597164\n",
      "The 4004 th iteration gives loss of 0.46415749957750635\n",
      "The 4005 th iteration gives loss of 0.4641085640047513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 4006 th iteration gives loss of 0.4640596457234228\n",
      "The 4007 th iteration gives loss of 0.4640107447155071\n",
      "The 4008 th iteration gives loss of 0.46396186096300884\n",
      "The 4009 th iteration gives loss of 0.46391299444798423\n",
      "The 4010 th iteration gives loss of 0.4638641451524866\n",
      "The 4011 th iteration gives loss of 0.4638153130585964\n",
      "The 4012 th iteration gives loss of 0.4637664981484285\n",
      "The 4013 th iteration gives loss of 0.4637177004041263\n",
      "The 4014 th iteration gives loss of 0.4636689198078593\n",
      "The 4015 th iteration gives loss of 0.46362015634180914\n",
      "The 4016 th iteration gives loss of 0.4635714099882082\n",
      "The 4017 th iteration gives loss of 0.46352268072926495\n",
      "The 4018 th iteration gives loss of 0.4634739685472588\n",
      "The 4019 th iteration gives loss of 0.4634252734244845\n",
      "The 4020 th iteration gives loss of 0.46337659534325987\n",
      "The 4021 th iteration gives loss of 0.4633279342859038\n",
      "The 4022 th iteration gives loss of 0.4632792902347905\n",
      "The 4023 th iteration gives loss of 0.4632306631723079\n",
      "The 4024 th iteration gives loss of 0.46318205308086724\n",
      "The 4025 th iteration gives loss of 0.46313345994291083\n",
      "The 4026 th iteration gives loss of 0.46308488374089823\n",
      "The 4027 th iteration gives loss of 0.4630363244573172\n",
      "The 4028 th iteration gives loss of 0.46298778207468017\n",
      "The 4029 th iteration gives loss of 0.46293925657552265\n",
      "The 4030 th iteration gives loss of 0.46289074794241325\n",
      "The 4031 th iteration gives loss of 0.4628422561579142\n",
      "The 4032 th iteration gives loss of 0.46279378120465203\n",
      "The 4033 th iteration gives loss of 0.46274532306526783\n",
      "The 4034 th iteration gives loss of 0.46269688172240664\n",
      "The 4035 th iteration gives loss of 0.4626484571587543\n",
      "The 4036 th iteration gives loss of 0.46260004935701554\n",
      "The 4037 th iteration gives loss of 0.46255165829993533\n",
      "The 4038 th iteration gives loss of 0.4625032839702595\n",
      "The 4039 th iteration gives loss of 0.46245492635076635\n",
      "The 4040 th iteration gives loss of 0.46240658542425966\n",
      "The 4041 th iteration gives loss of 0.4623582611735651\n",
      "The 4042 th iteration gives loss of 0.46230995358154325\n",
      "The 4043 th iteration gives loss of 0.46226166263106705\n",
      "The 4044 th iteration gives loss of 0.4622133883050324\n",
      "The 4045 th iteration gives loss of 0.4621651305863695\n",
      "The 4046 th iteration gives loss of 0.4621168894580176\n",
      "The 4047 th iteration gives loss of 0.4620686649029577\n",
      "The 4048 th iteration gives loss of 0.4620204569041864\n",
      "The 4049 th iteration gives loss of 0.4619722654447151\n",
      "The 4050 th iteration gives loss of 0.461924090507604\n",
      "The 4051 th iteration gives loss of 0.4618759320759032\n",
      "The 4052 th iteration gives loss of 0.4618277901327075\n",
      "The 4053 th iteration gives loss of 0.46177966466113146\n",
      "The 4054 th iteration gives loss of 0.4617315556443224\n",
      "The 4055 th iteration gives loss of 0.4616834630654339\n",
      "The 4056 th iteration gives loss of 0.46163538690766937\n",
      "The 4057 th iteration gives loss of 0.4615873271542166\n",
      "The 4058 th iteration gives loss of 0.46153928378830306\n",
      "The 4059 th iteration gives loss of 0.4614912567932145\n",
      "The 4060 th iteration gives loss of 0.46144324615220333\n",
      "The 4061 th iteration gives loss of 0.4613952518485897\n",
      "The 4062 th iteration gives loss of 0.46134727386569124\n",
      "The 4063 th iteration gives loss of 0.46129931218687087\n",
      "The 4064 th iteration gives loss of 0.4612513667954823\n",
      "The 4065 th iteration gives loss of 0.46120343767493815\n",
      "The 4066 th iteration gives loss of 0.4611555248086527\n",
      "The 4067 th iteration gives loss of 0.46110762818007145\n",
      "The 4068 th iteration gives loss of 0.4610597477726554\n",
      "The 4069 th iteration gives loss of 0.46101188356990214\n",
      "The 4070 th iteration gives loss of 0.46096403555532334\n",
      "The 4071 th iteration gives loss of 0.4609162037124391\n",
      "The 4072 th iteration gives loss of 0.4608683880248229\n",
      "The 4073 th iteration gives loss of 0.4608205884760657\n",
      "The 4074 th iteration gives loss of 0.46077280504976104\n",
      "The 4075 th iteration gives loss of 0.46072503772952916\n",
      "The 4076 th iteration gives loss of 0.4606772864990394\n",
      "The 4077 th iteration gives loss of 0.4606295513419538\n",
      "The 4078 th iteration gives loss of 0.4605818322419657\n",
      "The 4079 th iteration gives loss of 0.4605341291827986\n",
      "The 4080 th iteration gives loss of 0.46048644214820167\n",
      "The 4081 th iteration gives loss of 0.4604387711219354\n",
      "The 4082 th iteration gives loss of 0.46039111608777955\n",
      "The 4083 th iteration gives loss of 0.46034347702954836\n",
      "The 4084 th iteration gives loss of 0.46029585393109124\n",
      "The 4085 th iteration gives loss of 0.4602482467762427\n",
      "The 4086 th iteration gives loss of 0.46020065554889134\n",
      "The 4087 th iteration gives loss of 0.46015308023293466\n",
      "The 4088 th iteration gives loss of 0.4601055208122917\n",
      "The 4089 th iteration gives loss of 0.4600579772709303\n",
      "The 4090 th iteration gives loss of 0.4600104495927922\n",
      "The 4091 th iteration gives loss of 0.45996293776187525\n",
      "The 4092 th iteration gives loss of 0.45991544176219484\n",
      "The 4093 th iteration gives loss of 0.45986796157779986\n",
      "The 4094 th iteration gives loss of 0.45982049719273926\n",
      "The 4095 th iteration gives loss of 0.4597730485910839\n",
      "The 4096 th iteration gives loss of 0.4597256157569401\n",
      "The 4097 th iteration gives loss of 0.4596781986744326\n",
      "The 4098 th iteration gives loss of 0.4596307973277131\n",
      "The 4099 th iteration gives loss of 0.45958341170095807\n",
      "The 4100 th iteration gives loss of 0.45953604177834284\n",
      "The 4101 th iteration gives loss of 0.45948868754408967\n",
      "The 4102 th iteration gives loss of 0.45944134898243366\n",
      "The 4103 th iteration gives loss of 0.4593940260776358\n",
      "The 4104 th iteration gives loss of 0.45934671881396033\n",
      "The 4105 th iteration gives loss of 0.4592994271757183\n",
      "The 4106 th iteration gives loss of 0.4592521511472415\n",
      "The 4107 th iteration gives loss of 0.45920489071286064\n",
      "The 4108 th iteration gives loss of 0.4591576458569701\n",
      "The 4109 th iteration gives loss of 0.4591104165639185\n",
      "The 4110 th iteration gives loss of 0.4590632028181441\n",
      "The 4111 th iteration gives loss of 0.45901600460407205\n",
      "The 4112 th iteration gives loss of 0.4589688219061577\n",
      "The 4113 th iteration gives loss of 0.4589216547088864\n",
      "The 4114 th iteration gives loss of 0.45887450299674776\n",
      "The 4115 th iteration gives loss of 0.4588273667542553\n",
      "The 4116 th iteration gives loss of 0.4587802459659648\n",
      "The 4117 th iteration gives loss of 0.458733140616424\n",
      "The 4118 th iteration gives loss of 0.45868605069022084\n",
      "The 4119 th iteration gives loss of 0.4586389761719669\n",
      "The 4120 th iteration gives loss of 0.45859191704628927\n",
      "The 4121 th iteration gives loss of 0.45854487329782895\n",
      "The 4122 th iteration gives loss of 0.4584978449112654\n",
      "The 4123 th iteration gives loss of 0.45845083187128727\n",
      "The 4124 th iteration gives loss of 0.4584038341626129\n",
      "The 4125 th iteration gives loss of 0.4583568517699669\n",
      "The 4126 th iteration gives loss of 0.45830988467811706\n",
      "The 4127 th iteration gives loss of 0.458262932871825\n",
      "The 4128 th iteration gives loss of 0.4582159963358963\n",
      "The 4129 th iteration gives loss of 0.458169075055157\n",
      "The 4130 th iteration gives loss of 0.45812216901444336\n",
      "The 4131 th iteration gives loss of 0.4580752781986233\n",
      "The 4132 th iteration gives loss of 0.4580284025925644\n",
      "The 4133 th iteration gives loss of 0.4579815421811844\n",
      "The 4134 th iteration gives loss of 0.4579346969494062\n",
      "The 4135 th iteration gives loss of 0.4578878668821659\n",
      "The 4136 th iteration gives loss of 0.45784105196444597\n",
      "The 4137 th iteration gives loss of 0.4577942521812284\n",
      "The 4138 th iteration gives loss of 0.45774746751751366\n",
      "The 4139 th iteration gives loss of 0.4577006979583479\n",
      "The 4140 th iteration gives loss of 0.45765394348878097\n",
      "The 4141 th iteration gives loss of 0.4576072040938793\n",
      "The 4142 th iteration gives loss of 0.4575604797587357\n",
      "The 4143 th iteration gives loss of 0.4575137704684585\n",
      "The 4144 th iteration gives loss of 0.4574670762081829\n",
      "The 4145 th iteration gives loss of 0.45742039696307857\n",
      "The 4146 th iteration gives loss of 0.45737373271829795\n",
      "The 4147 th iteration gives loss of 0.4573270834590612\n",
      "The 4148 th iteration gives loss of 0.45728044917057453\n",
      "The 4149 th iteration gives loss of 0.4572338298380698\n",
      "The 4150 th iteration gives loss of 0.4571872254468152\n",
      "The 4151 th iteration gives loss of 0.45714063598208915\n",
      "The 4152 th iteration gives loss of 0.45709406142918596\n",
      "The 4153 th iteration gives loss of 0.45704750177343134\n",
      "The 4154 th iteration gives loss of 0.45700095700016446\n",
      "The 4155 th iteration gives loss of 0.4569544270947384\n",
      "The 4156 th iteration gives loss of 0.45690791204253933\n",
      "The 4157 th iteration gives loss of 0.4568614118289723\n",
      "The 4158 th iteration gives loss of 0.45681492643945737\n",
      "The 4159 th iteration gives loss of 0.4567684558594439\n",
      "The 4160 th iteration gives loss of 0.45672200007437247\n",
      "The 4161 th iteration gives loss of 0.4566755590697554\n",
      "The 4162 th iteration gives loss of 0.45662913283108525\n",
      "The 4163 th iteration gives loss of 0.4565827213438699\n",
      "The 4164 th iteration gives loss of 0.45653632459366633\n",
      "The 4165 th iteration gives loss of 0.45648994256602843\n",
      "The 4166 th iteration gives loss of 0.4564435752465592\n",
      "The 4167 th iteration gives loss of 0.45639722262084753\n",
      "The 4168 th iteration gives loss of 0.45635088467451423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 4169 th iteration gives loss of 0.4563045613932238\n",
      "The 4170 th iteration gives loss of 0.456258252762617\n",
      "The 4171 th iteration gives loss of 0.45621195876838355\n",
      "The 4172 th iteration gives loss of 0.4561656793962396\n",
      "The 4173 th iteration gives loss of 0.4561194146319054\n",
      "The 4174 th iteration gives loss of 0.4560731644611045\n",
      "The 4175 th iteration gives loss of 0.4560269288696114\n",
      "The 4176 th iteration gives loss of 0.45598070784322914\n",
      "The 4177 th iteration gives loss of 0.45593450136774527\n",
      "The 4178 th iteration gives loss of 0.4558883094289768\n",
      "The 4179 th iteration gives loss of 0.4558421320127754\n",
      "The 4180 th iteration gives loss of 0.45579596910499304\n",
      "The 4181 th iteration gives loss of 0.45574982069152253\n",
      "The 4182 th iteration gives loss of 0.4557036867582662\n",
      "The 4183 th iteration gives loss of 0.4556575672911322\n",
      "The 4184 th iteration gives loss of 0.45561146227607563\n",
      "The 4185 th iteration gives loss of 0.4555653716990508\n",
      "The 4186 th iteration gives loss of 0.45551929554604426\n",
      "The 4187 th iteration gives loss of 0.45547323380304505\n",
      "The 4188 th iteration gives loss of 0.4554271864560803\n",
      "The 4189 th iteration gives loss of 0.4553811534911907\n",
      "The 4190 th iteration gives loss of 0.4553351348944213\n",
      "The 4191 th iteration gives loss of 0.4552891306518636\n",
      "The 4192 th iteration gives loss of 0.4552431407495953\n",
      "The 4193 th iteration gives loss of 0.4551971651737541\n",
      "The 4194 th iteration gives loss of 0.45515120391046915\n",
      "The 4195 th iteration gives loss of 0.4551052569458804\n",
      "The 4196 th iteration gives loss of 0.45505932426617446\n",
      "The 4197 th iteration gives loss of 0.45501340585755284\n",
      "The 4198 th iteration gives loss of 0.4549675017062059\n",
      "The 4199 th iteration gives loss of 0.454921611798376\n",
      "The 4200 th iteration gives loss of 0.4548757361203339\n",
      "The 4201 th iteration gives loss of 0.45482987465832164\n",
      "The 4202 th iteration gives loss of 0.45478402739863455\n",
      "The 4203 th iteration gives loss of 0.45473819432758605\n",
      "The 4204 th iteration gives loss of 0.4546923754315041\n",
      "The 4205 th iteration gives loss of 0.454646570696729\n",
      "The 4206 th iteration gives loss of 0.45460078010962185\n",
      "The 4207 th iteration gives loss of 0.4545550036565703\n",
      "The 4208 th iteration gives loss of 0.45450924132398945\n",
      "The 4209 th iteration gives loss of 0.45446349309828155\n",
      "The 4210 th iteration gives loss of 0.4544177589659059\n",
      "The 4211 th iteration gives loss of 0.45437203891330363\n",
      "The 4212 th iteration gives loss of 0.45432633292696006\n",
      "The 4213 th iteration gives loss of 0.4542806409933747\n",
      "The 4214 th iteration gives loss of 0.4542349630990645\n",
      "The 4215 th iteration gives loss of 0.4541892992305616\n",
      "The 4216 th iteration gives loss of 0.45414364937441887\n",
      "The 4217 th iteration gives loss of 0.45409801351720674\n",
      "The 4218 th iteration gives loss of 0.45405239164552164\n",
      "The 4219 th iteration gives loss of 0.4540067837459668\n",
      "The 4220 th iteration gives loss of 0.45396118980517297\n",
      "The 4221 th iteration gives loss of 0.4539156098097839\n",
      "The 4222 th iteration gives loss of 0.4538700437464671\n",
      "The 4223 th iteration gives loss of 0.4538244916019\n",
      "The 4224 th iteration gives loss of 0.4537789533627917\n",
      "The 4225 th iteration gives loss of 0.45373342901586433\n",
      "The 4226 th iteration gives loss of 0.45368791854785195\n",
      "The 4227 th iteration gives loss of 0.453642421945509\n",
      "The 4228 th iteration gives loss of 0.4535969391956169\n",
      "The 4229 th iteration gives loss of 0.45355147028496634\n",
      "The 4230 th iteration gives loss of 0.45350601520037553\n",
      "The 4231 th iteration gives loss of 0.4534605739286728\n",
      "The 4232 th iteration gives loss of 0.45341514645670433\n",
      "The 4233 th iteration gives loss of 0.45336973277132286\n",
      "The 4234 th iteration gives loss of 0.4533243328594331\n",
      "The 4235 th iteration gives loss of 0.453278946707927\n",
      "The 4236 th iteration gives loss of 0.45323357430374583\n",
      "The 4237 th iteration gives loss of 0.4531882156338106\n",
      "The 4238 th iteration gives loss of 0.45314287068508646\n",
      "The 4239 th iteration gives loss of 0.4530975394445466\n",
      "The 4240 th iteration gives loss of 0.453052221899186\n",
      "The 4241 th iteration gives loss of 0.45300691803601795\n",
      "The 4242 th iteration gives loss of 0.4529616278420805\n",
      "The 4243 th iteration gives loss of 0.45291635130439173\n",
      "The 4244 th iteration gives loss of 0.4528710884100623\n",
      "The 4245 th iteration gives loss of 0.4528258391461415\n",
      "The 4246 th iteration gives loss of 0.4527806034997491\n",
      "The 4247 th iteration gives loss of 0.4527353814579888\n",
      "The 4248 th iteration gives loss of 0.45269017300802145\n",
      "The 4249 th iteration gives loss of 0.452644978136984\n",
      "The 4250 th iteration gives loss of 0.4525997968320486\n",
      "The 4251 th iteration gives loss of 0.4525546290804201\n",
      "The 4252 th iteration gives loss of 0.45250947486930215\n",
      "The 4253 th iteration gives loss of 0.4524643341859126\n",
      "The 4254 th iteration gives loss of 0.452419207017507\n",
      "The 4255 th iteration gives loss of 0.4523740933513416\n",
      "The 4256 th iteration gives loss of 0.4523289931746961\n",
      "The 4257 th iteration gives loss of 0.45228390647487804\n",
      "The 4258 th iteration gives loss of 0.4522388332391846\n",
      "The 4259 th iteration gives loss of 0.452193773454963\n",
      "The 4260 th iteration gives loss of 0.4521487271095419\n",
      "The 4261 th iteration gives loss of 0.45210369419031377\n",
      "The 4262 th iteration gives loss of 0.4520586746846544\n",
      "The 4263 th iteration gives loss of 0.4520136685799743\n",
      "The 4264 th iteration gives loss of 0.45196867586367195\n",
      "The 4265 th iteration gives loss of 0.4519236965232058\n",
      "The 4266 th iteration gives loss of 0.45187873054602234\n",
      "The 4267 th iteration gives loss of 0.45183377791959306\n",
      "The 4268 th iteration gives loss of 0.4517888386313954\n",
      "The 4269 th iteration gives loss of 0.4517439126689568\n",
      "The 4270 th iteration gives loss of 0.4516990000197913\n",
      "The 4271 th iteration gives loss of 0.4516541006714525\n",
      "The 4272 th iteration gives loss of 0.4516092146114761\n",
      "The 4273 th iteration gives loss of 0.45156434182746036\n",
      "The 4274 th iteration gives loss of 0.4515194823069856\n",
      "The 4275 th iteration gives loss of 0.45147463603766325\n",
      "The 4276 th iteration gives loss of 0.45142980300711727\n",
      "The 4277 th iteration gives loss of 0.45138498320300613\n",
      "The 4278 th iteration gives loss of 0.4513401766129825\n",
      "The 4279 th iteration gives loss of 0.45129538322471363\n",
      "The 4280 th iteration gives loss of 0.4512506030259177\n",
      "The 4281 th iteration gives loss of 0.4512058360042919\n",
      "The 4282 th iteration gives loss of 0.45116108214756095\n",
      "The 4283 th iteration gives loss of 0.45111634144348733\n",
      "The 4284 th iteration gives loss of 0.4510716138798269\n",
      "The 4285 th iteration gives loss of 0.45102689944436225\n",
      "The 4286 th iteration gives loss of 0.45098219812488866\n",
      "The 4287 th iteration gives loss of 0.4509375099092236\n",
      "The 4288 th iteration gives loss of 0.4508928347851886\n",
      "The 4289 th iteration gives loss of 0.4508481727406492\n",
      "The 4290 th iteration gives loss of 0.4508035237634627\n",
      "The 4291 th iteration gives loss of 0.45075888784149687\n",
      "The 4292 th iteration gives loss of 0.4507142649626638\n",
      "The 4293 th iteration gives loss of 0.45066965511487095\n",
      "The 4294 th iteration gives loss of 0.45062505828605615\n",
      "The 4295 th iteration gives loss of 0.45058047446416616\n",
      "The 4296 th iteration gives loss of 0.4505359036371739\n",
      "The 4297 th iteration gives loss of 0.4504913457930518\n",
      "The 4298 th iteration gives loss of 0.45044680091979933\n",
      "The 4299 th iteration gives loss of 0.45040226900543023\n",
      "The 4300 th iteration gives loss of 0.4503577500379798\n",
      "The 4301 th iteration gives loss of 0.4503132440054899\n",
      "The 4302 th iteration gives loss of 0.45026875089602236\n",
      "The 4303 th iteration gives loss of 0.450224270697673\n",
      "The 4304 th iteration gives loss of 0.4501798033985248\n",
      "The 4305 th iteration gives loss of 0.450135348986702\n",
      "The 4306 th iteration gives loss of 0.4500909074503368\n",
      "The 4307 th iteration gives loss of 0.45004647877756954\n",
      "The 4308 th iteration gives loss of 0.4500020629565598\n",
      "The 4309 th iteration gives loss of 0.4499576599754854\n",
      "The 4310 th iteration gives loss of 0.44991326982254964\n",
      "The 4311 th iteration gives loss of 0.44986889248596285\n",
      "The 4312 th iteration gives loss of 0.44982452795395284\n",
      "The 4313 th iteration gives loss of 0.44978017621476307\n",
      "The 4314 th iteration gives loss of 0.4497358372566546\n",
      "The 4315 th iteration gives loss of 0.4496915110678982\n",
      "The 4316 th iteration gives loss of 0.4496471976368027\n",
      "The 4317 th iteration gives loss of 0.449602896951666\n",
      "The 4318 th iteration gives loss of 0.4495586090008189\n",
      "The 4319 th iteration gives loss of 0.44951433377259753\n",
      "The 4320 th iteration gives loss of 0.44947007125535393\n",
      "The 4321 th iteration gives loss of 0.4494258214374808\n",
      "The 4322 th iteration gives loss of 0.44938158430735586\n",
      "The 4323 th iteration gives loss of 0.44933735985337975\n",
      "The 4324 th iteration gives loss of 0.4492931480639829\n",
      "The 4325 th iteration gives loss of 0.44924894892760314\n",
      "The 4326 th iteration gives loss of 0.4492047624326907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 4327 th iteration gives loss of 0.44916058856771607\n",
      "The 4328 th iteration gives loss of 0.4491164273211567\n",
      "The 4329 th iteration gives loss of 0.4490722786815242\n",
      "The 4330 th iteration gives loss of 0.4490281426373412\n",
      "The 4331 th iteration gives loss of 0.44898401917712355\n",
      "The 4332 th iteration gives loss of 0.4489399082894402\n",
      "The 4333 th iteration gives loss of 0.4488958099628295\n",
      "The 4334 th iteration gives loss of 0.44885172418590447\n",
      "The 4335 th iteration gives loss of 0.4488076509472312\n",
      "The 4336 th iteration gives loss of 0.44876359023543877\n",
      "The 4337 th iteration gives loss of 0.44871954203914866\n",
      "The 4338 th iteration gives loss of 0.44867550634700776\n",
      "The 4339 th iteration gives loss of 0.4486314831476741\n",
      "The 4340 th iteration gives loss of 0.4485874724298184\n",
      "The 4341 th iteration gives loss of 0.44854347418212626\n",
      "The 4342 th iteration gives loss of 0.4484994883933282\n",
      "The 4343 th iteration gives loss of 0.4484555150521146\n",
      "The 4344 th iteration gives loss of 0.4484115541472404\n",
      "The 4345 th iteration gives loss of 0.44836760566744505\n",
      "The 4346 th iteration gives loss of 0.44832366960151265\n",
      "The 4347 th iteration gives loss of 0.44827974593820435\n",
      "The 4348 th iteration gives loss of 0.44823583466633904\n",
      "The 4349 th iteration gives loss of 0.44819193577472366\n",
      "The 4350 th iteration gives loss of 0.4481480492521976\n",
      "The 4351 th iteration gives loss of 0.4481041750875913\n",
      "The 4352 th iteration gives loss of 0.4480603132697756\n",
      "The 4353 th iteration gives loss of 0.4480164637876056\n",
      "The 4354 th iteration gives loss of 0.4479726266299871\n",
      "The 4355 th iteration gives loss of 0.44792880178583433\n",
      "The 4356 th iteration gives loss of 0.44788498924405185\n",
      "The 4357 th iteration gives loss of 0.4478411889935865\n",
      "The 4358 th iteration gives loss of 0.44779740102339444\n",
      "The 4359 th iteration gives loss of 0.44775362532244506\n",
      "The 4360 th iteration gives loss of 0.44770986187970446\n",
      "The 4361 th iteration gives loss of 0.44766611068417705\n",
      "The 4362 th iteration gives loss of 0.4476223717248709\n",
      "The 4363 th iteration gives loss of 0.4475786449908356\n",
      "The 4364 th iteration gives loss of 0.4475349304710871\n",
      "The 4365 th iteration gives loss of 0.44749122815469233\n",
      "The 4366 th iteration gives loss of 0.44744753803072773\n",
      "The 4367 th iteration gives loss of 0.44740386008828587\n",
      "The 4368 th iteration gives loss of 0.4473601943164531\n",
      "The 4369 th iteration gives loss of 0.4473165407043595\n",
      "The 4370 th iteration gives loss of 0.447272899241139\n",
      "The 4371 th iteration gives loss of 0.44722926991593015\n",
      "The 4372 th iteration gives loss of 0.44718565271790456\n",
      "The 4373 th iteration gives loss of 0.44714204763624044\n",
      "The 4374 th iteration gives loss of 0.44709845466013143\n",
      "The 4375 th iteration gives loss of 0.44705487377876946\n",
      "The 4376 th iteration gives loss of 0.4470113049813914\n",
      "The 4377 th iteration gives loss of 0.4469677482572284\n",
      "The 4378 th iteration gives loss of 0.4469242035955416\n",
      "The 4379 th iteration gives loss of 0.44688067098558737\n",
      "The 4380 th iteration gives loss of 0.4468371504166434\n",
      "The 4381 th iteration gives loss of 0.44679364187801424\n",
      "The 4382 th iteration gives loss of 0.4467501453590186\n",
      "The 4383 th iteration gives loss of 0.44670666084896515\n",
      "The 4384 th iteration gives loss of 0.44666318833720403\n",
      "The 4385 th iteration gives loss of 0.44661972781307985\n",
      "The 4386 th iteration gives loss of 0.4465762792659718\n",
      "The 4387 th iteration gives loss of 0.44653284268525456\n",
      "The 4388 th iteration gives loss of 0.44648941806034537\n",
      "The 4389 th iteration gives loss of 0.44644600538063994\n",
      "The 4390 th iteration gives loss of 0.4464026046355806\n",
      "The 4391 th iteration gives loss of 0.4463592158145856\n",
      "The 4392 th iteration gives loss of 0.4463158389071331\n",
      "The 4393 th iteration gives loss of 0.4462724739026737\n",
      "The 4394 th iteration gives loss of 0.4462291207907093\n",
      "The 4395 th iteration gives loss of 0.4461857795607547\n",
      "The 4396 th iteration gives loss of 0.44614245020228543\n",
      "The 4397 th iteration gives loss of 0.4460991327048601\n",
      "The 4398 th iteration gives loss of 0.4460558270580069\n",
      "The 4399 th iteration gives loss of 0.44601253325129614\n",
      "The 4400 th iteration gives loss of 0.4459692512742846\n",
      "The 4401 th iteration gives loss of 0.4459259811165646\n",
      "The 4402 th iteration gives loss of 0.4458827227677381\n",
      "The 4403 th iteration gives loss of 0.4458394762174262\n",
      "The 4404 th iteration gives loss of 0.4457962414552583\n",
      "The 4405 th iteration gives loss of 0.445753018470865\n",
      "The 4406 th iteration gives loss of 0.4457098072539057\n",
      "The 4407 th iteration gives loss of 0.44566660779405787\n",
      "The 4408 th iteration gives loss of 0.44562342008099914\n",
      "The 4409 th iteration gives loss of 0.4455802441044354\n",
      "The 4410 th iteration gives loss of 0.44553707985408064\n",
      "The 4411 th iteration gives loss of 0.4454939273196689\n",
      "The 4412 th iteration gives loss of 0.44545078649092407\n",
      "The 4413 th iteration gives loss of 0.4454076573576181\n",
      "The 4414 th iteration gives loss of 0.44536453990951463\n",
      "The 4415 th iteration gives loss of 0.44532143413639635\n",
      "The 4416 th iteration gives loss of 0.44527834002807365\n",
      "The 4417 th iteration gives loss of 0.4452352575743537\n",
      "The 4418 th iteration gives loss of 0.4451921867650522\n",
      "The 4419 th iteration gives loss of 0.445149127590004\n",
      "The 4420 th iteration gives loss of 0.4451060800390868\n",
      "The 4421 th iteration gives loss of 0.44506304410216035\n",
      "The 4422 th iteration gives loss of 0.44502001976909866\n",
      "The 4423 th iteration gives loss of 0.4449770070298079\n",
      "The 4424 th iteration gives loss of 0.44493400587417964\n",
      "The 4425 th iteration gives loss of 0.4448910162921559\n",
      "The 4426 th iteration gives loss of 0.4448480382736612\n",
      "The 4427 th iteration gives loss of 0.4448050718086499\n",
      "The 4428 th iteration gives loss of 0.4447621168871001\n",
      "The 4429 th iteration gives loss of 0.44471917349895984\n",
      "The 4430 th iteration gives loss of 0.44467624163425695\n",
      "The 4431 th iteration gives loss of 0.44463332128296396\n",
      "The 4432 th iteration gives loss of 0.4445904124351222\n",
      "The 4433 th iteration gives loss of 0.4445475150807515\n",
      "The 4434 th iteration gives loss of 0.44450462920991923\n",
      "The 4435 th iteration gives loss of 0.4444617548126613\n",
      "The 4436 th iteration gives loss of 0.44441889187907224\n",
      "The 4437 th iteration gives loss of 0.44437604039921386\n",
      "The 4438 th iteration gives loss of 0.44433320036322077\n",
      "The 4439 th iteration gives loss of 0.44429037176117303\n",
      "The 4440 th iteration gives loss of 0.44424755458322407\n",
      "The 4441 th iteration gives loss of 0.4442047488195012\n",
      "The 4442 th iteration gives loss of 0.44416195446017664\n",
      "The 4443 th iteration gives loss of 0.44411917149539937\n",
      "The 4444 th iteration gives loss of 0.44407639991536035\n",
      "The 4445 th iteration gives loss of 0.44403363971024234\n",
      "The 4446 th iteration gives loss of 0.4439908908702868\n",
      "The 4447 th iteration gives loss of 0.44394815338568805\n",
      "The 4448 th iteration gives loss of 0.44390542724667376\n",
      "The 4449 th iteration gives loss of 0.4438627124435226\n",
      "The 4450 th iteration gives loss of 0.4438200089664724\n",
      "The 4451 th iteration gives loss of 0.4437773168058175\n",
      "The 4452 th iteration gives loss of 0.4437346359518234\n",
      "The 4453 th iteration gives loss of 0.4436919663948257\n",
      "The 4454 th iteration gives loss of 0.44364930812510245\n",
      "The 4455 th iteration gives loss of 0.4436066611330089\n",
      "The 4456 th iteration gives loss of 0.4435640254088788\n",
      "The 4457 th iteration gives loss of 0.4435214009430615\n",
      "The 4458 th iteration gives loss of 0.44347878772592314\n",
      "The 4459 th iteration gives loss of 0.44343618574785465\n",
      "The 4460 th iteration gives loss of 0.4433935949992482\n",
      "The 4461 th iteration gives loss of 0.4433510154705095\n",
      "The 4462 th iteration gives loss of 0.4433084471520604\n",
      "The 4463 th iteration gives loss of 0.4432658900343421\n",
      "The 4464 th iteration gives loss of 0.44322334410778425\n",
      "The 4465 th iteration gives loss of 0.4431808093628561\n",
      "The 4466 th iteration gives loss of 0.443138285790029\n",
      "The 4467 th iteration gives loss of 0.44309577337979233\n",
      "The 4468 th iteration gives loss of 0.44305327212264833\n",
      "The 4469 th iteration gives loss of 0.44301078200909294\n",
      "The 4470 th iteration gives loss of 0.44296830302966367\n",
      "The 4471 th iteration gives loss of 0.4429258351748934\n",
      "The 4472 th iteration gives loss of 0.44288337843534364\n",
      "The 4473 th iteration gives loss of 0.44284093280155934\n",
      "The 4474 th iteration gives loss of 0.4427984982641294\n",
      "The 4475 th iteration gives loss of 0.4427560748136423\n",
      "The 4476 th iteration gives loss of 0.44271366244068494\n",
      "The 4477 th iteration gives loss of 0.44267126113589883\n",
      "The 4478 th iteration gives loss of 0.4426288708898887\n",
      "The 4479 th iteration gives loss of 0.44258649169330383\n",
      "The 4480 th iteration gives loss of 0.4425441235368076\n",
      "The 4481 th iteration gives loss of 0.4425017664110398\n",
      "The 4482 th iteration gives loss of 0.4424594203066998\n",
      "The 4483 th iteration gives loss of 0.44241708521448164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 4484 th iteration gives loss of 0.4423747611250751\n",
      "The 4485 th iteration gives loss of 0.44233244802919475\n",
      "The 4486 th iteration gives loss of 0.4422901459175871\n",
      "The 4487 th iteration gives loss of 0.44224785478098533\n",
      "The 4488 th iteration gives loss of 0.4422055746101421\n",
      "The 4489 th iteration gives loss of 0.4421633053958242\n",
      "The 4490 th iteration gives loss of 0.44212104712881223\n",
      "The 4491 th iteration gives loss of 0.4420787997999017\n",
      "The 4492 th iteration gives loss of 0.4420365633998942\n",
      "The 4493 th iteration gives loss of 0.4419943379196055\n",
      "The 4494 th iteration gives loss of 0.4419521233498795\n",
      "The 4495 th iteration gives loss of 0.44190991968153515\n",
      "The 4496 th iteration gives loss of 0.44186772690543974\n",
      "The 4497 th iteration gives loss of 0.4418255450124618\n",
      "The 4498 th iteration gives loss of 0.4417833739934707\n",
      "The 4499 th iteration gives loss of 0.4417412138393718\n",
      "The 4500 th iteration gives loss of 0.441699064541069\n",
      "The 4501 th iteration gives loss of 0.4416569260894724\n",
      "The 4502 th iteration gives loss of 0.4416147984755087\n",
      "The 4503 th iteration gives loss of 0.44157268169012526\n",
      "The 4504 th iteration gives loss of 0.44153057572428184\n",
      "The 4505 th iteration gives loss of 0.44148848056893103\n",
      "The 4506 th iteration gives loss of 0.4414463962150619\n",
      "The 4507 th iteration gives loss of 0.44140432265365187\n",
      "The 4508 th iteration gives loss of 0.44136225987572997\n",
      "The 4509 th iteration gives loss of 0.4413202078722837\n",
      "The 4510 th iteration gives loss of 0.4412781666343428\n",
      "The 4511 th iteration gives loss of 0.4412361361529576\n",
      "The 4512 th iteration gives loss of 0.44119411641919265\n",
      "The 4513 th iteration gives loss of 0.4411521074240862\n",
      "The 4514 th iteration gives loss of 0.44111010915872806\n",
      "The 4515 th iteration gives loss of 0.44106812161419895\n",
      "The 4516 th iteration gives loss of 0.4410261447816124\n",
      "The 4517 th iteration gives loss of 0.440984178652062\n",
      "The 4518 th iteration gives loss of 0.4409422232166753\n",
      "The 4519 th iteration gives loss of 0.4409002784666112\n",
      "The 4520 th iteration gives loss of 0.4408583443929982\n",
      "The 4521 th iteration gives loss of 0.44081642098700347\n",
      "The 4522 th iteration gives loss of 0.44077450823980224\n",
      "The 4523 th iteration gives loss of 0.4407326061425655\n",
      "The 4524 th iteration gives loss of 0.44069071468650545\n",
      "The 4525 th iteration gives loss of 0.4406488338628175\n",
      "The 4526 th iteration gives loss of 0.44060696366273255\n",
      "The 4527 th iteration gives loss of 0.44056510407747684\n",
      "The 4528 th iteration gives loss of 0.44052325509830637\n",
      "The 4529 th iteration gives loss of 0.4404814167164633\n",
      "The 4530 th iteration gives loss of 0.4404395889232182\n",
      "The 4531 th iteration gives loss of 0.4403977717098532\n",
      "The 4532 th iteration gives loss of 0.44035596506766567\n",
      "The 4533 th iteration gives loss of 0.44031416898795933\n",
      "The 4534 th iteration gives loss of 0.4402723834620399\n",
      "The 4535 th iteration gives loss of 0.44023060848123824\n",
      "The 4536 th iteration gives loss of 0.44018884403689434\n",
      "The 4537 th iteration gives loss of 0.4401470901203554\n",
      "The 4538 th iteration gives loss of 0.44010534672299106\n",
      "The 4539 th iteration gives loss of 0.44006361383615844\n",
      "The 4540 th iteration gives loss of 0.4400218914512722\n",
      "The 4541 th iteration gives loss of 0.43998017955970997\n",
      "The 4542 th iteration gives loss of 0.43993847815288445\n",
      "The 4543 th iteration gives loss of 0.4398967872222192\n",
      "The 4544 th iteration gives loss of 0.43985510675914397\n",
      "The 4545 th iteration gives loss of 0.4398134367550963\n",
      "The 4546 th iteration gives loss of 0.43977177720154764\n",
      "The 4547 th iteration gives loss of 0.43973012808995815\n",
      "The 4548 th iteration gives loss of 0.43968848941180627\n",
      "The 4549 th iteration gives loss of 0.4396468611585747\n",
      "The 4550 th iteration gives loss of 0.4396052433217751\n",
      "The 4551 th iteration gives loss of 0.43956363589290803\n",
      "The 4552 th iteration gives loss of 0.43952203886352387\n",
      "The 4553 th iteration gives loss of 0.43948045222513527\n",
      "The 4554 th iteration gives loss of 0.4394388759692926\n",
      "The 4555 th iteration gives loss of 0.43939731008757604\n",
      "The 4556 th iteration gives loss of 0.43935575457152826\n",
      "The 4557 th iteration gives loss of 0.43931420941274635\n",
      "The 4558 th iteration gives loss of 0.4392726746028171\n",
      "The 4559 th iteration gives loss of 0.4392311501333603\n",
      "The 4560 th iteration gives loss of 0.439189635995977\n",
      "The 4561 th iteration gives loss of 0.4391481321823125\n",
      "The 4562 th iteration gives loss of 0.43910663868398175\n",
      "The 4563 th iteration gives loss of 0.43906515549262926\n",
      "The 4564 th iteration gives loss of 0.4390236825999485\n",
      "The 4565 th iteration gives loss of 0.4389822199975989\n",
      "The 4566 th iteration gives loss of 0.43894076767726187\n",
      "The 4567 th iteration gives loss of 0.43889932563063316\n",
      "The 4568 th iteration gives loss of 0.438857893849414\n",
      "The 4569 th iteration gives loss of 0.43881647232534415\n",
      "The 4570 th iteration gives loss of 0.4387750610501281\n",
      "The 4571 th iteration gives loss of 0.4387336600155234\n",
      "The 4572 th iteration gives loss of 0.43869226921325877\n",
      "The 4573 th iteration gives loss of 0.43865088863511736\n",
      "The 4574 th iteration gives loss of 0.4386095182728671\n",
      "The 4575 th iteration gives loss of 0.4385681581183033\n",
      "The 4576 th iteration gives loss of 0.4385268081631949\n",
      "The 4577 th iteration gives loss of 0.43848546839936553\n",
      "The 4578 th iteration gives loss of 0.4384441388186309\n",
      "The 4579 th iteration gives loss of 0.4384028194128344\n",
      "The 4580 th iteration gives loss of 0.4383615101738031\n",
      "The 4581 th iteration gives loss of 0.438320211093382\n",
      "The 4582 th iteration gives loss of 0.43827892216344067\n",
      "The 4583 th iteration gives loss of 0.43823764337585364\n",
      "The 4584 th iteration gives loss of 0.4381963747224991\n",
      "The 4585 th iteration gives loss of 0.4381551161952895\n",
      "The 4586 th iteration gives loss of 0.4381138677861117\n",
      "The 4587 th iteration gives loss of 0.43807262948688086\n",
      "The 4588 th iteration gives loss of 0.4380314012895381\n",
      "The 4589 th iteration gives loss of 0.43799018318601923\n",
      "The 4590 th iteration gives loss of 0.43794897516826226\n",
      "The 4591 th iteration gives loss of 0.43790777722824675\n",
      "The 4592 th iteration gives loss of 0.4378665893579442\n",
      "The 4593 th iteration gives loss of 0.4378254115493126\n",
      "The 4594 th iteration gives loss of 0.4377842437943629\n",
      "The 4595 th iteration gives loss of 0.43774308608510093\n",
      "The 4596 th iteration gives loss of 0.43770193841353733\n",
      "The 4597 th iteration gives loss of 0.43766080077169744\n",
      "The 4598 th iteration gives loss of 0.4376196731516063\n",
      "The 4599 th iteration gives loss of 0.43757855554532443\n",
      "The 4600 th iteration gives loss of 0.437537447944913\n",
      "The 4601 th iteration gives loss of 0.43749635034242895\n",
      "The 4602 th iteration gives loss of 0.4374552627299653\n",
      "The 4603 th iteration gives loss of 0.4374141850995979\n",
      "The 4604 th iteration gives loss of 0.43737311744343715\n",
      "The 4605 th iteration gives loss of 0.43733205975358114\n",
      "The 4606 th iteration gives loss of 0.4372910120221644\n",
      "The 4607 th iteration gives loss of 0.43724997424131656\n",
      "The 4608 th iteration gives loss of 0.4372089464031808\n",
      "The 4609 th iteration gives loss of 0.4371679284999054\n",
      "The 4610 th iteration gives loss of 0.4371269205236521\n",
      "The 4611 th iteration gives loss of 0.4370859224666079\n",
      "The 4612 th iteration gives loss of 0.4370449343209371\n",
      "The 4613 th iteration gives loss of 0.43700395607886794\n",
      "The 4614 th iteration gives loss of 0.43696298773257897\n",
      "The 4615 th iteration gives loss of 0.4369220292742984\n",
      "The 4616 th iteration gives loss of 0.43688108069626336\n",
      "The 4617 th iteration gives loss of 0.4368401419906945\n",
      "The 4618 th iteration gives loss of 0.4367992131498359\n",
      "The 4619 th iteration gives loss of 0.43675829416596623\n",
      "The 4620 th iteration gives loss of 0.43671738503133917\n",
      "The 4621 th iteration gives loss of 0.43667648573823264\n",
      "The 4622 th iteration gives loss of 0.4366355962789486\n",
      "The 4623 th iteration gives loss of 0.4365947166457735\n",
      "The 4624 th iteration gives loss of 0.4365538468310455\n",
      "The 4625 th iteration gives loss of 0.43651298682705136\n",
      "The 4626 th iteration gives loss of 0.43647213662614653\n",
      "The 4627 th iteration gives loss of 0.43643129622065674\n",
      "The 4628 th iteration gives loss of 0.43639046560295164\n",
      "The 4629 th iteration gives loss of 0.4363496447653667\n",
      "The 4630 th iteration gives loss of 0.4363088337002893\n",
      "The 4631 th iteration gives loss of 0.4362680324001032\n",
      "The 4632 th iteration gives loss of 0.4362272408572015\n",
      "The 4633 th iteration gives loss of 0.43618645906399156\n",
      "The 4634 th iteration gives loss of 0.4361456870128828\n",
      "The 4635 th iteration gives loss of 0.4361049246962967\n",
      "The 4636 th iteration gives loss of 0.43606417210666915\n",
      "The 4637 th iteration gives loss of 0.4360234292364396\n",
      "The 4638 th iteration gives loss of 0.4359826960780721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 4639 th iteration gives loss of 0.43594197262400863\n",
      "The 4640 th iteration gives loss of 0.4359012588667434\n",
      "The 4641 th iteration gives loss of 0.4358605547987639\n",
      "The 4642 th iteration gives loss of 0.43581986041255294\n",
      "The 4643 th iteration gives loss of 0.43577917570061936\n",
      "The 4644 th iteration gives loss of 0.4357385006554808\n",
      "The 4645 th iteration gives loss of 0.43569783526965467\n",
      "The 4646 th iteration gives loss of 0.4356571795356869\n",
      "The 4647 th iteration gives loss of 0.43561653344612433\n",
      "The 4648 th iteration gives loss of 0.4355758969934954\n",
      "The 4649 th iteration gives loss of 0.43553527017038535\n",
      "The 4650 th iteration gives loss of 0.43549465296937523\n",
      "The 4651 th iteration gives loss of 0.43545404538303456\n",
      "The 4652 th iteration gives loss of 0.4354134474039683\n",
      "The 4653 th iteration gives loss of 0.43537285902477485\n",
      "The 4654 th iteration gives loss of 0.4353322802380721\n",
      "The 4655 th iteration gives loss of 0.43529171103647957\n",
      "The 4656 th iteration gives loss of 0.4352511514126415\n",
      "The 4657 th iteration gives loss of 0.43521060135919837\n",
      "The 4658 th iteration gives loss of 0.43517006086879584\n",
      "The 4659 th iteration gives loss of 0.43512952993411763\n",
      "The 4660 th iteration gives loss of 0.43508900854781213\n",
      "The 4661 th iteration gives loss of 0.43504849670257884\n",
      "The 4662 th iteration gives loss of 0.4350079943911041\n",
      "The 4663 th iteration gives loss of 0.43496750160610076\n",
      "The 4664 th iteration gives loss of 0.4349270183402809\n",
      "The 4665 th iteration gives loss of 0.43488654458635434\n",
      "The 4666 th iteration gives loss of 0.43484608033705474\n",
      "The 4667 th iteration gives loss of 0.4348056255851506\n",
      "The 4668 th iteration gives loss of 0.4347651803233564\n",
      "The 4669 th iteration gives loss of 0.43472474454446347\n",
      "The 4670 th iteration gives loss of 0.43468431824122866\n",
      "The 4671 th iteration gives loss of 0.4346439014064237\n",
      "The 4672 th iteration gives loss of 0.4346034940328753\n",
      "The 4673 th iteration gives loss of 0.43456309611335636\n",
      "The 4674 th iteration gives loss of 0.43452270764066775\n",
      "The 4675 th iteration gives loss of 0.43448232860764835\n",
      "The 4676 th iteration gives loss of 0.43444195900712346\n",
      "The 4677 th iteration gives loss of 0.4344015988319289\n",
      "The 4678 th iteration gives loss of 0.4343612480749131\n",
      "The 4679 th iteration gives loss of 0.43432090672894175\n",
      "The 4680 th iteration gives loss of 0.43428057478687804\n",
      "The 4681 th iteration gives loss of 0.43424025224159757\n",
      "The 4682 th iteration gives loss of 0.43419993908597354\n",
      "The 4683 th iteration gives loss of 0.4341596353129257\n",
      "The 4684 th iteration gives loss of 0.4341193409153424\n",
      "The 4685 th iteration gives loss of 0.4340790558861584\n",
      "The 4686 th iteration gives loss of 0.434038780218279\n",
      "The 4687 th iteration gives loss of 0.4339985139046405\n",
      "The 4688 th iteration gives loss of 0.4339582569382049\n",
      "The 4689 th iteration gives loss of 0.43391800931190444\n",
      "The 4690 th iteration gives loss of 0.4338777710187115\n",
      "The 4691 th iteration gives loss of 0.43383754205158626\n",
      "The 4692 th iteration gives loss of 0.4337973224035279\n",
      "The 4693 th iteration gives loss of 0.43375711206751333\n",
      "The 4694 th iteration gives loss of 0.4337169110365563\n",
      "The 4695 th iteration gives loss of 0.4336767193036607\n",
      "The 4696 th iteration gives loss of 0.4336365368618329\n",
      "The 4697 th iteration gives loss of 0.4335963637041044\n",
      "The 4698 th iteration gives loss of 0.4335561998235298\n",
      "The 4699 th iteration gives loss of 0.43351604521314613\n",
      "The 4700 th iteration gives loss of 0.43347589986599694\n",
      "The 4701 th iteration gives loss of 0.4334357637751553\n",
      "The 4702 th iteration gives loss of 0.4333956369337112\n",
      "The 4703 th iteration gives loss of 0.43335551933472416\n",
      "The 4704 th iteration gives loss of 0.43331541097131343\n",
      "The 4705 th iteration gives loss of 0.43327531183654966\n",
      "The 4706 th iteration gives loss of 0.43323522192356023\n",
      "The 4707 th iteration gives loss of 0.43319514122546693\n",
      "The 4708 th iteration gives loss of 0.4331550697354116\n",
      "The 4709 th iteration gives loss of 0.43311500744651393\n",
      "The 4710 th iteration gives loss of 0.43307495435192767\n",
      "The 4711 th iteration gives loss of 0.4330349104448141\n",
      "The 4712 th iteration gives loss of 0.43299487571832446\n",
      "The 4713 th iteration gives loss of 0.43295485016566065\n",
      "The 4714 th iteration gives loss of 0.4329148337799739\n",
      "The 4715 th iteration gives loss of 0.4328748265544888\n",
      "The 4716 th iteration gives loss of 0.43283482848239924\n",
      "The 4717 th iteration gives loss of 0.43279483955690695\n",
      "The 4718 th iteration gives loss of 0.43275485977124456\n",
      "The 4719 th iteration gives loss of 0.43271488911863193\n",
      "The 4720 th iteration gives loss of 0.43267492759231657\n",
      "The 4721 th iteration gives loss of 0.43263497518554517\n",
      "The 4722 th iteration gives loss of 0.43259503189156645\n",
      "The 4723 th iteration gives loss of 0.43255509770364636\n",
      "The 4724 th iteration gives loss of 0.4325151726150729\n",
      "The 4725 th iteration gives loss of 0.4324752566191173\n",
      "The 4726 th iteration gives loss of 0.4324353497090802\n",
      "The 4727 th iteration gives loss of 0.43239545187826284\n",
      "The 4728 th iteration gives loss of 0.43235556311995715\n",
      "The 4729 th iteration gives loss of 0.43231568342751886\n",
      "The 4730 th iteration gives loss of 0.4322758127942441\n",
      "The 4731 th iteration gives loss of 0.43223595121348585\n",
      "The 4732 th iteration gives loss of 0.4321960986785879\n",
      "The 4733 th iteration gives loss of 0.4321562551829117\n",
      "The 4734 th iteration gives loss of 0.43211642071979467\n",
      "The 4735 th iteration gives loss of 0.43207659528263553\n",
      "The 4736 th iteration gives loss of 0.43203677886480907\n",
      "The 4737 th iteration gives loss of 0.4319969714597155\n",
      "The 4738 th iteration gives loss of 0.4319571730607339\n",
      "The 4739 th iteration gives loss of 0.4319173836612711\n",
      "The 4740 th iteration gives loss of 0.431877603254769\n",
      "The 4741 th iteration gives loss of 0.4318378318346297\n",
      "The 4742 th iteration gives loss of 0.4317980693943003\n",
      "The 4743 th iteration gives loss of 0.4317583159272152\n",
      "The 4744 th iteration gives loss of 0.43171857142683384\n",
      "The 4745 th iteration gives loss of 0.43167883588660805\n",
      "The 4746 th iteration gives loss of 0.4316391093000045\n",
      "The 4747 th iteration gives loss of 0.4315993916605169\n",
      "The 4748 th iteration gives loss of 0.4315596829616222\n",
      "The 4749 th iteration gives loss of 0.4315199831968138\n",
      "The 4750 th iteration gives loss of 0.4314802923595923\n",
      "The 4751 th iteration gives loss of 0.431440610443488\n",
      "The 4752 th iteration gives loss of 0.43140093744198776\n",
      "The 4753 th iteration gives loss of 0.4313612733486534\n",
      "The 4754 th iteration gives loss of 0.4313216181570114\n",
      "The 4755 th iteration gives loss of 0.4312819718606089\n",
      "The 4756 th iteration gives loss of 0.4312423344529897\n",
      "The 4757 th iteration gives loss of 0.43120270592773663\n",
      "The 4758 th iteration gives loss of 0.43116308627841693\n",
      "The 4759 th iteration gives loss of 0.43112347549860003\n",
      "The 4760 th iteration gives loss of 0.4310838735818851\n",
      "The 4761 th iteration gives loss of 0.4310442805218642\n",
      "The 4762 th iteration gives loss of 0.4310046963121584\n",
      "The 4763 th iteration gives loss of 0.43096512094636097\n",
      "The 4764 th iteration gives loss of 0.43092555441811636\n",
      "The 4765 th iteration gives loss of 0.4308859967210302\n",
      "The 4766 th iteration gives loss of 0.43084644784876314\n",
      "The 4767 th iteration gives loss of 0.43080690779495684\n",
      "The 4768 th iteration gives loss of 0.4307673765532798\n",
      "The 4769 th iteration gives loss of 0.4307278541173834\n",
      "The 4770 th iteration gives loss of 0.4306883404809448\n",
      "The 4771 th iteration gives loss of 0.43064883563763684\n",
      "The 4772 th iteration gives loss of 0.43060933958117625\n",
      "The 4773 th iteration gives loss of 0.43056985230524036\n",
      "The 4774 th iteration gives loss of 0.4305303738035415\n",
      "The 4775 th iteration gives loss of 0.43049090406979174\n",
      "The 4776 th iteration gives loss of 0.4304514430977231\n",
      "The 4777 th iteration gives loss of 0.430411990881067\n",
      "The 4778 th iteration gives loss of 0.43037254741355446\n",
      "The 4779 th iteration gives loss of 0.43033311268895885\n",
      "The 4780 th iteration gives loss of 0.43029368670100876\n",
      "The 4781 th iteration gives loss of 0.4302542694434676\n",
      "The 4782 th iteration gives loss of 0.43021486091012856\n",
      "The 4783 th iteration gives loss of 0.4301754610947753\n",
      "The 4784 th iteration gives loss of 0.4301360699911742\n",
      "The 4785 th iteration gives loss of 0.4300966875931528\n",
      "The 4786 th iteration gives loss of 0.43005731389450164\n",
      "The 4787 th iteration gives loss of 0.43001794888904166\n",
      "The 4788 th iteration gives loss of 0.42997859257057397\n",
      "The 4789 th iteration gives loss of 0.42993924493296193\n",
      "The 4790 th iteration gives loss of 0.42989990597003086\n",
      "The 4791 th iteration gives loss of 0.4298605756756217\n",
      "The 4792 th iteration gives loss of 0.4298212540435883\n",
      "The 4793 th iteration gives loss of 0.42978194106780526\n",
      "The 4794 th iteration gives loss of 0.4297426367421374\n",
      "The 4795 th iteration gives loss of 0.4297033410604752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 4796 th iteration gives loss of 0.4296640540166985\n",
      "The 4797 th iteration gives loss of 0.4296247756047034\n",
      "The 4798 th iteration gives loss of 0.4295855058183906\n",
      "The 4799 th iteration gives loss of 0.4295462446516831\n",
      "The 4800 th iteration gives loss of 0.42950699209847654\n",
      "The 4801 th iteration gives loss of 0.4294677481527282\n",
      "The 4802 th iteration gives loss of 0.4294285128083574\n",
      "The 4803 th iteration gives loss of 0.4293892860593256\n",
      "The 4804 th iteration gives loss of 0.4293500678995657\n",
      "The 4805 th iteration gives loss of 0.4293108583230433\n",
      "The 4806 th iteration gives loss of 0.42927165732372985\n",
      "The 4807 th iteration gives loss of 0.42923246489559364\n",
      "The 4808 th iteration gives loss of 0.4291932810326363\n",
      "The 4809 th iteration gives loss of 0.42915410572883833\n",
      "The 4810 th iteration gives loss of 0.4291149389781931\n",
      "The 4811 th iteration gives loss of 0.4290757807747239\n",
      "The 4812 th iteration gives loss of 0.4290366311124237\n",
      "The 4813 th iteration gives loss of 0.4289974899853271\n",
      "The 4814 th iteration gives loss of 0.4289583573874835\n",
      "The 4815 th iteration gives loss of 0.4289192333129099\n",
      "The 4816 th iteration gives loss of 0.42888011775567036\n",
      "The 4817 th iteration gives loss of 0.42884101070980707\n",
      "The 4818 th iteration gives loss of 0.4288019121693778\n",
      "The 4819 th iteration gives loss of 0.42876282212846734\n",
      "The 4820 th iteration gives loss of 0.4287237405811543\n",
      "The 4821 th iteration gives loss of 0.428684667521516\n",
      "The 4822 th iteration gives loss of 0.4286456029436523\n",
      "The 4823 th iteration gives loss of 0.42860654684165556\n",
      "The 4824 th iteration gives loss of 0.42856749920965037\n",
      "The 4825 th iteration gives loss of 0.4285284600417462\n",
      "The 4826 th iteration gives loss of 0.4284894293320615\n",
      "The 4827 th iteration gives loss of 0.4284504070747462\n",
      "The 4828 th iteration gives loss of 0.42841139326391914\n",
      "The 4829 th iteration gives loss of 0.42837238789375837\n",
      "The 4830 th iteration gives loss of 0.4283333909583988\n",
      "The 4831 th iteration gives loss of 0.42829440245199785\n",
      "The 4832 th iteration gives loss of 0.42825542236874786\n",
      "The 4833 th iteration gives loss of 0.4282164507028055\n",
      "The 4834 th iteration gives loss of 0.42817748744837186\n",
      "The 4835 th iteration gives loss of 0.42813853259964313\n",
      "The 4836 th iteration gives loss of 0.4280995861508241\n",
      "The 4837 th iteration gives loss of 0.4280606480961028\n",
      "The 4838 th iteration gives loss of 0.42802171842971704\n",
      "The 4839 th iteration gives loss of 0.42798279714588033\n",
      "The 4840 th iteration gives loss of 0.4279438842388443\n",
      "The 4841 th iteration gives loss of 0.4279049797028222\n",
      "The 4842 th iteration gives loss of 0.427866083532086\n",
      "The 4843 th iteration gives loss of 0.42782719572086825\n",
      "The 4844 th iteration gives loss of 0.4277883162634493\n",
      "The 4845 th iteration gives loss of 0.4277494451540936\n",
      "The 4846 th iteration gives loss of 0.42771058238707876\n",
      "The 4847 th iteration gives loss of 0.427671727956691\n",
      "The 4848 th iteration gives loss of 0.4276328818572131\n",
      "The 4849 th iteration gives loss of 0.42759404408297\n",
      "The 4850 th iteration gives loss of 0.4275552146282467\n",
      "The 4851 th iteration gives loss of 0.4275163934873582\n",
      "The 4852 th iteration gives loss of 0.42747758065463387\n",
      "The 4853 th iteration gives loss of 0.4274387761244104\n",
      "The 4854 th iteration gives loss of 0.42739997989101614\n",
      "The 4855 th iteration gives loss of 0.42736119194879635\n",
      "The 4856 th iteration gives loss of 0.42732241229212337\n",
      "The 4857 th iteration gives loss of 0.42728364091532195\n",
      "The 4858 th iteration gives loss of 0.4272448778127911\n",
      "The 4859 th iteration gives loss of 0.42720612297887545\n",
      "The 4860 th iteration gives loss of 0.42716737640799124\n",
      "The 4861 th iteration gives loss of 0.42712863809450197\n",
      "The 4862 th iteration gives loss of 0.42708990803281427\n",
      "The 4863 th iteration gives loss of 0.42705118621732774\n",
      "The 4864 th iteration gives loss of 0.42701247264245507\n",
      "The 4865 th iteration gives loss of 0.4269737673026206\n",
      "The 4866 th iteration gives loss of 0.42693507019225385\n",
      "The 4867 th iteration gives loss of 0.4268963813057702\n",
      "The 4868 th iteration gives loss of 0.4268577006376184\n",
      "The 4869 th iteration gives loss of 0.42681902818226497\n",
      "The 4870 th iteration gives loss of 0.4267803639341417\n",
      "The 4871 th iteration gives loss of 0.4267417078877111\n",
      "The 4872 th iteration gives loss of 0.4267030600374489\n",
      "The 4873 th iteration gives loss of 0.42666442037785046\n",
      "The 4874 th iteration gives loss of 0.42662578890337616\n",
      "The 4875 th iteration gives loss of 0.426587165608511\n",
      "The 4876 th iteration gives loss of 0.42654855048777335\n",
      "The 4877 th iteration gives loss of 0.42650994353565747\n",
      "The 4878 th iteration gives loss of 0.4264713447466839\n",
      "The 4879 th iteration gives loss of 0.42643275411538273\n",
      "The 4880 th iteration gives loss of 0.4263941716362668\n",
      "The 4881 th iteration gives loss of 0.4263555973038605\n",
      "The 4882 th iteration gives loss of 0.4263170311127288\n",
      "The 4883 th iteration gives loss of 0.42627847305739885\n",
      "The 4884 th iteration gives loss of 0.42623992313244147\n",
      "The 4885 th iteration gives loss of 0.4262013813324165\n",
      "The 4886 th iteration gives loss of 0.42616284765189466\n",
      "The 4887 th iteration gives loss of 0.42612432208543743\n",
      "The 4888 th iteration gives loss of 0.42608580462765533\n",
      "The 4889 th iteration gives loss of 0.42604729527312446\n",
      "The 4890 th iteration gives loss of 0.42600879401644987\n",
      "The 4891 th iteration gives loss of 0.42597030085223303\n",
      "The 4892 th iteration gives loss of 0.42593181577508543\n",
      "The 4893 th iteration gives loss of 0.4258933387796201\n",
      "The 4894 th iteration gives loss of 0.4258548698604845\n",
      "The 4895 th iteration gives loss of 0.4258164090122947\n",
      "The 4896 th iteration gives loss of 0.42577795622969555\n",
      "The 4897 th iteration gives loss of 0.4257395115073285\n",
      "The 4898 th iteration gives loss of 0.42570107483985753\n",
      "The 4899 th iteration gives loss of 0.4256626462219494\n",
      "The 4900 th iteration gives loss of 0.4256242256482631\n",
      "The 4901 th iteration gives loss of 0.4255858131134672\n",
      "The 4902 th iteration gives loss of 0.4255474086122567\n",
      "The 4903 th iteration gives loss of 0.4255090121393136\n",
      "The 4904 th iteration gives loss of 0.4254706236893394\n",
      "The 4905 th iteration gives loss of 0.425432243257034\n",
      "The 4906 th iteration gives loss of 0.425393870837108\n",
      "The 4907 th iteration gives loss of 0.4253555064242843\n",
      "The 4908 th iteration gives loss of 0.42531715001327575\n",
      "The 4909 th iteration gives loss of 0.4252788015988227\n",
      "The 4910 th iteration gives loss of 0.4252404611756656\n",
      "The 4911 th iteration gives loss of 0.425202128738534\n",
      "The 4912 th iteration gives loss of 0.425163804282191\n",
      "The 4913 th iteration gives loss of 0.4251254878013839\n",
      "The 4914 th iteration gives loss of 0.4250871792908939\n",
      "The 4915 th iteration gives loss of 0.42504887874547287\n",
      "The 4916 th iteration gives loss of 0.4250105861599239\n",
      "The 4917 th iteration gives loss of 0.4249723015290236\n",
      "The 4918 th iteration gives loss of 0.4249340248475455\n",
      "The 4919 th iteration gives loss of 0.42489575611031405\n",
      "The 4920 th iteration gives loss of 0.42485749531210354\n",
      "The 4921 th iteration gives loss of 0.4248192424477696\n",
      "The 4922 th iteration gives loss of 0.42478099751209836\n",
      "The 4923 th iteration gives loss of 0.42474276049992904\n",
      "The 4924 th iteration gives loss of 0.4247045314060934\n",
      "The 4925 th iteration gives loss of 0.42466631022541373\n",
      "The 4926 th iteration gives loss of 0.42462809695276543\n",
      "The 4927 th iteration gives loss of 0.4245898915829856\n",
      "The 4928 th iteration gives loss of 0.42455169411093935\n",
      "The 4929 th iteration gives loss of 0.42451350453147035\n",
      "The 4930 th iteration gives loss of 0.42447532283947786\n",
      "The 4931 th iteration gives loss of 0.42443714902984486\n",
      "The 4932 th iteration gives loss of 0.42439898309744334\n",
      "The 4933 th iteration gives loss of 0.42436082503715455\n",
      "The 4934 th iteration gives loss of 0.42432267484390623\n",
      "The 4935 th iteration gives loss of 0.42428453251258164\n",
      "The 4936 th iteration gives loss of 0.4242463980381088\n",
      "The 4937 th iteration gives loss of 0.4242082714154058\n",
      "The 4938 th iteration gives loss of 0.42417015263938185\n",
      "The 4939 th iteration gives loss of 0.4241320417049816\n",
      "The 4940 th iteration gives loss of 0.42409393860715244\n",
      "The 4941 th iteration gives loss of 0.42405584334082236\n",
      "The 4942 th iteration gives loss of 0.42401775590095225\n",
      "The 4943 th iteration gives loss of 0.42397967628249866\n",
      "The 4944 th iteration gives loss of 0.4239416044804289\n",
      "The 4945 th iteration gives loss of 0.4239035404897192\n",
      "The 4946 th iteration gives loss of 0.4238654843053413\n",
      "The 4947 th iteration gives loss of 0.4238274359222798\n",
      "The 4948 th iteration gives loss of 0.42378939533552934\n",
      "The 4949 th iteration gives loss of 0.4237513625400876\n",
      "The 4950 th iteration gives loss of 0.4237133375309564\n",
      "The 4951 th iteration gives loss of 0.42367532030314514\n",
      "The 4952 th iteration gives loss of 0.42363731085166817\n",
      "The 4953 th iteration gives loss of 0.4235993091715717\n",
      "The 4954 th iteration gives loss of 0.42356131525785384\n",
      "The 4955 th iteration gives loss of 0.4235233291055813\n",
      "The 4956 th iteration gives loss of 0.4234853507097675\n",
      "The 4957 th iteration gives loss of 0.42344738006547883\n",
      "The 4958 th iteration gives loss of 0.42340941716777153\n",
      "The 4959 th iteration gives loss of 0.4233714620117029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 4960 th iteration gives loss of 0.4233335145923455\n",
      "The 4961 th iteration gives loss of 0.423295574904771\n",
      "The 4962 th iteration gives loss of 0.42325764294406726\n",
      "The 4963 th iteration gives loss of 0.42321971870530234\n",
      "The 4964 th iteration gives loss of 0.4231818021835913\n",
      "The 4965 th iteration gives loss of 0.42314389337403086\n",
      "The 4966 th iteration gives loss of 0.42310599227172363\n",
      "The 4967 th iteration gives loss of 0.42306809887177543\n",
      "The 4968 th iteration gives loss of 0.4230302131693224\n",
      "The 4969 th iteration gives loss of 0.4229923351594755\n",
      "The 4970 th iteration gives loss of 0.42295446483738103\n",
      "The 4971 th iteration gives loss of 0.4229166021981596\n",
      "The 4972 th iteration gives loss of 0.42287874723697727\n",
      "The 4973 th iteration gives loss of 0.422840899948962\n",
      "The 4974 th iteration gives loss of 0.4228030603292848\n",
      "The 4975 th iteration gives loss of 0.42276522837311153\n",
      "The 4976 th iteration gives loss of 0.42272740407559534\n",
      "The 4977 th iteration gives loss of 0.42268958743192664\n",
      "The 4978 th iteration gives loss of 0.42265177843728424\n",
      "The 4979 th iteration gives loss of 0.42261397708685244\n",
      "The 4980 th iteration gives loss of 0.4225761833758269\n",
      "The 4981 th iteration gives loss of 0.4225383972994117\n",
      "The 4982 th iteration gives loss of 0.4225006188528167\n",
      "The 4983 th iteration gives loss of 0.42246284803124406\n",
      "The 4984 th iteration gives loss of 0.42242508482992946\n",
      "The 4985 th iteration gives loss of 0.42238732924408434\n",
      "The 4986 th iteration gives loss of 0.42234958126893807\n",
      "The 4987 th iteration gives loss of 0.4223118408997432\n",
      "The 4988 th iteration gives loss of 0.42227410813172334\n",
      "The 4989 th iteration gives loss of 0.42223638296014554\n",
      "The 4990 th iteration gives loss of 0.4221986653802635\n",
      "The 4991 th iteration gives loss of 0.42216095538733106\n",
      "The 4992 th iteration gives loss of 0.4221232529766204\n",
      "The 4993 th iteration gives loss of 0.42208555814340615\n",
      "The 4994 th iteration gives loss of 0.4220478708829673\n",
      "The 4995 th iteration gives loss of 0.4220101911905973\n",
      "The 4996 th iteration gives loss of 0.4219725190615878\n",
      "The 4997 th iteration gives loss of 0.4219348544912242\n",
      "The 4998 th iteration gives loss of 0.4218971974748221\n",
      "The 4999 th iteration gives loss of 0.4218595480076821\n",
      "The 5000 th iteration gives loss of 0.4218219060851366\n",
      "The 5001 th iteration gives loss of 0.4217842717025031\n",
      "The 5002 th iteration gives loss of 0.42174664485510677\n",
      "The 5003 th iteration gives loss of 0.42170902553828804\n",
      "The 5004 th iteration gives loss of 0.4216714137473844\n",
      "The 5005 th iteration gives loss of 0.4216338094777403\n",
      "The 5006 th iteration gives loss of 0.4215962127247086\n",
      "The 5007 th iteration gives loss of 0.42155862348364387\n",
      "The 5008 th iteration gives loss of 0.42152104174991656\n",
      "The 5009 th iteration gives loss of 0.42148346751890736\n",
      "The 5010 th iteration gives loss of 0.42144590078597244\n",
      "The 5011 th iteration gives loss of 0.42140834154651513\n",
      "The 5012 th iteration gives loss of 0.4213707897959033\n",
      "The 5013 th iteration gives loss of 0.42133324552955576\n",
      "The 5014 th iteration gives loss of 0.42129570874283906\n",
      "The 5015 th iteration gives loss of 0.42125817943118293\n",
      "The 5016 th iteration gives loss of 0.4212206575899964\n",
      "The 5017 th iteration gives loss of 0.4211831432147068\n",
      "The 5018 th iteration gives loss of 0.42114563630072044\n",
      "The 5019 th iteration gives loss of 0.4211081368434677\n",
      "The 5020 th iteration gives loss of 0.42107064483840534\n",
      "The 5021 th iteration gives loss of 0.42103316028094484\n",
      "The 5022 th iteration gives loss of 0.42099568316655744\n",
      "The 5023 th iteration gives loss of 0.4209582134906846\n",
      "The 5024 th iteration gives loss of 0.42092075124880063\n",
      "The 5025 th iteration gives loss of 0.42088329643634603\n",
      "The 5026 th iteration gives loss of 0.4208458490488053\n",
      "The 5027 th iteration gives loss of 0.42080840908165107\n",
      "The 5028 th iteration gives loss of 0.42077097653036966\n",
      "The 5029 th iteration gives loss of 0.4207335513904426\n",
      "The 5030 th iteration gives loss of 0.42069613365737324\n",
      "The 5031 th iteration gives loss of 0.4206587233266501\n",
      "The 5032 th iteration gives loss of 0.42062132039379474\n",
      "The 5033 th iteration gives loss of 0.42058392485430474\n",
      "The 5034 th iteration gives loss of 0.42054653670370246\n",
      "The 5035 th iteration gives loss of 0.4205091559375005\n",
      "The 5036 th iteration gives loss of 0.4204717825512362\n",
      "The 5037 th iteration gives loss of 0.4204344165404416\n",
      "The 5038 th iteration gives loss of 0.42039705790065285\n",
      "The 5039 th iteration gives loss of 0.42035970662741956\n",
      "The 5040 th iteration gives loss of 0.4203223627162936\n",
      "The 5041 th iteration gives loss of 0.42028502616283203\n",
      "The 5042 th iteration gives loss of 0.42024769696260916\n",
      "The 5043 th iteration gives loss of 0.42021037511116643\n",
      "The 5044 th iteration gives loss of 0.4201730606040948\n",
      "The 5045 th iteration gives loss of 0.4201357534369737\n",
      "The 5046 th iteration gives loss of 0.4200984536053717\n",
      "The 5047 th iteration gives loss of 0.42006116110489744\n",
      "The 5048 th iteration gives loss of 0.4200238759311387\n",
      "The 5049 th iteration gives loss of 0.4199865980797058\n",
      "The 5050 th iteration gives loss of 0.41994932754620257\n",
      "The 5051 th iteration gives loss of 0.4199120643262422\n",
      "The 5052 th iteration gives loss of 0.41987480841543606\n",
      "The 5053 th iteration gives loss of 0.4198375598094248\n",
      "The 5054 th iteration gives loss of 0.41980031850381927\n",
      "The 5055 th iteration gives loss of 0.4197630844942668\n",
      "The 5056 th iteration gives loss of 0.4197258577764053\n",
      "The 5057 th iteration gives loss of 0.41968863834588\n",
      "The 5058 th iteration gives loss of 0.4196514261983463\n",
      "The 5059 th iteration gives loss of 0.4196142213294604\n",
      "The 5060 th iteration gives loss of 0.41957702373490297\n",
      "The 5061 th iteration gives loss of 0.4195398334103124\n",
      "The 5062 th iteration gives loss of 0.41950265035137274\n",
      "The 5063 th iteration gives loss of 0.41946547455376976\n",
      "The 5064 th iteration gives loss of 0.4194283060131852\n",
      "The 5065 th iteration gives loss of 0.4193911447253184\n",
      "The 5066 th iteration gives loss of 0.4193539906858674\n",
      "The 5067 th iteration gives loss of 0.41931684389051227\n",
      "The 5068 th iteration gives loss of 0.4192797043349786\n",
      "The 5069 th iteration gives loss of 0.4192425720149697\n",
      "The 5070 th iteration gives loss of 0.41920544692620776\n",
      "The 5071 th iteration gives loss of 0.419168329064419\n",
      "The 5072 th iteration gives loss of 0.4191312184253338\n",
      "The 5073 th iteration gives loss of 0.41909411500467975\n",
      "The 5074 th iteration gives loss of 0.41905701879819834\n",
      "The 5075 th iteration gives loss of 0.4190199298016411\n",
      "The 5076 th iteration gives loss of 0.41898284801075764\n",
      "The 5077 th iteration gives loss of 0.4189457734213029\n",
      "The 5078 th iteration gives loss of 0.4189087060290348\n",
      "The 5079 th iteration gives loss of 0.41887164582971886\n",
      "The 5080 th iteration gives loss of 0.41883459281912744\n",
      "The 5081 th iteration gives loss of 0.41879754699304356\n",
      "The 5082 th iteration gives loss of 0.418760508347262\n",
      "The 5083 th iteration gives loss of 0.41872347687754974\n",
      "The 5084 th iteration gives loss of 0.4186864525797133\n",
      "The 5085 th iteration gives loss of 0.41864943544954836\n",
      "The 5086 th iteration gives loss of 0.41861242548285166\n",
      "The 5087 th iteration gives loss of 0.41857542267545056\n",
      "The 5088 th iteration gives loss of 0.41853842702314714\n",
      "The 5089 th iteration gives loss of 0.41850143852175586\n",
      "The 5090 th iteration gives loss of 0.4184644571671079\n",
      "The 5091 th iteration gives loss of 0.4184274829550484\n",
      "The 5092 th iteration gives loss of 0.4183905158814008\n",
      "The 5093 th iteration gives loss of 0.41835355594200396\n",
      "The 5094 th iteration gives loss of 0.4183166031327006\n",
      "The 5095 th iteration gives loss of 0.41827965744937357\n",
      "The 5096 th iteration gives loss of 0.41824271888784864\n",
      "The 5097 th iteration gives loss of 0.41820578744399756\n",
      "The 5098 th iteration gives loss of 0.4181688631136893\n",
      "The 5099 th iteration gives loss of 0.41813194589279645\n",
      "The 5100 th iteration gives loss of 0.41809503577719476\n",
      "The 5101 th iteration gives loss of 0.41805813276277315\n",
      "The 5102 th iteration gives loss of 0.4180212368454135\n",
      "The 5103 th iteration gives loss of 0.41798434802101375\n",
      "The 5104 th iteration gives loss of 0.4179474662854804\n",
      "The 5105 th iteration gives loss of 0.4179105916347123\n",
      "The 5106 th iteration gives loss of 0.4178737240646082\n",
      "The 5107 th iteration gives loss of 0.41783686357109656\n",
      "The 5108 th iteration gives loss of 0.4178000101500977\n",
      "The 5109 th iteration gives loss of 0.4177631637975214\n",
      "The 5110 th iteration gives loss of 0.41772632450931724\n",
      "The 5111 th iteration gives loss of 0.4176894922814131\n",
      "The 5112 th iteration gives loss of 0.4176526671097425\n",
      "The 5113 th iteration gives loss of 0.41761584899026233\n",
      "The 5114 th iteration gives loss of 0.4175790379189091\n",
      "The 5115 th iteration gives loss of 0.4175422338916489\n",
      "The 5116 th iteration gives loss of 0.4175054369044495\n",
      "The 5117 th iteration gives loss of 0.41746864695325864\n",
      "The 5118 th iteration gives loss of 0.41743186403406585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5119 th iteration gives loss of 0.4173950881428372\n",
      "The 5120 th iteration gives loss of 0.4173583192755518\n",
      "The 5121 th iteration gives loss of 0.41732155742820337\n",
      "The 5122 th iteration gives loss of 0.41728480259677714\n",
      "The 5123 th iteration gives loss of 0.4172480547772779\n",
      "The 5124 th iteration gives loss of 0.41721131396569605\n",
      "The 5125 th iteration gives loss of 0.41717458015804243\n",
      "The 5126 th iteration gives loss of 0.4171378533503398\n",
      "The 5127 th iteration gives loss of 0.41710113353858536\n",
      "The 5128 th iteration gives loss of 0.4170644207188099\n",
      "The 5129 th iteration gives loss of 0.4170277148870491\n",
      "The 5130 th iteration gives loss of 0.4169910160393239\n",
      "The 5131 th iteration gives loss of 0.4169543241716774\n",
      "The 5132 th iteration gives loss of 0.4169176392801458\n",
      "The 5133 th iteration gives loss of 0.4168809613607703\n",
      "The 5134 th iteration gives loss of 0.4168442904096177\n",
      "The 5135 th iteration gives loss of 0.41680762642273683\n",
      "The 5136 th iteration gives loss of 0.4167709693961831\n",
      "The 5137 th iteration gives loss of 0.4167343193260391\n",
      "The 5138 th iteration gives loss of 0.4166976762083658\n",
      "The 5139 th iteration gives loss of 0.4166610400392415\n",
      "The 5140 th iteration gives loss of 0.41662441081473367\n",
      "The 5141 th iteration gives loss of 0.41658778853094525\n",
      "The 5142 th iteration gives loss of 0.41655117318397933\n",
      "The 5143 th iteration gives loss of 0.4165145647699077\n",
      "The 5144 th iteration gives loss of 0.4164779632848323\n",
      "The 5145 th iteration gives loss of 0.41644136872487153\n",
      "The 5146 th iteration gives loss of 0.41640478108614287\n",
      "The 5147 th iteration gives loss of 0.4163682003647379\n",
      "The 5148 th iteration gives loss of 0.41633162655679773\n",
      "The 5149 th iteration gives loss of 0.41629505965844893\n",
      "The 5150 th iteration gives loss of 0.41625849966580786\n",
      "The 5151 th iteration gives loss of 0.4162219465750092\n",
      "The 5152 th iteration gives loss of 0.41618540038219815\n",
      "The 5153 th iteration gives loss of 0.41614886108352994\n",
      "The 5154 th iteration gives loss of 0.4161123286751443\n",
      "The 5155 th iteration gives loss of 0.41607580315319376\n",
      "The 5156 th iteration gives loss of 0.41603928451384325\n",
      "The 5157 th iteration gives loss of 0.41600277275325215\n",
      "The 5158 th iteration gives loss of 0.41596626786759827\n",
      "The 5159 th iteration gives loss of 0.41592976985304086\n",
      "The 5160 th iteration gives loss of 0.4158932787057711\n",
      "The 5161 th iteration gives loss of 0.41585679442197\n",
      "The 5162 th iteration gives loss of 0.4158203169978216\n",
      "The 5163 th iteration gives loss of 0.415783846429527\n",
      "The 5164 th iteration gives loss of 0.41574738271327555\n",
      "The 5165 th iteration gives loss of 0.4157109258452629\n",
      "The 5166 th iteration gives loss of 0.41567447582172057\n",
      "The 5167 th iteration gives loss of 0.4156380326388455\n",
      "The 5168 th iteration gives loss of 0.41560159629285165\n",
      "The 5169 th iteration gives loss of 0.4155651667799676\n",
      "The 5170 th iteration gives loss of 0.4155287440964111\n",
      "The 5171 th iteration gives loss of 0.4154923282384266\n",
      "The 5172 th iteration gives loss of 0.41545591920223246\n",
      "The 5173 th iteration gives loss of 0.4154195169840844\n",
      "The 5174 th iteration gives loss of 0.4153831215802201\n",
      "The 5175 th iteration gives loss of 0.4153467329868906\n",
      "The 5176 th iteration gives loss of 0.4153103512003554\n",
      "The 5177 th iteration gives loss of 0.41527397621686024\n",
      "The 5178 th iteration gives loss of 0.4152376080326746\n",
      "The 5179 th iteration gives loss of 0.4152012466440714\n",
      "The 5180 th iteration gives loss of 0.4151648920473203\n",
      "The 5181 th iteration gives loss of 0.4151285442387143\n",
      "The 5182 th iteration gives loss of 0.41509220321451884\n",
      "The 5183 th iteration gives loss of 0.41505586897103464\n",
      "The 5184 th iteration gives loss of 0.41501954150452786\n",
      "The 5185 th iteration gives loss of 0.41498322081130534\n",
      "The 5186 th iteration gives loss of 0.41494690688768954\n",
      "The 5187 th iteration gives loss of 0.41491059972996847\n",
      "The 5188 th iteration gives loss of 0.4148742993344449\n",
      "The 5189 th iteration gives loss of 0.4148380056974487\n",
      "The 5190 th iteration gives loss of 0.41480171881529004\n",
      "The 5191 th iteration gives loss of 0.4147654386842972\n",
      "The 5192 th iteration gives loss of 0.41472916530079396\n",
      "The 5193 th iteration gives loss of 0.4146928986611249\n",
      "The 5194 th iteration gives loss of 0.41465663876162534\n",
      "The 5195 th iteration gives loss of 0.41462038559862174\n",
      "The 5196 th iteration gives loss of 0.41458413916847114\n",
      "The 5197 th iteration gives loss of 0.41454789946752685\n",
      "The 5198 th iteration gives loss of 0.4145116664921547\n",
      "The 5199 th iteration gives loss of 0.41447544023869465\n",
      "The 5200 th iteration gives loss of 0.41443922070351613\n",
      "The 5201 th iteration gives loss of 0.41440300788300916\n",
      "The 5202 th iteration gives loss of 0.41436680177351726\n",
      "The 5203 th iteration gives loss of 0.41433060237144903\n",
      "The 5204 th iteration gives loss of 0.414294409673166\n",
      "The 5205 th iteration gives loss of 0.4142582236750601\n",
      "The 5206 th iteration gives loss of 0.41422204437353444\n",
      "The 5207 th iteration gives loss of 0.4141858717649735\n",
      "The 5208 th iteration gives loss of 0.41414970584576705\n",
      "The 5209 th iteration gives loss of 0.41411354661235195\n",
      "The 5210 th iteration gives loss of 0.4140773940611224\n",
      "The 5211 th iteration gives loss of 0.4140412481884832\n",
      "The 5212 th iteration gives loss of 0.4140051089908618\n",
      "The 5213 th iteration gives loss of 0.413968976464681\n",
      "The 5214 th iteration gives loss of 0.4139328506063788\n",
      "The 5215 th iteration gives loss of 0.41389673141236305\n",
      "The 5216 th iteration gives loss of 0.41386061887910414\n",
      "The 5217 th iteration gives loss of 0.4138245130030107\n",
      "The 5218 th iteration gives loss of 0.4137884137805452\n",
      "The 5219 th iteration gives loss of 0.4137523212081617\n",
      "The 5220 th iteration gives loss of 0.41371623528230184\n",
      "The 5221 th iteration gives loss of 0.4136801559994228\n",
      "The 5222 th iteration gives loss of 0.4136440833559965\n",
      "The 5223 th iteration gives loss of 0.4136080173484954\n",
      "The 5224 th iteration gives loss of 0.4135719579733829\n",
      "The 5225 th iteration gives loss of 0.41353590522713013\n",
      "The 5226 th iteration gives loss of 0.4134998591062299\n",
      "The 5227 th iteration gives loss of 0.4134638196071596\n",
      "The 5228 th iteration gives loss of 0.41342778672640906\n",
      "The 5229 th iteration gives loss of 0.41339176046047743\n",
      "The 5230 th iteration gives loss of 0.4133557408058643\n",
      "The 5231 th iteration gives loss of 0.4133197277590532\n",
      "The 5232 th iteration gives loss of 0.41328372131656443\n",
      "The 5233 th iteration gives loss of 0.4132477214749128\n",
      "The 5234 th iteration gives loss of 0.41321172823060437\n",
      "The 5235 th iteration gives loss of 0.4131757415801683\n",
      "The 5236 th iteration gives loss of 0.41313976152012793\n",
      "The 5237 th iteration gives loss of 0.4131037880470041\n",
      "The 5238 th iteration gives loss of 0.41306782115732904\n",
      "The 5239 th iteration gives loss of 0.4130318608476458\n",
      "The 5240 th iteration gives loss of 0.4129959071144876\n",
      "The 5241 th iteration gives loss of 0.4129599599544089\n",
      "The 5242 th iteration gives loss of 0.4129240193639471\n",
      "The 5243 th iteration gives loss of 0.4128880853396687\n",
      "The 5244 th iteration gives loss of 0.4128521578781219\n",
      "The 5245 th iteration gives loss of 0.4128162369758801\n",
      "The 5246 th iteration gives loss of 0.41278032262949427\n",
      "The 5247 th iteration gives loss of 0.4127444148355599\n",
      "The 5248 th iteration gives loss of 0.4127085135906228\n",
      "The 5249 th iteration gives loss of 0.4126726188912876\n",
      "The 5250 th iteration gives loss of 0.4126367307341264\n",
      "The 5251 th iteration gives loss of 0.4126008491157217\n",
      "The 5252 th iteration gives loss of 0.4125649740326758\n",
      "The 5253 th iteration gives loss of 0.4125291054815714\n",
      "The 5254 th iteration gives loss of 0.41249324345901717\n",
      "The 5255 th iteration gives loss of 0.41245738796163117\n",
      "The 5256 th iteration gives loss of 0.4124215389859844\n",
      "The 5257 th iteration gives loss of 0.4123856965287362\n",
      "The 5258 th iteration gives loss of 0.41234986058646883\n",
      "The 5259 th iteration gives loss of 0.4123140311558253\n",
      "The 5260 th iteration gives loss of 0.4122782082334143\n",
      "The 5261 th iteration gives loss of 0.4122423918158775\n",
      "The 5262 th iteration gives loss of 0.41220658189984466\n",
      "The 5263 th iteration gives loss of 0.41217077848194816\n",
      "The 5264 th iteration gives loss of 0.4121349815588337\n",
      "The 5265 th iteration gives loss of 0.4120991911271501\n",
      "The 5266 th iteration gives loss of 0.4120634071835481\n",
      "The 5267 th iteration gives loss of 0.41202762972467655\n",
      "The 5268 th iteration gives loss of 0.4119918587471893\n",
      "The 5269 th iteration gives loss of 0.4119560942477607\n",
      "The 5270 th iteration gives loss of 0.4119203362230533\n",
      "The 5271 th iteration gives loss of 0.41188458466973815\n",
      "The 5272 th iteration gives loss of 0.4118488395844926\n",
      "The 5273 th iteration gives loss of 0.41181310096399143\n",
      "The 5274 th iteration gives loss of 0.4117773688049116\n",
      "The 5275 th iteration gives loss of 0.41174164310395195\n",
      "The 5276 th iteration gives loss of 0.4117059238577926\n",
      "The 5277 th iteration gives loss of 0.4116702110631422\n",
      "The 5278 th iteration gives loss of 0.4116345047166821\n",
      "The 5279 th iteration gives loss of 0.4115988048151398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5280 th iteration gives loss of 0.4115631113552014\n",
      "The 5281 th iteration gives loss of 0.41152742433357287\n",
      "The 5282 th iteration gives loss of 0.4114917437470011\n",
      "The 5283 th iteration gives loss of 0.41145606959217806\n",
      "The 5284 th iteration gives loss of 0.41142040186583995\n",
      "The 5285 th iteration gives loss of 0.4113847405646994\n",
      "The 5286 th iteration gives loss of 0.4113490856855116\n",
      "The 5287 th iteration gives loss of 0.41131343722498703\n",
      "The 5288 th iteration gives loss of 0.4112777951798791\n",
      "The 5289 th iteration gives loss of 0.4112421595469205\n",
      "The 5290 th iteration gives loss of 0.41120653032287896\n",
      "The 5291 th iteration gives loss of 0.4111709075044923\n",
      "The 5292 th iteration gives loss of 0.41113529108850655\n",
      "The 5293 th iteration gives loss of 0.4110996810716971\n",
      "The 5294 th iteration gives loss of 0.41106407745082324\n",
      "The 5295 th iteration gives loss of 0.4110284802226465\n",
      "The 5296 th iteration gives loss of 0.4109928893839449\n",
      "The 5297 th iteration gives loss of 0.41095730493149113\n",
      "The 5298 th iteration gives loss of 0.41092172686205714\n",
      "The 5299 th iteration gives loss of 0.410886155172439\n",
      "The 5300 th iteration gives loss of 0.4108505898594085\n",
      "The 5301 th iteration gives loss of 0.41081503091978416\n",
      "The 5302 th iteration gives loss of 0.4107794783503184\n",
      "The 5303 th iteration gives loss of 0.41074393214785093\n",
      "The 5304 th iteration gives loss of 0.4107083923091643\n",
      "The 5305 th iteration gives loss of 0.4106728588310632\n",
      "The 5306 th iteration gives loss of 0.4106373317103665\n",
      "The 5307 th iteration gives loss of 0.41060181094386805\n",
      "The 5308 th iteration gives loss of 0.41056629652841237\n",
      "The 5309 th iteration gives loss of 0.4105307884608127\n",
      "The 5310 th iteration gives loss of 0.4104952867378875\n",
      "The 5311 th iteration gives loss of 0.4104597913564695\n",
      "The 5312 th iteration gives loss of 0.4104243023134026\n",
      "The 5313 th iteration gives loss of 0.4103888196055058\n",
      "The 5314 th iteration gives loss of 0.41035334322965555\n",
      "The 5315 th iteration gives loss of 0.410317873182647\n",
      "The 5316 th iteration gives loss of 0.4102824094613703\n",
      "The 5317 th iteration gives loss of 0.41024695206265066\n",
      "The 5318 th iteration gives loss of 0.41021150098336634\n",
      "The 5319 th iteration gives loss of 0.41017605622036335\n",
      "The 5320 th iteration gives loss of 0.41014061777050925\n",
      "The 5321 th iteration gives loss of 0.41010518563067966\n",
      "The 5322 th iteration gives loss of 0.4100697597977363\n",
      "The 5323 th iteration gives loss of 0.4100343402685584\n",
      "The 5324 th iteration gives loss of 0.40999892704003504\n",
      "The 5325 th iteration gives loss of 0.409963520109032\n",
      "The 5326 th iteration gives loss of 0.40992811947243796\n",
      "The 5327 th iteration gives loss of 0.4098927251271642\n",
      "The 5328 th iteration gives loss of 0.4098573370700746\n",
      "The 5329 th iteration gives loss of 0.4098219552981\n",
      "The 5330 th iteration gives loss of 0.4097865798081251\n",
      "The 5331 th iteration gives loss of 0.40975121059705366\n",
      "The 5332 th iteration gives loss of 0.4097158476617975\n",
      "The 5333 th iteration gives loss of 0.40968049099928217\n",
      "The 5334 th iteration gives loss of 0.4096451406064107\n",
      "The 5335 th iteration gives loss of 0.4096097964801086\n",
      "The 5336 th iteration gives loss of 0.4095744586172977\n",
      "The 5337 th iteration gives loss of 0.40953912701491335\n",
      "The 5338 th iteration gives loss of 0.4095038016698792\n",
      "The 5339 th iteration gives loss of 0.40946848257914453\n",
      "The 5340 th iteration gives loss of 0.40943316973962846\n",
      "The 5341 th iteration gives loss of 0.4093978631482754\n",
      "The 5342 th iteration gives loss of 0.40936256280205796\n",
      "The 5343 th iteration gives loss of 0.4093272686979063\n",
      "The 5344 th iteration gives loss of 0.40929198083278245\n",
      "The 5345 th iteration gives loss of 0.4092566992036322\n",
      "The 5346 th iteration gives loss of 0.40922142380743215\n",
      "The 5347 th iteration gives loss of 0.4091861546411364\n",
      "The 5348 th iteration gives loss of 0.4091508917017291\n",
      "The 5349 th iteration gives loss of 0.4091156349861709\n",
      "The 5350 th iteration gives loss of 0.40908038449142864\n",
      "The 5351 th iteration gives loss of 0.4090451402144964\n",
      "The 5352 th iteration gives loss of 0.4090099021523673\n",
      "The 5353 th iteration gives loss of 0.4089746703020074\n",
      "The 5354 th iteration gives loss of 0.4089394446604096\n",
      "The 5355 th iteration gives loss of 0.4089042252245821\n",
      "The 5356 th iteration gives loss of 0.40886901199152187\n",
      "The 5357 th iteration gives loss of 0.4088338049582235\n",
      "The 5358 th iteration gives loss of 0.4087986041216857\n",
      "The 5359 th iteration gives loss of 0.4087634094789356\n",
      "The 5360 th iteration gives loss of 0.40872822102696826\n",
      "The 5361 th iteration gives loss of 0.40869303876281504\n",
      "The 5362 th iteration gives loss of 0.4086578626834742\n",
      "The 5363 th iteration gives loss of 0.4086226927859987\n",
      "The 5364 th iteration gives loss of 0.4085875290673924\n",
      "The 5365 th iteration gives loss of 0.408552371524691\n",
      "The 5366 th iteration gives loss of 0.40851722015493946\n",
      "The 5367 th iteration gives loss of 0.4084820749551528\n",
      "The 5368 th iteration gives loss of 0.40844693592240355\n",
      "The 5369 th iteration gives loss of 0.4084118030537168\n",
      "The 5370 th iteration gives loss of 0.40837667634612207\n",
      "The 5371 th iteration gives loss of 0.40834155579671394\n",
      "The 5372 th iteration gives loss of 0.4083064414025175\n",
      "The 5373 th iteration gives loss of 0.4082713331606005\n",
      "The 5374 th iteration gives loss of 0.40823623106803264\n",
      "The 5375 th iteration gives loss of 0.40820113512187234\n",
      "The 5376 th iteration gives loss of 0.4081660453191866\n",
      "The 5377 th iteration gives loss of 0.4081309616570559\n",
      "The 5378 th iteration gives loss of 0.4080958841325437\n",
      "The 5379 th iteration gives loss of 0.40806081274274514\n",
      "The 5380 th iteration gives loss of 0.4080257474847456\n",
      "The 5381 th iteration gives loss of 0.4079906883556113\n",
      "The 5382 th iteration gives loss of 0.4079556353524555\n",
      "The 5383 th iteration gives loss of 0.407920588472366\n",
      "The 5384 th iteration gives loss of 0.4078855477124342\n",
      "The 5385 th iteration gives loss of 0.4078505130697551\n",
      "The 5386 th iteration gives loss of 0.40781548454144956\n",
      "The 5387 th iteration gives loss of 0.4077804621246229\n",
      "The 5388 th iteration gives loss of 0.40774544581637945\n",
      "The 5389 th iteration gives loss of 0.4077104356138377\n",
      "The 5390 th iteration gives loss of 0.40767543151411495\n",
      "The 5391 th iteration gives loss of 0.407640433514333\n",
      "The 5392 th iteration gives loss of 0.4076054416116128\n",
      "The 5393 th iteration gives loss of 0.40757045580309753\n",
      "The 5394 th iteration gives loss of 0.40753547608590246\n",
      "The 5395 th iteration gives loss of 0.4075005024571731\n",
      "The 5396 th iteration gives loss of 0.40746553491404985\n",
      "The 5397 th iteration gives loss of 0.40743057345367745\n",
      "The 5398 th iteration gives loss of 0.40739561807318136\n",
      "The 5399 th iteration gives loss of 0.4073606687697274\n",
      "The 5400 th iteration gives loss of 0.4073257255404767\n",
      "The 5401 th iteration gives loss of 0.4072907883825655\n",
      "The 5402 th iteration gives loss of 0.4072558572931654\n",
      "The 5403 th iteration gives loss of 0.40722093226944117\n",
      "The 5404 th iteration gives loss of 0.40718601330854964\n",
      "The 5405 th iteration gives loss of 0.40715110040766195\n",
      "The 5406 th iteration gives loss of 0.40711619356394624\n",
      "The 5407 th iteration gives loss of 0.40708129277460164\n",
      "The 5408 th iteration gives loss of 0.4070463980367812\n",
      "The 5409 th iteration gives loss of 0.40701150934768376\n",
      "The 5410 th iteration gives loss of 0.4069766267045016\n",
      "The 5411 th iteration gives loss of 0.40694175010441086\n",
      "The 5412 th iteration gives loss of 0.4069068795445996\n",
      "The 5413 th iteration gives loss of 0.4068720150222851\n",
      "The 5414 th iteration gives loss of 0.406837156534658\n",
      "The 5415 th iteration gives loss of 0.4068023040789175\n",
      "The 5416 th iteration gives loss of 0.40676745765225836\n",
      "The 5417 th iteration gives loss of 0.40673261725191895\n",
      "The 5418 th iteration gives loss of 0.40669778287508146\n",
      "The 5419 th iteration gives loss of 0.4066629545189885\n",
      "The 5420 th iteration gives loss of 0.40662813218085214\n",
      "The 5421 th iteration gives loss of 0.4065933158578843\n",
      "The 5422 th iteration gives loss of 0.40655850554733225\n",
      "The 5423 th iteration gives loss of 0.4065237012464095\n",
      "The 5424 th iteration gives loss of 0.4064889029523407\n",
      "The 5425 th iteration gives loss of 0.40645411066238846\n",
      "The 5426 th iteration gives loss of 0.4064193243737802\n",
      "The 5427 th iteration gives loss of 0.4063845440837489\n",
      "The 5428 th iteration gives loss of 0.40634976978955184\n",
      "The 5429 th iteration gives loss of 0.4063150014884418\n",
      "The 5430 th iteration gives loss of 0.4062802391776561\n",
      "The 5431 th iteration gives loss of 0.4062454828544628\n",
      "The 5432 th iteration gives loss of 0.4062107325161192\n",
      "The 5433 th iteration gives loss of 0.40617598815988515\n",
      "The 5434 th iteration gives loss of 0.40614124978302485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5435 th iteration gives loss of 0.406106517382814\n",
      "The 5436 th iteration gives loss of 0.4060717909565126\n",
      "The 5437 th iteration gives loss of 0.40603707050140436\n",
      "The 5438 th iteration gives loss of 0.406002356014777\n",
      "The 5439 th iteration gives loss of 0.40596764749390407\n",
      "The 5440 th iteration gives loss of 0.4059329449360698\n",
      "The 5441 th iteration gives loss of 0.40589824833854365\n",
      "The 5442 th iteration gives loss of 0.40586355769864496\n",
      "The 5443 th iteration gives loss of 0.40582887301366266\n",
      "The 5444 th iteration gives loss of 0.40579419428089003\n",
      "The 5445 th iteration gives loss of 0.40575952149762545\n",
      "The 5446 th iteration gives loss of 0.4057248546611848\n",
      "The 5447 th iteration gives loss of 0.40569019376886395\n",
      "The 5448 th iteration gives loss of 0.4056555388179715\n",
      "The 5449 th iteration gives loss of 0.4056208898058278\n",
      "The 5450 th iteration gives loss of 0.40558624672974636\n",
      "The 5451 th iteration gives loss of 0.4055516095870549\n",
      "The 5452 th iteration gives loss of 0.40551697837507533\n",
      "The 5453 th iteration gives loss of 0.4054823530911364\n",
      "The 5454 th iteration gives loss of 0.4054477337325439\n",
      "The 5455 th iteration gives loss of 0.40541312029664883\n",
      "The 5456 th iteration gives loss of 0.4053785127807873\n",
      "The 5457 th iteration gives loss of 0.4053439111823087\n",
      "The 5458 th iteration gives loss of 0.4053093154985347\n",
      "The 5459 th iteration gives loss of 0.40527472572682266\n",
      "The 5460 th iteration gives loss of 0.40524014186451046\n",
      "The 5461 th iteration gives loss of 0.4052055639089659\n",
      "The 5462 th iteration gives loss of 0.40517099185753136\n",
      "The 5463 th iteration gives loss of 0.4051364257075665\n",
      "The 5464 th iteration gives loss of 0.40510186545643834\n",
      "The 5465 th iteration gives loss of 0.4050673111015024\n",
      "The 5466 th iteration gives loss of 0.40503276264013116\n",
      "The 5467 th iteration gives loss of 0.4049982200696968\n",
      "The 5468 th iteration gives loss of 0.4049636833875651\n",
      "The 5469 th iteration gives loss of 0.40492915259111883\n",
      "The 5470 th iteration gives loss of 0.40489462767773393\n",
      "The 5471 th iteration gives loss of 0.40486010864479155\n",
      "The 5472 th iteration gives loss of 0.40482559548968405\n",
      "The 5473 th iteration gives loss of 0.4047910882097971\n",
      "The 5474 th iteration gives loss of 0.4047565868025197\n",
      "The 5475 th iteration gives loss of 0.4047220912652495\n",
      "The 5476 th iteration gives loss of 0.40468760159538736\n",
      "The 5477 th iteration gives loss of 0.4046531177903279\n",
      "The 5478 th iteration gives loss of 0.4046186398474731\n",
      "The 5479 th iteration gives loss of 0.4045841677642305\n",
      "The 5480 th iteration gives loss of 0.4045497015380231\n",
      "The 5481 th iteration gives loss of 0.404515241166257\n",
      "The 5482 th iteration gives loss of 0.4044807866463357\n",
      "The 5483 th iteration gives loss of 0.40444633797568685\n",
      "The 5484 th iteration gives loss of 0.4044118951517441\n",
      "The 5485 th iteration gives loss of 0.4043774581719312\n",
      "The 5486 th iteration gives loss of 0.4043430270336534\n",
      "The 5487 th iteration gives loss of 0.4043086017343616\n",
      "The 5488 th iteration gives loss of 0.40427418227146694\n",
      "The 5489 th iteration gives loss of 0.4042397686424431\n",
      "The 5490 th iteration gives loss of 0.4042053608447117\n",
      "The 5491 th iteration gives loss of 0.4041709588757206\n",
      "The 5492 th iteration gives loss of 0.40413656273289594\n",
      "The 5493 th iteration gives loss of 0.40410217241372204\n",
      "The 5494 th iteration gives loss of 0.4040677879156067\n",
      "The 5495 th iteration gives loss of 0.4040334092360569\n",
      "The 5496 th iteration gives loss of 0.40399903637247925\n",
      "The 5497 th iteration gives loss of 0.403964669322375\n",
      "The 5498 th iteration gives loss of 0.4039303080831902\n",
      "The 5499 th iteration gives loss of 0.40389595265239026\n",
      "The 5500 th iteration gives loss of 0.40386160302745483\n",
      "The 5501 th iteration gives loss of 0.4038272592058459\n",
      "The 5502 th iteration gives loss of 0.4037929211850481\n",
      "The 5503 th iteration gives loss of 0.4037585889625333\n",
      "The 5504 th iteration gives loss of 0.4037242625357911\n",
      "The 5505 th iteration gives loss of 0.40368994190229734\n",
      "The 5506 th iteration gives loss of 0.40365562705954317\n",
      "The 5507 th iteration gives loss of 0.4036213180050206\n",
      "The 5508 th iteration gives loss of 0.4035870147362177\n",
      "The 5509 th iteration gives loss of 0.4035527172506342\n",
      "The 5510 th iteration gives loss of 0.40351842554578254\n",
      "The 5511 th iteration gives loss of 0.4034841396191503\n",
      "The 5512 th iteration gives loss of 0.40344985946824286\n",
      "The 5513 th iteration gives loss of 0.40341558509056735\n",
      "The 5514 th iteration gives loss of 0.40338131648363496\n",
      "The 5515 th iteration gives loss of 0.403347053644967\n",
      "The 5516 th iteration gives loss of 0.403312796572074\n",
      "The 5517 th iteration gives loss of 0.40327854526247287\n",
      "The 5518 th iteration gives loss of 0.4032442997136948\n",
      "The 5519 th iteration gives loss of 0.4032100599232637\n",
      "The 5520 th iteration gives loss of 0.4031758258886958\n",
      "The 5521 th iteration gives loss of 0.4031415976075268\n",
      "The 5522 th iteration gives loss of 0.40310737507730593\n",
      "The 5523 th iteration gives loss of 0.40307315829555723\n",
      "The 5524 th iteration gives loss of 0.40303894725981465\n",
      "The 5525 th iteration gives loss of 0.4030047419676342\n",
      "The 5526 th iteration gives loss of 0.4029705424165483\n",
      "The 5527 th iteration gives loss of 0.40293634860411476\n",
      "The 5528 th iteration gives loss of 0.4029021605278848\n",
      "The 5529 th iteration gives loss of 0.40286797818540615\n",
      "The 5530 th iteration gives loss of 0.40283380157423565\n",
      "The 5531 th iteration gives loss of 0.4027996306919266\n",
      "The 5532 th iteration gives loss of 0.4027654655360633\n",
      "The 5533 th iteration gives loss of 0.4027313061041883\n",
      "The 5534 th iteration gives loss of 0.40269715239388115\n",
      "The 5535 th iteration gives loss of 0.40266300440271213\n",
      "The 5536 th iteration gives loss of 0.4026288621282563\n",
      "The 5537 th iteration gives loss of 0.4025947255680822\n",
      "The 5538 th iteration gives loss of 0.40256059471977973\n",
      "The 5539 th iteration gives loss of 0.4025264695809115\n",
      "The 5540 th iteration gives loss of 0.40249235014907675\n",
      "The 5541 th iteration gives loss of 0.4024582364218681\n",
      "The 5542 th iteration gives loss of 0.40242412839686376\n",
      "The 5543 th iteration gives loss of 0.402390026071672\n",
      "The 5544 th iteration gives loss of 0.4023559294438667\n",
      "The 5545 th iteration gives loss of 0.40232183851106396\n",
      "The 5546 th iteration gives loss of 0.40228775327085897\n",
      "The 5547 th iteration gives loss of 0.4022536737208525\n",
      "The 5548 th iteration gives loss of 0.40221959985867367\n",
      "The 5549 th iteration gives loss of 0.40218553168190313\n",
      "The 5550 th iteration gives loss of 0.4021514691881578\n",
      "The 5551 th iteration gives loss of 0.40211741237507154\n",
      "The 5552 th iteration gives loss of 0.4020833612402381\n",
      "The 5553 th iteration gives loss of 0.4020493157813018\n",
      "The 5554 th iteration gives loss of 0.4020152759958674\n",
      "The 5555 th iteration gives loss of 0.4019812418815763\n",
      "The 5556 th iteration gives loss of 0.40194721343603385\n",
      "The 5557 th iteration gives loss of 0.4019131906568974\n",
      "The 5558 th iteration gives loss of 0.40187917354179337\n",
      "The 5559 th iteration gives loss of 0.4018451620883594\n",
      "The 5560 th iteration gives loss of 0.4018111562942182\n",
      "The 5561 th iteration gives loss of 0.4017771561570351\n",
      "The 5562 th iteration gives loss of 0.4017431616744433\n",
      "The 5563 th iteration gives loss of 0.40170917284410745\n",
      "The 5564 th iteration gives loss of 0.40167518966365773\n",
      "The 5565 th iteration gives loss of 0.401641212130744\n",
      "The 5566 th iteration gives loss of 0.4016072402430401\n",
      "The 5567 th iteration gives loss of 0.40157327399819487\n",
      "The 5568 th iteration gives loss of 0.40153931339388227\n",
      "The 5569 th iteration gives loss of 0.401505358427749\n",
      "The 5570 th iteration gives loss of 0.40147140909747353\n",
      "The 5571 th iteration gives loss of 0.40143746540071207\n",
      "The 5572 th iteration gives loss of 0.4014035273351436\n",
      "The 5573 th iteration gives loss of 0.40136959489844526\n",
      "The 5574 th iteration gives loss of 0.4013356680882979\n",
      "The 5575 th iteration gives loss of 0.401301746902374\n",
      "The 5576 th iteration gives loss of 0.40126783133835997\n",
      "The 5577 th iteration gives loss of 0.4012339213939429\n",
      "The 5578 th iteration gives loss of 0.40120001706681196\n",
      "The 5579 th iteration gives loss of 0.4011661183546502\n",
      "The 5580 th iteration gives loss of 0.40113222525515785\n",
      "The 5581 th iteration gives loss of 0.4010983377660195\n",
      "The 5582 th iteration gives loss of 0.4010644558849399\n",
      "The 5583 th iteration gives loss of 0.40103057960962357\n",
      "The 5584 th iteration gives loss of 0.400996708937783\n",
      "The 5585 th iteration gives loss of 0.4009628438671063\n",
      "The 5586 th iteration gives loss of 0.40092898439531965\n",
      "The 5587 th iteration gives loss of 0.4008951305201078\n",
      "The 5588 th iteration gives loss of 0.4008612822392207\n",
      "The 5589 th iteration gives loss of 0.4008274395503419\n",
      "The 5590 th iteration gives loss of 0.40079360245121637\n",
      "The 5591 th iteration gives loss of 0.40075977093955406\n",
      "The 5592 th iteration gives loss of 0.4007259450130762\n",
      "The 5593 th iteration gives loss of 0.40069212466951587\n",
      "The 5594 th iteration gives loss of 0.40065830990660967\n",
      "The 5595 th iteration gives loss of 0.40062450072207756\n",
      "The 5596 th iteration gives loss of 0.40059069711365153\n",
      "The 5597 th iteration gives loss of 0.40055689907908437\n",
      "The 5598 th iteration gives loss of 0.4005231066161071\n",
      "The 5599 th iteration gives loss of 0.40048931972246304\n",
      "The 5600 th iteration gives loss of 0.4004555383958939\n",
      "The 5601 th iteration gives loss of 0.4004217626341562\n",
      "The 5602 th iteration gives loss of 0.4003879924349961\n",
      "The 5603 th iteration gives loss of 0.4003542277961601\n",
      "The 5604 th iteration gives loss of 0.40032046871540844\n",
      "The 5605 th iteration gives loss of 0.4002867151905018\n",
      "The 5606 th iteration gives loss of 0.40025296721920484\n",
      "The 5607 th iteration gives loss of 0.4002192247992761\n",
      "The 5608 th iteration gives loss of 0.4001854879284685\n",
      "The 5609 th iteration gives loss of 0.40015175660457414\n",
      "The 5610 th iteration gives loss of 0.40011803082535224\n",
      "The 5611 th iteration gives loss of 0.40008431058856425\n",
      "The 5612 th iteration gives loss of 0.4000505958920043\n",
      "The 5613 th iteration gives loss of 0.40001688673343044\n",
      "The 5614 th iteration gives loss of 0.39998318311064873\n",
      "The 5615 th iteration gives loss of 0.3999494850214197\n",
      "The 5616 th iteration gives loss of 0.3999157924635438\n",
      "The 5617 th iteration gives loss of 0.3998821054347908\n",
      "The 5618 th iteration gives loss of 0.399848423932988\n",
      "The 5619 th iteration gives loss of 0.3998147479558956\n",
      "The 5620 th iteration gives loss of 0.39978107750132263\n",
      "The 5621 th iteration gives loss of 0.39974741256705737\n",
      "The 5622 th iteration gives loss of 0.39971375315090324\n",
      "The 5623 th iteration gives loss of 0.39968009925066567\n",
      "The 5624 th iteration gives loss of 0.3996464508641661\n",
      "The 5625 th iteration gives loss of 0.3996128079891882\n",
      "The 5626 th iteration gives loss of 0.3995791706235534\n",
      "The 5627 th iteration gives loss of 0.3995455387650772\n",
      "The 5628 th iteration gives loss of 0.399511912411571\n",
      "The 5629 th iteration gives loss of 0.399478291560846\n",
      "The 5630 th iteration gives loss of 0.3994446762107444\n",
      "The 5631 th iteration gives loss of 0.39941106635906476\n",
      "The 5632 th iteration gives loss of 0.3993774620036538\n",
      "The 5633 th iteration gives loss of 0.3993438631423201\n",
      "The 5634 th iteration gives loss of 0.3993102697729039\n",
      "The 5635 th iteration gives loss of 0.3992766818932365\n",
      "The 5636 th iteration gives loss of 0.39924309950114634\n",
      "The 5637 th iteration gives loss of 0.3992095225944771\n",
      "The 5638 th iteration gives loss of 0.39917595117108035\n",
      "The 5639 th iteration gives loss of 0.3991423852287762\n",
      "The 5640 th iteration gives loss of 0.39910882476543386\n",
      "The 5641 th iteration gives loss of 0.3990752697788701\n",
      "The 5642 th iteration gives loss of 0.39904172026697077\n",
      "The 5643 th iteration gives loss of 0.3990081762275521\n",
      "The 5644 th iteration gives loss of 0.3989746376584859\n",
      "The 5645 th iteration gives loss of 0.398941104557642\n",
      "The 5646 th iteration gives loss of 0.39890757692284984\n",
      "The 5647 th iteration gives loss of 0.3988740547520019\n",
      "The 5648 th iteration gives loss of 0.3988405380429431\n",
      "The 5649 th iteration gives loss of 0.39880702679353985\n",
      "The 5650 th iteration gives loss of 0.3987735210016664\n",
      "The 5651 th iteration gives loss of 0.3987400206651987\n",
      "The 5652 th iteration gives loss of 0.39870652578200794\n",
      "The 5653 th iteration gives loss of 0.39867303634995627\n",
      "The 5654 th iteration gives loss of 0.3986395523669409\n",
      "The 5655 th iteration gives loss of 0.3986060738308247\n",
      "The 5656 th iteration gives loss of 0.3985726007395106\n",
      "The 5657 th iteration gives loss of 0.39853913309087413\n",
      "The 5658 th iteration gives loss of 0.39850567088279715\n",
      "The 5659 th iteration gives loss of 0.3984722141131909\n",
      "The 5660 th iteration gives loss of 0.3984387627799187\n",
      "The 5661 th iteration gives loss of 0.3984053168808845\n",
      "The 5662 th iteration gives loss of 0.3983718764140111\n",
      "The 5663 th iteration gives loss of 0.39833844137716057\n",
      "The 5664 th iteration gives loss of 0.3983050117682634\n",
      "The 5665 th iteration gives loss of 0.3982715875852111\n",
      "The 5666 th iteration gives loss of 0.3982381688259141\n",
      "The 5667 th iteration gives loss of 0.39820475548827194\n",
      "The 5668 th iteration gives loss of 0.39817134757020717\n",
      "The 5669 th iteration gives loss of 0.39813794506963024\n",
      "The 5670 th iteration gives loss of 0.3981045479844553\n",
      "The 5671 th iteration gives loss of 0.39807115631260526\n",
      "The 5672 th iteration gives loss of 0.3980377700520002\n",
      "The 5673 th iteration gives loss of 0.39800438920055375\n",
      "The 5674 th iteration gives loss of 0.3979710137561893\n",
      "The 5675 th iteration gives loss of 0.39793764371684875\n",
      "The 5676 th iteration gives loss of 0.39790427908046194\n",
      "The 5677 th iteration gives loss of 0.39787091984495015\n",
      "The 5678 th iteration gives loss of 0.397837566008253\n",
      "The 5679 th iteration gives loss of 0.39780421756830797\n",
      "The 5680 th iteration gives loss of 0.39777087452304427\n",
      "The 5681 th iteration gives loss of 0.3977375368704227\n",
      "The 5682 th iteration gives loss of 0.39770420460836625\n",
      "The 5683 th iteration gives loss of 0.39767087773484466\n",
      "The 5684 th iteration gives loss of 0.3976375562477857\n",
      "The 5685 th iteration gives loss of 0.3976042401451343\n",
      "The 5686 th iteration gives loss of 0.3975709294248554\n",
      "The 5687 th iteration gives loss of 0.39753762408491455\n",
      "The 5688 th iteration gives loss of 0.3975043241232526\n",
      "The 5689 th iteration gives loss of 0.3974710295378297\n",
      "The 5690 th iteration gives loss of 0.39743774032661555\n",
      "The 5691 th iteration gives loss of 0.39740445648757083\n",
      "The 5692 th iteration gives loss of 0.3973711780186638\n",
      "The 5693 th iteration gives loss of 0.3973379049178493\n",
      "The 5694 th iteration gives loss of 0.39730463718311654\n",
      "The 5695 th iteration gives loss of 0.39727137481242847\n",
      "The 5696 th iteration gives loss of 0.3972381178037698\n",
      "The 5697 th iteration gives loss of 0.39720486615509515\n",
      "The 5698 th iteration gives loss of 0.3971716198644112\n",
      "The 5699 th iteration gives loss of 0.39713837892969434\n",
      "The 5700 th iteration gives loss of 0.39710514334891467\n",
      "The 5701 th iteration gives loss of 0.3970719131200571\n",
      "The 5702 th iteration gives loss of 0.3970386882411256\n",
      "The 5703 th iteration gives loss of 0.39700546871011877\n",
      "The 5704 th iteration gives loss of 0.3969722545249988\n",
      "The 5705 th iteration gives loss of 0.39693904568379107\n",
      "The 5706 th iteration gives loss of 0.3969058421844683\n",
      "The 5707 th iteration gives loss of 0.39687264402504646\n",
      "The 5708 th iteration gives loss of 0.3968394512035217\n",
      "The 5709 th iteration gives loss of 0.3968062637178999\n",
      "The 5710 th iteration gives loss of 0.39677308156618696\n",
      "The 5711 th iteration gives loss of 0.3967399047463874\n",
      "The 5712 th iteration gives loss of 0.3967067332565142\n",
      "The 5713 th iteration gives loss of 0.3966735670945804\n",
      "The 5714 th iteration gives loss of 0.3966404062586059\n",
      "The 5715 th iteration gives loss of 0.39660725074659964\n",
      "The 5716 th iteration gives loss of 0.3965741005565925\n",
      "The 5717 th iteration gives loss of 0.39654095568657616\n",
      "The 5718 th iteration gives loss of 0.3965078161346081\n",
      "The 5719 th iteration gives loss of 0.3964746818987118\n",
      "The 5720 th iteration gives loss of 0.3964415529768879\n",
      "The 5721 th iteration gives loss of 0.39640842936720033\n",
      "The 5722 th iteration gives loss of 0.3963753110676588\n",
      "The 5723 th iteration gives loss of 0.3963421980762948\n",
      "The 5724 th iteration gives loss of 0.39630909039116025\n",
      "The 5725 th iteration gives loss of 0.39627598801028835\n",
      "The 5726 th iteration gives loss of 0.396242890931725\n",
      "The 5727 th iteration gives loss of 0.3962097991535066\n",
      "The 5728 th iteration gives loss of 0.3961767126736758\n",
      "The 5729 th iteration gives loss of 0.3961436314902766\n",
      "The 5730 th iteration gives loss of 0.39611055560136865\n",
      "The 5731 th iteration gives loss of 0.39607748500499906\n",
      "The 5732 th iteration gives loss of 0.39604441969923676\n",
      "The 5733 th iteration gives loss of 0.3960113596821114\n",
      "The 5734 th iteration gives loss of 0.39597830495169156\n",
      "The 5735 th iteration gives loss of 0.3959452555060357\n",
      "The 5736 th iteration gives loss of 0.3959122113432163\n",
      "The 5737 th iteration gives loss of 0.39587917246128856\n",
      "The 5738 th iteration gives loss of 0.3958461388583198\n",
      "The 5739 th iteration gives loss of 0.3958131105323813\n",
      "The 5740 th iteration gives loss of 0.39578008748153465\n",
      "The 5741 th iteration gives loss of 0.39574706970386164\n",
      "The 5742 th iteration gives loss of 0.3957140571974332\n",
      "The 5743 th iteration gives loss of 0.3956810499603343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5744 th iteration gives loss of 0.3956480479906332\n",
      "The 5745 th iteration gives loss of 0.3956150512864234\n",
      "The 5746 th iteration gives loss of 0.39558205984577316\n",
      "The 5747 th iteration gives loss of 0.39554907366677083\n",
      "The 5748 th iteration gives loss of 0.395516092747517\n",
      "The 5749 th iteration gives loss of 0.3954831170860855\n",
      "The 5750 th iteration gives loss of 0.39545014668057726\n",
      "The 5751 th iteration gives loss of 0.39541718152907573\n",
      "The 5752 th iteration gives loss of 0.395384221629692\n",
      "The 5753 th iteration gives loss of 0.3953512669805018\n",
      "The 5754 th iteration gives loss of 0.3953183175796389\n",
      "The 5755 th iteration gives loss of 0.39528537342516307\n",
      "The 5756 th iteration gives loss of 0.3952524345152081\n",
      "The 5757 th iteration gives loss of 0.3952195008478693\n",
      "The 5758 th iteration gives loss of 0.3951865724212604\n",
      "The 5759 th iteration gives loss of 0.3951536492334829\n",
      "The 5760 th iteration gives loss of 0.395120731282657\n",
      "The 5761 th iteration gives loss of 0.3950878185668832\n",
      "The 5762 th iteration gives loss of 0.39505491108428903\n",
      "The 5763 th iteration gives loss of 0.3950220088329955\n",
      "The 5764 th iteration gives loss of 0.3949891118111263\n",
      "The 5765 th iteration gives loss of 0.3949562200167857\n",
      "The 5766 th iteration gives loss of 0.39492333344811265\n",
      "The 5767 th iteration gives loss of 0.39489045210322715\n",
      "The 5768 th iteration gives loss of 0.3948575759802479\n",
      "The 5769 th iteration gives loss of 0.3948247050773274\n",
      "The 5770 th iteration gives loss of 0.394791839392598\n",
      "The 5771 th iteration gives loss of 0.3947589789241679\n",
      "The 5772 th iteration gives loss of 0.3947261236701921\n",
      "The 5773 th iteration gives loss of 0.39469327362881673\n",
      "The 5774 th iteration gives loss of 0.3946604287981666\n",
      "The 5775 th iteration gives loss of 0.3946275891763801\n",
      "The 5776 th iteration gives loss of 0.39459475476160877\n",
      "The 5777 th iteration gives loss of 0.3945619255519961\n",
      "The 5778 th iteration gives loss of 0.394529101545701\n",
      "The 5779 th iteration gives loss of 0.3944962827408752\n",
      "The 5780 th iteration gives loss of 0.39446346913565666\n",
      "The 5781 th iteration gives loss of 0.3944306607282064\n",
      "The 5782 th iteration gives loss of 0.3943978575166758\n",
      "The 5783 th iteration gives loss of 0.3943650594992352\n",
      "The 5784 th iteration gives loss of 0.3943322666740351\n",
      "The 5785 th iteration gives loss of 0.39429947903924534\n",
      "The 5786 th iteration gives loss of 0.39426669659302116\n",
      "The 5787 th iteration gives loss of 0.3942339193335307\n",
      "The 5788 th iteration gives loss of 0.39420114725894567\n",
      "The 5789 th iteration gives loss of 0.394168380367436\n",
      "The 5790 th iteration gives loss of 0.39413561865717095\n",
      "The 5791 th iteration gives loss of 0.3941028621263161\n",
      "The 5792 th iteration gives loss of 0.39407011077306253\n",
      "The 5793 th iteration gives loss of 0.39403736459559546\n",
      "The 5794 th iteration gives loss of 0.3940046235920709\n",
      "The 5795 th iteration gives loss of 0.39397188776069003\n",
      "The 5796 th iteration gives loss of 0.39393915709961896\n",
      "The 5797 th iteration gives loss of 0.39390643160705685\n",
      "The 5798 th iteration gives loss of 0.39387371128118603\n",
      "The 5799 th iteration gives loss of 0.39384099612020007\n",
      "The 5800 th iteration gives loss of 0.3938082861222911\n",
      "The 5801 th iteration gives loss of 0.3937755812856446\n",
      "The 5802 th iteration gives loss of 0.39374288160846244\n",
      "The 5803 th iteration gives loss of 0.39371018708894023\n",
      "The 5804 th iteration gives loss of 0.3936774977252798\n",
      "The 5805 th iteration gives loss of 0.3936448135156832\n",
      "The 5806 th iteration gives loss of 0.3936121344583421\n",
      "The 5807 th iteration gives loss of 0.39357946055148135\n",
      "The 5808 th iteration gives loss of 0.3935467917932852\n",
      "The 5809 th iteration gives loss of 0.3935141281819818\n",
      "The 5810 th iteration gives loss of 0.3934814697157625\n",
      "The 5811 th iteration gives loss of 0.3934488163928548\n",
      "The 5812 th iteration gives loss of 0.39341616821146974\n",
      "The 5813 th iteration gives loss of 0.3933835251698219\n",
      "The 5814 th iteration gives loss of 0.39335088726612266\n",
      "The 5815 th iteration gives loss of 0.3933182544986123\n",
      "The 5816 th iteration gives loss of 0.3932856268654945\n",
      "The 5817 th iteration gives loss of 0.3932530043650029\n",
      "The 5818 th iteration gives loss of 0.3932203869953567\n",
      "The 5819 th iteration gives loss of 0.3931877747547881\n",
      "The 5820 th iteration gives loss of 0.39315516764153696\n",
      "The 5821 th iteration gives loss of 0.3931225656538151\n",
      "The 5822 th iteration gives loss of 0.3930899687898657\n",
      "The 5823 th iteration gives loss of 0.39305737704791827\n",
      "The 5824 th iteration gives loss of 0.3930247904262216\n",
      "The 5825 th iteration gives loss of 0.3929922089229949\n",
      "The 5826 th iteration gives loss of 0.39295963253649135\n",
      "The 5827 th iteration gives loss of 0.3929270612649585\n",
      "The 5828 th iteration gives loss of 0.39289449510664715\n",
      "The 5829 th iteration gives loss of 0.39286193405978553\n",
      "The 5830 th iteration gives loss of 0.3928293781226282\n",
      "The 5831 th iteration gives loss of 0.39279682729342685\n",
      "The 5832 th iteration gives loss of 0.3927642815704289\n",
      "The 5833 th iteration gives loss of 0.3927317409518917\n",
      "The 5834 th iteration gives loss of 0.3926992054360725\n",
      "The 5835 th iteration gives loss of 0.39266667502122826\n",
      "The 5836 th iteration gives loss of 0.39263414970562377\n",
      "The 5837 th iteration gives loss of 0.3926016294875085\n",
      "The 5838 th iteration gives loss of 0.3925691143651464\n",
      "The 5839 th iteration gives loss of 0.39253660433681486\n",
      "The 5840 th iteration gives loss of 0.3925040994007686\n",
      "The 5841 th iteration gives loss of 0.3924715995552832\n",
      "The 5842 th iteration gives loss of 0.392439104798618\n",
      "The 5843 th iteration gives loss of 0.3924066151290542\n",
      "The 5844 th iteration gives loss of 0.39237413054486564\n",
      "The 5845 th iteration gives loss of 0.3923416510443233\n",
      "The 5846 th iteration gives loss of 0.39230917662571074\n",
      "The 5847 th iteration gives loss of 0.39227670728730046\n",
      "The 5848 th iteration gives loss of 0.3922442430273904\n",
      "The 5849 th iteration gives loss of 0.3922117838442374\n",
      "The 5850 th iteration gives loss of 0.392179329736146\n",
      "The 5851 th iteration gives loss of 0.3921468807014094\n",
      "The 5852 th iteration gives loss of 0.39211443673828494\n",
      "The 5853 th iteration gives loss of 0.39208199784508185\n",
      "The 5854 th iteration gives loss of 0.3920495640200943\n",
      "The 5855 th iteration gives loss of 0.3920171352616139\n",
      "The 5856 th iteration gives loss of 0.3919847115679371\n",
      "The 5857 th iteration gives loss of 0.3919522929373567\n",
      "The 5858 th iteration gives loss of 0.39191987936818107\n",
      "The 5859 th iteration gives loss of 0.39188747085869524\n",
      "The 5860 th iteration gives loss of 0.3918550674072104\n",
      "The 5861 th iteration gives loss of 0.3918226690120352\n",
      "The 5862 th iteration gives loss of 0.3917902756714784\n",
      "The 5863 th iteration gives loss of 0.39175788738383155\n",
      "The 5864 th iteration gives loss of 0.3917255041474225\n",
      "The 5865 th iteration gives loss of 0.3916931259605558\n",
      "The 5866 th iteration gives loss of 0.391660752821529\n",
      "The 5867 th iteration gives loss of 0.39162838472868444\n",
      "The 5868 th iteration gives loss of 0.3915960216803206\n",
      "The 5869 th iteration gives loss of 0.3915636636747742\n",
      "The 5870 th iteration gives loss of 0.39153131071033764\n",
      "The 5871 th iteration gives loss of 0.39149896278534896\n",
      "The 5872 th iteration gives loss of 0.39146661989814274\n",
      "The 5873 th iteration gives loss of 0.3914342820470269\n",
      "The 5874 th iteration gives loss of 0.39140194923033617\n",
      "The 5875 th iteration gives loss of 0.39136962144639886\n",
      "The 5876 th iteration gives loss of 0.39133729869354567\n",
      "The 5877 th iteration gives loss of 0.39130498097009997\n",
      "The 5878 th iteration gives loss of 0.39127266827441926\n",
      "The 5879 th iteration gives loss of 0.39124036060481554\n",
      "The 5880 th iteration gives loss of 0.39120805795963326\n",
      "The 5881 th iteration gives loss of 0.39117576033721974\n",
      "The 5882 th iteration gives loss of 0.39114346773591063\n",
      "The 5883 th iteration gives loss of 0.39111118015404517\n",
      "The 5884 th iteration gives loss of 0.3910788975899743\n",
      "The 5885 th iteration gives loss of 0.39104662004203294\n",
      "The 5886 th iteration gives loss of 0.3910143475085962\n",
      "The 5887 th iteration gives loss of 0.39098207998798984\n",
      "The 5888 th iteration gives loss of 0.39094981747855406\n",
      "The 5889 th iteration gives loss of 0.39091755997866157\n",
      "The 5890 th iteration gives loss of 0.39088530748667544\n",
      "The 5891 th iteration gives loss of 0.39085306000093384\n",
      "The 5892 th iteration gives loss of 0.3908208175198035\n",
      "The 5893 th iteration gives loss of 0.3907885800416421\n",
      "The 5894 th iteration gives loss of 0.39075634756481614\n",
      "The 5895 th iteration gives loss of 0.39072412008767476\n",
      "The 5896 th iteration gives loss of 0.39069189760859346\n",
      "The 5897 th iteration gives loss of 0.3906596801259434\n",
      "The 5898 th iteration gives loss of 0.39062746763809086\n",
      "The 5899 th iteration gives loss of 0.39059526014339724\n",
      "The 5900 th iteration gives loss of 0.3905630576402454\n",
      "The 5901 th iteration gives loss of 0.390530860127001\n",
      "The 5902 th iteration gives loss of 0.390498667602033\n",
      "The 5903 th iteration gives loss of 0.3904664800637342\n",
      "The 5904 th iteration gives loss of 0.3904342975104673\n",
      "The 5905 th iteration gives loss of 0.3904021199406273\n",
      "The 5906 th iteration gives loss of 0.39036994735258346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5907 th iteration gives loss of 0.39033777974471956\n",
      "The 5908 th iteration gives loss of 0.39030561711542866\n",
      "The 5909 th iteration gives loss of 0.3902734594631004\n",
      "The 5910 th iteration gives loss of 0.3902413067861193\n",
      "The 5911 th iteration gives loss of 0.39020915908285575\n",
      "The 5912 th iteration gives loss of 0.39017701635173874\n",
      "The 5913 th iteration gives loss of 0.3901448785911284\n",
      "The 5914 th iteration gives loss of 0.3901127457994415\n",
      "The 5915 th iteration gives loss of 0.39008061797505533\n",
      "The 5916 th iteration gives loss of 0.39004849511638695\n",
      "The 5917 th iteration gives loss of 0.39001637722181737\n",
      "The 5918 th iteration gives loss of 0.38998426428977445\n",
      "The 5919 th iteration gives loss of 0.3899521563186352\n",
      "The 5920 th iteration gives loss of 0.3899200533068228\n",
      "The 5921 th iteration gives loss of 0.3898879552527173\n",
      "The 5922 th iteration gives loss of 0.38985586215476004\n",
      "The 5923 th iteration gives loss of 0.3898237740113406\n",
      "The 5924 th iteration gives loss of 0.38979169082088155\n",
      "The 5925 th iteration gives loss of 0.3897596125817764\n",
      "The 5926 th iteration gives loss of 0.3897275392924579\n",
      "The 5927 th iteration gives loss of 0.38969547095134566\n",
      "The 5928 th iteration gives loss of 0.3896634075568422\n",
      "The 5929 th iteration gives loss of 0.38963134910738456\n",
      "The 5930 th iteration gives loss of 0.38959929560136863\n",
      "The 5931 th iteration gives loss of 0.3895672470372342\n",
      "The 5932 th iteration gives loss of 0.3895352034133978\n",
      "The 5933 th iteration gives loss of 0.3895031647283044\n",
      "The 5934 th iteration gives loss of 0.38947113098035346\n",
      "The 5935 th iteration gives loss of 0.38943910216799404\n",
      "The 5936 th iteration gives loss of 0.3894070782896471\n",
      "The 5937 th iteration gives loss of 0.3893750593437458\n",
      "The 5938 th iteration gives loss of 0.3893430453287331\n",
      "The 5939 th iteration gives loss of 0.38931103624303565\n",
      "The 5940 th iteration gives loss of 0.38927903208508247\n",
      "The 5941 th iteration gives loss of 0.38924703285332557\n",
      "The 5942 th iteration gives loss of 0.3892150385462068\n",
      "The 5943 th iteration gives loss of 0.38918304916216595\n",
      "The 5944 th iteration gives loss of 0.3891510646996421\n",
      "The 5945 th iteration gives loss of 0.3891190851570722\n",
      "The 5946 th iteration gives loss of 0.3890871105329146\n",
      "The 5947 th iteration gives loss of 0.3890551408256026\n",
      "The 5948 th iteration gives loss of 0.38902317603360176\n",
      "The 5949 th iteration gives loss of 0.3889912161553678\n",
      "The 5950 th iteration gives loss of 0.38895926118933966\n",
      "The 5951 th iteration gives loss of 0.388927311133978\n",
      "The 5952 th iteration gives loss of 0.38889536598772956\n",
      "The 5953 th iteration gives loss of 0.3888634257490693\n",
      "The 5954 th iteration gives loss of 0.3888314904164362\n",
      "The 5955 th iteration gives loss of 0.3887995599883076\n",
      "The 5956 th iteration gives loss of 0.38876763446313095\n",
      "The 5957 th iteration gives loss of 0.38873571383938826\n",
      "The 5958 th iteration gives loss of 0.38870379811552186\n",
      "The 5959 th iteration gives loss of 0.388671887290015\n",
      "The 5960 th iteration gives loss of 0.38863998136133326\n",
      "The 5961 th iteration gives loss of 0.3886080803279394\n",
      "The 5962 th iteration gives loss of 0.38857618418831597\n",
      "The 5963 th iteration gives loss of 0.38854429294092235\n",
      "The 5964 th iteration gives loss of 0.38851240658423997\n",
      "The 5965 th iteration gives loss of 0.3884805251167502\n",
      "The 5966 th iteration gives loss of 0.3884486485369225\n",
      "The 5967 th iteration gives loss of 0.3884167768432355\n",
      "The 5968 th iteration gives loss of 0.3883849100341832\n",
      "The 5969 th iteration gives loss of 0.3883530481082249\n",
      "The 5970 th iteration gives loss of 0.3883211910638617\n",
      "The 5971 th iteration gives loss of 0.388289338899579\n",
      "The 5972 th iteration gives loss of 0.3882574916138532\n",
      "The 5973 th iteration gives loss of 0.388225649205175\n",
      "The 5974 th iteration gives loss of 0.38819381167203937\n",
      "The 5975 th iteration gives loss of 0.3881619790129235\n",
      "The 5976 th iteration gives loss of 0.38813015122634087\n",
      "The 5977 th iteration gives loss of 0.38809832831077207\n",
      "The 5978 th iteration gives loss of 0.38806651026471706\n",
      "The 5979 th iteration gives loss of 0.38803469708667016\n",
      "The 5980 th iteration gives loss of 0.3880028887751359\n",
      "The 5981 th iteration gives loss of 0.38797108532861313\n",
      "The 5982 th iteration gives loss of 0.38793928674559297\n",
      "The 5983 th iteration gives loss of 0.3879074930245997\n",
      "The 5984 th iteration gives loss of 0.387875704164117\n",
      "The 5985 th iteration gives loss of 0.38784392016266395\n",
      "The 5986 th iteration gives loss of 0.38781214101873474\n",
      "The 5987 th iteration gives loss of 0.3877803667308537\n",
      "The 5988 th iteration gives loss of 0.3877485972975138\n",
      "The 5989 th iteration gives loss of 0.3877168327172413\n",
      "The 5990 th iteration gives loss of 0.3876850729885488\n",
      "The 5991 th iteration gives loss of 0.3876533181099519\n",
      "The 5992 th iteration gives loss of 0.38762156807995535\n",
      "The 5993 th iteration gives loss of 0.38758982289710203\n",
      "The 5994 th iteration gives loss of 0.3875580825598852\n",
      "The 5995 th iteration gives loss of 0.3875263470668285\n",
      "The 5996 th iteration gives loss of 0.3874946164164716\n",
      "The 5997 th iteration gives loss of 0.38746289060732486\n",
      "The 5998 th iteration gives loss of 0.3874311696379237\n",
      "The 5999 th iteration gives loss of 0.3873994535067825\n",
      "The 6000 th iteration gives loss of 0.38736774221243364\n",
      "The 6001 th iteration gives loss of 0.38733603575340825\n",
      "The 6002 th iteration gives loss of 0.38730433412823695\n",
      "The 6003 th iteration gives loss of 0.3872726373354622\n",
      "The 6004 th iteration gives loss of 0.38724094537360315\n",
      "The 6005 th iteration gives loss of 0.3872092582412046\n",
      "The 6006 th iteration gives loss of 0.3871775759367981\n",
      "The 6007 th iteration gives loss of 0.3871458984589243\n",
      "The 6008 th iteration gives loss of 0.3871142258061259\n",
      "The 6009 th iteration gives loss of 0.3870825579769368\n",
      "The 6010 th iteration gives loss of 0.3870508949699045\n",
      "The 6011 th iteration gives loss of 0.38701923678357025\n",
      "The 6012 th iteration gives loss of 0.38698758341649053\n",
      "The 6013 th iteration gives loss of 0.3869559348672009\n",
      "The 6014 th iteration gives loss of 0.38692429113425136\n",
      "The 6015 th iteration gives loss of 0.38689265221619223\n",
      "The 6016 th iteration gives loss of 0.3868610181115793\n",
      "The 6017 th iteration gives loss of 0.386829388818968\n",
      "The 6018 th iteration gives loss of 0.38679776433689267\n",
      "The 6019 th iteration gives loss of 0.3867661446639348\n",
      "The 6020 th iteration gives loss of 0.38673452979863343\n",
      "The 6021 th iteration gives loss of 0.3867029197395548\n",
      "The 6022 th iteration gives loss of 0.38667131448525827\n",
      "The 6023 th iteration gives loss of 0.38663971403430325\n",
      "The 6024 th iteration gives loss of 0.3866081183852565\n",
      "The 6025 th iteration gives loss of 0.3865765275366797\n",
      "The 6026 th iteration gives loss of 0.3865449414871241\n",
      "The 6027 th iteration gives loss of 0.3865133602351741\n",
      "The 6028 th iteration gives loss of 0.3864817837793974\n",
      "The 6029 th iteration gives loss of 0.3864502121183641\n",
      "The 6030 th iteration gives loss of 0.3864186452506434\n",
      "The 6031 th iteration gives loss of 0.38638708317480447\n",
      "The 6032 th iteration gives loss of 0.3863555258894038\n",
      "The 6033 th iteration gives loss of 0.38632397339304914\n",
      "The 6034 th iteration gives loss of 0.3862924256843049\n",
      "The 6035 th iteration gives loss of 0.38626088276175174\n",
      "The 6036 th iteration gives loss of 0.3862293446239572\n",
      "The 6037 th iteration gives loss of 0.3861978112695164\n",
      "The 6038 th iteration gives loss of 0.3861662826969953\n",
      "The 6039 th iteration gives loss of 0.38613475890499127\n",
      "The 6040 th iteration gives loss of 0.38610323989208256\n",
      "The 6041 th iteration gives loss of 0.38607172565686726\n",
      "The 6042 th iteration gives loss of 0.38604021619793155\n",
      "The 6043 th iteration gives loss of 0.3860087115138385\n",
      "The 6044 th iteration gives loss of 0.3859772116031958\n",
      "The 6045 th iteration gives loss of 0.3859457164646043\n",
      "The 6046 th iteration gives loss of 0.38591422609665815\n",
      "The 6047 th iteration gives loss of 0.3858827404979324\n",
      "The 6048 th iteration gives loss of 0.3858512596670384\n",
      "The 6049 th iteration gives loss of 0.3858197836025613\n",
      "The 6050 th iteration gives loss of 0.3857883123031193\n",
      "The 6051 th iteration gives loss of 0.3857568457672947\n",
      "The 6052 th iteration gives loss of 0.385725383993696\n",
      "The 6053 th iteration gives loss of 0.38569392698092425\n",
      "The 6054 th iteration gives loss of 0.38566247472758414\n",
      "The 6055 th iteration gives loss of 0.38563102723228015\n",
      "The 6056 th iteration gives loss of 0.3855995844936239\n",
      "The 6057 th iteration gives loss of 0.3855681465102231\n",
      "The 6058 th iteration gives loss of 0.38553671328067635\n",
      "The 6059 th iteration gives loss of 0.3855052848036049\n",
      "The 6060 th iteration gives loss of 0.3854738610776137\n",
      "The 6061 th iteration gives loss of 0.3854424421013299\n",
      "The 6062 th iteration gives loss of 0.385411027873363\n",
      "The 6063 th iteration gives loss of 0.38537961839231444\n",
      "The 6064 th iteration gives loss of 0.3853482136568062\n",
      "The 6065 th iteration gives loss of 0.38531681366547793\n",
      "The 6066 th iteration gives loss of 0.38528541841693115\n",
      "The 6067 th iteration gives loss of 0.3852540279097895\n",
      "The 6068 th iteration gives loss of 0.3852226421426862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 6069 th iteration gives loss of 0.3851912611142314\n",
      "The 6070 th iteration gives loss of 0.3851598848230558\n",
      "The 6071 th iteration gives loss of 0.3851285132677908\n",
      "The 6072 th iteration gives loss of 0.385097146447059\n",
      "The 6073 th iteration gives loss of 0.38506578435948285\n",
      "The 6074 th iteration gives loss of 0.38503442700371143\n",
      "The 6075 th iteration gives loss of 0.385003074378371\n",
      "The 6076 th iteration gives loss of 0.3849717264820828\n",
      "The 6077 th iteration gives loss of 0.3849403833134989\n",
      "The 6078 th iteration gives loss of 0.38490904487125027\n",
      "The 6079 th iteration gives loss of 0.3848777111539596\n",
      "The 6080 th iteration gives loss of 0.3848463821602801\n",
      "The 6081 th iteration gives loss of 0.3848150578888488\n",
      "The 6082 th iteration gives loss of 0.38478373833830326\n",
      "The 6083 th iteration gives loss of 0.38475242350728767\n",
      "The 6084 th iteration gives loss of 0.3847211133944525\n",
      "The 6085 th iteration gives loss of 0.3846898079984314\n",
      "The 6086 th iteration gives loss of 0.3846585073178908\n",
      "The 6087 th iteration gives loss of 0.38462721135146105\n",
      "The 6088 th iteration gives loss of 0.38459592009779237\n",
      "The 6089 th iteration gives loss of 0.384564633555528\n",
      "The 6090 th iteration gives loss of 0.3845333517233367\n",
      "The 6091 th iteration gives loss of 0.3845020745998613\n",
      "The 6092 th iteration gives loss of 0.3844708021837682\n",
      "The 6093 th iteration gives loss of 0.38443953447369356\n",
      "The 6094 th iteration gives loss of 0.3844082714683145\n",
      "The 6095 th iteration gives loss of 0.3843770131662626\n",
      "The 6096 th iteration gives loss of 0.38434575956621714\n",
      "The 6097 th iteration gives loss of 0.38431451066683275\n",
      "The 6098 th iteration gives loss of 0.38428326646676797\n",
      "The 6099 th iteration gives loss of 0.38425202696470356\n",
      "The 6100 th iteration gives loss of 0.38422079215929095\n",
      "The 6101 th iteration gives loss of 0.38418956204919485\n",
      "The 6102 th iteration gives loss of 0.3841583366330824\n",
      "The 6103 th iteration gives loss of 0.38412711590962006\n",
      "The 6104 th iteration gives loss of 0.38409589987748866\n",
      "The 6105 th iteration gives loss of 0.3840646885353378\n",
      "The 6106 th iteration gives loss of 0.38403348188185776\n",
      "The 6107 th iteration gives loss of 0.38400227991571084\n",
      "The 6108 th iteration gives loss of 0.38397108263557744\n",
      "The 6109 th iteration gives loss of 0.3839398900401372\n",
      "The 6110 th iteration gives loss of 0.3839087021280563\n",
      "The 6111 th iteration gives loss of 0.3838775188980167\n",
      "The 6112 th iteration gives loss of 0.38384634034871\n",
      "The 6113 th iteration gives loss of 0.38381516647880154\n",
      "The 6114 th iteration gives loss of 0.3837839972869727\n",
      "The 6115 th iteration gives loss of 0.38375283277191924\n",
      "The 6116 th iteration gives loss of 0.3837216729323157\n",
      "The 6117 th iteration gives loss of 0.38369051776685753\n",
      "The 6118 th iteration gives loss of 0.38365936727422656\n",
      "The 6119 th iteration gives loss of 0.38362822145310466\n",
      "The 6120 th iteration gives loss of 0.38359708030218187\n",
      "The 6121 th iteration gives loss of 0.38356594382015086\n",
      "The 6122 th iteration gives loss of 0.38353481200571155\n",
      "The 6123 th iteration gives loss of 0.38350368485755415\n",
      "The 6124 th iteration gives loss of 0.3834725623743646\n",
      "The 6125 th iteration gives loss of 0.3834414445548442\n",
      "The 6126 th iteration gives loss of 0.38341033139768727\n",
      "The 6127 th iteration gives loss of 0.3833792229015945\n",
      "The 6128 th iteration gives loss of 0.38334811906527105\n",
      "The 6129 th iteration gives loss of 0.38331701988740136\n",
      "The 6130 th iteration gives loss of 0.38328592536670936\n",
      "The 6131 th iteration gives loss of 0.38325483550187034\n",
      "The 6132 th iteration gives loss of 0.3832237502916104\n",
      "The 6133 th iteration gives loss of 0.38319266973461846\n",
      "The 6134 th iteration gives loss of 0.3831615938296141\n",
      "The 6135 th iteration gives loss of 0.38313052257530955\n",
      "The 6136 th iteration gives loss of 0.38309945597040446\n",
      "The 6137 th iteration gives loss of 0.38306839401360165\n",
      "The 6138 th iteration gives loss of 0.3830373367036246\n",
      "The 6139 th iteration gives loss of 0.38300628403917375\n",
      "The 6140 th iteration gives loss of 0.3829752360189752\n",
      "The 6141 th iteration gives loss of 0.3829441926417392\n",
      "The 6142 th iteration gives loss of 0.3829131539061823\n",
      "The 6143 th iteration gives loss of 0.38288211981102144\n",
      "The 6144 th iteration gives loss of 0.3828510903549825\n",
      "The 6145 th iteration gives loss of 0.3828200655367718\n",
      "The 6146 th iteration gives loss of 0.38278904535511543\n",
      "The 6147 th iteration gives loss of 0.38275802980873774\n",
      "The 6148 th iteration gives loss of 0.38272701889636584\n",
      "The 6149 th iteration gives loss of 0.3826960126167159\n",
      "The 6150 th iteration gives loss of 0.3826650109685118\n",
      "The 6151 th iteration gives loss of 0.3826340139504793\n",
      "The 6152 th iteration gives loss of 0.38260302156135745\n",
      "The 6153 th iteration gives loss of 0.38257203379987936\n",
      "The 6154 th iteration gives loss of 0.38254105066475125\n",
      "The 6155 th iteration gives loss of 0.3825100721547331\n",
      "The 6156 th iteration gives loss of 0.38247909826853677\n",
      "The 6157 th iteration gives loss of 0.38244812900490394\n",
      "The 6158 th iteration gives loss of 0.3824171643625759\n",
      "The 6159 th iteration gives loss of 0.38238620434027376\n",
      "The 6160 th iteration gives loss of 0.38235524893674644\n",
      "The 6161 th iteration gives loss of 0.38232429815072655\n",
      "The 6162 th iteration gives loss of 0.3822933519809545\n",
      "The 6163 th iteration gives loss of 0.38226241042617465\n",
      "The 6164 th iteration gives loss of 0.38223147348513925\n",
      "The 6165 th iteration gives loss of 0.3822005411565744\n",
      "The 6166 th iteration gives loss of 0.3821696134392316\n",
      "The 6167 th iteration gives loss of 0.38213869033185294\n",
      "The 6168 th iteration gives loss of 0.3821077718331841\n",
      "The 6169 th iteration gives loss of 0.3820768579419728\n",
      "The 6170 th iteration gives loss of 0.38204594865697633\n",
      "The 6171 th iteration gives loss of 0.38201504397693314\n",
      "The 6172 th iteration gives loss of 0.3819841439006042\n",
      "The 6173 th iteration gives loss of 0.38195324842673034\n",
      "The 6174 th iteration gives loss of 0.3819223575540931\n",
      "The 6175 th iteration gives loss of 0.3818914712814198\n",
      "The 6176 th iteration gives loss of 0.3818605896074711\n",
      "The 6177 th iteration gives loss of 0.38182971253099474\n",
      "The 6178 th iteration gives loss of 0.3817988400507728\n",
      "The 6179 th iteration gives loss of 0.3817679721655539\n",
      "The 6180 th iteration gives loss of 0.3817371088740945\n",
      "The 6181 th iteration gives loss of 0.38170625017516063\n",
      "The 6182 th iteration gives loss of 0.38167539606751194\n",
      "The 6183 th iteration gives loss of 0.381644546549913\n",
      "The 6184 th iteration gives loss of 0.381613701621126\n",
      "The 6185 th iteration gives loss of 0.38158286127992613\n",
      "The 6186 th iteration gives loss of 0.38155202552506445\n",
      "The 6187 th iteration gives loss of 0.3815211943553317\n",
      "The 6188 th iteration gives loss of 0.381490367769487\n",
      "The 6189 th iteration gives loss of 0.38145954576628704\n",
      "The 6190 th iteration gives loss of 0.3814287283445178\n",
      "The 6191 th iteration gives loss of 0.381397915502943\n",
      "The 6192 th iteration gives loss of 0.38136710724035644\n",
      "The 6193 th iteration gives loss of 0.38133630355550824\n",
      "The 6194 th iteration gives loss of 0.38130550444719197\n",
      "The 6195 th iteration gives loss of 0.38127470991418444\n",
      "The 6196 th iteration gives loss of 0.38124391995526097\n",
      "The 6197 th iteration gives loss of 0.3812131345691919\n",
      "The 6198 th iteration gives loss of 0.3811823537547666\n",
      "The 6199 th iteration gives loss of 0.3811515775107568\n",
      "The 6200 th iteration gives loss of 0.3811208058359602\n",
      "The 6201 th iteration gives loss of 0.3810900387291475\n",
      "The 6202 th iteration gives loss of 0.38105927618911895\n",
      "The 6203 th iteration gives loss of 0.38102851821464306\n",
      "The 6204 th iteration gives loss of 0.380997764804528\n",
      "The 6205 th iteration gives loss of 0.38096701595754173\n",
      "The 6206 th iteration gives loss of 0.38093627167248184\n",
      "The 6207 th iteration gives loss of 0.3809055319481362\n",
      "The 6208 th iteration gives loss of 0.38087479678330294\n",
      "The 6209 th iteration gives loss of 0.3808440661767593\n",
      "The 6210 th iteration gives loss of 0.38081334012731216\n",
      "The 6211 th iteration gives loss of 0.3807826186337632\n",
      "The 6212 th iteration gives loss of 0.3807519016948928\n",
      "The 6213 th iteration gives loss of 0.38072118930951154\n",
      "The 6214 th iteration gives loss of 0.3806904814764095\n",
      "The 6215 th iteration gives loss of 0.38065977819438\n",
      "The 6216 th iteration gives loss of 0.38062907946223573\n",
      "The 6217 th iteration gives loss of 0.38059838527876666\n",
      "The 6218 th iteration gives loss of 0.38056769564278764\n",
      "The 6219 th iteration gives loss of 0.3805370105530928\n",
      "The 6220 th iteration gives loss of 0.3805063300084892\n",
      "The 6221 th iteration gives loss of 0.38047565400778494\n",
      "The 6222 th iteration gives loss of 0.38044498254978415\n",
      "The 6223 th iteration gives loss of 0.38041431563329664\n",
      "The 6224 th iteration gives loss of 0.3803836532571259\n",
      "The 6225 th iteration gives loss of 0.38035299542008877\n",
      "The 6226 th iteration gives loss of 0.3803223421209868\n",
      "The 6227 th iteration gives loss of 0.38029169335864116\n",
      "The 6228 th iteration gives loss of 0.38026104913186753\n",
      "The 6229 th iteration gives loss of 0.3802304094394688\n",
      "The 6230 th iteration gives loss of 0.3801997742802633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 6231 th iteration gives loss of 0.3801691436530744\n",
      "The 6232 th iteration gives loss of 0.3801385175567095\n",
      "The 6233 th iteration gives loss of 0.3801078959899959\n",
      "The 6234 th iteration gives loss of 0.3800772789517578\n",
      "The 6235 th iteration gives loss of 0.3800466664407945\n",
      "The 6236 th iteration gives loss of 0.3800160584559455\n",
      "The 6237 th iteration gives loss of 0.3799854549960163\n",
      "The 6238 th iteration gives loss of 0.37995485605986035\n",
      "The 6239 th iteration gives loss of 0.37992426164628085\n",
      "The 6240 th iteration gives loss of 0.3798936717540996\n",
      "The 6241 th iteration gives loss of 0.3798630863821549\n",
      "The 6242 th iteration gives loss of 0.37983250552927644\n",
      "The 6243 th iteration gives loss of 0.3798019291942868\n",
      "The 6244 th iteration gives loss of 0.3797713573760125\n",
      "The 6245 th iteration gives loss of 0.37974079007328876\n",
      "The 6246 th iteration gives loss of 0.3797102272849409\n",
      "The 6247 th iteration gives loss of 0.37967966900981603\n",
      "The 6248 th iteration gives loss of 0.37964911524674144\n",
      "The 6249 th iteration gives loss of 0.37961856599454813\n",
      "The 6250 th iteration gives loss of 0.3795880212520717\n",
      "The 6251 th iteration gives loss of 0.37955748101815706\n",
      "The 6252 th iteration gives loss of 0.3795269452916425\n",
      "The 6253 th iteration gives loss of 0.37949641407135265\n",
      "The 6254 th iteration gives loss of 0.37946588735613684\n",
      "The 6255 th iteration gives loss of 0.3794353651448513\n",
      "The 6256 th iteration gives loss of 0.3794048474363099\n",
      "The 6257 th iteration gives loss of 0.37937433422937356\n",
      "The 6258 th iteration gives loss of 0.3793438255228843\n",
      "The 6259 th iteration gives loss of 0.3793133213156902\n",
      "The 6260 th iteration gives loss of 0.379282821606634\n",
      "The 6261 th iteration gives loss of 0.37925232639455897\n",
      "The 6262 th iteration gives loss of 0.3792218356783222\n",
      "The 6263 th iteration gives loss of 0.3791913494567577\n",
      "The 6264 th iteration gives loss of 0.379160867728735\n",
      "The 6265 th iteration gives loss of 0.3791303904930971\n",
      "The 6266 th iteration gives loss of 0.3790999177486912\n",
      "The 6267 th iteration gives loss of 0.3790694494943848\n",
      "The 6268 th iteration gives loss of 0.37903898572901984\n",
      "The 6269 th iteration gives loss of 0.3790085264514562\n",
      "The 6270 th iteration gives loss of 0.3789780716605462\n",
      "The 6271 th iteration gives loss of 0.37894762135514526\n",
      "The 6272 th iteration gives loss of 0.37891717553411464\n",
      "The 6273 th iteration gives loss of 0.3788867341963245\n",
      "The 6274 th iteration gives loss of 0.37885629734062526\n",
      "The 6275 th iteration gives loss of 0.3788258649658788\n",
      "The 6276 th iteration gives loss of 0.3787954370709411\n",
      "The 6277 th iteration gives loss of 0.3787650136546887\n",
      "The 6278 th iteration gives loss of 0.37873459471598514\n",
      "The 6279 th iteration gives loss of 0.3787041802536831\n",
      "The 6280 th iteration gives loss of 0.3786737702666557\n",
      "The 6281 th iteration gives loss of 0.3786433647537749\n",
      "The 6282 th iteration gives loss of 0.37861296371390973\n",
      "The 6283 th iteration gives loss of 0.3785825671459248\n",
      "The 6284 th iteration gives loss of 0.37855217504868344\n",
      "The 6285 th iteration gives loss of 0.37852178742107356\n",
      "The 6286 th iteration gives loss of 0.3784914042619556\n",
      "The 6287 th iteration gives loss of 0.37846102557020217\n",
      "The 6288 th iteration gives loss of 0.3784306513446931\n",
      "The 6289 th iteration gives loss of 0.37840028158429695\n",
      "The 6290 th iteration gives loss of 0.3783699162878928\n",
      "The 6291 th iteration gives loss of 0.3783395554543668\n",
      "The 6292 th iteration gives loss of 0.3783091990825838\n",
      "The 6293 th iteration gives loss of 0.37827884717142907\n",
      "The 6294 th iteration gives loss of 0.3782484997197869\n",
      "The 6295 th iteration gives loss of 0.37821815672652254\n",
      "The 6296 th iteration gives loss of 0.3781878181905303\n",
      "The 6297 th iteration gives loss of 0.37815748411070327\n",
      "The 6298 th iteration gives loss of 0.37812715448590156\n",
      "The 6299 th iteration gives loss of 0.37809682931504024\n",
      "The 6300 th iteration gives loss of 0.3780665085969748\n",
      "The 6301 th iteration gives loss of 0.37803619233060437\n",
      "The 6302 th iteration gives loss of 0.3780058805148146\n",
      "The 6303 th iteration gives loss of 0.37797557314849667\n",
      "The 6304 th iteration gives loss of 0.3779452702305482\n",
      "The 6305 th iteration gives loss of 0.37791497175984734\n",
      "The 6306 th iteration gives loss of 0.37788467773528395\n",
      "The 6307 th iteration gives loss of 0.37785438815576117\n",
      "The 6308 th iteration gives loss of 0.37782410302017966\n",
      "The 6309 th iteration gives loss of 0.3777938223274035\n",
      "The 6310 th iteration gives loss of 0.37776354607635526\n",
      "The 6311 th iteration gives loss of 0.37773327426592085\n",
      "The 6312 th iteration gives loss of 0.37770300689499864\n",
      "The 6313 th iteration gives loss of 0.37767274396248945\n",
      "The 6314 th iteration gives loss of 0.3776424854672848\n",
      "The 6315 th iteration gives loss of 0.37761223140830197\n",
      "The 6316 th iteration gives loss of 0.37758198178442615\n",
      "The 6317 th iteration gives loss of 0.37755173659457275\n",
      "The 6318 th iteration gives loss of 0.37752149583763184\n",
      "The 6319 th iteration gives loss of 0.37749125951250345\n",
      "The 6320 th iteration gives loss of 0.37746102761810774\n",
      "The 6321 th iteration gives loss of 0.37743080015333497\n",
      "The 6322 th iteration gives loss of 0.37740057711711134\n",
      "The 6323 th iteration gives loss of 0.37737035850832695\n",
      "The 6324 th iteration gives loss of 0.37734014432588503\n",
      "The 6325 th iteration gives loss of 0.37730993456871303\n",
      "The 6326 th iteration gives loss of 0.37727972923572106\n",
      "The 6327 th iteration gives loss of 0.3772495283258134\n",
      "The 6328 th iteration gives loss of 0.37721933183790435\n",
      "The 6329 th iteration gives loss of 0.37718913977089324\n",
      "The 6330 th iteration gives loss of 0.37715895212372647\n",
      "The 6331 th iteration gives loss of 0.37712876889528657\n",
      "The 6332 th iteration gives loss of 0.3770985900845004\n",
      "The 6333 th iteration gives loss of 0.37706841569029637\n",
      "The 6334 th iteration gives loss of 0.3770382457115825\n",
      "The 6335 th iteration gives loss of 0.37700808014727083\n",
      "The 6336 th iteration gives loss of 0.3769779189962907\n",
      "The 6337 th iteration gives loss of 0.3769477622575565\n",
      "The 6338 th iteration gives loss of 0.3769176099299929\n",
      "The 6339 th iteration gives loss of 0.376887462012526\n",
      "The 6340 th iteration gives loss of 0.37685731850407483\n",
      "The 6341 th iteration gives loss of 0.37682717940355975\n",
      "The 6342 th iteration gives loss of 0.376797044709909\n",
      "The 6343 th iteration gives loss of 0.3767669144220553\n",
      "The 6344 th iteration gives loss of 0.37673678853891657\n",
      "The 6345 th iteration gives loss of 0.3767066670594209\n",
      "The 6346 th iteration gives loss of 0.3766765499825092\n",
      "The 6347 th iteration gives loss of 0.3766464373070882\n",
      "The 6348 th iteration gives loss of 0.37661632903211173\n",
      "The 6349 th iteration gives loss of 0.37658622515650186\n",
      "The 6350 th iteration gives loss of 0.3765561256791905\n",
      "The 6351 th iteration gives loss of 0.37652603059910383\n",
      "The 6352 th iteration gives loss of 0.37649593991517927\n",
      "The 6353 th iteration gives loss of 0.37646585362636187\n",
      "The 6354 th iteration gives loss of 0.3764357717315783\n",
      "The 6355 th iteration gives loss of 0.3764056942297778\n",
      "The 6356 th iteration gives loss of 0.3763756211198817\n",
      "The 6357 th iteration gives loss of 0.3763455524008329\n",
      "The 6358 th iteration gives loss of 0.3763154880715708\n",
      "The 6359 th iteration gives loss of 0.37628542813104854\n",
      "The 6360 th iteration gives loss of 0.37625537257818986\n",
      "The 6361 th iteration gives loss of 0.3762253214119357\n",
      "The 6362 th iteration gives loss of 0.37619527463124053\n",
      "The 6363 th iteration gives loss of 0.3761652322350479\n",
      "The 6364 th iteration gives loss of 0.3761351942223047\n",
      "The 6365 th iteration gives loss of 0.3761051605919448\n",
      "The 6366 th iteration gives loss of 0.37607513134292947\n",
      "The 6367 th iteration gives loss of 0.3760451064741838\n",
      "The 6368 th iteration gives loss of 0.37601508598468264\n",
      "The 6369 th iteration gives loss of 0.375985069873351\n",
      "The 6370 th iteration gives loss of 0.37595505813915403\n",
      "The 6371 th iteration gives loss of 0.375925050781041\n",
      "The 6372 th iteration gives loss of 0.37589504779795413\n",
      "The 6373 th iteration gives loss of 0.3758650491888562\n",
      "The 6374 th iteration gives loss of 0.3758350549526995\n",
      "The 6375 th iteration gives loss of 0.37580506508843026\n",
      "The 6376 th iteration gives loss of 0.37577507959502\n",
      "The 6377 th iteration gives loss of 0.37574509847139914\n",
      "The 6378 th iteration gives loss of 0.37571512171655547\n",
      "The 6379 th iteration gives loss of 0.3756851493294294\n",
      "The 6380 th iteration gives loss of 0.3756551813089809\n",
      "The 6381 th iteration gives loss of 0.375625217654169\n",
      "The 6382 th iteration gives loss of 0.3755952583639573\n",
      "The 6383 th iteration gives loss of 0.3755653034373042\n",
      "The 6384 th iteration gives loss of 0.3755353528731678\n",
      "The 6385 th iteration gives loss of 0.37550540667052107\n",
      "The 6386 th iteration gives loss of 0.37547546482832106\n",
      "The 6387 th iteration gives loss of 0.37544552734553666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 6388 th iteration gives loss of 0.3754155942211229\n",
      "The 6389 th iteration gives loss of 0.3753856654540524\n",
      "The 6390 th iteration gives loss of 0.3753557410433004\n",
      "The 6391 th iteration gives loss of 0.375325820987831\n",
      "The 6392 th iteration gives loss of 0.3752959052866036\n",
      "The 6393 th iteration gives loss of 0.37526599393859367\n",
      "The 6394 th iteration gives loss of 0.37523608694277155\n",
      "The 6395 th iteration gives loss of 0.3752061842981127\n",
      "The 6396 th iteration gives loss of 0.37517628600359176\n",
      "The 6397 th iteration gives loss of 0.37514639205817274\n",
      "The 6398 th iteration gives loss of 0.37511650246083095\n",
      "The 6399 th iteration gives loss of 0.3750866172105502\n",
      "The 6400 th iteration gives loss of 0.3750567363062992\n",
      "The 6401 th iteration gives loss of 0.37502685974704597\n",
      "The 6402 th iteration gives loss of 0.37499698753178906\n",
      "The 6403 th iteration gives loss of 0.3749671196594771\n",
      "The 6404 th iteration gives loss of 0.37493725612912376\n",
      "The 6405 th iteration gives loss of 0.37490739693968295\n",
      "The 6406 th iteration gives loss of 0.374877542090144\n",
      "The 6407 th iteration gives loss of 0.37484769157949177\n",
      "The 6408 th iteration gives loss of 0.37481784540669727\n",
      "The 6409 th iteration gives loss of 0.37478800357076214\n",
      "The 6410 th iteration gives loss of 0.374758166070652\n",
      "The 6411 th iteration gives loss of 0.37472833290535273\n",
      "The 6412 th iteration gives loss of 0.37469850407387034\n",
      "The 6413 th iteration gives loss of 0.3746686795751731\n",
      "The 6414 th iteration gives loss of 0.3746388594082495\n",
      "The 6415 th iteration gives loss of 0.37460904357208796\n",
      "The 6416 th iteration gives loss of 0.3745792320656801\n",
      "The 6417 th iteration gives loss of 0.3745494248880216\n",
      "The 6418 th iteration gives loss of 0.37451962203809264\n",
      "The 6419 th iteration gives loss of 0.3744898235148816\n",
      "The 6420 th iteration gives loss of 0.3744600293173946\n",
      "The 6421 th iteration gives loss of 0.3744302394446208\n",
      "The 6422 th iteration gives loss of 0.37440045389554416\n",
      "The 6423 th iteration gives loss of 0.37437067266917523\n",
      "The 6424 th iteration gives loss of 0.3743408957644855\n",
      "The 6425 th iteration gives loss of 0.37431112318049464\n",
      "The 6426 th iteration gives loss of 0.37428135491619047\n",
      "The 6427 th iteration gives loss of 0.3742515909705746\n",
      "The 6428 th iteration gives loss of 0.37422183134262565\n",
      "The 6429 th iteration gives loss of 0.37419207603138205\n",
      "The 6430 th iteration gives loss of 0.3741623250358003\n",
      "The 6431 th iteration gives loss of 0.37413257835491864\n",
      "The 6432 th iteration gives loss of 0.37410283598771743\n",
      "The 6433 th iteration gives loss of 0.3740730979332051\n",
      "The 6434 th iteration gives loss of 0.374043364190376\n",
      "The 6435 th iteration gives loss of 0.37401363475826194\n",
      "The 6436 th iteration gives loss of 0.37398390963583905\n",
      "The 6437 th iteration gives loss of 0.37395418882212467\n",
      "The 6438 th iteration gives loss of 0.373924472316122\n",
      "The 6439 th iteration gives loss of 0.37389476011684225\n",
      "The 6440 th iteration gives loss of 0.3738650522232942\n",
      "The 6441 th iteration gives loss of 0.3738353486344843\n",
      "The 6442 th iteration gives loss of 0.3738056493494173\n",
      "The 6443 th iteration gives loss of 0.37377595436710953\n",
      "The 6444 th iteration gives loss of 0.37374626368657893\n",
      "The 6445 th iteration gives loss of 0.373716577306818\n",
      "The 6446 th iteration gives loss of 0.3736868952268589\n",
      "The 6447 th iteration gives loss of 0.3736572174457072\n",
      "The 6448 th iteration gives loss of 0.3736275439623804\n",
      "The 6449 th iteration gives loss of 0.3735978747758892\n",
      "The 6450 th iteration gives loss of 0.3735682098852684\n",
      "The 6451 th iteration gives loss of 0.37353854928951036\n",
      "The 6452 th iteration gives loss of 0.3735088929876342\n",
      "The 6453 th iteration gives loss of 0.37347924097867524\n",
      "The 6454 th iteration gives loss of 0.37344959326164356\n",
      "The 6455 th iteration gives loss of 0.37341994983555604\n",
      "The 6456 th iteration gives loss of 0.3733903106994372\n",
      "The 6457 th iteration gives loss of 0.3733606758523059\n",
      "The 6458 th iteration gives loss of 0.37333104529318983\n",
      "The 6459 th iteration gives loss of 0.37330141902110425\n",
      "The 6460 th iteration gives loss of 0.37327179703508506\n",
      "The 6461 th iteration gives loss of 0.37324217933413956\n",
      "The 6462 th iteration gives loss of 0.37321256591730717\n",
      "The 6463 th iteration gives loss of 0.37318295678361557\n",
      "The 6464 th iteration gives loss of 0.3731533519320842\n",
      "The 6465 th iteration gives loss of 0.373123751361736\n",
      "The 6466 th iteration gives loss of 0.3730941550716022\n",
      "The 6467 th iteration gives loss of 0.37306456306073166\n",
      "The 6468 th iteration gives loss of 0.3730349753281317\n",
      "The 6469 th iteration gives loss of 0.37300539187283643\n",
      "The 6470 th iteration gives loss of 0.37297581269387803\n",
      "The 6471 th iteration gives loss of 0.3729462377902955\n",
      "The 6472 th iteration gives loss of 0.3729166671611164\n",
      "The 6473 th iteration gives loss of 0.3728871008053815\n",
      "The 6474 th iteration gives loss of 0.3728575387221171\n",
      "The 6475 th iteration gives loss of 0.3728279809103638\n",
      "The 6476 th iteration gives loss of 0.37279842736914975\n",
      "The 6477 th iteration gives loss of 0.37276887809751663\n",
      "The 6478 th iteration gives loss of 0.3727393330944998\n",
      "The 6479 th iteration gives loss of 0.37270979235914464\n",
      "The 6480 th iteration gives loss of 0.37268025589048986\n",
      "The 6481 th iteration gives loss of 0.3726507236875669\n",
      "The 6482 th iteration gives loss of 0.37262119574942254\n",
      "The 6483 th iteration gives loss of 0.3725916720751028\n",
      "The 6484 th iteration gives loss of 0.3725621526636385\n",
      "The 6485 th iteration gives loss of 0.37253263751406296\n",
      "The 6486 th iteration gives loss of 0.37250312662545193\n",
      "The 6487 th iteration gives loss of 0.37247361999682604\n",
      "The 6488 th iteration gives loss of 0.3724441176272317\n",
      "The 6489 th iteration gives loss of 0.3724146195157291\n",
      "The 6490 th iteration gives loss of 0.3723851256613529\n",
      "The 6491 th iteration gives loss of 0.3723556360631443\n",
      "The 6492 th iteration gives loss of 0.3723261507201716\n",
      "The 6493 th iteration gives loss of 0.37229666963146146\n",
      "The 6494 th iteration gives loss of 0.37226719279607257\n",
      "The 6495 th iteration gives loss of 0.37223772021305984\n",
      "The 6496 th iteration gives loss of 0.3722082518814728\n",
      "The 6497 th iteration gives loss of 0.3721787878003576\n",
      "The 6498 th iteration gives loss of 0.37214932796876155\n",
      "The 6499 th iteration gives loss of 0.3721198723857532\n",
      "The 6500 th iteration gives loss of 0.37209042105037443\n",
      "The 6501 th iteration gives loss of 0.3720609739616923\n",
      "The 6502 th iteration gives loss of 0.3720315311187458\n",
      "The 6503 th iteration gives loss of 0.3720020925206086\n",
      "The 6504 th iteration gives loss of 0.3719726581663302\n",
      "The 6505 th iteration gives loss of 0.37194322805495755\n",
      "The 6506 th iteration gives loss of 0.3719138021855652\n",
      "The 6507 th iteration gives loss of 0.37188438055719314\n",
      "The 6508 th iteration gives loss of 0.37185496316891764\n",
      "The 6509 th iteration gives loss of 0.37182555001979745\n",
      "The 6510 th iteration gives loss of 0.37179614110888387\n",
      "The 6511 th iteration gives loss of 0.37176673643525415\n",
      "The 6512 th iteration gives loss of 0.37173733599796077\n",
      "The 6513 th iteration gives loss of 0.3717079397960628\n",
      "The 6514 th iteration gives loss of 0.3716785478286326\n",
      "The 6515 th iteration gives loss of 0.371649160094731\n",
      "The 6516 th iteration gives loss of 0.37161977659341894\n",
      "The 6517 th iteration gives loss of 0.37159039732377946\n",
      "The 6518 th iteration gives loss of 0.37156102228485677\n",
      "The 6519 th iteration gives loss of 0.3715316514757287\n",
      "The 6520 th iteration gives loss of 0.37150228489547166\n",
      "The 6521 th iteration gives loss of 0.37147292254314307\n",
      "The 6522 th iteration gives loss of 0.3714435644178213\n",
      "The 6523 th iteration gives loss of 0.37141421051857354\n",
      "The 6524 th iteration gives loss of 0.37138486084446387\n",
      "The 6525 th iteration gives loss of 0.3713555153945705\n",
      "The 6526 th iteration gives loss of 0.37132617416797126\n",
      "The 6527 th iteration gives loss of 0.3712968371637245\n",
      "The 6528 th iteration gives loss of 0.371267504380924\n",
      "The 6529 th iteration gives loss of 0.3712381758186306\n",
      "The 6530 th iteration gives loss of 0.37120885147591637\n",
      "The 6531 th iteration gives loss of 0.37117953135186765\n",
      "The 6532 th iteration gives loss of 0.37115021544556065\n",
      "The 6533 th iteration gives loss of 0.3711209037560699\n",
      "The 6534 th iteration gives loss of 0.37109159628245875\n",
      "The 6535 th iteration gives loss of 0.37106229302384364\n",
      "The 6536 th iteration gives loss of 0.37103299397926404\n",
      "The 6537 th iteration gives loss of 0.3710036991478202\n",
      "The 6538 th iteration gives loss of 0.3709744085285893\n",
      "The 6539 th iteration gives loss of 0.3709451221206592\n",
      "The 6540 th iteration gives loss of 0.37091583992310523\n",
      "The 6541 th iteration gives loss of 0.3708865619350011\n",
      "The 6542 th iteration gives loss of 0.3708572881554428\n",
      "The 6543 th iteration gives loss of 0.37082801858352266\n",
      "The 6544 th iteration gives loss of 0.37079875321831485\n",
      "The 6545 th iteration gives loss of 0.37076949205890336\n",
      "The 6546 th iteration gives loss of 0.3707402351043874\n",
      "The 6547 th iteration gives loss of 0.3707109823538372\n",
      "The 6548 th iteration gives loss of 0.37068173380634667\n",
      "The 6549 th iteration gives loss of 0.3706524894610076\n",
      "The 6550 th iteration gives loss of 0.3706232493169019\n",
      "The 6551 th iteration gives loss of 0.37059401337312364\n",
      "The 6552 th iteration gives loss of 0.37056478162877177\n",
      "The 6553 th iteration gives loss of 0.37053555408292355\n",
      "The 6554 th iteration gives loss of 0.3705063307346811\n",
      "The 6555 th iteration gives loss of 0.3704771115831406\n",
      "The 6556 th iteration gives loss of 0.37044789662737687\n",
      "The 6557 th iteration gives loss of 0.37041868586649335\n",
      "The 6558 th iteration gives loss of 0.3703894792995881\n",
      "The 6559 th iteration gives loss of 0.3703602769257516\n",
      "The 6560 th iteration gives loss of 0.3703310787440952\n",
      "The 6561 th iteration gives loss of 0.37030188475368614\n",
      "The 6562 th iteration gives loss of 0.37027269495365295\n",
      "The 6563 th iteration gives loss of 0.37024350934307687\n",
      "The 6564 th iteration gives loss of 0.3702143279210577\n",
      "The 6565 th iteration gives loss of 0.37018515068669366\n",
      "The 6566 th iteration gives loss of 0.37015597763908814\n",
      "The 6567 th iteration gives loss of 0.37012680877734316\n",
      "The 6568 th iteration gives loss of 0.37009764410054713\n",
      "The 6569 th iteration gives loss of 0.37006848360782013\n",
      "The 6570 th iteration gives loss of 0.37003932729825684\n",
      "The 6571 th iteration gives loss of 0.3700101751709631\n",
      "The 6572 th iteration gives loss of 0.36998102722503334\n",
      "The 6573 th iteration gives loss of 0.3699518834595922\n",
      "The 6574 th iteration gives loss of 0.36992274387372087\n",
      "The 6575 th iteration gives loss of 0.3698936084665483\n",
      "The 6576 th iteration gives loss of 0.36986447723715304\n",
      "The 6577 th iteration gives loss of 0.3698353501846667\n",
      "The 6578 th iteration gives loss of 0.3698062273081873\n",
      "The 6579 th iteration gives loss of 0.3697771086068213\n",
      "The 6580 th iteration gives loss of 0.369747994079698\n",
      "The 6581 th iteration gives loss of 0.36971888372590145\n",
      "The 6582 th iteration gives loss of 0.3696897775445523\n",
      "The 6583 th iteration gives loss of 0.36966067553475923\n",
      "The 6584 th iteration gives loss of 0.3696315776956412\n",
      "The 6585 th iteration gives loss of 0.3696024840263043\n",
      "The 6586 th iteration gives loss of 0.36957339452585936\n",
      "The 6587 th iteration gives loss of 0.3695443091934278\n",
      "The 6588 th iteration gives loss of 0.36951522802812037\n",
      "The 6589 th iteration gives loss of 0.36948615102904975\n",
      "The 6590 th iteration gives loss of 0.36945707819533785\n",
      "The 6591 th iteration gives loss of 0.36942800952609833\n",
      "The 6592 th iteration gives loss of 0.369398945020441\n",
      "The 6593 th iteration gives loss of 0.369369884677495\n",
      "The 6594 th iteration gives loss of 0.36934082849636785\n",
      "The 6595 th iteration gives loss of 0.3693117764761852\n",
      "The 6596 th iteration gives loss of 0.3692827286160657\n",
      "The 6597 th iteration gives loss of 0.3692536849151261\n",
      "The 6598 th iteration gives loss of 0.36922464537250405\n",
      "The 6599 th iteration gives loss of 0.3691956099872956\n",
      "The 6600 th iteration gives loss of 0.3691665787586349\n",
      "The 6601 th iteration gives loss of 0.36913755168564566\n",
      "The 6602 th iteration gives loss of 0.36910852876745337\n",
      "The 6603 th iteration gives loss of 0.36907951000318406\n",
      "The 6604 th iteration gives loss of 0.3690504953919462\n",
      "The 6605 th iteration gives loss of 0.3690214849328766\n",
      "The 6606 th iteration gives loss of 0.3689924786251016\n",
      "The 6607 th iteration gives loss of 0.3689634764677541\n",
      "The 6608 th iteration gives loss of 0.36893447845995864\n",
      "The 6609 th iteration gives loss of 0.3689054846008335\n",
      "The 6610 th iteration gives loss of 0.368876494889509\n",
      "The 6611 th iteration gives loss of 0.36884750932511134\n",
      "The 6612 th iteration gives loss of 0.36881852790679664\n",
      "The 6613 th iteration gives loss of 0.3687895506336621\n",
      "The 6614 th iteration gives loss of 0.36876057750485536\n",
      "The 6615 th iteration gives loss of 0.36873160851949727\n",
      "The 6616 th iteration gives loss of 0.3687026436767349\n",
      "The 6617 th iteration gives loss of 0.3686736829756931\n",
      "The 6618 th iteration gives loss of 0.36864472641550056\n",
      "The 6619 th iteration gives loss of 0.3686157739953025\n",
      "The 6620 th iteration gives loss of 0.36858682571423\n",
      "The 6621 th iteration gives loss of 0.3685578815714075\n",
      "The 6622 th iteration gives loss of 0.3685289415659826\n",
      "The 6623 th iteration gives loss of 0.3685000056970876\n",
      "The 6624 th iteration gives loss of 0.3684710739638657\n",
      "The 6625 th iteration gives loss of 0.3684421463654544\n",
      "The 6626 th iteration gives loss of 0.3684132229009774\n",
      "The 6627 th iteration gives loss of 0.3683843035695877\n",
      "The 6628 th iteration gives loss of 0.36835538837043047\n",
      "The 6629 th iteration gives loss of 0.368326477302635\n",
      "The 6630 th iteration gives loss of 0.36829757036533123\n",
      "The 6631 th iteration gives loss of 0.3682686675576848\n",
      "The 6632 th iteration gives loss of 0.3682397688788265\n",
      "The 6633 th iteration gives loss of 0.3682108743278909\n",
      "The 6634 th iteration gives loss of 0.368181983904034\n",
      "The 6635 th iteration gives loss of 0.36815309760638654\n",
      "The 6636 th iteration gives loss of 0.36812421543412127\n",
      "The 6637 th iteration gives loss of 0.3680953373863585\n",
      "The 6638 th iteration gives loss of 0.3680664634622474\n",
      "The 6639 th iteration gives loss of 0.3680375936609369\n",
      "The 6640 th iteration gives loss of 0.368008727981575\n",
      "The 6641 th iteration gives loss of 0.36797986642330105\n",
      "The 6642 th iteration gives loss of 0.36795100898527167\n",
      "The 6643 th iteration gives loss of 0.3679221556666338\n",
      "The 6644 th iteration gives loss of 0.36789330646654084\n",
      "The 6645 th iteration gives loss of 0.3678644613841363\n",
      "The 6646 th iteration gives loss of 0.36783562041856777\n",
      "The 6647 th iteration gives loss of 0.3678067835690076\n",
      "The 6648 th iteration gives loss of 0.3677779508345874\n",
      "The 6649 th iteration gives loss of 0.36774912221446043\n",
      "The 6650 th iteration gives loss of 0.36772029770778514\n",
      "The 6651 th iteration gives loss of 0.3676914773137067\n",
      "The 6652 th iteration gives loss of 0.3676626610313955\n",
      "The 6653 th iteration gives loss of 0.36763384885999056\n",
      "The 6654 th iteration gives loss of 0.3676050407986479\n",
      "The 6655 th iteration gives loss of 0.3675762368465463\n",
      "The 6656 th iteration gives loss of 0.3675474370028084\n",
      "The 6657 th iteration gives loss of 0.36751864126661477\n",
      "The 6658 th iteration gives loss of 0.36748984963711984\n",
      "The 6659 th iteration gives loss of 0.3674610621134822\n",
      "The 6660 th iteration gives loss of 0.36743227869484085\n",
      "The 6661 th iteration gives loss of 0.3674034993803854\n",
      "The 6662 th iteration gives loss of 0.3673747241692583\n",
      "The 6663 th iteration gives loss of 0.3673459530606168\n",
      "The 6664 th iteration gives loss of 0.36731718605363445\n",
      "The 6665 th iteration gives loss of 0.3672884231474634\n",
      "The 6666 th iteration gives loss of 0.3672596643412744\n",
      "The 6667 th iteration gives loss of 0.36723090963423627\n",
      "The 6668 th iteration gives loss of 0.36720215902548886\n",
      "The 6669 th iteration gives loss of 0.36717341251421726\n",
      "The 6670 th iteration gives loss of 0.36714467009957746\n",
      "The 6671 th iteration gives loss of 0.3671159317807376\n",
      "The 6672 th iteration gives loss of 0.3670871975568622\n",
      "The 6673 th iteration gives loss of 0.3670584674271198\n",
      "The 6674 th iteration gives loss of 0.36702974139066197\n",
      "The 6675 th iteration gives loss of 0.36700101944667757\n",
      "The 6676 th iteration gives loss of 0.36697230159432764\n",
      "The 6677 th iteration gives loss of 0.3669435878327861\n",
      "The 6678 th iteration gives loss of 0.3669148781612175\n",
      "The 6679 th iteration gives loss of 0.3668861725787794\n",
      "The 6680 th iteration gives loss of 0.36685747108466193\n",
      "The 6681 th iteration gives loss of 0.3668287736780275\n",
      "The 6682 th iteration gives loss of 0.3668000803580416\n",
      "The 6683 th iteration gives loss of 0.3667713911238855\n",
      "The 6684 th iteration gives loss of 0.36674270597473796\n",
      "The 6685 th iteration gives loss of 0.3667140249097582\n",
      "The 6686 th iteration gives loss of 0.3666853479281315\n",
      "The 6687 th iteration gives loss of 0.36665667502902205\n",
      "The 6688 th iteration gives loss of 0.3666280062116031\n",
      "The 6689 th iteration gives loss of 0.36659934147506346\n",
      "The 6690 th iteration gives loss of 0.36657068081857475\n",
      "The 6691 th iteration gives loss of 0.3665420242413059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 6692 th iteration gives loss of 0.36651337174243837\n",
      "The 6693 th iteration gives loss of 0.3664847233211544\n",
      "The 6694 th iteration gives loss of 0.36645607897663807\n",
      "The 6695 th iteration gives loss of 0.36642743870804734\n",
      "The 6696 th iteration gives loss of 0.366398802514576\n",
      "The 6697 th iteration gives loss of 0.36637017039541053\n",
      "The 6698 th iteration gives loss of 0.36634154234971517\n",
      "The 6699 th iteration gives loss of 0.3663129183766819\n",
      "The 6700 th iteration gives loss of 0.36628429847548943\n",
      "The 6701 th iteration gives loss of 0.3662556826453176\n",
      "The 6702 th iteration gives loss of 0.3662270708853548\n",
      "The 6703 th iteration gives loss of 0.3661984631947852\n",
      "The 6704 th iteration gives loss of 0.366169859572796\n",
      "The 6705 th iteration gives loss of 0.3661412600185519\n",
      "The 6706 th iteration gives loss of 0.36611266453125296\n",
      "The 6707 th iteration gives loss of 0.3660840731100857\n",
      "The 6708 th iteration gives loss of 0.36605548575423924\n",
      "The 6709 th iteration gives loss of 0.3660269024628884\n",
      "The 6710 th iteration gives loss of 0.365998323235229\n",
      "The 6711 th iteration gives loss of 0.3659697480704482\n",
      "The 6712 th iteration gives loss of 0.365941176967732\n",
      "The 6713 th iteration gives loss of 0.36591260992627195\n",
      "The 6714 th iteration gives loss of 0.36588404694525245\n",
      "The 6715 th iteration gives loss of 0.3658554880238662\n",
      "The 6716 th iteration gives loss of 0.36582693316131404\n",
      "The 6717 th iteration gives loss of 0.3657983823567681\n",
      "The 6718 th iteration gives loss of 0.36576983560943166\n",
      "The 6719 th iteration gives loss of 0.36574129291849444\n",
      "The 6720 th iteration gives loss of 0.36571275428315664\n",
      "The 6721 th iteration gives loss of 0.36568421970260134\n",
      "The 6722 th iteration gives loss of 0.3656556891760213\n",
      "The 6723 th iteration gives loss of 0.3656271627026142\n",
      "The 6724 th iteration gives loss of 0.36559864028158595\n",
      "The 6725 th iteration gives loss of 0.36557012191212684\n",
      "The 6726 th iteration gives loss of 0.36554160759341764\n",
      "The 6727 th iteration gives loss of 0.36551309732466974\n",
      "The 6728 th iteration gives loss of 0.36548459110507464\n",
      "The 6729 th iteration gives loss of 0.3654560889338296\n",
      "The 6730 th iteration gives loss of 0.36542759081013937\n",
      "The 6731 th iteration gives loss of 0.36539909673319043\n",
      "The 6732 th iteration gives loss of 0.36537060670219423\n",
      "The 6733 th iteration gives loss of 0.36534212071635064\n",
      "The 6734 th iteration gives loss of 0.36531363877484685\n",
      "The 6735 th iteration gives loss of 0.36528516087690205\n",
      "The 6736 th iteration gives loss of 0.3652566870217023\n",
      "The 6737 th iteration gives loss of 0.3652282172084489\n",
      "The 6738 th iteration gives loss of 0.365199751436351\n",
      "The 6739 th iteration gives loss of 0.3651712897046165\n",
      "The 6740 th iteration gives loss of 0.36514283201244285\n",
      "The 6741 th iteration gives loss of 0.3651143783590332\n",
      "The 6742 th iteration gives loss of 0.365085928743591\n",
      "The 6743 th iteration gives loss of 0.36505748316532444\n",
      "The 6744 th iteration gives loss of 0.36502904162344185\n",
      "The 6745 th iteration gives loss of 0.365000604117138\n",
      "The 6746 th iteration gives loss of 0.364972170645629\n",
      "The 6747 th iteration gives loss of 0.36494374120811823\n",
      "The 6748 th iteration gives loss of 0.364915315803822\n",
      "The 6749 th iteration gives loss of 0.3648868944319395\n",
      "The 6750 th iteration gives loss of 0.3648584770916788\n",
      "The 6751 th iteration gives loss of 0.3648300637822486\n",
      "The 6752 th iteration gives loss of 0.36480165450286883\n",
      "The 6753 th iteration gives loss of 0.36477324925273624\n",
      "The 6754 th iteration gives loss of 0.3647448480310732\n",
      "The 6755 th iteration gives loss of 0.36471645083708315\n",
      "The 6756 th iteration gives loss of 0.3646880576699804\n",
      "The 6757 th iteration gives loss of 0.36465966852898274\n",
      "The 6758 th iteration gives loss of 0.36463128341329104\n",
      "The 6759 th iteration gives loss of 0.3646029023221215\n",
      "The 6760 th iteration gives loss of 0.3645745252547108\n",
      "The 6761 th iteration gives loss of 0.3645461522102351\n",
      "The 6762 th iteration gives loss of 0.3645177831879402\n",
      "The 6763 th iteration gives loss of 0.36448941818702885\n",
      "The 6764 th iteration gives loss of 0.3644610572067201\n",
      "The 6765 th iteration gives loss of 0.3644327002462328\n",
      "The 6766 th iteration gives loss of 0.3644043473047691\n",
      "The 6767 th iteration gives loss of 0.36437599838155543\n",
      "The 6768 th iteration gives loss of 0.36434765347581455\n",
      "The 6769 th iteration gives loss of 0.3643193125867597\n",
      "The 6770 th iteration gives loss of 0.36429097571361957\n",
      "The 6771 th iteration gives loss of 0.36426264285561394\n",
      "The 6772 th iteration gives loss of 0.36423431401192957\n",
      "The 6773 th iteration gives loss of 0.3642059891818255\n",
      "The 6774 th iteration gives loss of 0.36417766836450377\n",
      "The 6775 th iteration gives loss of 0.364149351559206\n",
      "The 6776 th iteration gives loss of 0.3641210387651233\n",
      "The 6777 th iteration gives loss of 0.3640927299815049\n",
      "The 6778 th iteration gives loss of 0.36406442520755755\n",
      "The 6779 th iteration gives loss of 0.3640361244425319\n",
      "The 6780 th iteration gives loss of 0.3640078276856115\n",
      "The 6781 th iteration gives loss of 0.36397953493603774\n",
      "The 6782 th iteration gives loss of 0.36395124619303776\n",
      "The 6783 th iteration gives loss of 0.36392296145584213\n",
      "The 6784 th iteration gives loss of 0.36389468072367087\n",
      "The 6785 th iteration gives loss of 0.3638664039957642\n",
      "The 6786 th iteration gives loss of 0.36383813127132325\n",
      "The 6787 th iteration gives loss of 0.36380986254960035\n",
      "The 6788 th iteration gives loss of 0.36378159782980285\n",
      "The 6789 th iteration gives loss of 0.3637533371111634\n",
      "The 6790 th iteration gives loss of 0.36372508039293305\n",
      "The 6791 th iteration gives loss of 0.36369682767431233\n",
      "The 6792 th iteration gives loss of 0.36366857895454635\n",
      "The 6793 th iteration gives loss of 0.3636403342328673\n",
      "The 6794 th iteration gives loss of 0.36361209350849916\n",
      "The 6795 th iteration gives loss of 0.3635838567806722\n",
      "The 6796 th iteration gives loss of 0.36355562404861874\n",
      "The 6797 th iteration gives loss of 0.3635273953115818\n",
      "The 6798 th iteration gives loss of 0.3634991705687863\n",
      "The 6799 th iteration gives loss of 0.3634709498194646\n",
      "The 6800 th iteration gives loss of 0.36344273306284947\n",
      "The 6801 th iteration gives loss of 0.3634145202981866\n",
      "The 6802 th iteration gives loss of 0.3633863115246856\n",
      "The 6803 th iteration gives loss of 0.36335810674161656\n",
      "The 6804 th iteration gives loss of 0.3633299059481879\n",
      "The 6805 th iteration gives loss of 0.3633017091436422\n",
      "The 6806 th iteration gives loss of 0.36327351632722377\n",
      "The 6807 th iteration gives loss of 0.3632453274981721\n",
      "The 6808 th iteration gives loss of 0.36321714265571475\n",
      "The 6809 th iteration gives loss of 0.36318896179909504\n",
      "The 6810 th iteration gives loss of 0.3631607849275457\n",
      "The 6811 th iteration gives loss of 0.3631326120403111\n",
      "The 6812 th iteration gives loss of 0.36310444313663676\n",
      "The 6813 th iteration gives loss of 0.3630762782157486\n",
      "The 6814 th iteration gives loss of 0.3630481172768984\n",
      "The 6815 th iteration gives loss of 0.3630199603193217\n",
      "The 6816 th iteration gives loss of 0.3629918073422657\n",
      "The 6817 th iteration gives loss of 0.36296365834496774\n",
      "The 6818 th iteration gives loss of 0.3629355133266685\n",
      "The 6819 th iteration gives loss of 0.3629073722866191\n",
      "The 6820 th iteration gives loss of 0.36287923522405435\n",
      "The 6821 th iteration gives loss of 0.3628511021382259\n",
      "The 6822 th iteration gives loss of 0.3628229730283825\n",
      "The 6823 th iteration gives loss of 0.3627948478937515\n",
      "The 6824 th iteration gives loss of 0.36276672673358934\n",
      "The 6825 th iteration gives loss of 0.36273860954713566\n",
      "The 6826 th iteration gives loss of 0.3627104963336489\n",
      "The 6827 th iteration gives loss of 0.36268238709237477\n",
      "The 6828 th iteration gives loss of 0.36265428182254195\n",
      "The 6829 th iteration gives loss of 0.36262618052341716\n",
      "The 6830 th iteration gives loss of 0.3625980831942452\n",
      "The 6831 th iteration gives loss of 0.36256998983426286\n",
      "The 6832 th iteration gives loss of 0.3625419004427388\n",
      "The 6833 th iteration gives loss of 0.36251381501890517\n",
      "The 6834 th iteration gives loss of 0.36248573356201474\n",
      "The 6835 th iteration gives loss of 0.3624576560713291\n",
      "The 6836 th iteration gives loss of 0.36242958254609037\n",
      "The 6837 th iteration gives loss of 0.3624015129855577\n",
      "The 6838 th iteration gives loss of 0.3623734473889666\n",
      "The 6839 th iteration gives loss of 0.36234538575558695\n",
      "The 6840 th iteration gives loss of 0.36231732808465955\n",
      "The 6841 th iteration gives loss of 0.36228927437544434\n",
      "The 6842 th iteration gives loss of 0.36226122462720506\n",
      "The 6843 th iteration gives loss of 0.36223317883917044\n",
      "The 6844 th iteration gives loss of 0.36220513701062024\n",
      "The 6845 th iteration gives loss of 0.36217709914079593\n",
      "The 6846 th iteration gives loss of 0.3621490652289492\n",
      "The 6847 th iteration gives loss of 0.3621210352743473\n",
      "The 6848 th iteration gives loss of 0.3620930092762489\n",
      "The 6849 th iteration gives loss of 0.3620649872339062\n",
      "The 6850 th iteration gives loss of 0.362036969146578\n",
      "The 6851 th iteration gives loss of 0.36200895501351665\n",
      "The 6852 th iteration gives loss of 0.3619809448339694\n",
      "The 6853 th iteration gives loss of 0.3619529386072141\n",
      "The 6854 th iteration gives loss of 0.3619249363325097\n",
      "The 6855 th iteration gives loss of 0.3618969380091173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 6856 th iteration gives loss of 0.3618689436362769\n",
      "The 6857 th iteration gives loss of 0.36184095321326987\n",
      "The 6858 th iteration gives loss of 0.36181296673935986\n",
      "The 6859 th iteration gives loss of 0.3617849842137883\n",
      "The 6860 th iteration gives loss of 0.3617570056358362\n",
      "The 6861 th iteration gives loss of 0.3617290310047453\n",
      "The 6862 th iteration gives loss of 0.3617010603197994\n",
      "The 6863 th iteration gives loss of 0.3616730935802521\n",
      "The 6864 th iteration gives loss of 0.36164513078537536\n",
      "The 6865 th iteration gives loss of 0.3616171719344207\n",
      "The 6866 th iteration gives loss of 0.36158921702665997\n",
      "The 6867 th iteration gives loss of 0.3615612660613624\n",
      "The 6868 th iteration gives loss of 0.3615333190377843\n",
      "The 6869 th iteration gives loss of 0.36150537595518273\n",
      "The 6870 th iteration gives loss of 0.3614774368128467\n",
      "The 6871 th iteration gives loss of 0.36144950161003514\n",
      "The 6872 th iteration gives loss of 0.3614215703460176\n",
      "The 6873 th iteration gives loss of 0.36139364302004756\n",
      "The 6874 th iteration gives loss of 0.3613657196314113\n",
      "The 6875 th iteration gives loss of 0.36133780017936623\n",
      "The 6876 th iteration gives loss of 0.3613098846631918\n",
      "The 6877 th iteration gives loss of 0.36128197308214544\n",
      "The 6878 th iteration gives loss of 0.3612540654355112\n",
      "The 6879 th iteration gives loss of 0.3612261617225434\n",
      "The 6880 th iteration gives loss of 0.3611982619425194\n",
      "The 6881 th iteration gives loss of 0.36117036609471803\n",
      "The 6882 th iteration gives loss of 0.3611424741783971\n",
      "The 6883 th iteration gives loss of 0.36111458619283193\n",
      "The 6884 th iteration gives loss of 0.3610867021373166\n",
      "The 6885 th iteration gives loss of 0.36105882201109846\n",
      "The 6886 th iteration gives loss of 0.361030945813455\n",
      "The 6887 th iteration gives loss of 0.36100307354367495\n",
      "The 6888 th iteration gives loss of 0.3609752052010135\n",
      "The 6889 th iteration gives loss of 0.36094734078475793\n",
      "The 6890 th iteration gives loss of 0.3609194802941793\n",
      "The 6891 th iteration gives loss of 0.36089162372855155\n",
      "The 6892 th iteration gives loss of 0.3608637710871547\n",
      "The 6893 th iteration gives loss of 0.3608359223692696\n",
      "The 6894 th iteration gives loss of 0.3608080775741617\n",
      "The 6895 th iteration gives loss of 0.3607802367011157\n",
      "The 6896 th iteration gives loss of 0.3607523997493991\n",
      "The 6897 th iteration gives loss of 0.36072456671830394\n",
      "The 6898 th iteration gives loss of 0.3606967376071043\n",
      "The 6899 th iteration gives loss of 0.3606689124150837\n",
      "The 6900 th iteration gives loss of 0.3606410911415156\n",
      "The 6901 th iteration gives loss of 0.360613273785685\n",
      "The 6902 th iteration gives loss of 0.3605854603468666\n",
      "The 6903 th iteration gives loss of 0.3605576508243343\n",
      "The 6904 th iteration gives loss of 0.36052984521738707\n",
      "The 6905 th iteration gives loss of 0.36050204352529525\n",
      "The 6906 th iteration gives loss of 0.3604742457473404\n",
      "The 6907 th iteration gives loss of 0.36044645188280644\n",
      "The 6908 th iteration gives loss of 0.36041866193097805\n",
      "The 6909 th iteration gives loss of 0.3603908758911322\n",
      "The 6910 th iteration gives loss of 0.3603630937625585\n",
      "The 6911 th iteration gives loss of 0.3603353155445558\n",
      "The 6912 th iteration gives loss of 0.36030754123639114\n",
      "The 6913 th iteration gives loss of 0.36027977083734813\n",
      "The 6914 th iteration gives loss of 0.36025200434671395\n",
      "The 6915 th iteration gives loss of 0.36022424176377843\n",
      "The 6916 th iteration gives loss of 0.3601964830878256\n",
      "The 6917 th iteration gives loss of 0.36016872831814883\n",
      "The 6918 th iteration gives loss of 0.36014097745401463\n",
      "The 6919 th iteration gives loss of 0.3601132304947321\n",
      "The 6920 th iteration gives loss of 0.3600854874395897\n",
      "The 6921 th iteration gives loss of 0.36005774828786585\n",
      "The 6922 th iteration gives loss of 0.3600300130388446\n",
      "The 6923 th iteration gives loss of 0.36000228169183307\n",
      "The 6924 th iteration gives loss of 0.35997455424610547\n",
      "The 6925 th iteration gives loss of 0.3599468307009562\n",
      "The 6926 th iteration gives loss of 0.3599191110556686\n",
      "The 6927 th iteration gives loss of 0.35989139530955305\n",
      "The 6928 th iteration gives loss of 0.35986368346188574\n",
      "The 6929 th iteration gives loss of 0.3598359755119602\n",
      "The 6930 th iteration gives loss of 0.35980827145906213\n",
      "The 6931 th iteration gives loss of 0.3597805713025094\n",
      "The 6932 th iteration gives loss of 0.35975287504156667\n",
      "The 6933 th iteration gives loss of 0.3597251826755309\n",
      "The 6934 th iteration gives loss of 0.3596974942037122\n",
      "The 6935 th iteration gives loss of 0.35966980962538986\n",
      "The 6936 th iteration gives loss of 0.3596421289398755\n",
      "The 6937 th iteration gives loss of 0.3596144521464416\n",
      "The 6938 th iteration gives loss of 0.35958677924439125\n",
      "The 6939 th iteration gives loss of 0.3595591102330198\n",
      "The 6940 th iteration gives loss of 0.3595314451116349\n",
      "The 6941 th iteration gives loss of 0.3595037838795208\n",
      "The 6942 th iteration gives loss of 0.359476126535981\n",
      "The 6943 th iteration gives loss of 0.3594484730803151\n",
      "The 6944 th iteration gives loss of 0.3594208235118136\n",
      "The 6945 th iteration gives loss of 0.3593931778297695\n",
      "The 6946 th iteration gives loss of 0.3593655360334944\n",
      "The 6947 th iteration gives loss of 0.35933789812227845\n",
      "The 6948 th iteration gives loss of 0.35931026409542316\n",
      "The 6949 th iteration gives loss of 0.3592826339522285\n",
      "The 6950 th iteration gives loss of 0.35925500769199964\n",
      "The 6951 th iteration gives loss of 0.35922738531403564\n",
      "The 6952 th iteration gives loss of 0.3591997668176326\n",
      "The 6953 th iteration gives loss of 0.3591721522020927\n",
      "The 6954 th iteration gives loss of 0.3591445414667255\n",
      "The 6955 th iteration gives loss of 0.35911693461082556\n",
      "The 6956 th iteration gives loss of 0.3590893316336843\n",
      "The 6957 th iteration gives loss of 0.3590617325346317\n",
      "The 6958 th iteration gives loss of 0.3590341373129499\n",
      "The 6959 th iteration gives loss of 0.35900654596795595\n",
      "The 6960 th iteration gives loss of 0.3589789584989434\n",
      "The 6961 th iteration gives loss of 0.3589513749052215\n",
      "The 6962 th iteration gives loss of 0.35892379518608797\n",
      "The 6963 th iteration gives loss of 0.3588962193408595\n",
      "The 6964 th iteration gives loss of 0.35886864736883495\n",
      "The 6965 th iteration gives loss of 0.3588410792693241\n",
      "The 6966 th iteration gives loss of 0.358813515041629\n",
      "The 6967 th iteration gives loss of 0.35878595468505436\n",
      "The 6968 th iteration gives loss of 0.35875839819891575\n",
      "The 6969 th iteration gives loss of 0.3587308455825242\n",
      "The 6970 th iteration gives loss of 0.35870329683518265\n",
      "The 6971 th iteration gives loss of 0.35867575195618706\n",
      "The 6972 th iteration gives loss of 0.3586482109448632\n",
      "The 6973 th iteration gives loss of 0.3586206738005173\n",
      "The 6974 th iteration gives loss of 0.35859314052245417\n",
      "The 6975 th iteration gives loss of 0.358565611109978\n",
      "The 6976 th iteration gives loss of 0.3585380855624135\n",
      "The 6977 th iteration gives loss of 0.3585105638790672\n",
      "The 6978 th iteration gives loss of 0.3584830460592384\n",
      "The 6979 th iteration gives loss of 0.3584555321022558\n",
      "The 6980 th iteration gives loss of 0.3584280220074205\n",
      "The 6981 th iteration gives loss of 0.35840051577405035\n",
      "The 6982 th iteration gives loss of 0.3583730134014472\n",
      "The 6983 th iteration gives loss of 0.35834551488894517\n",
      "The 6984 th iteration gives loss of 0.3583180202358349\n",
      "The 6985 th iteration gives loss of 0.3582905294414441\n",
      "The 6986 th iteration gives loss of 0.35826304250507646\n",
      "The 6987 th iteration gives loss of 0.35823555942605095\n",
      "The 6988 th iteration gives loss of 0.3582080802036895\n",
      "The 6989 th iteration gives loss of 0.3581806048373064\n",
      "The 6990 th iteration gives loss of 0.35815313332621196\n",
      "The 6991 th iteration gives loss of 0.3581256656697299\n",
      "The 6992 th iteration gives loss of 0.35809820186716024\n",
      "The 6993 th iteration gives loss of 0.35807074191783467\n",
      "The 6994 th iteration gives loss of 0.3580432858210752\n",
      "The 6995 th iteration gives loss of 0.3580158335761839\n",
      "The 6996 th iteration gives loss of 0.35798838518248344\n",
      "The 6997 th iteration gives loss of 0.3579609406392945\n",
      "The 6998 th iteration gives loss of 0.35793349994593177\n",
      "The 6999 th iteration gives loss of 0.35790606310172784\n",
      "The 7000 th iteration gives loss of 0.35787863010598847\n",
      "The 7001 th iteration gives loss of 0.3578512009580305\n",
      "The 7002 th iteration gives loss of 0.3578237756571843\n",
      "The 7003 th iteration gives loss of 0.3577963542027772\n",
      "The 7004 th iteration gives loss of 0.3577689365941117\n",
      "The 7005 th iteration gives loss of 0.3577415228305156\n",
      "The 7006 th iteration gives loss of 0.3577141129113174\n",
      "The 7007 th iteration gives loss of 0.3576867068358358\n",
      "The 7008 th iteration gives loss of 0.357659304603387\n",
      "The 7009 th iteration gives loss of 0.3576319062133045\n",
      "The 7010 th iteration gives loss of 0.3576045116648998\n",
      "The 7011 th iteration gives loss of 0.3575771209575079\n",
      "The 7012 th iteration gives loss of 0.3575497340904466\n",
      "The 7013 th iteration gives loss of 0.35752235106304403\n",
      "The 7014 th iteration gives loss of 0.3574949718746208\n",
      "The 7015 th iteration gives loss of 0.3574675965244988\n",
      "The 7016 th iteration gives loss of 0.35744022501200584\n",
      "The 7017 th iteration gives loss of 0.3574128573364788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 7018 th iteration gives loss of 0.35738549349722387\n",
      "The 7019 th iteration gives loss of 0.3573581334935938\n",
      "The 7020 th iteration gives loss of 0.35733077732489815\n",
      "The 7021 th iteration gives loss of 0.3573034249904588\n",
      "The 7022 th iteration gives loss of 0.35727607648960974\n",
      "The 7023 th iteration gives loss of 0.35724873182168293\n",
      "The 7024 th iteration gives loss of 0.35722139098600425\n",
      "The 7025 th iteration gives loss of 0.35719405398189563\n",
      "The 7026 th iteration gives loss of 0.3571667208087033\n",
      "The 7027 th iteration gives loss of 0.35713939146574125\n",
      "The 7028 th iteration gives loss of 0.3571120659523443\n",
      "The 7029 th iteration gives loss of 0.3570847442678417\n",
      "The 7030 th iteration gives loss of 0.35705742641157295\n",
      "The 7031 th iteration gives loss of 0.3570301123828442\n",
      "The 7032 th iteration gives loss of 0.3570028021810115\n",
      "The 7033 th iteration gives loss of 0.3569754958054032\n",
      "The 7034 th iteration gives loss of 0.3569481932553423\n",
      "The 7035 th iteration gives loss of 0.3569208945301644\n",
      "The 7036 th iteration gives loss of 0.356893599629197\n",
      "The 7037 th iteration gives loss of 0.35686630855178375\n",
      "The 7038 th iteration gives loss of 0.3568390212972559\n",
      "The 7039 th iteration gives loss of 0.356811737864943\n",
      "The 7040 th iteration gives loss of 0.35678445825416727\n",
      "The 7041 th iteration gives loss of 0.3567571824642904\n",
      "The 7042 th iteration gives loss of 0.35672991049463665\n",
      "The 7043 th iteration gives loss of 0.3567026423445258\n",
      "The 7044 th iteration gives loss of 0.35667537801331745\n",
      "The 7045 th iteration gives loss of 0.3566481175003296\n",
      "The 7046 th iteration gives loss of 0.356620860804892\n",
      "The 7047 th iteration gives loss of 0.35659360792637723\n",
      "The 7048 th iteration gives loss of 0.35656635886408367\n",
      "The 7049 th iteration gives loss of 0.35653911361736784\n",
      "The 7050 th iteration gives loss of 0.3565118721855604\n",
      "The 7051 th iteration gives loss of 0.35648463456800017\n",
      "The 7052 th iteration gives loss of 0.35645740076403315\n",
      "The 7053 th iteration gives loss of 0.35643017077297756\n",
      "The 7054 th iteration gives loss of 0.35640294459420047\n",
      "The 7055 th iteration gives loss of 0.35637572222702063\n",
      "The 7056 th iteration gives loss of 0.3563485036707862\n",
      "The 7057 th iteration gives loss of 0.3563212889248407\n",
      "The 7058 th iteration gives loss of 0.35629407798851537\n",
      "The 7059 th iteration gives loss of 0.35626687086115244\n",
      "The 7060 th iteration gives loss of 0.3562396675421082\n",
      "The 7061 th iteration gives loss of 0.35621246803070256\n",
      "The 7062 th iteration gives loss of 0.35618527232628155\n",
      "The 7063 th iteration gives loss of 0.35615808042819624\n",
      "The 7064 th iteration gives loss of 0.3561308923357862\n",
      "The 7065 th iteration gives loss of 0.35610370804839697\n",
      "The 7066 th iteration gives loss of 0.3560765275653607\n",
      "The 7067 th iteration gives loss of 0.35604935088602724\n",
      "The 7068 th iteration gives loss of 0.35602217800975144\n",
      "The 7069 th iteration gives loss of 0.35599500893585867\n",
      "The 7070 th iteration gives loss of 0.35596784366370965\n",
      "The 7071 th iteration gives loss of 0.3559406821926283\n",
      "The 7072 th iteration gives loss of 0.35591352452198854\n",
      "The 7073 th iteration gives loss of 0.3558863706511126\n",
      "The 7074 th iteration gives loss of 0.35585922057935276\n",
      "The 7075 th iteration gives loss of 0.3558320743060501\n",
      "The 7076 th iteration gives loss of 0.3558049318305778\n",
      "The 7077 th iteration gives loss of 0.35577779315224456\n",
      "The 7078 th iteration gives loss of 0.3557506582704269\n",
      "The 7079 th iteration gives loss of 0.3557235271844612\n",
      "The 7080 th iteration gives loss of 0.3556963998936956\n",
      "The 7081 th iteration gives loss of 0.35566927639747264\n",
      "The 7082 th iteration gives loss of 0.3556421566951552\n",
      "The 7083 th iteration gives loss of 0.3556150407860803\n",
      "The 7084 th iteration gives loss of 0.35558792866960576\n",
      "The 7085 th iteration gives loss of 0.35556082034507225\n",
      "The 7086 th iteration gives loss of 0.35553371581182813\n",
      "The 7087 th iteration gives loss of 0.3555066150692382\n",
      "The 7088 th iteration gives loss of 0.35547951811664036\n",
      "The 7089 th iteration gives loss of 0.3554524249533918\n",
      "The 7090 th iteration gives loss of 0.35542533557884687\n",
      "The 7091 th iteration gives loss of 0.3553982499923404\n",
      "The 7092 th iteration gives loss of 0.3553711681932408\n",
      "The 7093 th iteration gives loss of 0.3553440901808967\n",
      "The 7094 th iteration gives loss of 0.35531701595465304\n",
      "The 7095 th iteration gives loss of 0.3552899455138852\n",
      "The 7096 th iteration gives loss of 0.3552628788579169\n",
      "The 7097 th iteration gives loss of 0.35523581598611786\n",
      "The 7098 th iteration gives loss of 0.35520875689783665\n",
      "The 7099 th iteration gives loss of 0.3551817015924306\n",
      "The 7100 th iteration gives loss of 0.3551546500692596\n",
      "The 7101 th iteration gives loss of 0.3551276023276742\n",
      "The 7102 th iteration gives loss of 0.3551005583670223\n",
      "The 7103 th iteration gives loss of 0.3550735181866601\n",
      "The 7104 th iteration gives loss of 0.35504648178596143\n",
      "The 7105 th iteration gives loss of 0.355019449164272\n",
      "The 7106 th iteration gives loss of 0.35499242032093714\n",
      "The 7107 th iteration gives loss of 0.35496539525532456\n",
      "The 7108 th iteration gives loss of 0.35493837396678973\n",
      "The 7109 th iteration gives loss of 0.3549113564546959\n",
      "The 7110 th iteration gives loss of 0.35488434271839936\n",
      "The 7111 th iteration gives loss of 0.3548573327572447\n",
      "The 7112 th iteration gives loss of 0.35483032657060776\n",
      "The 7113 th iteration gives loss of 0.3548033241578369\n",
      "The 7114 th iteration gives loss of 0.3547763255182939\n",
      "The 7115 th iteration gives loss of 0.35474933065133984\n",
      "The 7116 th iteration gives loss of 0.35472233955632515\n",
      "The 7117 th iteration gives loss of 0.3546953522326264\n",
      "The 7118 th iteration gives loss of 0.35466836867959356\n",
      "The 7119 th iteration gives loss of 0.3546413888965948\n",
      "The 7120 th iteration gives loss of 0.3546144128829781\n",
      "The 7121 th iteration gives loss of 0.35458744063812175\n",
      "The 7122 th iteration gives loss of 0.35456047216137765\n",
      "The 7123 th iteration gives loss of 0.3545335074521048\n",
      "The 7124 th iteration gives loss of 0.3545065465096639\n",
      "The 7125 th iteration gives loss of 0.3544795893334343\n",
      "The 7126 th iteration gives loss of 0.35445263592276716\n",
      "The 7127 th iteration gives loss of 0.354425686277021\n",
      "The 7128 th iteration gives loss of 0.3543987403955705\n",
      "The 7129 th iteration gives loss of 0.3543717982777657\n",
      "The 7130 th iteration gives loss of 0.3543448599229958\n",
      "The 7131 th iteration gives loss of 0.35431792533060247\n",
      "The 7132 th iteration gives loss of 0.354290994499942\n",
      "The 7133 th iteration gives loss of 0.35426406743041333\n",
      "The 7134 th iteration gives loss of 0.35423714412135904\n",
      "The 7135 th iteration gives loss of 0.35421022457214657\n",
      "The 7136 th iteration gives loss of 0.35418330878213905\n",
      "The 7137 th iteration gives loss of 0.35415639675071764\n",
      "The 7138 th iteration gives loss of 0.35412948847724096\n",
      "The 7139 th iteration gives loss of 0.35410258396107075\n",
      "The 7140 th iteration gives loss of 0.3540756832015837\n",
      "The 7141 th iteration gives loss of 0.35404878619815106\n",
      "The 7142 th iteration gives loss of 0.35402189295011727\n",
      "The 7143 th iteration gives loss of 0.3539950034568693\n",
      "The 7144 th iteration gives loss of 0.3539681177177796\n",
      "The 7145 th iteration gives loss of 0.35394123573220476\n",
      "The 7146 th iteration gives loss of 0.35391435749952416\n",
      "The 7147 th iteration gives loss of 0.35388748301910344\n",
      "The 7148 th iteration gives loss of 0.35386061229031307\n",
      "The 7149 th iteration gives loss of 0.3538337453125173\n",
      "The 7150 th iteration gives loss of 0.35380688208509475\n",
      "The 7151 th iteration gives loss of 0.3537800226074125\n",
      "The 7152 th iteration gives loss of 0.35375316687884206\n",
      "The 7153 th iteration gives loss of 0.3537263148987589\n",
      "The 7154 th iteration gives loss of 0.3536994666665246\n",
      "The 7155 th iteration gives loss of 0.3536726221815237\n",
      "The 7156 th iteration gives loss of 0.35364578144312203\n",
      "The 7157 th iteration gives loss of 0.3536189444506876\n",
      "The 7158 th iteration gives loss of 0.35359211120359785\n",
      "The 7159 th iteration gives loss of 0.3535652817012327\n",
      "The 7160 th iteration gives loss of 0.3535384559429655\n",
      "The 7161 th iteration gives loss of 0.3535116339281584\n",
      "The 7162 th iteration gives loss of 0.3534848156561851\n",
      "The 7163 th iteration gives loss of 0.3534580011264328\n",
      "The 7164 th iteration gives loss of 0.3534311903382678\n",
      "The 7165 th iteration gives loss of 0.35340438329107404\n",
      "The 7166 th iteration gives loss of 0.3533775799842188\n",
      "The 7167 th iteration gives loss of 0.3533507804170783\n",
      "The 7168 th iteration gives loss of 0.35332398458902414\n",
      "The 7169 th iteration gives loss of 0.3532971924994407\n",
      "The 7170 th iteration gives loss of 0.353270404147705\n",
      "The 7171 th iteration gives loss of 0.35324361953319816\n",
      "The 7172 th iteration gives loss of 0.3532168386552793\n",
      "The 7173 th iteration gives loss of 0.35319006151334037\n",
      "The 7174 th iteration gives loss of 0.3531632881067562\n",
      "The 7175 th iteration gives loss of 0.3531365184349075\n",
      "The 7176 th iteration gives loss of 0.3531097524971675\n",
      "The 7177 th iteration gives loss of 0.35308299029291484\n",
      "The 7178 th iteration gives loss of 0.3530562318215409\n",
      "The 7179 th iteration gives loss of 0.3530294770824061\n",
      "The 7180 th iteration gives loss of 0.3530027260748969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 7181 th iteration gives loss of 0.3529759787983986\n",
      "The 7182 th iteration gives loss of 0.3529492352522893\n",
      "The 7183 th iteration gives loss of 0.3529224954359464\n",
      "The 7184 th iteration gives loss of 0.3528957593487539\n",
      "The 7185 th iteration gives loss of 0.3528690269900871\n",
      "The 7186 th iteration gives loss of 0.3528422983593303\n",
      "The 7187 th iteration gives loss of 0.3528155734558769\n",
      "The 7188 th iteration gives loss of 0.3527888522790961\n",
      "The 7189 th iteration gives loss of 0.3527621348283733\n",
      "The 7190 th iteration gives loss of 0.3527354211030883\n",
      "The 7191 th iteration gives loss of 0.35270871110262114\n",
      "The 7192 th iteration gives loss of 0.3526820048263691\n",
      "The 7193 th iteration gives loss of 0.35265530227369957\n",
      "The 7194 th iteration gives loss of 0.3526286034440089\n",
      "The 7195 th iteration gives loss of 0.35260190833666977\n",
      "The 7196 th iteration gives loss of 0.3525752169510775\n",
      "The 7197 th iteration gives loss of 0.352548529286605\n",
      "The 7198 th iteration gives loss of 0.35252184534264824\n",
      "The 7199 th iteration gives loss of 0.3524951651185836\n",
      "The 7200 th iteration gives loss of 0.3524684886138028\n",
      "The 7201 th iteration gives loss of 0.3524418158276892\n",
      "The 7202 th iteration gives loss of 0.35241514675963315\n",
      "The 7203 th iteration gives loss of 0.3523884814090128\n",
      "The 7204 th iteration gives loss of 0.3523618197752261\n",
      "The 7205 th iteration gives loss of 0.35233516185764235\n",
      "The 7206 th iteration gives loss of 0.3523085076556556\n",
      "The 7207 th iteration gives loss of 0.35228185716866806\n",
      "The 7208 th iteration gives loss of 0.3522552103960512\n",
      "The 7209 th iteration gives loss of 0.3522285673372032\n",
      "The 7210 th iteration gives loss of 0.3522019279915044\n",
      "The 7211 th iteration gives loss of 0.3521752923583407\n",
      "The 7212 th iteration gives loss of 0.3521486604371188\n",
      "The 7213 th iteration gives loss of 0.35212203222720817\n",
      "The 7214 th iteration gives loss of 0.352095407728005\n",
      "The 7215 th iteration gives loss of 0.3520687869389027\n",
      "The 7216 th iteration gives loss of 0.35204216985928416\n",
      "The 7217 th iteration gives loss of 0.35201555648854865\n",
      "The 7218 th iteration gives loss of 0.35198894682608983\n",
      "The 7219 th iteration gives loss of 0.3519623408712783\n",
      "The 7220 th iteration gives loss of 0.3519357386235246\n",
      "The 7221 th iteration gives loss of 0.3519091400822133\n",
      "The 7222 th iteration gives loss of 0.35188254524672913\n",
      "The 7223 th iteration gives loss of 0.35185595411647735\n",
      "The 7224 th iteration gives loss of 0.3518293666908474\n",
      "The 7225 th iteration gives loss of 0.3518027829692196\n",
      "The 7226 th iteration gives loss of 0.3517762029510104\n",
      "The 7227 th iteration gives loss of 0.35174962663559656\n",
      "The 7228 th iteration gives loss of 0.35172305402236614\n",
      "The 7229 th iteration gives loss of 0.35169648511072077\n",
      "The 7230 th iteration gives loss of 0.3516699199000559\n",
      "The 7231 th iteration gives loss of 0.3516433583897691\n",
      "The 7232 th iteration gives loss of 0.3516168005792445\n",
      "The 7233 th iteration gives loss of 0.351590246467874\n",
      "The 7234 th iteration gives loss of 0.35156369605507176\n",
      "The 7235 th iteration gives loss of 0.35153714934022023\n",
      "The 7236 th iteration gives loss of 0.3515106063227187\n",
      "The 7237 th iteration gives loss of 0.35148406700196066\n",
      "The 7238 th iteration gives loss of 0.3514575313773454\n",
      "The 7239 th iteration gives loss of 0.35143099944826595\n",
      "The 7240 th iteration gives loss of 0.3514044712141073\n",
      "The 7241 th iteration gives loss of 0.3513779466742932\n",
      "The 7242 th iteration gives loss of 0.35135142582821194\n",
      "The 7243 th iteration gives loss of 0.3513249086752464\n",
      "The 7244 th iteration gives loss of 0.35129839521481226\n",
      "The 7245 th iteration gives loss of 0.3512718854462894\n",
      "The 7246 th iteration gives loss of 0.3512453793690934\n",
      "The 7247 th iteration gives loss of 0.35121887698261534\n",
      "The 7248 th iteration gives loss of 0.3511923782862552\n",
      "The 7249 th iteration gives loss of 0.35116588327940046\n",
      "The 7250 th iteration gives loss of 0.3511393919614718\n",
      "The 7251 th iteration gives loss of 0.3511129043318685\n",
      "The 7252 th iteration gives loss of 0.35108642038997034\n",
      "The 7253 th iteration gives loss of 0.3510599401351999\n",
      "The 7254 th iteration gives loss of 0.3510334635669367\n",
      "The 7255 th iteration gives loss of 0.3510069906845925\n",
      "The 7256 th iteration gives loss of 0.35098052148757597\n",
      "The 7257 th iteration gives loss of 0.35095405597526647\n",
      "The 7258 th iteration gives loss of 0.35092759414707697\n",
      "The 7259 th iteration gives loss of 0.3509011360024218\n",
      "The 7260 th iteration gives loss of 0.3508746815406873\n",
      "The 7261 th iteration gives loss of 0.35084823076128574\n",
      "The 7262 th iteration gives loss of 0.3508217836636174\n",
      "The 7263 th iteration gives loss of 0.35079534024707687\n",
      "The 7264 th iteration gives loss of 0.350768900511085\n",
      "The 7265 th iteration gives loss of 0.3507424644550263\n",
      "The 7266 th iteration gives loss of 0.35071603207831736\n",
      "The 7267 th iteration gives loss of 0.35068960338035055\n",
      "The 7268 th iteration gives loss of 0.350663178360543\n",
      "The 7269 th iteration gives loss of 0.3506367570182895\n",
      "The 7270 th iteration gives loss of 0.35061033935299973\n",
      "The 7271 th iteration gives loss of 0.35058392536407845\n",
      "The 7272 th iteration gives loss of 0.3505575150509329\n",
      "The 7273 th iteration gives loss of 0.3505311084129693\n",
      "The 7274 th iteration gives loss of 0.35050470544958706\n",
      "The 7275 th iteration gives loss of 0.35047830616019326\n",
      "The 7276 th iteration gives loss of 0.3504519105441998\n",
      "The 7277 th iteration gives loss of 0.35042551860101084\n",
      "The 7278 th iteration gives loss of 0.3503991303300273\n",
      "The 7279 th iteration gives loss of 0.3503727457306726\n",
      "The 7280 th iteration gives loss of 0.3503463648023458\n",
      "The 7281 th iteration gives loss of 0.35031998754444305\n",
      "The 7282 th iteration gives loss of 0.3502936139563847\n",
      "The 7283 th iteration gives loss of 0.3502672440375737\n",
      "The 7284 th iteration gives loss of 0.3502408777874294\n",
      "The 7285 th iteration gives loss of 0.3502145152053493\n",
      "The 7286 th iteration gives loss of 0.3501881562907386\n",
      "The 7287 th iteration gives loss of 0.35016180104302413\n",
      "The 7288 th iteration gives loss of 0.3501354494616003\n",
      "The 7289 th iteration gives loss of 0.35010910154587993\n",
      "The 7290 th iteration gives loss of 0.3500827572952834\n",
      "The 7291 th iteration gives loss of 0.35005641670920645\n",
      "The 7292 th iteration gives loss of 0.3500300797870636\n",
      "The 7293 th iteration gives loss of 0.35000374652826216\n",
      "The 7294 th iteration gives loss of 0.34997741693222745\n",
      "The 7295 th iteration gives loss of 0.349951090998363\n",
      "The 7296 th iteration gives loss of 0.34992476872608225\n",
      "The 7297 th iteration gives loss of 0.3498984501147905\n",
      "The 7298 th iteration gives loss of 0.34987213516390375\n",
      "The 7299 th iteration gives loss of 0.3498458238728353\n",
      "The 7300 th iteration gives loss of 0.3498195162409998\n",
      "The 7301 th iteration gives loss of 0.34979321226780435\n",
      "The 7302 th iteration gives loss of 0.3497669119526617\n",
      "The 7303 th iteration gives loss of 0.34974061529498535\n",
      "The 7304 th iteration gives loss of 0.3497143222942147\n",
      "The 7305 th iteration gives loss of 0.3496880329497253\n",
      "The 7306 th iteration gives loss of 0.3496617472609541\n",
      "The 7307 th iteration gives loss of 0.34963546522730343\n",
      "The 7308 th iteration gives loss of 0.3496091868482016\n",
      "The 7309 th iteration gives loss of 0.34958291212304776\n",
      "The 7310 th iteration gives loss of 0.3495566410512596\n",
      "The 7311 th iteration gives loss of 0.3495303736322692\n",
      "The 7312 th iteration gives loss of 0.3495041098654766\n",
      "The 7313 th iteration gives loss of 0.34947784975030505\n",
      "The 7314 th iteration gives loss of 0.34945159328616826\n",
      "The 7315 th iteration gives loss of 0.3494253404724808\n",
      "The 7316 th iteration gives loss of 0.349399091308659\n",
      "The 7317 th iteration gives loss of 0.34937284579412187\n",
      "The 7318 th iteration gives loss of 0.34934660392828887\n",
      "The 7319 th iteration gives loss of 0.34932036571057845\n",
      "The 7320 th iteration gives loss of 0.3492941311403959\n",
      "The 7321 th iteration gives loss of 0.3492679002171678\n",
      "The 7322 th iteration gives loss of 0.34924167294031994\n",
      "The 7323 th iteration gives loss of 0.34921544930925713\n",
      "The 7324 th iteration gives loss of 0.3491892293234091\n",
      "The 7325 th iteration gives loss of 0.34916301298219327\n",
      "The 7326 th iteration gives loss of 0.3491368002850239\n",
      "The 7327 th iteration gives loss of 0.34911059123131644\n",
      "The 7328 th iteration gives loss of 0.3490843858204984\n",
      "The 7329 th iteration gives loss of 0.3490581840519906\n",
      "The 7330 th iteration gives loss of 0.3490319859252104\n",
      "The 7331 th iteration gives loss of 0.3490057914395819\n",
      "The 7332 th iteration gives loss of 0.348979600594518\n",
      "The 7333 th iteration gives loss of 0.34895341338944624\n",
      "The 7334 th iteration gives loss of 0.3489272298237767\n",
      "The 7335 th iteration gives loss of 0.34890104989694576\n",
      "The 7336 th iteration gives loss of 0.34887487360837266\n",
      "The 7337 th iteration gives loss of 0.34884870095746795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 7338 th iteration gives loss of 0.34882253194366125\n",
      "The 7339 th iteration gives loss of 0.3487963665663722\n",
      "The 7340 th iteration gives loss of 0.34877020482502924\n",
      "The 7341 th iteration gives loss of 0.348744046719057\n",
      "The 7342 th iteration gives loss of 0.3487178922478824\n",
      "The 7343 th iteration gives loss of 0.3486917414108997\n",
      "The 7344 th iteration gives loss of 0.3486655942075628\n",
      "The 7345 th iteration gives loss of 0.34863945063728186\n",
      "The 7346 th iteration gives loss of 0.34861331069948837\n",
      "The 7347 th iteration gives loss of 0.34858717439359815\n",
      "The 7348 th iteration gives loss of 0.34856104171904284\n",
      "The 7349 th iteration gives loss of 0.3485349126752525\n",
      "The 7350 th iteration gives loss of 0.34850878726163564\n",
      "The 7351 th iteration gives loss of 0.3484826654776255\n",
      "The 7352 th iteration gives loss of 0.3484565473226519\n",
      "The 7353 th iteration gives loss of 0.3484304327961434\n",
      "The 7354 th iteration gives loss of 0.3484043218975103\n",
      "The 7355 th iteration gives loss of 0.34837821462619\n",
      "The 7356 th iteration gives loss of 0.3483521109816139\n",
      "The 7357 th iteration gives loss of 0.34832601096319465\n",
      "The 7358 th iteration gives loss of 0.3482999145703628\n",
      "The 7359 th iteration gives loss of 0.3482738218025612\n",
      "The 7360 th iteration gives loss of 0.3482477326592037\n",
      "The 7361 th iteration gives loss of 0.34822164713971665\n",
      "The 7362 th iteration gives loss of 0.3481955652435302\n",
      "The 7363 th iteration gives loss of 0.3481694869700741\n",
      "The 7364 th iteration gives loss of 0.34814341231876994\n",
      "The 7365 th iteration gives loss of 0.34811734128905825\n",
      "The 7366 th iteration gives loss of 0.34809127388036376\n",
      "The 7367 th iteration gives loss of 0.34806521009211216\n",
      "The 7368 th iteration gives loss of 0.3480391499237396\n",
      "The 7369 th iteration gives loss of 0.3480130933746661\n",
      "The 7370 th iteration gives loss of 0.3479870404443219\n",
      "The 7371 th iteration gives loss of 0.34796099113214457\n",
      "The 7372 th iteration gives loss of 0.34793494543755754\n",
      "The 7373 th iteration gives loss of 0.3479089033599996\n",
      "The 7374 th iteration gives loss of 0.3478828648988889\n",
      "The 7375 th iteration gives loss of 0.3478568300536703\n",
      "The 7376 th iteration gives loss of 0.34783079882376905\n",
      "The 7377 th iteration gives loss of 0.3478047712086168\n",
      "The 7378 th iteration gives loss of 0.3477787472076464\n",
      "The 7379 th iteration gives loss of 0.3477527268202813\n",
      "The 7380 th iteration gives loss of 0.34772671004596656\n",
      "The 7381 th iteration gives loss of 0.3477006968841221\n",
      "The 7382 th iteration gives loss of 0.34767468733417906\n",
      "The 7383 th iteration gives loss of 0.3476486813955861\n",
      "The 7384 th iteration gives loss of 0.34762267906777\n",
      "The 7385 th iteration gives loss of 0.34759668035015745\n",
      "The 7386 th iteration gives loss of 0.34757068524219437\n",
      "The 7387 th iteration gives loss of 0.3475446937433037\n",
      "The 7388 th iteration gives loss of 0.3475187058529102\n",
      "The 7389 th iteration gives loss of 0.34749272157047256\n",
      "The 7390 th iteration gives loss of 0.34746674089540686\n",
      "The 7391 th iteration gives loss of 0.3474407638271494\n",
      "The 7392 th iteration gives loss of 0.34741479036514183\n",
      "The 7393 th iteration gives loss of 0.3473888205088195\n",
      "The 7394 th iteration gives loss of 0.34736285425761376\n",
      "The 7395 th iteration gives loss of 0.3473368916109575\n",
      "The 7396 th iteration gives loss of 0.34731093256829465\n",
      "The 7397 th iteration gives loss of 0.3472849771290584\n",
      "The 7398 th iteration gives loss of 0.34725902529267677\n",
      "The 7399 th iteration gives loss of 0.34723307705860085\n",
      "The 7400 th iteration gives loss of 0.34720713242625334\n",
      "The 7401 th iteration gives loss of 0.34718119139507453\n",
      "The 7402 th iteration gives loss of 0.34715525396450797\n",
      "The 7403 th iteration gives loss of 0.3471293201339861\n",
      "The 7404 th iteration gives loss of 0.34710338990294304\n",
      "The 7405 th iteration gives loss of 0.34707746327082295\n",
      "The 7406 th iteration gives loss of 0.3470515402370654\n",
      "The 7407 th iteration gives loss of 0.3470256208010988\n",
      "The 7408 th iteration gives loss of 0.3469997049623769\n",
      "The 7409 th iteration gives loss of 0.34697379272031553\n",
      "The 7410 th iteration gives loss of 0.346947884074375\n",
      "The 7411 th iteration gives loss of 0.3469219790239986\n",
      "The 7412 th iteration gives loss of 0.3468960775685967\n",
      "The 7413 th iteration gives loss of 0.3468701797076333\n",
      "The 7414 th iteration gives loss of 0.3468442854405383\n",
      "The 7415 th iteration gives loss of 0.3468183947667606\n",
      "The 7416 th iteration gives loss of 0.3467925076857164\n",
      "The 7417 th iteration gives loss of 0.34676662419687465\n",
      "The 7418 th iteration gives loss of 0.34674074429966745\n",
      "The 7419 th iteration gives loss of 0.3467148679935412\n",
      "The 7420 th iteration gives loss of 0.3466889952779227\n",
      "The 7421 th iteration gives loss of 0.34666312615226413\n",
      "The 7422 th iteration gives loss of 0.34663726061599925\n",
      "The 7423 th iteration gives loss of 0.3466113986685738\n",
      "The 7424 th iteration gives loss of 0.34658554030942657\n",
      "The 7425 th iteration gives loss of 0.34655968553800337\n",
      "The 7426 th iteration gives loss of 0.3465338343537492\n",
      "The 7427 th iteration gives loss of 0.3465079867561006\n",
      "The 7428 th iteration gives loss of 0.34648214274450856\n",
      "The 7429 th iteration gives loss of 0.346456302318408\n",
      "The 7430 th iteration gives loss of 0.34643046547724515\n",
      "The 7431 th iteration gives loss of 0.3464046322204574\n",
      "The 7432 th iteration gives loss of 0.346378802547498\n",
      "The 7433 th iteration gives loss of 0.3463529764578204\n",
      "The 7434 th iteration gives loss of 0.34632715395084696\n",
      "The 7435 th iteration gives loss of 0.3463013350260347\n",
      "The 7436 th iteration gives loss of 0.34627551968281656\n",
      "The 7437 th iteration gives loss of 0.34624970792064924\n",
      "The 7438 th iteration gives loss of 0.34622389973897083\n",
      "The 7439 th iteration gives loss of 0.34619809513724376\n",
      "The 7440 th iteration gives loss of 0.3461722941148945\n",
      "The 7441 th iteration gives loss of 0.3461464966713653\n",
      "The 7442 th iteration gives loss of 0.3461207028061189\n",
      "The 7443 th iteration gives loss of 0.34609491251859265\n",
      "The 7444 th iteration gives loss of 0.3460691258082338\n",
      "The 7445 th iteration gives loss of 0.34604334267448517\n",
      "The 7446 th iteration gives loss of 0.346017563116796\n",
      "The 7447 th iteration gives loss of 0.3459917871346163\n",
      "The 7448 th iteration gives loss of 0.3459660147273839\n",
      "The 7449 th iteration gives loss of 0.3459402458945645\n",
      "The 7450 th iteration gives loss of 0.3459144806355984\n",
      "The 7451 th iteration gives loss of 0.34588871894992074\n",
      "The 7452 th iteration gives loss of 0.34586296083698737\n",
      "The 7453 th iteration gives loss of 0.34583720629625403\n",
      "The 7454 th iteration gives loss of 0.3458114553271638\n",
      "The 7455 th iteration gives loss of 0.345785707929167\n",
      "The 7456 th iteration gives loss of 0.34575996410169946\n",
      "The 7457 th iteration gives loss of 0.34573422384421953\n",
      "The 7458 th iteration gives loss of 0.3457084871561877\n",
      "The 7459 th iteration gives loss of 0.34568275403703685\n",
      "The 7460 th iteration gives loss of 0.345657024486233\n",
      "The 7461 th iteration gives loss of 0.34563129850320246\n",
      "The 7462 th iteration gives loss of 0.34560557608742715\n",
      "The 7463 th iteration gives loss of 0.3455798572383364\n",
      "The 7464 th iteration gives loss of 0.34555414195537276\n",
      "The 7465 th iteration gives loss of 0.3455284302380008\n",
      "The 7466 th iteration gives loss of 0.34550272208567495\n",
      "The 7467 th iteration gives loss of 0.3454770174978358\n",
      "The 7468 th iteration gives loss of 0.3454513164739436\n",
      "The 7469 th iteration gives loss of 0.34542561901344265\n",
      "The 7470 th iteration gives loss of 0.34539992511579115\n",
      "The 7471 th iteration gives loss of 0.34537423478043794\n",
      "The 7472 th iteration gives loss of 0.34534854800683135\n",
      "The 7473 th iteration gives loss of 0.3453228647944364\n",
      "The 7474 th iteration gives loss of 0.34529718514269675\n",
      "The 7475 th iteration gives loss of 0.34527150905105225\n",
      "The 7476 th iteration gives loss of 0.3452458365189771\n",
      "The 7477 th iteration gives loss of 0.3452201675459188\n",
      "The 7478 th iteration gives loss of 0.34519450213132125\n",
      "The 7479 th iteration gives loss of 0.3451688402746539\n",
      "The 7480 th iteration gives loss of 0.34514318197535937\n",
      "The 7481 th iteration gives loss of 0.3451175272328985\n",
      "The 7482 th iteration gives loss of 0.34509187604672004\n",
      "The 7483 th iteration gives loss of 0.3450662284162745\n",
      "The 7484 th iteration gives loss of 0.3450405843410289\n",
      "The 7485 th iteration gives loss of 0.34501494382043235\n",
      "The 7486 th iteration gives loss of 0.344989306853937\n",
      "The 7487 th iteration gives loss of 0.3449636734410046\n",
      "The 7488 th iteration gives loss of 0.34493804358107877\n",
      "The 7489 th iteration gives loss of 0.34491241727362243\n",
      "The 7490 th iteration gives loss of 0.34488679451810067\n",
      "The 7491 th iteration gives loss of 0.34486117531395144\n",
      "The 7492 th iteration gives loss of 0.34483555966064733\n",
      "The 7493 th iteration gives loss of 0.34480994755764094\n",
      "The 7494 th iteration gives loss of 0.3447843390043794\n",
      "The 7495 th iteration gives loss of 0.34475873400033186\n",
      "The 7496 th iteration gives loss of 0.3447331325449439\n",
      "The 7497 th iteration gives loss of 0.344707534637681\n",
      "The 7498 th iteration gives loss of 0.3446819402780035\n",
      "The 7499 th iteration gives loss of 0.34465634946534707\n",
      "The 7500 th iteration gives loss of 0.3446307621992065\n",
      "The 7501 th iteration gives loss of 0.3446051784790203\n",
      "The 7502 th iteration gives loss of 0.3445795983042332\n",
      "The 7503 th iteration gives loss of 0.3445540216743264\n",
      "The 7504 th iteration gives loss of 0.34452844858874526\n",
      "The 7505 th iteration gives loss of 0.34450287904695787\n",
      "The 7506 th iteration gives loss of 0.34447731304841095\n",
      "The 7507 th iteration gives loss of 0.34445175059257543\n",
      "The 7508 th iteration gives loss of 0.3444261916789084\n",
      "The 7509 th iteration gives loss of 0.34440063630687207\n",
      "The 7510 th iteration gives loss of 0.3443750844759126\n",
      "The 7511 th iteration gives loss of 0.3443495361855013\n",
      "The 7512 th iteration gives loss of 0.3443239914350947\n",
      "The 7513 th iteration gives loss of 0.34429845022416417\n",
      "The 7514 th iteration gives loss of 0.344272912552153\n",
      "The 7515 th iteration gives loss of 0.3442473784185355\n",
      "The 7516 th iteration gives loss of 0.34422184782276205\n",
      "The 7517 th iteration gives loss of 0.3441963207643079\n",
      "The 7518 th iteration gives loss of 0.3441707972426257\n",
      "The 7519 th iteration gives loss of 0.3441452772571779\n",
      "The 7520 th iteration gives loss of 0.34411976080742224\n",
      "The 7521 th iteration gives loss of 0.3440942478928259\n",
      "The 7522 th iteration gives loss of 0.3440687385128494\n",
      "The 7523 th iteration gives loss of 0.3440432326669597\n",
      "The 7524 th iteration gives loss of 0.34401773035461747\n",
      "The 7525 th iteration gives loss of 0.3439922315752749\n",
      "The 7526 th iteration gives loss of 0.3439667363284092\n",
      "The 7527 th iteration gives loss of 0.34394124461347986\n",
      "The 7528 th iteration gives loss of 0.3439157564299513\n",
      "The 7529 th iteration gives loss of 0.34389027177727677\n",
      "The 7530 th iteration gives loss of 0.3438647906549272\n",
      "The 7531 th iteration gives loss of 0.34383931306237336\n",
      "The 7532 th iteration gives loss of 0.3438138389990806\n",
      "The 7533 th iteration gives loss of 0.34378836846449734\n",
      "The 7534 th iteration gives loss of 0.34376290145809907\n",
      "The 7535 th iteration gives loss of 0.34373743797933914\n",
      "The 7536 th iteration gives loss of 0.34371197802769765\n",
      "The 7537 th iteration gives loss of 0.34368652160264096\n",
      "The 7538 th iteration gives loss of 0.34366106870362567\n",
      "The 7539 th iteration gives loss of 0.34363561933012454\n",
      "The 7540 th iteration gives loss of 0.3436101734815828\n",
      "The 7541 th iteration gives loss of 0.3435847311574881\n",
      "The 7542 th iteration gives loss of 0.3435592923573043\n",
      "The 7543 th iteration gives loss of 0.34353385708049244\n",
      "The 7544 th iteration gives loss of 0.34350842532651776\n",
      "The 7545 th iteration gives loss of 0.34348299709485414\n",
      "The 7546 th iteration gives loss of 0.34345757238495217\n",
      "The 7547 th iteration gives loss of 0.3434321511962956\n",
      "The 7548 th iteration gives loss of 0.3434067335283524\n",
      "The 7549 th iteration gives loss of 0.3433813193805779\n",
      "The 7550 th iteration gives loss of 0.34335590875244953\n",
      "The 7551 th iteration gives loss of 0.34333050164342654\n",
      "The 7552 th iteration gives loss of 0.3433050980529808\n",
      "The 7553 th iteration gives loss of 0.343279697980588\n",
      "The 7554 th iteration gives loss of 0.3432543014257088\n",
      "The 7555 th iteration gives loss of 0.34322890838781406\n",
      "The 7556 th iteration gives loss of 0.34320351886636946\n",
      "The 7557 th iteration gives loss of 0.3431781328608429\n",
      "The 7558 th iteration gives loss of 0.34315275037071025\n",
      "The 7559 th iteration gives loss of 0.3431273713954397\n",
      "The 7560 th iteration gives loss of 0.34310199593449014\n",
      "The 7561 th iteration gives loss of 0.3430766239873385\n",
      "The 7562 th iteration gives loss of 0.3430512555534628\n",
      "The 7563 th iteration gives loss of 0.34302589063232386\n",
      "The 7564 th iteration gives loss of 0.3430005292233982\n",
      "The 7565 th iteration gives loss of 0.3429751713261483\n",
      "The 7566 th iteration gives loss of 0.34294981694004534\n",
      "The 7567 th iteration gives loss of 0.34292446606456406\n",
      "The 7568 th iteration gives loss of 0.342899118699177\n",
      "The 7569 th iteration gives loss of 0.34287377484334847\n",
      "The 7570 th iteration gives loss of 0.34284843449656377\n",
      "The 7571 th iteration gives loss of 0.34282309765827385\n",
      "The 7572 th iteration gives loss of 0.3427977643279677\n",
      "The 7573 th iteration gives loss of 0.34277243450511596\n",
      "The 7574 th iteration gives loss of 0.342747108189172\n",
      "The 7575 th iteration gives loss of 0.3427217853796333\n",
      "The 7576 th iteration gives loss of 0.34269646607595566\n",
      "The 7577 th iteration gives loss of 0.3426711502776236\n",
      "The 7578 th iteration gives loss of 0.3426458379841034\n",
      "The 7579 th iteration gives loss of 0.3426205291948626\n",
      "The 7580 th iteration gives loss of 0.34259522390938035\n",
      "The 7581 th iteration gives loss of 0.3425699221271256\n",
      "The 7582 th iteration gives loss of 0.34254462384758533\n",
      "The 7583 th iteration gives loss of 0.34251932907021587\n",
      "The 7584 th iteration gives loss of 0.34249403779449883\n",
      "The 7585 th iteration gives loss of 0.34246875001990673\n",
      "The 7586 th iteration gives loss of 0.342443465745923\n",
      "The 7587 th iteration gives loss of 0.34241818497201676\n",
      "The 7588 th iteration gives loss of 0.34239290769765596\n",
      "The 7589 th iteration gives loss of 0.3423676339223224\n",
      "The 7590 th iteration gives loss of 0.3423423636454841\n",
      "The 7591 th iteration gives loss of 0.342317096866627\n",
      "The 7592 th iteration gives loss of 0.3422918335852192\n",
      "The 7593 th iteration gives loss of 0.34226657380073716\n",
      "The 7594 th iteration gives loss of 0.34224131751265513\n",
      "The 7595 th iteration gives loss of 0.34221606472044785\n",
      "The 7596 th iteration gives loss of 0.3421908154236011\n",
      "The 7597 th iteration gives loss of 0.34216556962157696\n",
      "The 7598 th iteration gives loss of 0.34214032731386607\n",
      "The 7599 th iteration gives loss of 0.3421150884999302\n",
      "The 7600 th iteration gives loss of 0.34208985317925505\n",
      "The 7601 th iteration gives loss of 0.34206462135132304\n",
      "The 7602 th iteration gives loss of 0.34203939301560216\n",
      "The 7603 th iteration gives loss of 0.34201416817157265\n",
      "The 7604 th iteration gives loss of 0.3419889468187086\n",
      "The 7605 th iteration gives loss of 0.3419637289564956\n",
      "The 7606 th iteration gives loss of 0.3419385145844017\n",
      "The 7607 th iteration gives loss of 0.3419133037019136\n",
      "The 7608 th iteration gives loss of 0.3418880963085019\n",
      "The 7609 th iteration gives loss of 0.34186289240365014\n",
      "The 7610 th iteration gives loss of 0.3418376919868418\n",
      "The 7611 th iteration gives loss of 0.34181249505754446\n",
      "The 7612 th iteration gives loss of 0.3417873016152515\n",
      "The 7613 th iteration gives loss of 0.3417621116594261\n",
      "The 7614 th iteration gives loss of 0.34173692518954646\n",
      "The 7615 th iteration gives loss of 0.34171174220510075\n",
      "The 7616 th iteration gives loss of 0.3416865627055751\n",
      "The 7617 th iteration gives loss of 0.3416613866904457\n",
      "The 7618 th iteration gives loss of 0.34163621415917067\n",
      "The 7619 th iteration gives loss of 0.3416110451112618\n",
      "The 7620 th iteration gives loss of 0.3415858795461853\n",
      "The 7621 th iteration gives loss of 0.341560717463416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 7622 th iteration gives loss of 0.3415355588624417\n",
      "The 7623 th iteration gives loss of 0.341510403742742\n",
      "The 7624 th iteration gives loss of 0.34148525210380504\n",
      "The 7625 th iteration gives loss of 0.3414601039450922\n",
      "The 7626 th iteration gives loss of 0.3414349592661093\n",
      "The 7627 th iteration gives loss of 0.3414098180663199\n",
      "The 7628 th iteration gives loss of 0.3413846803452107\n",
      "The 7629 th iteration gives loss of 0.34135954610227015\n",
      "The 7630 th iteration gives loss of 0.34133441533696746\n",
      "The 7631 th iteration gives loss of 0.3413092880487956\n",
      "The 7632 th iteration gives loss of 0.3412841642372369\n",
      "The 7633 th iteration gives loss of 0.34125904390176853\n",
      "The 7634 th iteration gives loss of 0.34123392704187405\n",
      "The 7635 th iteration gives loss of 0.34120881365703126\n",
      "The 7636 th iteration gives loss of 0.3411837037467316\n",
      "The 7637 th iteration gives loss of 0.34115859731046017\n",
      "The 7638 th iteration gives loss of 0.34113349434769347\n",
      "The 7639 th iteration gives loss of 0.341108394857915\n",
      "The 7640 th iteration gives loss of 0.3410832988406163\n",
      "The 7641 th iteration gives loss of 0.34105820629527045\n",
      "The 7642 th iteration gives loss of 0.34103311722136886\n",
      "The 7643 th iteration gives loss of 0.3410080316184061\n",
      "The 7644 th iteration gives loss of 0.34098294948584823\n",
      "The 7645 th iteration gives loss of 0.3409578708231797\n",
      "The 7646 th iteration gives loss of 0.3409327956298968\n",
      "The 7647 th iteration gives loss of 0.34090772390547586\n",
      "The 7648 th iteration gives loss of 0.34088265564941334\n",
      "The 7649 th iteration gives loss of 0.34085759086117423\n",
      "The 7650 th iteration gives loss of 0.3408325295402688\n",
      "The 7651 th iteration gives loss of 0.3408074716861605\n",
      "The 7652 th iteration gives loss of 0.34078241729834946\n",
      "The 7653 th iteration gives loss of 0.3407573663763227\n",
      "The 7654 th iteration gives loss of 0.34073231891955374\n",
      "The 7655 th iteration gives loss of 0.34070727492754066\n",
      "The 7656 th iteration gives loss of 0.3406822343997561\n",
      "The 7657 th iteration gives loss of 0.3406571973357071\n",
      "The 7658 th iteration gives loss of 0.34063216373486255\n",
      "The 7659 th iteration gives loss of 0.34060713359670824\n",
      "The 7660 th iteration gives loss of 0.34058210692075214\n",
      "The 7661 th iteration gives loss of 0.3405570837064621\n",
      "The 7662 th iteration gives loss of 0.34053206395332764\n",
      "The 7663 th iteration gives loss of 0.3405070476608497\n",
      "The 7664 th iteration gives loss of 0.3404820348284991\n",
      "The 7665 th iteration gives loss of 0.3404570254557736\n",
      "The 7666 th iteration gives loss of 0.34043201954216273\n",
      "The 7667 th iteration gives loss of 0.34040701708713905\n",
      "The 7668 th iteration gives loss of 0.3403820180902193\n",
      "The 7669 th iteration gives loss of 0.3403570225508645\n",
      "The 7670 th iteration gives loss of 0.34033203046857835\n",
      "The 7671 th iteration gives loss of 0.34030704184284427\n",
      "The 7672 th iteration gives loss of 0.3402820566731461\n",
      "The 7673 th iteration gives loss of 0.340257074958991\n",
      "The 7674 th iteration gives loss of 0.3402320966998555\n",
      "The 7675 th iteration gives loss of 0.3402071218952211\n",
      "The 7676 th iteration gives loss of 0.34018215054459905\n",
      "The 7677 th iteration gives loss of 0.34015718264746203\n",
      "The 7678 th iteration gives loss of 0.3401322182033103\n",
      "The 7679 th iteration gives loss of 0.34010725721162943\n",
      "The 7680 th iteration gives loss of 0.34008229967190784\n",
      "The 7681 th iteration gives loss of 0.34005734558362766\n",
      "The 7682 th iteration gives loss of 0.34003239494630116\n",
      "The 7683 th iteration gives loss of 0.3400074477594072\n",
      "The 7684 th iteration gives loss of 0.33998250402243557\n",
      "The 7685 th iteration gives loss of 0.33995756373487407\n",
      "The 7686 th iteration gives loss of 0.3399326268962259\n",
      "The 7687 th iteration gives loss of 0.3399076935059804\n",
      "The 7688 th iteration gives loss of 0.3398827635636117\n",
      "The 7689 th iteration gives loss of 0.33985783706862643\n",
      "The 7690 th iteration gives loss of 0.339832914020522\n",
      "The 7691 th iteration gives loss of 0.3398079944187827\n",
      "The 7692 th iteration gives loss of 0.33978307826289794\n",
      "The 7693 th iteration gives loss of 0.33975816555237104\n",
      "The 7694 th iteration gives loss of 0.3397332562866784\n",
      "The 7695 th iteration gives loss of 0.3397083504653295\n",
      "The 7696 th iteration gives loss of 0.339683448087802\n",
      "The 7697 th iteration gives loss of 0.33965854915359944\n",
      "The 7698 th iteration gives loss of 0.33963365366222154\n",
      "The 7699 th iteration gives loss of 0.3396087616131428\n",
      "The 7700 th iteration gives loss of 0.3395838730058595\n",
      "The 7701 th iteration gives loss of 0.3395589878398841\n",
      "The 7702 th iteration gives loss of 0.33953410611470247\n",
      "The 7703 th iteration gives loss of 0.339509227829795\n",
      "The 7704 th iteration gives loss of 0.33948435298466567\n",
      "The 7705 th iteration gives loss of 0.33945948157881006\n",
      "The 7706 th iteration gives loss of 0.3394346136117201\n",
      "The 7707 th iteration gives loss of 0.33940974908288823\n",
      "The 7708 th iteration gives loss of 0.33938488799182376\n",
      "The 7709 th iteration gives loss of 0.3393600303380033\n",
      "The 7710 th iteration gives loss of 0.33933517612093245\n",
      "The 7711 th iteration gives loss of 0.33931032534010575\n",
      "The 7712 th iteration gives loss of 0.3392854779950134\n",
      "The 7713 th iteration gives loss of 0.3392606340851544\n",
      "The 7714 th iteration gives loss of 0.33923579361002343\n",
      "The 7715 th iteration gives loss of 0.33921095656911565\n",
      "The 7716 th iteration gives loss of 0.3391861229619315\n",
      "The 7717 th iteration gives loss of 0.33916129278795676\n",
      "The 7718 th iteration gives loss of 0.33913646604671377\n",
      "The 7719 th iteration gives loss of 0.33911164273766925\n",
      "The 7720 th iteration gives loss of 0.33908682286033337\n",
      "The 7721 th iteration gives loss of 0.33906200641419654\n",
      "The 7722 th iteration gives loss of 0.33903719339876115\n",
      "The 7723 th iteration gives loss of 0.3390123838135299\n",
      "The 7724 th iteration gives loss of 0.33898757765798554\n",
      "The 7725 th iteration gives loss of 0.3389627749316361\n",
      "The 7726 th iteration gives loss of 0.3389379756339825\n",
      "The 7727 th iteration gives loss of 0.33891317976451296\n",
      "The 7728 th iteration gives loss of 0.33888838732273124\n",
      "The 7729 th iteration gives loss of 0.3388635983081384\n",
      "The 7730 th iteration gives loss of 0.33883881272022554\n",
      "The 7731 th iteration gives loss of 0.33881403055848275\n",
      "The 7732 th iteration gives loss of 0.33878925182242714\n",
      "The 7733 th iteration gives loss of 0.33876447651154534\n",
      "The 7734 th iteration gives loss of 0.33873970462534436\n",
      "The 7735 th iteration gives loss of 0.33871493616331994\n",
      "The 7736 th iteration gives loss of 0.3386901711249707\n",
      "The 7737 th iteration gives loss of 0.33866540950979734\n",
      "The 7738 th iteration gives loss of 0.3386406513172923\n",
      "The 7739 th iteration gives loss of 0.33861589654695884\n",
      "The 7740 th iteration gives loss of 0.33859114519831274\n",
      "The 7741 th iteration gives loss of 0.33856639727082144\n",
      "The 7742 th iteration gives loss of 0.33854165276401793\n",
      "The 7743 th iteration gives loss of 0.338516911677386\n",
      "The 7744 th iteration gives loss of 0.33849217401041665\n",
      "The 7745 th iteration gives loss of 0.33846743976262617\n",
      "The 7746 th iteration gives loss of 0.33844270893351713\n",
      "The 7747 th iteration gives loss of 0.3384179815225824\n",
      "The 7748 th iteration gives loss of 0.33839325752932387\n",
      "The 7749 th iteration gives loss of 0.3383685369532458\n",
      "The 7750 th iteration gives loss of 0.3383438197938493\n",
      "The 7751 th iteration gives loss of 0.3383191060506203\n",
      "The 7752 th iteration gives loss of 0.3382943957230808\n",
      "The 7753 th iteration gives loss of 0.3382696888107225\n",
      "The 7754 th iteration gives loss of 0.338244985313057\n",
      "The 7755 th iteration gives loss of 0.3382202852295736\n",
      "The 7756 th iteration gives loss of 0.33819558855978316\n",
      "The 7757 th iteration gives loss of 0.33817089530317923\n",
      "The 7758 th iteration gives loss of 0.33814620545927776\n",
      "The 7759 th iteration gives loss of 0.3381215190275716\n",
      "The 7760 th iteration gives loss of 0.3380968360075683\n",
      "The 7761 th iteration gives loss of 0.33807215639875543\n",
      "The 7762 th iteration gives loss of 0.33804748020066416\n",
      "The 7763 th iteration gives loss of 0.33802280741277574\n",
      "The 7764 th iteration gives loss of 0.3379981380346053\n",
      "The 7765 th iteration gives loss of 0.33797347206564265\n",
      "The 7766 th iteration gives loss of 0.337948809505406\n",
      "The 7767 th iteration gives loss of 0.33792415035339135\n",
      "The 7768 th iteration gives loss of 0.33789949460910845\n",
      "The 7769 th iteration gives loss of 0.33787484227204906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 7770 th iteration gives loss of 0.33785019334173333\n",
      "The 7771 th iteration gives loss of 0.33782554781765534\n",
      "The 7772 th iteration gives loss of 0.33780090569932186\n",
      "The 7773 th iteration gives loss of 0.33777626698623753\n",
      "The 7774 th iteration gives loss of 0.3377516316779012\n",
      "The 7775 th iteration gives loss of 0.3377269997738322\n",
      "The 7776 th iteration gives loss of 0.3377023712735295\n",
      "The 7777 th iteration gives loss of 0.33767774617649254\n",
      "The 7778 th iteration gives loss of 0.3376531244822342\n",
      "The 7779 th iteration gives loss of 0.3376285061902552\n",
      "The 7780 th iteration gives loss of 0.3376038913000657\n",
      "The 7781 th iteration gives loss of 0.3375792798111687\n",
      "The 7782 th iteration gives loss of 0.3375546717230696\n",
      "The 7783 th iteration gives loss of 0.33753006703526434\n",
      "The 7784 th iteration gives loss of 0.337505465747277\n",
      "The 7785 th iteration gives loss of 0.3374808678586062\n",
      "The 7786 th iteration gives loss of 0.33745627336876166\n",
      "The 7787 th iteration gives loss of 0.3374316822772413\n",
      "The 7788 th iteration gives loss of 0.3374070945835639\n",
      "The 7789 th iteration gives loss of 0.3373825102872328\n",
      "The 7790 th iteration gives loss of 0.3373579293877486\n",
      "The 7791 th iteration gives loss of 0.3373333518846238\n",
      "The 7792 th iteration gives loss of 0.33730877777736984\n",
      "The 7793 th iteration gives loss of 0.33728420706548967\n",
      "The 7794 th iteration gives loss of 0.3372596397484901\n",
      "The 7795 th iteration gives loss of 0.33723507582586904\n",
      "The 7796 th iteration gives loss of 0.33721051529714996\n",
      "The 7797 th iteration gives loss of 0.3371859581618421\n",
      "The 7798 th iteration gives loss of 0.3371614044194517\n",
      "The 7799 th iteration gives loss of 0.3371368540694816\n",
      "The 7800 th iteration gives loss of 0.3371123071114341\n",
      "The 7801 th iteration gives loss of 0.3370877635448207\n",
      "The 7802 th iteration gives loss of 0.33706322336916317\n",
      "The 7803 th iteration gives loss of 0.33703868658396624\n",
      "The 7804 th iteration gives loss of 0.3370141531887353\n",
      "The 7805 th iteration gives loss of 0.3369896231829848\n",
      "The 7806 th iteration gives loss of 0.3369650965662072\n",
      "The 7807 th iteration gives loss of 0.3369405733379347\n",
      "The 7808 th iteration gives loss of 0.33691605349766135\n",
      "The 7809 th iteration gives loss of 0.33689153704490254\n",
      "The 7810 th iteration gives loss of 0.33686702397916846\n",
      "The 7811 th iteration gives loss of 0.33684251429997\n",
      "The 7812 th iteration gives loss of 0.33681800800681133\n",
      "The 7813 th iteration gives loss of 0.33679350509921396\n",
      "The 7814 th iteration gives loss of 0.33676900557668615\n",
      "The 7815 th iteration gives loss of 0.3367445094387187\n",
      "The 7816 th iteration gives loss of 0.336720016684844\n",
      "The 7817 th iteration gives loss of 0.33669552731457286\n",
      "The 7818 th iteration gives loss of 0.33667104132740744\n",
      "The 7819 th iteration gives loss of 0.3366465587228591\n",
      "The 7820 th iteration gives loss of 0.3366220795004431\n",
      "The 7821 th iteration gives loss of 0.3365976036596788\n",
      "The 7822 th iteration gives loss of 0.336573131200063\n",
      "The 7823 th iteration gives loss of 0.3365486621211145\n",
      "The 7824 th iteration gives loss of 0.3365241964223444\n",
      "The 7825 th iteration gives loss of 0.33649973410327144\n",
      "The 7826 th iteration gives loss of 0.3364752751633924\n",
      "The 7827 th iteration gives loss of 0.336450819602226\n",
      "The 7828 th iteration gives loss of 0.3364263674192857\n",
      "The 7829 th iteration gives loss of 0.33640191861408864\n",
      "The 7830 th iteration gives loss of 0.3363774731861439\n",
      "The 7831 th iteration gives loss of 0.33635303113497317\n",
      "The 7832 th iteration gives loss of 0.3363285924600737\n",
      "The 7833 th iteration gives loss of 0.3363041571609697\n",
      "The 7834 th iteration gives loss of 0.3362797252371712\n",
      "The 7835 th iteration gives loss of 0.33625529668818893\n",
      "The 7836 th iteration gives loss of 0.3362308715135424\n",
      "The 7837 th iteration gives loss of 0.3362064497127339\n",
      "The 7838 th iteration gives loss of 0.33618203128529206\n",
      "The 7839 th iteration gives loss of 0.3361576162307245\n",
      "The 7840 th iteration gives loss of 0.3361332045485428\n",
      "The 7841 th iteration gives loss of 0.3361087962382661\n",
      "The 7842 th iteration gives loss of 0.33608439129940304\n",
      "The 7843 th iteration gives loss of 0.33605998973146994\n",
      "The 7844 th iteration gives loss of 0.33603559153398416\n",
      "The 7845 th iteration gives loss of 0.3360111967064569\n",
      "The 7846 th iteration gives loss of 0.33598680524841096\n",
      "The 7847 th iteration gives loss of 0.3359624171593497\n",
      "The 7848 th iteration gives loss of 0.3359380324387948\n",
      "The 7849 th iteration gives loss of 0.3359136510862637\n",
      "The 7850 th iteration gives loss of 0.3358892731012691\n",
      "The 7851 th iteration gives loss of 0.3358648984833215\n",
      "The 7852 th iteration gives loss of 0.3358405272319433\n",
      "The 7853 th iteration gives loss of 0.33581615934665315\n",
      "The 7854 th iteration gives loss of 0.33579179482695837\n",
      "The 7855 th iteration gives loss of 0.33576743367238604\n",
      "The 7856 th iteration gives loss of 0.33574307588243657\n",
      "The 7857 th iteration gives loss of 0.335718721456644\n",
      "The 7858 th iteration gives loss of 0.3356943703945129\n",
      "The 7859 th iteration gives loss of 0.33567002269556157\n",
      "The 7860 th iteration gives loss of 0.3356456783593198\n",
      "The 7861 th iteration gives loss of 0.335621337385282\n",
      "The 7862 th iteration gives loss of 0.3355969997729747\n",
      "The 7863 th iteration gives loss of 0.3355726655219188\n",
      "The 7864 th iteration gives loss of 0.33554833463162886\n",
      "The 7865 th iteration gives loss of 0.33552400710163105\n",
      "The 7866 th iteration gives loss of 0.3354996829314281\n",
      "The 7867 th iteration gives loss of 0.33547536212054335\n",
      "The 7868 th iteration gives loss of 0.335451044668503\n",
      "The 7869 th iteration gives loss of 0.3354267305748177\n",
      "The 7870 th iteration gives loss of 0.335402419838997\n",
      "The 7871 th iteration gives loss of 0.3353781124605806\n",
      "The 7872 th iteration gives loss of 0.3353538084390707\n",
      "The 7873 th iteration gives loss of 0.335329507774\n",
      "The 7874 th iteration gives loss of 0.335305210464858\n",
      "The 7875 th iteration gives loss of 0.3352809165111888\n",
      "The 7876 th iteration gives loss of 0.335256625912502\n",
      "The 7877 th iteration gives loss of 0.3352323386683287\n",
      "The 7878 th iteration gives loss of 0.33520805477817706\n",
      "The 7879 th iteration gives loss of 0.33518377424157125\n",
      "The 7880 th iteration gives loss of 0.3351594970580236\n",
      "The 7881 th iteration gives loss of 0.3351352232270515\n",
      "The 7882 th iteration gives loss of 0.3351109527481793\n",
      "The 7883 th iteration gives loss of 0.33508668562093996\n",
      "The 7884 th iteration gives loss of 0.3350624218448384\n",
      "The 7885 th iteration gives loss of 0.3350381614193888\n",
      "The 7886 th iteration gives loss of 0.3350139043441316\n",
      "The 7887 th iteration gives loss of 0.3349896506185659\n",
      "The 7888 th iteration gives loss of 0.3349654002422291\n",
      "The 7889 th iteration gives loss of 0.33494115321463813\n",
      "The 7890 th iteration gives loss of 0.334916909535308\n",
      "The 7891 th iteration gives loss of 0.33489266920376665\n",
      "The 7892 th iteration gives loss of 0.3348684322195199\n",
      "The 7893 th iteration gives loss of 0.3348441985821091\n",
      "The 7894 th iteration gives loss of 0.33481996829104393\n",
      "The 7895 th iteration gives loss of 0.334795741345849\n",
      "The 7896 th iteration gives loss of 0.33477151774603553\n",
      "The 7897 th iteration gives loss of 0.3347472974911403\n",
      "The 7898 th iteration gives loss of 0.3347230805806781\n",
      "The 7899 th iteration gives loss of 0.33469886701418033\n",
      "The 7900 th iteration gives loss of 0.3346746567911502\n",
      "The 7901 th iteration gives loss of 0.3346504499111265\n",
      "The 7902 th iteration gives loss of 0.33462624637361793\n",
      "The 7903 th iteration gives loss of 0.33460204617815525\n",
      "The 7904 th iteration gives loss of 0.3345778493242682\n",
      "The 7905 th iteration gives loss of 0.3345536558114623\n",
      "The 7906 th iteration gives loss of 0.33452946563927594\n",
      "The 7907 th iteration gives loss of 0.3345052788072236\n",
      "The 7908 th iteration gives loss of 0.3344810953148251\n",
      "The 7909 th iteration gives loss of 0.33445691516160486\n",
      "The 7910 th iteration gives loss of 0.3344327383470948\n",
      "The 7911 th iteration gives loss of 0.33440856487081394\n",
      "The 7912 th iteration gives loss of 0.3343843947322906\n",
      "The 7913 th iteration gives loss of 0.33436022793103126\n",
      "The 7914 th iteration gives loss of 0.3343360644665735\n",
      "The 7915 th iteration gives loss of 0.33431190433844704\n",
      "The 7916 th iteration gives loss of 0.33428774754616364\n",
      "The 7917 th iteration gives loss of 0.33426359408924927\n",
      "The 7918 th iteration gives loss of 0.33423944396723415\n",
      "The 7919 th iteration gives loss of 0.33421529717963083\n",
      "The 7920 th iteration gives loss of 0.33419115372598013\n",
      "The 7921 th iteration gives loss of 0.33416701360579254\n",
      "The 7922 th iteration gives loss of 0.3341428768186035\n",
      "The 7923 th iteration gives loss of 0.33411874336393316\n",
      "The 7924 th iteration gives loss of 0.3340946132413144\n",
      "The 7925 th iteration gives loss of 0.3340704864502486\n",
      "The 7926 th iteration gives loss of 0.3340463629902829\n",
      "The 7927 th iteration gives loss of 0.3340222428609444\n",
      "The 7928 th iteration gives loss of 0.3339981260617417\n",
      "The 7929 th iteration gives loss of 0.3339740125922058\n",
      "The 7930 th iteration gives loss of 0.3339499024518819\n",
      "The 7931 th iteration gives loss of 0.3339257956402707\n",
      "The 7932 th iteration gives loss of 0.33390169215690224\n",
      "The 7933 th iteration gives loss of 0.33387759200132316\n",
      "The 7934 th iteration gives loss of 0.33385349517303003\n",
      "The 7935 th iteration gives loss of 0.3338294016715732\n",
      "The 7936 th iteration gives loss of 0.33380531149645976\n",
      "The 7937 th iteration gives loss of 0.33378122464723536\n",
      "The 7938 th iteration gives loss of 0.33375714112342414\n",
      "The 7939 th iteration gives loss of 0.3337330609245395\n",
      "The 7940 th iteration gives loss of 0.33370898405011656\n",
      "The 7941 th iteration gives loss of 0.3336849104996752\n",
      "The 7942 th iteration gives loss of 0.3336608402727558\n",
      "The 7943 th iteration gives loss of 0.3336367733688786\n",
      "The 7944 th iteration gives loss of 0.33361270978756963\n",
      "The 7945 th iteration gives loss of 0.3335886495283603\n",
      "The 7946 th iteration gives loss of 0.33356459259078103\n",
      "The 7947 th iteration gives loss of 0.33354053897434394\n",
      "The 7948 th iteration gives loss of 0.33351648867859285\n",
      "The 7949 th iteration gives loss of 0.3334924417030451\n",
      "The 7950 th iteration gives loss of 0.33346839804724177\n",
      "The 7951 th iteration gives loss of 0.33344435771070935\n",
      "The 7952 th iteration gives loss of 0.3334203206929665\n",
      "The 7953 th iteration gives loss of 0.33339628699354307\n",
      "The 7954 th iteration gives loss of 0.33337225661196984\n",
      "The 7955 th iteration gives loss of 0.3333482295477794\n",
      "The 7956 th iteration gives loss of 0.33332420580050465\n",
      "The 7957 th iteration gives loss of 0.33330018536965944\n",
      "The 7958 th iteration gives loss of 0.33327616825478534\n",
      "The 7959 th iteration gives loss of 0.33325215445541506\n",
      "The 7960 th iteration gives loss of 0.33322814397106487\n",
      "The 7961 th iteration gives loss of 0.3332041368012609\n",
      "The 7962 th iteration gives loss of 0.333180132945548\n",
      "The 7963 th iteration gives loss of 0.3331561324034459\n",
      "The 7964 th iteration gives loss of 0.33313213517449997\n",
      "The 7965 th iteration gives loss of 0.3331081412582188\n",
      "The 7966 th iteration gives loss of 0.3330841506541555\n",
      "The 7967 th iteration gives loss of 0.3330601633618238\n",
      "The 7968 th iteration gives loss of 0.3330361793807555\n",
      "The 7969 th iteration gives loss of 0.3330121987104762\n",
      "The 7970 th iteration gives loss of 0.33298822135052897\n",
      "The 7971 th iteration gives loss of 0.33296424730043434\n",
      "The 7972 th iteration gives loss of 0.3329402765597353\n",
      "The 7973 th iteration gives loss of 0.3329163091279468\n",
      "The 7974 th iteration gives loss of 0.3328923450046124\n",
      "The 7975 th iteration gives loss of 0.33286838418926135\n",
      "The 7976 th iteration gives loss of 0.33284442668142755\n",
      "The 7977 th iteration gives loss of 0.33282047248063434\n",
      "The 7978 th iteration gives loss of 0.3327965215864206\n",
      "The 7979 th iteration gives loss of 0.3327725739983085\n",
      "The 7980 th iteration gives loss of 0.332748629715833\n",
      "The 7981 th iteration gives loss of 0.33272468873852984\n",
      "The 7982 th iteration gives loss of 0.3327007510659311\n",
      "The 7983 th iteration gives loss of 0.33267681669756977\n",
      "The 7984 th iteration gives loss of 0.33265288563297146\n",
      "The 7985 th iteration gives loss of 0.3326289578716776\n",
      "The 7986 th iteration gives loss of 0.33260503341321845\n",
      "The 7987 th iteration gives loss of 0.3325811122571181\n",
      "The 7988 th iteration gives loss of 0.3325571944029111\n",
      "The 7989 th iteration gives loss of 0.3325332798501444\n",
      "The 7990 th iteration gives loss of 0.33250936859834\n",
      "The 7991 th iteration gives loss of 0.33248546064702117\n",
      "The 7992 th iteration gives loss of 0.3324615559957424\n",
      "The 7993 th iteration gives loss of 0.3324376546440227\n",
      "The 7994 th iteration gives loss of 0.332413756591397\n",
      "The 7995 th iteration gives loss of 0.33238986183740526\n",
      "The 7996 th iteration gives loss of 0.3323659703815725\n",
      "The 7997 th iteration gives loss of 0.3323420822234418\n",
      "The 7998 th iteration gives loss of 0.33231819736253276\n",
      "The 7999 th iteration gives loss of 0.3322943157983968\n",
      "The 8000 th iteration gives loss of 0.3322704375305565\n",
      "The 8001 th iteration gives loss of 0.33224656255855756\n",
      "The 8002 th iteration gives loss of 0.3322226908819177\n",
      "The 8003 th iteration gives loss of 0.33219882250018573\n",
      "The 8004 th iteration gives loss of 0.33217495741288233\n",
      "The 8005 th iteration gives loss of 0.3321510956195587\n",
      "The 8006 th iteration gives loss of 0.3321272371197304\n",
      "The 8007 th iteration gives loss of 0.33210338191294886\n",
      "The 8008 th iteration gives loss of 0.3320795299987354\n",
      "The 8009 th iteration gives loss of 0.332055681376644\n",
      "The 8010 th iteration gives loss of 0.3320318360461914\n",
      "The 8011 th iteration gives loss of 0.33200799400692943\n",
      "The 8012 th iteration gives loss of 0.33198415525837693\n",
      "The 8013 th iteration gives loss of 0.33196031980007806\n",
      "The 8014 th iteration gives loss of 0.3319364876315673\n",
      "The 8015 th iteration gives loss of 0.3319126587523848\n",
      "The 8016 th iteration gives loss of 0.33188883316205675\n",
      "The 8017 th iteration gives loss of 0.33186501086012754\n",
      "The 8018 th iteration gives loss of 0.33184119184612376\n",
      "The 8019 th iteration gives loss of 0.3318173761195946\n",
      "The 8020 th iteration gives loss of 0.331793563680068\n",
      "The 8021 th iteration gives loss of 0.3317697545270842\n",
      "The 8022 th iteration gives loss of 0.33174594866017343\n",
      "The 8023 th iteration gives loss of 0.33172214607887585\n",
      "The 8024 th iteration gives loss of 0.3316983467827406\n",
      "The 8025 th iteration gives loss of 0.3316745507712933\n",
      "The 8026 th iteration gives loss of 0.33165075804406513\n",
      "The 8027 th iteration gives loss of 0.33162696860060326\n",
      "The 8028 th iteration gives loss of 0.331603182440438\n",
      "The 8029 th iteration gives loss of 0.33157939956310634\n",
      "The 8030 th iteration gives loss of 0.33155561996814537\n",
      "The 8031 th iteration gives loss of 0.33153184365510585\n",
      "The 8032 th iteration gives loss of 0.33150807062351567\n",
      "The 8033 th iteration gives loss of 0.33148430087291053\n",
      "The 8034 th iteration gives loss of 0.33146053440283746\n",
      "The 8035 th iteration gives loss of 0.3314367712128209\n",
      "The 8036 th iteration gives loss of 0.3314130113023977\n",
      "The 8037 th iteration gives loss of 0.33138925467112623\n",
      "The 8038 th iteration gives loss of 0.3313655013185355\n",
      "The 8039 th iteration gives loss of 0.33134175124415366\n",
      "The 8040 th iteration gives loss of 0.33131800444752624\n",
      "The 8041 th iteration gives loss of 0.3312942609281987\n",
      "The 8042 th iteration gives loss of 0.3312705206856986\n",
      "The 8043 th iteration gives loss of 0.33124678371957805\n",
      "The 8044 th iteration gives loss of 0.33122305002936336\n",
      "The 8045 th iteration gives loss of 0.33119931961459137\n",
      "The 8046 th iteration gives loss of 0.3311755924748196\n",
      "The 8047 th iteration gives loss of 0.33115186860957546\n",
      "The 8048 th iteration gives loss of 0.33112814801839435\n",
      "The 8049 th iteration gives loss of 0.3311044307008204\n",
      "The 8050 th iteration gives loss of 0.3310807166563915\n",
      "The 8051 th iteration gives loss of 0.33105700588465997\n",
      "The 8052 th iteration gives loss of 0.3310332983851507\n",
      "The 8053 th iteration gives loss of 0.33100959415740433\n",
      "The 8054 th iteration gives loss of 0.33098589320097405\n",
      "The 8055 th iteration gives loss of 0.3309621955153826\n",
      "The 8056 th iteration gives loss of 0.3309385011001841\n",
      "The 8057 th iteration gives loss of 0.33091480995491424\n",
      "The 8058 th iteration gives loss of 0.3308911220791081\n",
      "The 8059 th iteration gives loss of 0.3308674374723225\n",
      "The 8060 th iteration gives loss of 0.33084375613406664\n",
      "The 8061 th iteration gives loss of 0.330820078063913\n",
      "The 8062 th iteration gives loss of 0.3307964032613943\n",
      "The 8063 th iteration gives loss of 0.33077273172604876\n",
      "The 8064 th iteration gives loss of 0.3307490634574112\n",
      "The 8065 th iteration gives loss of 0.33072539845503485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 8066 th iteration gives loss of 0.330701736718456\n",
      "The 8067 th iteration gives loss of 0.33067807824720746\n",
      "The 8068 th iteration gives loss of 0.3306544230408495\n",
      "The 8069 th iteration gives loss of 0.33063077109891054\n",
      "The 8070 th iteration gives loss of 0.33060712242092677\n",
      "The 8071 th iteration gives loss of 0.33058347700646246\n",
      "The 8072 th iteration gives loss of 0.33055983485502927\n",
      "The 8073 th iteration gives loss of 0.33053619596620415\n",
      "The 8074 th iteration gives loss of 0.3305125603395045\n",
      "The 8075 th iteration gives loss of 0.33048892797447793\n",
      "The 8076 th iteration gives loss of 0.3304652988706623\n",
      "The 8077 th iteration gives loss of 0.3304416730276056\n",
      "The 8078 th iteration gives loss of 0.3304180504448608\n",
      "The 8079 th iteration gives loss of 0.3303944311219599\n",
      "The 8080 th iteration gives loss of 0.33037081505844257\n",
      "The 8081 th iteration gives loss of 0.3303472022538645\n",
      "The 8082 th iteration gives loss of 0.33032359270774586\n",
      "The 8083 th iteration gives loss of 0.33029998641965175\n",
      "The 8084 th iteration gives loss of 0.3302763833891227\n",
      "The 8085 th iteration gives loss of 0.33025278361569793\n",
      "The 8086 th iteration gives loss of 0.3302291870989209\n",
      "The 8087 th iteration gives loss of 0.33020559383833514\n",
      "The 8088 th iteration gives loss of 0.33018200383348284\n",
      "The 8089 th iteration gives loss of 0.33015841708390764\n",
      "The 8090 th iteration gives loss of 0.3301348335891535\n",
      "The 8091 th iteration gives loss of 0.33011125334877417\n",
      "The 8092 th iteration gives loss of 0.3300876763623003\n",
      "The 8093 th iteration gives loss of 0.3300641026292827\n",
      "The 8094 th iteration gives loss of 0.3300405321492631\n",
      "The 8095 th iteration gives loss of 0.33001696492179083\n",
      "The 8096 th iteration gives loss of 0.32999340094641055\n",
      "The 8097 th iteration gives loss of 0.32996984022266024\n",
      "The 8098 th iteration gives loss of 0.3299462827500859\n",
      "The 8099 th iteration gives loss of 0.32992272852824445\n",
      "The 8100 th iteration gives loss of 0.32989917755666387\n",
      "The 8101 th iteration gives loss of 0.3298756298348993\n",
      "The 8102 th iteration gives loss of 0.32985208536249133\n",
      "The 8103 th iteration gives loss of 0.3298285441389935\n",
      "The 8104 th iteration gives loss of 0.32980500616393355\n",
      "The 8105 th iteration gives loss of 0.32978147143687125\n",
      "The 8106 th iteration gives loss of 0.3297579399573542\n",
      "The 8107 th iteration gives loss of 0.329734411724928\n",
      "The 8108 th iteration gives loss of 0.32971088673912397\n",
      "The 8109 th iteration gives loss of 0.3296873649995002\n",
      "The 8110 th iteration gives loss of 0.32966384650560815\n",
      "The 8111 th iteration gives loss of 0.32964033125697695\n",
      "The 8112 th iteration gives loss of 0.32961681925316594\n",
      "The 8113 th iteration gives loss of 0.3295933104937093\n",
      "The 8114 th iteration gives loss of 0.32956980497818134\n",
      "The 8115 th iteration gives loss of 0.3295463027061044\n",
      "The 8116 th iteration gives loss of 0.32952280367702536\n",
      "The 8117 th iteration gives loss of 0.3294993078904925\n",
      "The 8118 th iteration gives loss of 0.3294758153460585\n",
      "The 8119 th iteration gives loss of 0.3294523260432723\n",
      "The 8120 th iteration gives loss of 0.3294288399816696\n",
      "The 8121 th iteration gives loss of 0.3294053571608115\n",
      "The 8122 th iteration gives loss of 0.3293818775802298\n",
      "The 8123 th iteration gives loss of 0.32935840123948823\n",
      "The 8124 th iteration gives loss of 0.32933492813811843\n",
      "The 8125 th iteration gives loss of 0.3293114582756777\n",
      "The 8126 th iteration gives loss of 0.32928799165171424\n",
      "The 8127 th iteration gives loss of 0.3292645282657714\n",
      "The 8128 th iteration gives loss of 0.32924106811739773\n",
      "The 8129 th iteration gives loss of 0.32921761120614446\n",
      "The 8130 th iteration gives loss of 0.32919415753155645\n",
      "The 8131 th iteration gives loss of 0.32917070709318674\n",
      "The 8132 th iteration gives loss of 0.32914725989057564\n",
      "The 8133 th iteration gives loss of 0.32912381592328394\n",
      "The 8134 th iteration gives loss of 0.3291003751908448\n",
      "The 8135 th iteration gives loss of 0.3290769376928156\n",
      "The 8136 th iteration gives loss of 0.32905350342874345\n",
      "The 8137 th iteration gives loss of 0.3290300723981794\n",
      "The 8138 th iteration gives loss of 0.32900664460067425\n",
      "The 8139 th iteration gives loss of 0.32898322003576497\n",
      "The 8140 th iteration gives loss of 0.3289597987030086\n",
      "The 8141 th iteration gives loss of 0.3289363806019583\n",
      "The 8142 th iteration gives loss of 0.3289129657321641\n",
      "The 8143 th iteration gives loss of 0.3288895540931632\n",
      "The 8144 th iteration gives loss of 0.32886614568451367\n",
      "The 8145 th iteration gives loss of 0.32884274050576207\n",
      "The 8146 th iteration gives loss of 0.3288193385564628\n",
      "The 8147 th iteration gives loss of 0.3287959398361593\n",
      "The 8148 th iteration gives loss of 0.3287725443444044\n",
      "The 8149 th iteration gives loss of 0.3287491520807526\n",
      "The 8150 th iteration gives loss of 0.3287257630447509\n",
      "The 8151 th iteration gives loss of 0.328702377235946\n",
      "The 8152 th iteration gives loss of 0.32867899465389133\n",
      "The 8153 th iteration gives loss of 0.3286556152981385\n",
      "The 8154 th iteration gives loss of 0.3286322391682285\n",
      "The 8155 th iteration gives loss of 0.32860886626372865\n",
      "The 8156 th iteration gives loss of 0.3285854965841787\n",
      "The 8157 th iteration gives loss of 0.32856213012913227\n",
      "The 8158 th iteration gives loss of 0.3285387668981372\n",
      "The 8159 th iteration gives loss of 0.3285154068907395\n",
      "The 8160 th iteration gives loss of 0.3284920501065078\n",
      "The 8161 th iteration gives loss of 0.3284686965449808\n",
      "The 8162 th iteration gives loss of 0.328445346205712\n",
      "The 8163 th iteration gives loss of 0.32842199908824815\n",
      "The 8164 th iteration gives loss of 0.32839865519214795\n",
      "The 8165 th iteration gives loss of 0.3283753145169626\n",
      "The 8166 th iteration gives loss of 0.32835197706224656\n",
      "The 8167 th iteration gives loss of 0.32832864282753726\n",
      "The 8168 th iteration gives loss of 0.32830531181239453\n",
      "The 8169 th iteration gives loss of 0.32828198401637154\n",
      "The 8170 th iteration gives loss of 0.32825865943902394\n",
      "The 8171 th iteration gives loss of 0.32823533807990046\n",
      "The 8172 th iteration gives loss of 0.328212019938554\n",
      "The 8173 th iteration gives loss of 0.3281887050145296\n",
      "The 8174 th iteration gives loss of 0.3281653933073833\n",
      "The 8175 th iteration gives loss of 0.3281420848166782\n",
      "The 8176 th iteration gives loss of 0.3281187795419517\n",
      "The 8177 th iteration gives loss of 0.3280954774827732\n",
      "The 8178 th iteration gives loss of 0.3280721786386808\n",
      "The 8179 th iteration gives loss of 0.3280488830092308\n",
      "The 8180 th iteration gives loss of 0.32802559059397945\n",
      "The 8181 th iteration gives loss of 0.3280023013924833\n",
      "The 8182 th iteration gives loss of 0.3279790154042852\n",
      "The 8183 th iteration gives loss of 0.327955732628947\n",
      "The 8184 th iteration gives loss of 0.32793245306602087\n",
      "The 8185 th iteration gives loss of 0.32790917671505054\n",
      "The 8186 th iteration gives loss of 0.32788590357560554\n",
      "The 8187 th iteration gives loss of 0.32786263364723217\n",
      "The 8188 th iteration gives loss of 0.3278393669294794\n",
      "The 8189 th iteration gives loss of 0.32781610342190004\n",
      "The 8190 th iteration gives loss of 0.32779284312406703\n",
      "The 8191 th iteration gives loss of 0.3277695860355116\n",
      "The 8192 th iteration gives loss of 0.32774633215579824\n",
      "The 8193 th iteration gives loss of 0.327723081484482\n",
      "The 8194 th iteration gives loss of 0.3276998340211145\n",
      "The 8195 th iteration gives loss of 0.32767658976525327\n",
      "The 8196 th iteration gives loss of 0.3276533487164457\n",
      "The 8197 th iteration gives loss of 0.32763011087425703\n",
      "The 8198 th iteration gives loss of 0.3276068762382299\n",
      "The 8199 th iteration gives loss of 0.32758364480792834\n",
      "The 8200 th iteration gives loss of 0.3275604165829036\n",
      "The 8201 th iteration gives loss of 0.32753719156271605\n",
      "The 8202 th iteration gives loss of 0.3275139697469125\n",
      "The 8203 th iteration gives loss of 0.32749075113504783\n",
      "The 8204 th iteration gives loss of 0.32746753572668097\n",
      "The 8205 th iteration gives loss of 0.3274443235213784\n",
      "The 8206 th iteration gives loss of 0.32742111451867906\n",
      "The 8207 th iteration gives loss of 0.3273979087181436\n",
      "The 8208 th iteration gives loss of 0.3273747061193212\n",
      "The 8209 th iteration gives loss of 0.3273515067217886\n",
      "The 8210 th iteration gives loss of 0.32732831052508743\n",
      "The 8211 th iteration gives loss of 0.3273051175287631\n",
      "The 8212 th iteration gives loss of 0.3272819277323868\n",
      "The 8213 th iteration gives loss of 0.3272587411355162\n",
      "The 8214 th iteration gives loss of 0.3272355577377052\n",
      "The 8215 th iteration gives loss of 0.32721237753849564\n",
      "The 8216 th iteration gives loss of 0.3271892005374617\n",
      "The 8217 th iteration gives loss of 0.3271660267341545\n",
      "The 8218 th iteration gives loss of 0.32714285612812244\n",
      "The 8219 th iteration gives loss of 0.3271196887189352\n",
      "The 8220 th iteration gives loss of 0.32709652450613896\n",
      "The 8221 th iteration gives loss of 0.3270733634893063\n",
      "The 8222 th iteration gives loss of 0.32705020566797344\n",
      "The 8223 th iteration gives loss of 0.3270270510417105\n",
      "The 8224 th iteration gives loss of 0.32700389961006715\n",
      "The 8225 th iteration gives loss of 0.32698075137260874\n",
      "The 8226 th iteration gives loss of 0.32695760632889104\n",
      "The 8227 th iteration gives loss of 0.32693446447846697\n",
      "The 8228 th iteration gives loss of 0.32691132582089055\n",
      "The 8229 th iteration gives loss of 0.3268881903557305\n",
      "The 8230 th iteration gives loss of 0.3268650580825356\n",
      "The 8231 th iteration gives loss of 0.3268419290008747\n",
      "The 8232 th iteration gives loss of 0.3268188031102934\n",
      "The 8233 th iteration gives loss of 0.32679568041035906\n",
      "The 8234 th iteration gives loss of 0.32677256090062096\n",
      "The 8235 th iteration gives loss of 0.32674944458063604\n",
      "The 8236 th iteration gives loss of 0.32672633144997515\n",
      "The 8237 th iteration gives loss of 0.3267032215081902\n",
      "The 8238 th iteration gives loss of 0.3266801147548385\n",
      "The 8239 th iteration gives loss of 0.32665701118948354\n",
      "The 8240 th iteration gives loss of 0.32663391081167303\n",
      "The 8241 th iteration gives loss of 0.3266108136209699\n",
      "The 8242 th iteration gives loss of 0.32658771961693334\n",
      "The 8243 th iteration gives loss of 0.32656462879913134\n",
      "The 8244 th iteration gives loss of 0.3265415411671127\n",
      "The 8245 th iteration gives loss of 0.326518456720443\n",
      "The 8246 th iteration gives loss of 0.3264953754586767\n",
      "The 8247 th iteration gives loss of 0.32647229738137157\n",
      "The 8248 th iteration gives loss of 0.32644922248809066\n",
      "The 8249 th iteration gives loss of 0.3264261507783959\n",
      "The 8250 th iteration gives loss of 0.3264030822518386\n",
      "The 8251 th iteration gives loss of 0.3263800169079905\n",
      "The 8252 th iteration gives loss of 0.32635695474639675\n",
      "The 8253 th iteration gives loss of 0.32633389576662464\n",
      "The 8254 th iteration gives loss of 0.32631083996823457\n",
      "The 8255 th iteration gives loss of 0.32628778735079284\n",
      "The 8256 th iteration gives loss of 0.32626473791384947\n",
      "The 8257 th iteration gives loss of 0.32624169165695793\n",
      "The 8258 th iteration gives loss of 0.3262186485797035\n",
      "The 8259 th iteration gives loss of 0.3261956086816222\n",
      "The 8260 th iteration gives loss of 0.32617257196228155\n",
      "The 8261 th iteration gives loss of 0.32614953842124295\n",
      "The 8262 th iteration gives loss of 0.3261265080580768\n",
      "The 8263 th iteration gives loss of 0.3261034808723268\n",
      "The 8264 th iteration gives loss of 0.3260804568635568\n",
      "The 8265 th iteration gives loss of 0.3260574360313472\n",
      "The 8266 th iteration gives loss of 0.3260344183752373\n",
      "The 8267 th iteration gives loss of 0.3260114038948\n",
      "The 8268 th iteration gives loss of 0.3259883925895911\n",
      "The 8269 th iteration gives loss of 0.3259653844591712\n",
      "The 8270 th iteration gives loss of 0.32594237950310095\n",
      "The 8271 th iteration gives loss of 0.32591937772095453\n",
      "The 8272 th iteration gives loss of 0.32589637911226926\n",
      "The 8273 th iteration gives loss of 0.3258733836766182\n",
      "The 8274 th iteration gives loss of 0.32585039141357786\n",
      "The 8275 th iteration gives loss of 0.325827402322693\n",
      "The 8276 th iteration gives loss of 0.3258044164035285\n",
      "The 8277 th iteration gives loss of 0.32578143365564527\n",
      "The 8278 th iteration gives loss of 0.3257584540786164\n",
      "The 8279 th iteration gives loss of 0.3257354776719831\n",
      "The 8280 th iteration gives loss of 0.325712504435322\n",
      "The 8281 th iteration gives loss of 0.32568953436819514\n",
      "The 8282 th iteration gives loss of 0.32566656747016065\n",
      "The 8283 th iteration gives loss of 0.3256436037407828\n",
      "The 8284 th iteration gives loss of 0.32562064317962963\n",
      "The 8285 th iteration gives loss of 0.3255976857862688\n",
      "The 8286 th iteration gives loss of 0.32557473156024447\n",
      "The 8287 th iteration gives loss of 0.32555178050113354\n",
      "The 8288 th iteration gives loss of 0.32552883260848386\n",
      "The 8289 th iteration gives loss of 0.3255058878818729\n",
      "The 8290 th iteration gives loss of 0.3254829463208465\n",
      "The 8291 th iteration gives loss of 0.3254600079249972\n",
      "The 8292 th iteration gives loss of 0.3254370726938634\n",
      "The 8293 th iteration gives loss of 0.3254141406270154\n",
      "The 8294 th iteration gives loss of 0.32539121172401986\n",
      "The 8295 th iteration gives loss of 0.3253682859844291\n",
      "The 8296 th iteration gives loss of 0.3253453634078305\n",
      "The 8297 th iteration gives loss of 0.32532244399376914\n",
      "The 8298 th iteration gives loss of 0.32529952774180737\n",
      "The 8299 th iteration gives loss of 0.3252766146515163\n",
      "The 8300 th iteration gives loss of 0.32525370472245985\n",
      "The 8301 th iteration gives loss of 0.3252307979541965\n",
      "The 8302 th iteration gives loss of 0.32520789434629543\n",
      "The 8303 th iteration gives loss of 0.32518499389832006\n",
      "The 8304 th iteration gives loss of 0.32516209660984347\n",
      "The 8305 th iteration gives loss of 0.32513920248041045\n",
      "The 8306 th iteration gives loss of 0.32511631150959336\n",
      "The 8307 th iteration gives loss of 0.3250934236969617\n",
      "The 8308 th iteration gives loss of 0.3250705390420736\n",
      "The 8309 th iteration gives loss of 0.3250476575444965\n",
      "The 8310 th iteration gives loss of 0.32502477920381234\n",
      "The 8311 th iteration gives loss of 0.3250019040195597\n",
      "The 8312 th iteration gives loss of 0.3249790319913116\n",
      "The 8313 th iteration gives loss of 0.32495616311863096\n",
      "The 8314 th iteration gives loss of 0.3249332974010965\n",
      "The 8315 th iteration gives loss of 0.32491043483825544\n",
      "The 8316 th iteration gives loss of 0.32488757542969404\n",
      "The 8317 th iteration gives loss of 0.3248647191749584\n",
      "The 8318 th iteration gives loss of 0.3248418660736195\n",
      "The 8319 th iteration gives loss of 0.3248190161252476\n",
      "The 8320 th iteration gives loss of 0.32479616932940275\n",
      "The 8321 th iteration gives loss of 0.3247733256856528\n",
      "The 8322 th iteration gives loss of 0.3247504851935695\n",
      "The 8323 th iteration gives loss of 0.32472764785271013\n",
      "The 8324 th iteration gives loss of 0.32470481366264564\n",
      "The 8325 th iteration gives loss of 0.32468198262293674\n",
      "The 8326 th iteration gives loss of 0.32465915473315277\n",
      "The 8327 th iteration gives loss of 0.3246363299928634\n",
      "The 8328 th iteration gives loss of 0.324613508401638\n",
      "The 8329 th iteration gives loss of 0.32459068995902624\n",
      "The 8330 th iteration gives loss of 0.3245678746646068\n",
      "The 8331 th iteration gives loss of 0.3245450625179542\n",
      "The 8332 th iteration gives loss of 0.3245222535186167\n",
      "The 8333 th iteration gives loss of 0.324499447666173\n",
      "The 8334 th iteration gives loss of 0.3244766449601929\n",
      "The 8335 th iteration gives loss of 0.32445384540023353\n",
      "The 8336 th iteration gives loss of 0.32443104898586456\n",
      "The 8337 th iteration gives loss of 0.3244082557166538\n",
      "The 8338 th iteration gives loss of 0.32438546559217074\n",
      "The 8339 th iteration gives loss of 0.3243626786119782\n",
      "The 8340 th iteration gives loss of 0.324339894775652\n",
      "The 8341 th iteration gives loss of 0.3243171140827463\n",
      "The 8342 th iteration gives loss of 0.3242943365328406\n",
      "The 8343 th iteration gives loss of 0.324271562125497\n",
      "The 8344 th iteration gives loss of 0.3242487908602858\n",
      "The 8345 th iteration gives loss of 0.32422602273677165\n",
      "The 8346 th iteration gives loss of 0.3242032577545289\n",
      "The 8347 th iteration gives loss of 0.3241804959131195\n",
      "The 8348 th iteration gives loss of 0.32415773721210733\n",
      "The 8349 th iteration gives loss of 0.32413498165107335\n",
      "The 8350 th iteration gives loss of 0.3241122292295704\n",
      "The 8351 th iteration gives loss of 0.32408947994718346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 8352 th iteration gives loss of 0.3240667338034669\n",
      "The 8353 th iteration gives loss of 0.324043990797995\n",
      "The 8354 th iteration gives loss of 0.3240212509303303\n",
      "The 8355 th iteration gives loss of 0.32399851420005066\n",
      "The 8356 th iteration gives loss of 0.32397578060672294\n",
      "The 8357 th iteration gives loss of 0.32395305014991294\n",
      "The 8358 th iteration gives loss of 0.3239303228291907\n",
      "The 8359 th iteration gives loss of 0.3239075986441134\n",
      "The 8360 th iteration gives loss of 0.3238848775942747\n",
      "The 8361 th iteration gives loss of 0.3238621596792319\n",
      "The 8362 th iteration gives loss of 0.3238394448985367\n",
      "The 8363 th iteration gives loss of 0.3238167332517873\n",
      "The 8364 th iteration gives loss of 0.32379402473853847\n",
      "The 8365 th iteration gives loss of 0.32377131935836456\n",
      "The 8366 th iteration gives loss of 0.3237486171108192\n",
      "The 8367 th iteration gives loss of 0.32372591799548767\n",
      "The 8368 th iteration gives loss of 0.3237032220119417\n",
      "The 8369 th iteration gives loss of 0.323680529159742\n",
      "The 8370 th iteration gives loss of 0.32365783943847004\n",
      "The 8371 th iteration gives loss of 0.32363515284767863\n",
      "The 8372 th iteration gives loss of 0.32361246938695376\n",
      "The 8373 th iteration gives loss of 0.3235897890558539\n",
      "The 8374 th iteration gives loss of 0.32356711185395265\n",
      "The 8375 th iteration gives loss of 0.3235444377808243\n",
      "The 8376 th iteration gives loss of 0.3235217668360375\n",
      "The 8377 th iteration gives loss of 0.3234990990191543\n",
      "The 8378 th iteration gives loss of 0.32347643432975615\n",
      "The 8379 th iteration gives loss of 0.32345377276740933\n",
      "The 8380 th iteration gives loss of 0.3234311143316868\n",
      "The 8381 th iteration gives loss of 0.32340845902216725\n",
      "The 8382 th iteration gives loss of 0.3233858068383981\n",
      "The 8383 th iteration gives loss of 0.32336315777996133\n",
      "The 8384 th iteration gives loss of 0.3233405118464377\n",
      "The 8385 th iteration gives loss of 0.3233178690373806\n",
      "The 8386 th iteration gives loss of 0.323295229352392\n",
      "The 8387 th iteration gives loss of 0.32327259279100634\n",
      "The 8388 th iteration gives loss of 0.32324995935281814\n",
      "The 8389 th iteration gives loss of 0.3232273290373892\n",
      "The 8390 th iteration gives loss of 0.32320470184429284\n",
      "The 8391 th iteration gives loss of 0.3231820777731031\n",
      "The 8392 th iteration gives loss of 0.3231594568233823\n",
      "The 8393 th iteration gives loss of 0.32313683899471063\n",
      "The 8394 th iteration gives loss of 0.3231142242866633\n",
      "The 8395 th iteration gives loss of 0.3230916126988054\n",
      "The 8396 th iteration gives loss of 0.3230690042307148\n",
      "The 8397 th iteration gives loss of 0.32304639888196207\n",
      "The 8398 th iteration gives loss of 0.3230237966521136\n",
      "The 8399 th iteration gives loss of 0.32300119754073603\n",
      "The 8400 th iteration gives loss of 0.32297860154741664\n",
      "The 8401 th iteration gives loss of 0.32295600867171986\n",
      "The 8402 th iteration gives loss of 0.3229334189132168\n",
      "The 8403 th iteration gives loss of 0.32291083227149947\n",
      "The 8404 th iteration gives loss of 0.3228882487461086\n",
      "The 8405 th iteration gives loss of 0.32286566833662866\n",
      "The 8406 th iteration gives loss of 0.3228430910426463\n",
      "The 8407 th iteration gives loss of 0.3228205168637142\n",
      "The 8408 th iteration gives loss of 0.3227979457994255\n",
      "The 8409 th iteration gives loss of 0.32277537784933635\n",
      "The 8410 th iteration gives loss of 0.32275281301302466\n",
      "The 8411 th iteration gives loss of 0.3227302512900672\n",
      "The 8412 th iteration gives loss of 0.32270769268003685\n",
      "The 8413 th iteration gives loss of 0.3226851371824975\n",
      "The 8414 th iteration gives loss of 0.3226625847970363\n",
      "The 8415 th iteration gives loss of 0.32264003552321996\n",
      "The 8416 th iteration gives loss of 0.3226174893606139\n",
      "The 8417 th iteration gives loss of 0.3225949463087966\n",
      "The 8418 th iteration gives loss of 0.3225724063673461\n",
      "The 8419 th iteration gives loss of 0.32254986953585035\n",
      "The 8420 th iteration gives loss of 0.3225273358138589\n",
      "The 8421 th iteration gives loss of 0.3225048052009494\n",
      "The 8422 th iteration gives loss of 0.3224822776967057\n",
      "The 8423 th iteration gives loss of 0.32245975330069865\n",
      "The 8424 th iteration gives loss of 0.32243723201249985\n",
      "The 8425 th iteration gives loss of 0.32241471383169035\n",
      "The 8426 th iteration gives loss of 0.3223921987578278\n",
      "The 8427 th iteration gives loss of 0.3223696867905045\n",
      "The 8428 th iteration gives loss of 0.3223471779292813\n",
      "The 8429 th iteration gives loss of 0.322324672173736\n",
      "The 8430 th iteration gives loss of 0.322302169523455\n",
      "The 8431 th iteration gives loss of 0.3222796699779948\n",
      "The 8432 th iteration gives loss of 0.32225717353695\n",
      "The 8433 th iteration gives loss of 0.3222346801998771\n",
      "The 8434 th iteration gives loss of 0.32221218996635875\n",
      "The 8435 th iteration gives loss of 0.32218970283597514\n",
      "The 8436 th iteration gives loss of 0.3221672188082923\n",
      "The 8437 th iteration gives loss of 0.32214473788288356\n",
      "The 8438 th iteration gives loss of 0.3221222600593358\n",
      "The 8439 th iteration gives loss of 0.322099785337222\n",
      "The 8440 th iteration gives loss of 0.32207731371611104\n",
      "The 8441 th iteration gives loss of 0.3220548451955737\n",
      "The 8442 th iteration gives loss of 0.3220323797751997\n",
      "The 8443 th iteration gives loss of 0.32200991745456586\n",
      "The 8444 th iteration gives loss of 0.32198745823322844\n",
      "The 8445 th iteration gives loss of 0.32196500211077134\n",
      "The 8446 th iteration gives loss of 0.32194254908677833\n",
      "The 8447 th iteration gives loss of 0.321920099160825\n",
      "The 8448 th iteration gives loss of 0.32189765233247464\n",
      "The 8449 th iteration gives loss of 0.32187520860131424\n",
      "The 8450 th iteration gives loss of 0.321852767966922\n",
      "The 8451 th iteration gives loss of 0.3218303304288627\n",
      "The 8452 th iteration gives loss of 0.3218078959867232\n",
      "The 8453 th iteration gives loss of 0.3217854646400735\n",
      "The 8454 th iteration gives loss of 0.32176303638849163\n",
      "The 8455 th iteration gives loss of 0.3217406112315597\n",
      "The 8456 th iteration gives loss of 0.32171818916883904\n",
      "The 8457 th iteration gives loss of 0.3216957701999268\n",
      "The 8458 th iteration gives loss of 0.32167335432437627\n",
      "The 8459 th iteration gives loss of 0.32165094154178814\n",
      "The 8460 th iteration gives loss of 0.3216285318517319\n",
      "The 8461 th iteration gives loss of 0.32160612525377785\n",
      "The 8462 th iteration gives loss of 0.3215837217474984\n",
      "The 8463 th iteration gives loss of 0.3215613213324855\n",
      "The 8464 th iteration gives loss of 0.3215389240083093\n",
      "The 8465 th iteration gives loss of 0.3215165297745475\n",
      "The 8466 th iteration gives loss of 0.32149413863077536\n",
      "The 8467 th iteration gives loss of 0.32147175057657235\n",
      "The 8468 th iteration gives loss of 0.3214493656115216\n",
      "The 8469 th iteration gives loss of 0.3214269837351833\n",
      "The 8470 th iteration gives loss of 0.32140460494715284\n",
      "The 8471 th iteration gives loss of 0.3213822292470065\n",
      "The 8472 th iteration gives loss of 0.3213598566343048\n",
      "The 8473 th iteration gives loss of 0.32133748710864213\n",
      "The 8474 th iteration gives loss of 0.321315120669599\n",
      "The 8475 th iteration gives loss of 0.32129275731674695\n",
      "The 8476 th iteration gives loss of 0.3212703970496542\n",
      "The 8477 th iteration gives loss of 0.3212480398679151\n",
      "The 8478 th iteration gives loss of 0.3212256857711012\n",
      "The 8479 th iteration gives loss of 0.3212033347587886\n",
      "The 8480 th iteration gives loss of 0.32118098683055984\n",
      "The 8481 th iteration gives loss of 0.3211586419859963\n",
      "The 8482 th iteration gives loss of 0.3211363002246634\n",
      "The 8483 th iteration gives loss of 0.32111396154615257\n",
      "The 8484 th iteration gives loss of 0.32109162595003743\n",
      "The 8485 th iteration gives loss of 0.3210692934358914\n",
      "The 8486 th iteration gives loss of 0.32104696400329674\n",
      "The 8487 th iteration gives loss of 0.3210246376518447\n",
      "The 8488 th iteration gives loss of 0.3210023143811017\n",
      "The 8489 th iteration gives loss of 0.32097999419064516\n",
      "The 8490 th iteration gives loss of 0.3209576770800606\n",
      "The 8491 th iteration gives loss of 0.32093536304892145\n",
      "The 8492 th iteration gives loss of 0.3209130520968216\n",
      "The 8493 th iteration gives loss of 0.32089074422332853\n",
      "The 8494 th iteration gives loss of 0.3208684394280157\n",
      "The 8495 th iteration gives loss of 0.3208461377104785\n",
      "The 8496 th iteration gives loss of 0.32082383907027984\n",
      "The 8497 th iteration gives loss of 0.3208015435070017\n",
      "The 8498 th iteration gives loss of 0.3207792510202264\n",
      "The 8499 th iteration gives loss of 0.3207569616095455\n",
      "The 8500 th iteration gives loss of 0.32073467527452554\n",
      "The 8501 th iteration gives loss of 0.32071239201475854\n",
      "The 8502 th iteration gives loss of 0.32069011182980983\n",
      "The 8503 th iteration gives loss of 0.3206678347192616\n",
      "The 8504 th iteration gives loss of 0.3206455606827057\n",
      "The 8505 th iteration gives loss of 0.3206232897197141\n",
      "The 8506 th iteration gives loss of 0.3206010218298644\n",
      "The 8507 th iteration gives loss of 0.3205787570127442\n",
      "The 8508 th iteration gives loss of 0.32055649526793223\n",
      "The 8509 th iteration gives loss of 0.3205342365950056\n",
      "The 8510 th iteration gives loss of 0.32051198099354106\n",
      "The 8511 th iteration gives loss of 0.3204897284631298\n",
      "The 8512 th iteration gives loss of 0.32046747900334294\n",
      "The 8513 th iteration gives loss of 0.3204452326137673\n",
      "The 8514 th iteration gives loss of 0.32042298929398544\n",
      "The 8515 th iteration gives loss of 0.32040074904357635\n",
      "The 8516 th iteration gives loss of 0.32037851186211985\n",
      "The 8517 th iteration gives loss of 0.32035627774919767\n",
      "The 8518 th iteration gives loss of 0.32033404670438265\n",
      "The 8519 th iteration gives loss of 0.3203118187272722\n",
      "The 8520 th iteration gives loss of 0.3202895938174425\n",
      "The 8521 th iteration gives loss of 0.32026737197445415\n",
      "The 8522 th iteration gives loss of 0.32024515319792196\n",
      "The 8523 th iteration gives loss of 0.32022293748740766\n",
      "The 8524 th iteration gives loss of 0.3202007248424989\n",
      "The 8525 th iteration gives loss of 0.3201785152627679\n",
      "The 8526 th iteration gives loss of 0.32015630874780365\n",
      "The 8527 th iteration gives loss of 0.3201341052971891\n",
      "The 8528 th iteration gives loss of 0.3201119049105099\n",
      "The 8529 th iteration gives loss of 0.3200897075873486\n",
      "The 8530 th iteration gives loss of 0.32006751332727174\n",
      "The 8531 th iteration gives loss of 0.3200453221298676\n",
      "The 8532 th iteration gives loss of 0.3200231339947289\n",
      "The 8533 th iteration gives loss of 0.32000094892143366\n",
      "The 8534 th iteration gives loss of 0.3199787669095581\n",
      "The 8535 th iteration gives loss of 0.31995658795868415\n",
      "The 8536 th iteration gives loss of 0.3199344120684018\n",
      "The 8537 th iteration gives loss of 0.3199122392382929\n",
      "The 8538 th iteration gives loss of 0.31989006946792947\n",
      "The 8539 th iteration gives loss of 0.31986790275691723\n",
      "The 8540 th iteration gives loss of 0.31984573910480724\n",
      "The 8541 th iteration gives loss of 0.31982357851120086\n",
      "The 8542 th iteration gives loss of 0.3198014209756872\n",
      "The 8543 th iteration gives loss of 0.31977926649783345\n",
      "The 8544 th iteration gives loss of 0.319757115077235\n",
      "The 8545 th iteration gives loss of 0.3197349667134623\n",
      "The 8546 th iteration gives loss of 0.31971282140611706\n",
      "The 8547 th iteration gives loss of 0.3196906791547661\n",
      "The 8548 th iteration gives loss of 0.31966853995900013\n",
      "The 8549 th iteration gives loss of 0.31964640381839343\n",
      "The 8550 th iteration gives loss of 0.31962427073254934\n",
      "The 8551 th iteration gives loss of 0.3196021407010299\n",
      "The 8552 th iteration gives loss of 0.31958001372343275\n",
      "The 8553 th iteration gives loss of 0.31955788979933053\n",
      "The 8554 th iteration gives loss of 0.3195357689283128\n",
      "The 8555 th iteration gives loss of 0.319513651109968\n",
      "The 8556 th iteration gives loss of 0.3194915363438668\n",
      "The 8557 th iteration gives loss of 0.3194694246296084\n",
      "The 8558 th iteration gives loss of 0.3194473159667667\n",
      "The 8559 th iteration gives loss of 0.3194252103549349\n",
      "The 8560 th iteration gives loss of 0.31940310779369013\n",
      "The 8561 th iteration gives loss of 0.31938100828260974\n",
      "The 8562 th iteration gives loss of 0.31935891182128867\n",
      "The 8563 th iteration gives loss of 0.3193368184093202\n",
      "The 8564 th iteration gives loss of 0.3193147280462598\n",
      "The 8565 th iteration gives loss of 0.31929264073172414\n",
      "The 8566 th iteration gives loss of 0.31927055646528313\n",
      "The 8567 th iteration gives loss of 0.31924847524651423\n",
      "The 8568 th iteration gives loss of 0.31922639707500633\n",
      "The 8569 th iteration gives loss of 0.3192043219503564\n",
      "The 8570 th iteration gives loss of 0.319182249872132\n",
      "The 8571 th iteration gives loss of 0.31916018083993525\n",
      "The 8572 th iteration gives loss of 0.31913811485333365\n",
      "The 8573 th iteration gives loss of 0.31911605191191583\n",
      "The 8574 th iteration gives loss of 0.31909399201528604\n",
      "The 8575 th iteration gives loss of 0.3190719351630126\n",
      "The 8576 th iteration gives loss of 0.31904988135467294\n",
      "The 8577 th iteration gives loss of 0.3190278305898736\n",
      "The 8578 th iteration gives loss of 0.31900578286817577\n",
      "The 8579 th iteration gives loss of 0.318983738189191\n",
      "The 8580 th iteration gives loss of 0.318961696552493\n",
      "The 8581 th iteration gives loss of 0.3189396579576619\n",
      "The 8582 th iteration gives loss of 0.3189176224042845\n",
      "The 8583 th iteration gives loss of 0.31889558989195604\n",
      "The 8584 th iteration gives loss of 0.31887356042024734\n",
      "The 8585 th iteration gives loss of 0.3188515339887593\n",
      "The 8586 th iteration gives loss of 0.3188295105970708\n",
      "The 8587 th iteration gives loss of 0.3188074902447744\n",
      "The 8588 th iteration gives loss of 0.3187854729314466\n",
      "The 8589 th iteration gives loss of 0.3187634586566775\n",
      "The 8590 th iteration gives loss of 0.3187414474200548\n",
      "The 8591 th iteration gives loss of 0.3187194392211593\n",
      "The 8592 th iteration gives loss of 0.3186974340595857\n",
      "The 8593 th iteration gives loss of 0.3186754319349209\n",
      "The 8594 th iteration gives loss of 0.3186534328467423\n",
      "The 8595 th iteration gives loss of 0.31863143679463624\n",
      "The 8596 th iteration gives loss of 0.3186094437781971\n",
      "The 8597 th iteration gives loss of 0.3185874537970071\n",
      "The 8598 th iteration gives loss of 0.31856546685065495\n",
      "The 8599 th iteration gives loss of 0.31854348293873247\n",
      "The 8600 th iteration gives loss of 0.3185215020608163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 8601 th iteration gives loss of 0.3184995242165003\n",
      "The 8602 th iteration gives loss of 0.31847754940536743\n",
      "The 8603 th iteration gives loss of 0.3184555776270026\n",
      "The 8604 th iteration gives loss of 0.3184336088810047\n",
      "The 8605 th iteration gives loss of 0.31841164316695514\n",
      "The 8606 th iteration gives loss of 0.3183896804844322\n",
      "The 8607 th iteration gives loss of 0.3183677208330318\n",
      "The 8608 th iteration gives loss of 0.3183457642123427\n",
      "The 8609 th iteration gives loss of 0.31832381062195464\n",
      "The 8610 th iteration gives loss of 0.31830186006144223\n",
      "The 8611 th iteration gives loss of 0.3182799125304031\n",
      "The 8612 th iteration gives loss of 0.3182579680284221\n",
      "The 8613 th iteration gives loss of 0.3182360265550956\n",
      "The 8614 th iteration gives loss of 0.31821408811000274\n",
      "The 8615 th iteration gives loss of 0.3181921526927266\n",
      "The 8616 th iteration gives loss of 0.3181702203028596\n",
      "The 8617 th iteration gives loss of 0.31814829093999414\n",
      "The 8618 th iteration gives loss of 0.31812636460372273\n",
      "The 8619 th iteration gives loss of 0.3181044412936286\n",
      "The 8620 th iteration gives loss of 0.31808252100929324\n",
      "The 8621 th iteration gives loss of 0.3180606037503086\n",
      "The 8622 th iteration gives loss of 0.3180386895162575\n",
      "The 8623 th iteration gives loss of 0.31801677830674185\n",
      "The 8624 th iteration gives loss of 0.3179948701213409\n",
      "The 8625 th iteration gives loss of 0.3179729649596506\n",
      "The 8626 th iteration gives loss of 0.31795106282125063\n",
      "The 8627 th iteration gives loss of 0.3179291637057367\n",
      "The 8628 th iteration gives loss of 0.3179072676126919\n",
      "The 8629 th iteration gives loss of 0.3178853745417027\n",
      "The 8630 th iteration gives loss of 0.31786348449237156\n",
      "The 8631 th iteration gives loss of 0.3178415974642752\n",
      "The 8632 th iteration gives loss of 0.3178197134570075\n",
      "The 8633 th iteration gives loss of 0.3177978324701622\n",
      "The 8634 th iteration gives loss of 0.3177759545033161\n",
      "The 8635 th iteration gives loss of 0.31775407955606955\n",
      "The 8636 th iteration gives loss of 0.3177322076280083\n",
      "The 8637 th iteration gives loss of 0.3177103387187188\n",
      "The 8638 th iteration gives loss of 0.31768847282779344\n",
      "The 8639 th iteration gives loss of 0.31766660995482476\n",
      "The 8640 th iteration gives loss of 0.31764475009939913\n",
      "The 8641 th iteration gives loss of 0.3176228932611011\n",
      "The 8642 th iteration gives loss of 0.31760103943952267\n",
      "The 8643 th iteration gives loss of 0.3175791886342637\n",
      "The 8644 th iteration gives loss of 0.31755734084490744\n",
      "The 8645 th iteration gives loss of 0.31753549607104\n",
      "The 8646 th iteration gives loss of 0.3175136543122497\n",
      "The 8647 th iteration gives loss of 0.3174918155681325\n",
      "The 8648 th iteration gives loss of 0.31746997983827735\n",
      "The 8649 th iteration gives loss of 0.31744814712228075\n",
      "The 8650 th iteration gives loss of 0.3174263174197177\n",
      "The 8651 th iteration gives loss of 0.3174044907301927\n",
      "The 8652 th iteration gives loss of 0.31738266705329227\n",
      "The 8653 th iteration gives loss of 0.31736084638859785\n",
      "The 8654 th iteration gives loss of 0.3173390287357134\n",
      "The 8655 th iteration gives loss of 0.31731721409422\n",
      "The 8656 th iteration gives loss of 0.3172954024637116\n",
      "The 8657 th iteration gives loss of 0.31727359384378656\n",
      "The 8658 th iteration gives loss of 0.3172517882340228\n",
      "The 8659 th iteration gives loss of 0.3172299856340198\n",
      "The 8660 th iteration gives loss of 0.31720818604335016\n",
      "The 8661 th iteration gives loss of 0.3171863894616358\n",
      "The 8662 th iteration gives loss of 0.31716459588845\n",
      "The 8663 th iteration gives loss of 0.31714280532337585\n",
      "The 8664 th iteration gives loss of 0.3171210177660289\n",
      "The 8665 th iteration gives loss of 0.31709923321597144\n",
      "The 8666 th iteration gives loss of 0.3170774516728218\n",
      "The 8667 th iteration gives loss of 0.31705567313615846\n",
      "The 8668 th iteration gives loss of 0.3170338976055698\n",
      "The 8669 th iteration gives loss of 0.31701212508064797\n",
      "The 8670 th iteration gives loss of 0.31699035556098354\n",
      "The 8671 th iteration gives loss of 0.31696858904617714\n",
      "The 8672 th iteration gives loss of 0.31694682553581754\n",
      "The 8673 th iteration gives loss of 0.31692506502949\n",
      "The 8674 th iteration gives loss of 0.3169033075267867\n",
      "The 8675 th iteration gives loss of 0.3168815530273155\n",
      "The 8676 th iteration gives loss of 0.31685980153065474\n",
      "The 8677 th iteration gives loss of 0.3168380530363858\n",
      "The 8678 th iteration gives loss of 0.3168163075441173\n",
      "The 8679 th iteration gives loss of 0.3167945650534366\n",
      "The 8680 th iteration gives loss of 0.31677282556394015\n",
      "The 8681 th iteration gives loss of 0.31675108907520955\n",
      "The 8682 th iteration gives loss of 0.3167293555868498\n",
      "The 8683 th iteration gives loss of 0.3167076250984468\n",
      "The 8684 th iteration gives loss of 0.31668589760959376\n",
      "The 8685 th iteration gives loss of 0.31666417311987316\n",
      "The 8686 th iteration gives loss of 0.3166424516289004\n",
      "The 8687 th iteration gives loss of 0.31662073313624645\n",
      "The 8688 th iteration gives loss of 0.3165990176415192\n",
      "The 8689 th iteration gives loss of 0.31657730514429233\n",
      "The 8690 th iteration gives loss of 0.31655559564418306\n",
      "The 8691 th iteration gives loss of 0.3165338891407725\n",
      "The 8692 th iteration gives loss of 0.316512185633647\n",
      "The 8693 th iteration gives loss of 0.31649048512241124\n",
      "The 8694 th iteration gives loss of 0.31646878760664854\n",
      "The 8695 th iteration gives loss of 0.3164470930859599\n",
      "The 8696 th iteration gives loss of 0.3164254015599428\n",
      "The 8697 th iteration gives loss of 0.31640371302816334\n",
      "The 8698 th iteration gives loss of 0.31638202749025046\n",
      "The 8699 th iteration gives loss of 0.31636034494578114\n",
      "The 8700 th iteration gives loss of 0.3163386653943526\n",
      "The 8701 th iteration gives loss of 0.31631698883554826\n",
      "The 8702 th iteration gives loss of 0.3162953152689626\n",
      "The 8703 th iteration gives loss of 0.3162736446941981\n",
      "The 8704 th iteration gives loss of 0.31625197711085656\n",
      "The 8705 th iteration gives loss of 0.3162303125185056\n",
      "The 8706 th iteration gives loss of 0.3162086509167644\n",
      "The 8707 th iteration gives loss of 0.31618699230521835\n",
      "The 8708 th iteration gives loss of 0.3161653366834602\n",
      "The 8709 th iteration gives loss of 0.31614368405107685\n",
      "The 8710 th iteration gives loss of 0.3161220344076652\n",
      "The 8711 th iteration gives loss of 0.3161003877528296\n",
      "The 8712 th iteration gives loss of 0.3160787440861639\n",
      "The 8713 th iteration gives loss of 0.3160571034072477\n",
      "The 8714 th iteration gives loss of 0.31603546571568997\n",
      "The 8715 th iteration gives loss of 0.3160138310110848\n",
      "The 8716 th iteration gives loss of 0.3159921992930107\n",
      "The 8717 th iteration gives loss of 0.31597057056107986\n",
      "The 8718 th iteration gives loss of 0.315948944814876\n",
      "The 8719 th iteration gives loss of 0.31592732205399315\n",
      "The 8720 th iteration gives loss of 0.31590570227804204\n",
      "The 8721 th iteration gives loss of 0.31588408548659797\n",
      "The 8722 th iteration gives loss of 0.3158624716792712\n",
      "The 8723 th iteration gives loss of 0.3158408608556487\n",
      "The 8724 th iteration gives loss of 0.31581925301532066\n",
      "The 8725 th iteration gives loss of 0.3157976481578844\n",
      "The 8726 th iteration gives loss of 0.3157760462829452\n",
      "The 8727 th iteration gives loss of 0.31575444739009056\n",
      "The 8728 th iteration gives loss of 0.3157328514789097\n",
      "The 8729 th iteration gives loss of 0.3157112585490217\n",
      "The 8730 th iteration gives loss of 0.31568966859999015\n",
      "The 8731 th iteration gives loss of 0.31566808163142596\n",
      "The 8732 th iteration gives loss of 0.31564649764293307\n",
      "The 8733 th iteration gives loss of 0.3156249166340919\n",
      "The 8734 th iteration gives loss of 0.3156033386045069\n",
      "The 8735 th iteration gives loss of 0.3155817635537613\n",
      "The 8736 th iteration gives loss of 0.31556019148146913\n",
      "The 8737 th iteration gives loss of 0.31553862238722064\n",
      "The 8738 th iteration gives loss of 0.31551705627059695\n",
      "The 8739 th iteration gives loss of 0.31549549313121644\n",
      "The 8740 th iteration gives loss of 0.3154739329686612\n",
      "The 8741 th iteration gives loss of 0.31545237578252616\n",
      "The 8742 th iteration gives loss of 0.3154308215724171\n",
      "The 8743 th iteration gives loss of 0.31540927033791544\n",
      "The 8744 th iteration gives loss of 0.3153877220786312\n",
      "The 8745 th iteration gives loss of 0.31536617679415824\n",
      "The 8746 th iteration gives loss of 0.3153446344840875\n",
      "The 8747 th iteration gives loss of 0.31532309514802676\n",
      "The 8748 th iteration gives loss of 0.31530155878555777\n",
      "The 8749 th iteration gives loss of 0.3152800253962903\n",
      "The 8750 th iteration gives loss of 0.3152584949798025\n",
      "The 8751 th iteration gives loss of 0.3152369675357124\n",
      "The 8752 th iteration gives loss of 0.3152154430636058\n",
      "The 8753 th iteration gives loss of 0.31519392156307274\n",
      "The 8754 th iteration gives loss of 0.3151724030337249\n",
      "The 8755 th iteration gives loss of 0.3151508874751569\n",
      "The 8756 th iteration gives loss of 0.3151293748869472\n",
      "The 8757 th iteration gives loss of 0.3151078652687144\n",
      "The 8758 th iteration gives loss of 0.3150863586200538\n",
      "The 8759 th iteration gives loss of 0.3150648549405527\n",
      "The 8760 th iteration gives loss of 0.315043354229809\n",
      "The 8761 th iteration gives loss of 0.3150218564874285\n",
      "The 8762 th iteration gives loss of 0.31500036171300194\n",
      "The 8763 th iteration gives loss of 0.31497886990613305\n",
      "The 8764 th iteration gives loss of 0.31495738106640586\n",
      "The 8765 th iteration gives loss of 0.3149358951934258\n",
      "The 8766 th iteration gives loss of 0.31491441228680017\n",
      "The 8767 th iteration gives loss of 0.3148929323461056\n",
      "The 8768 th iteration gives loss of 0.31487145537095973\n",
      "The 8769 th iteration gives loss of 0.3148499813609537\n",
      "The 8770 th iteration gives loss of 0.3148285103156809\n",
      "The 8771 th iteration gives loss of 0.31480704223473777\n",
      "The 8772 th iteration gives loss of 0.314785577117738\n",
      "The 8773 th iteration gives loss of 0.31476411496426754\n",
      "The 8774 th iteration gives loss of 0.31474265577391547\n",
      "The 8775 th iteration gives loss of 0.3147211995462912\n",
      "The 8776 th iteration gives loss of 0.31469974628098885\n",
      "The 8777 th iteration gives loss of 0.31467829597762376\n",
      "The 8778 th iteration gives loss of 0.31465684863576515\n",
      "The 8779 th iteration gives loss of 0.3146354042550375\n",
      "The 8780 th iteration gives loss of 0.3146139628350217\n",
      "The 8781 th iteration gives loss of 0.3145925243753179\n",
      "The 8782 th iteration gives loss of 0.3145710888755339\n",
      "The 8783 th iteration gives loss of 0.3145496563352601\n",
      "The 8784 th iteration gives loss of 0.314528226754103\n",
      "The 8785 th iteration gives loss of 0.31450680013165605\n",
      "The 8786 th iteration gives loss of 0.3144853764675119\n",
      "The 8787 th iteration gives loss of 0.31446395576128744\n",
      "The 8788 th iteration gives loss of 0.31444253801256095\n",
      "The 8789 th iteration gives loss of 0.3144211232209459\n",
      "The 8790 th iteration gives loss of 0.3143997113860321\n",
      "The 8791 th iteration gives loss of 0.3143783025074246\n",
      "The 8792 th iteration gives loss of 0.31435689658472266\n",
      "The 8793 th iteration gives loss of 0.31433549361751756\n",
      "The 8794 th iteration gives loss of 0.31431409360542073\n",
      "The 8795 th iteration gives loss of 0.314292696548022\n",
      "The 8796 th iteration gives loss of 0.31427130244493123\n",
      "The 8797 th iteration gives loss of 0.31424991129573343\n",
      "The 8798 th iteration gives loss of 0.31422852310003424\n",
      "The 8799 th iteration gives loss of 0.3142071378574353\n",
      "The 8800 th iteration gives loss of 0.31418575556753553\n",
      "The 8801 th iteration gives loss of 0.31416437622993604\n",
      "The 8802 th iteration gives loss of 0.3141429998442354\n",
      "The 8803 th iteration gives loss of 0.3141216264100327\n",
      "The 8804 th iteration gives loss of 0.31410025592692625\n",
      "The 8805 th iteration gives loss of 0.31407888839452064\n",
      "The 8806 th iteration gives loss of 0.3140575238124114\n",
      "The 8807 th iteration gives loss of 0.31403616218019464\n",
      "The 8808 th iteration gives loss of 0.3140148034974819\n",
      "The 8809 th iteration gives loss of 0.31399344776386096\n",
      "The 8810 th iteration gives loss of 0.3139720949789484\n",
      "The 8811 th iteration gives loss of 0.313950745142327\n",
      "The 8812 th iteration gives loss of 0.31392939825360316\n",
      "The 8813 th iteration gives loss of 0.3139080543123823\n",
      "The 8814 th iteration gives loss of 0.3138867133182616\n",
      "The 8815 th iteration gives loss of 0.3138653752708451\n",
      "The 8816 th iteration gives loss of 0.3138440401697256\n",
      "The 8817 th iteration gives loss of 0.3138227080145086\n",
      "The 8818 th iteration gives loss of 0.3138013788047933\n",
      "The 8819 th iteration gives loss of 0.3137800525401766\n",
      "The 8820 th iteration gives loss of 0.31375872922026793\n",
      "The 8821 th iteration gives loss of 0.31373740884466517\n",
      "The 8822 th iteration gives loss of 0.3137160914129661\n",
      "The 8823 th iteration gives loss of 0.3136947769247693\n",
      "The 8824 th iteration gives loss of 0.3136734653796906\n",
      "The 8825 th iteration gives loss of 0.31365215677732117\n",
      "The 8826 th iteration gives loss of 0.3136308511172509\n",
      "The 8827 th iteration gives loss of 0.3136095483990978\n",
      "The 8828 th iteration gives loss of 0.313588248622455\n",
      "The 8829 th iteration gives loss of 0.3135669517869252\n",
      "The 8830 th iteration gives loss of 0.3135456578921024\n",
      "The 8831 th iteration gives loss of 0.31352436693761393\n",
      "The 8832 th iteration gives loss of 0.31350307892302914\n",
      "The 8833 th iteration gives loss of 0.31348179384796676\n",
      "The 8834 th iteration gives loss of 0.3134605117120277\n",
      "The 8835 th iteration gives loss of 0.3134392325148171\n",
      "The 8836 th iteration gives loss of 0.3134179562559233\n",
      "The 8837 th iteration gives loss of 0.3133966829349634\n",
      "The 8838 th iteration gives loss of 0.31337541255152307\n",
      "The 8839 th iteration gives loss of 0.3133541451052136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 8840 th iteration gives loss of 0.31333288059564124\n",
      "The 8841 th iteration gives loss of 0.3133116190224025\n",
      "The 8842 th iteration gives loss of 0.3132903603850963\n",
      "The 8843 th iteration gives loss of 0.31326910468333247\n",
      "The 8844 th iteration gives loss of 0.31324785191670834\n",
      "The 8845 th iteration gives loss of 0.31322660208482844\n",
      "The 8846 th iteration gives loss of 0.31320535518728665\n",
      "The 8847 th iteration gives loss of 0.3131841112237014\n",
      "The 8848 th iteration gives loss of 0.31316287019365674\n",
      "The 8849 th iteration gives loss of 0.31314163209677626\n",
      "The 8850 th iteration gives loss of 0.3131203969326444\n",
      "The 8851 th iteration gives loss of 0.3130991647008684\n",
      "The 8852 th iteration gives loss of 0.31307793540105494\n",
      "The 8853 th iteration gives loss of 0.31305670903280697\n",
      "The 8854 th iteration gives loss of 0.31303548559571753\n",
      "The 8855 th iteration gives loss of 0.31301426508940255\n",
      "The 8856 th iteration gives loss of 0.31299304751346585\n",
      "The 8857 th iteration gives loss of 0.3129718328674958\n",
      "The 8858 th iteration gives loss of 0.3129506211511064\n",
      "The 8859 th iteration gives loss of 0.31292941236388805\n",
      "The 8860 th iteration gives loss of 0.3129082065054572\n",
      "The 8861 th iteration gives loss of 0.31288700357541904\n",
      "The 8862 th iteration gives loss of 0.31286580357336835\n",
      "The 8863 th iteration gives loss of 0.31284460649890705\n",
      "The 8864 th iteration gives loss of 0.31282341235165095\n",
      "The 8865 th iteration gives loss of 0.3128022211311911\n",
      "The 8866 th iteration gives loss of 0.31278103283713665\n",
      "The 8867 th iteration gives loss of 0.3127598474690895\n",
      "The 8868 th iteration gives loss of 0.3127386650266435\n",
      "The 8869 th iteration gives loss of 0.3127174855094203\n",
      "The 8870 th iteration gives loss of 0.3126963089170117\n",
      "The 8871 th iteration gives loss of 0.312675135249031\n",
      "The 8872 th iteration gives loss of 0.31265396450507627\n",
      "The 8873 th iteration gives loss of 0.31263279668475363\n",
      "The 8874 th iteration gives loss of 0.31261163178766355\n",
      "The 8875 th iteration gives loss of 0.31259046981340316\n",
      "The 8876 th iteration gives loss of 0.31256931076159317\n",
      "The 8877 th iteration gives loss of 0.3125481546318212\n",
      "The 8878 th iteration gives loss of 0.31252700142370243\n",
      "The 8879 th iteration gives loss of 0.31250585113683776\n",
      "The 8880 th iteration gives loss of 0.3124847037708291\n",
      "The 8881 th iteration gives loss of 0.3124635593252902\n",
      "The 8882 th iteration gives loss of 0.3124424177998121\n",
      "The 8883 th iteration gives loss of 0.31242127919400703\n",
      "The 8884 th iteration gives loss of 0.31240014350747153\n",
      "The 8885 th iteration gives loss of 0.31237901073982444\n",
      "The 8886 th iteration gives loss of 0.31235788089066024\n",
      "The 8887 th iteration gives loss of 0.312336753959593\n",
      "The 8888 th iteration gives loss of 0.3123156299462103\n",
      "The 8889 th iteration gives loss of 0.3122945088501255\n",
      "The 8890 th iteration gives loss of 0.3122733906709514\n",
      "The 8891 th iteration gives loss of 0.31225227540829337\n",
      "The 8892 th iteration gives loss of 0.3122311630617379\n",
      "The 8893 th iteration gives loss of 0.3122100536309114\n",
      "The 8894 th iteration gives loss of 0.3121889471153964\n",
      "The 8895 th iteration gives loss of 0.31216784351481003\n",
      "The 8896 th iteration gives loss of 0.31214674282876415\n",
      "The 8897 th iteration gives loss of 0.3121256450568626\n",
      "The 8898 th iteration gives loss of 0.3121045501987028\n",
      "The 8899 th iteration gives loss of 0.3120834582538897\n",
      "The 8900 th iteration gives loss of 0.31206236922203373\n",
      "The 8901 th iteration gives loss of 0.31204128310274454\n",
      "The 8902 th iteration gives loss of 0.31202019989561613\n",
      "The 8903 th iteration gives loss of 0.3119991196002677\n",
      "The 8904 th iteration gives loss of 0.3119780422162884\n",
      "The 8905 th iteration gives loss of 0.3119569677432994\n",
      "The 8906 th iteration gives loss of 0.31193589618088996\n",
      "The 8907 th iteration gives loss of 0.3119148275286865\n",
      "The 8908 th iteration gives loss of 0.3118937617862757\n",
      "The 8909 th iteration gives loss of 0.31187269895327757\n",
      "The 8910 th iteration gives loss of 0.31185163902928736\n",
      "The 8911 th iteration gives loss of 0.3118305820139198\n",
      "The 8912 th iteration gives loss of 0.31180952790678035\n",
      "The 8913 th iteration gives loss of 0.3117884767074678\n",
      "The 8914 th iteration gives loss of 0.3117674284155907\n",
      "The 8915 th iteration gives loss of 0.3117463830307551\n",
      "The 8916 th iteration gives loss of 0.3117253405525664\n",
      "The 8917 th iteration gives loss of 0.31170430098064\n",
      "The 8918 th iteration gives loss of 0.3116832643145771\n",
      "The 8919 th iteration gives loss of 0.31166223055398334\n",
      "The 8920 th iteration gives loss of 0.3116411996984615\n",
      "The 8921 th iteration gives loss of 0.31162017174761975\n",
      "The 8922 th iteration gives loss of 0.3115991467010752\n",
      "The 8923 th iteration gives loss of 0.31157812455842343\n",
      "The 8924 th iteration gives loss of 0.3115571053192767\n",
      "The 8925 th iteration gives loss of 0.31153608898323226\n",
      "The 8926 th iteration gives loss of 0.3115150755498996\n",
      "The 8927 th iteration gives loss of 0.31149406501889565\n",
      "The 8928 th iteration gives loss of 0.3114730573898195\n",
      "The 8929 th iteration gives loss of 0.311452052662284\n",
      "The 8930 th iteration gives loss of 0.31143105083588335\n",
      "The 8931 th iteration gives loss of 0.31141005191024146\n",
      "The 8932 th iteration gives loss of 0.31138905588494964\n",
      "The 8933 th iteration gives loss of 0.3113680627596353\n",
      "The 8934 th iteration gives loss of 0.31134707253387855\n",
      "The 8935 th iteration gives loss of 0.3113260852073107\n",
      "The 8936 th iteration gives loss of 0.3113051007795327\n",
      "The 8937 th iteration gives loss of 0.31128411925014465\n",
      "The 8938 th iteration gives loss of 0.31126314061876864\n",
      "The 8939 th iteration gives loss of 0.31124216488498857\n",
      "The 8940 th iteration gives loss of 0.31122119204843784\n",
      "The 8941 th iteration gives loss of 0.3112002221087043\n",
      "The 8942 th iteration gives loss of 0.3111792550654037\n",
      "The 8943 th iteration gives loss of 0.31115829091814773\n",
      "The 8944 th iteration gives loss of 0.31113732966653823\n",
      "The 8945 th iteration gives loss of 0.31111637131018327\n",
      "The 8946 th iteration gives loss of 0.31109541584868644\n",
      "The 8947 th iteration gives loss of 0.31107446328167665\n",
      "The 8948 th iteration gives loss of 0.31105351360873723\n",
      "The 8949 th iteration gives loss of 0.3110325668294912\n",
      "The 8950 th iteration gives loss of 0.3110116229435406\n",
      "The 8951 th iteration gives loss of 0.3109906819504898\n",
      "The 8952 th iteration gives loss of 0.31096974384995346\n",
      "The 8953 th iteration gives loss of 0.3109488086415378\n",
      "The 8954 th iteration gives loss of 0.3109278763248572\n",
      "The 8955 th iteration gives loss of 0.3109069468995132\n",
      "The 8956 th iteration gives loss of 0.3108860203651155\n",
      "The 8957 th iteration gives loss of 0.31086509672127444\n",
      "The 8958 th iteration gives loss of 0.31084417596760316\n",
      "The 8959 th iteration gives loss of 0.31082325810369416\n",
      "The 8960 th iteration gives loss of 0.3108023431291709\n",
      "The 8961 th iteration gives loss of 0.31078143104363865\n",
      "The 8962 th iteration gives loss of 0.3107605218467072\n",
      "The 8963 th iteration gives loss of 0.3107396155379777\n",
      "The 8964 th iteration gives loss of 0.31071871211706853\n",
      "The 8965 th iteration gives loss of 0.31069781158358933\n",
      "The 8966 th iteration gives loss of 0.3106769139371344\n",
      "The 8967 th iteration gives loss of 0.3106560191773222\n",
      "The 8968 th iteration gives loss of 0.3106351273037721\n",
      "The 8969 th iteration gives loss of 0.310614238316086\n",
      "The 8970 th iteration gives loss of 0.31059335221386714\n",
      "The 8971 th iteration gives loss of 0.3105724689967233\n",
      "The 8972 th iteration gives loss of 0.31055158866427757\n",
      "The 8973 th iteration gives loss of 0.31053071121612164\n",
      "The 8974 th iteration gives loss of 0.31050983665188403\n",
      "The 8975 th iteration gives loss of 0.3104889649711619\n",
      "The 8976 th iteration gives loss of 0.3104680961735658\n",
      "The 8977 th iteration gives loss of 0.3104472302587095\n",
      "The 8978 th iteration gives loss of 0.310426367226203\n",
      "The 8979 th iteration gives loss of 0.310405507075651\n",
      "The 8980 th iteration gives loss of 0.3103846498066685\n",
      "The 8981 th iteration gives loss of 0.3103637954188639\n",
      "The 8982 th iteration gives loss of 0.31034294391183703\n",
      "The 8983 th iteration gives loss of 0.31032209528521704\n",
      "The 8984 th iteration gives loss of 0.3103012495386084\n",
      "The 8985 th iteration gives loss of 0.3102804066716031\n",
      "The 8986 th iteration gives loss of 0.31025956668383115\n",
      "The 8987 th iteration gives loss of 0.310238729574896\n",
      "The 8988 th iteration gives loss of 0.3102178953444034\n",
      "The 8989 th iteration gives loss of 0.3101970639919725\n",
      "The 8990 th iteration gives loss of 0.3101762355172131\n",
      "The 8991 th iteration gives loss of 0.31015540991972623\n",
      "The 8992 th iteration gives loss of 0.310134587199122\n",
      "The 8993 th iteration gives loss of 0.3101137673550266\n",
      "The 8994 th iteration gives loss of 0.3100929503870421\n",
      "The 8995 th iteration gives loss of 0.310072136294771\n",
      "The 8996 th iteration gives loss of 0.3100513250778338\n",
      "The 8997 th iteration gives loss of 0.31003051673583587\n",
      "The 8998 th iteration gives loss of 0.3100097112684001\n",
      "The 8999 th iteration gives loss of 0.3099889086751155\n",
      "The 9000 th iteration gives loss of 0.30996810895561017\n",
      "The 9001 th iteration gives loss of 0.30994731210948356\n",
      "The 9002 th iteration gives loss of 0.3099265181363599\n",
      "The 9003 th iteration gives loss of 0.3099057270358402\n",
      "The 9004 th iteration gives loss of 0.30988493880754925\n",
      "The 9005 th iteration gives loss of 0.3098641534510766\n",
      "The 9006 th iteration gives loss of 0.3098433709660565\n",
      "The 9007 th iteration gives loss of 0.3098225913520702\n",
      "The 9008 th iteration gives loss of 0.30980181460875117\n",
      "The 9009 th iteration gives loss of 0.3097810407357109\n",
      "The 9010 th iteration gives loss of 0.30976026973255927\n",
      "The 9011 th iteration gives loss of 0.3097395015989015\n",
      "The 9012 th iteration gives loss of 0.3097187363343487\n",
      "The 9013 th iteration gives loss of 0.3096979739385214\n",
      "The 9014 th iteration gives loss of 0.3096772144110291\n",
      "The 9015 th iteration gives loss of 0.30965645775147216\n",
      "The 9016 th iteration gives loss of 0.30963570395947043\n",
      "The 9017 th iteration gives loss of 0.3096149530346389\n",
      "The 9018 th iteration gives loss of 0.30959420497658896\n",
      "The 9019 th iteration gives loss of 0.30957345978492556\n",
      "The 9020 th iteration gives loss of 0.309552717459262\n",
      "The 9021 th iteration gives loss of 0.3095319779992224\n",
      "The 9022 th iteration gives loss of 0.30951124140440417\n",
      "The 9023 th iteration gives loss of 0.30949050767441905\n",
      "The 9024 th iteration gives loss of 0.3094697768088968\n",
      "The 9025 th iteration gives loss of 0.30944904880742274\n",
      "The 9026 th iteration gives loss of 0.3094283236696361\n",
      "The 9027 th iteration gives loss of 0.30940760139513307\n",
      "The 9028 th iteration gives loss of 0.30938688198352987\n",
      "The 9029 th iteration gives loss of 0.30936616543443674\n",
      "The 9030 th iteration gives loss of 0.30934545174746686\n",
      "The 9031 th iteration gives loss of 0.3093247409222408\n",
      "The 9032 th iteration gives loss of 0.3093040329583654\n",
      "The 9033 th iteration gives loss of 0.3092833278554433\n",
      "The 9034 th iteration gives loss of 0.3092626256131002\n",
      "The 9035 th iteration gives loss of 0.30924192623094077\n",
      "The 9036 th iteration gives loss of 0.30922122970859456\n",
      "The 9037 th iteration gives loss of 0.3092005360456456\n",
      "The 9038 th iteration gives loss of 0.3091798452417291\n",
      "The 9039 th iteration gives loss of 0.3091591572964545\n",
      "The 9040 th iteration gives loss of 0.309138472209424\n",
      "The 9041 th iteration gives loss of 0.3091177899802722\n",
      "The 9042 th iteration gives loss of 0.3090971106085875\n",
      "The 9043 th iteration gives loss of 0.30907643409399765\n",
      "The 9044 th iteration gives loss of 0.3090557604361147\n",
      "The 9045 th iteration gives loss of 0.30903508963454157\n",
      "The 9046 th iteration gives loss of 0.3090144216889036\n",
      "The 9047 th iteration gives loss of 0.30899375659881895\n",
      "The 9048 th iteration gives loss of 0.3089730943638786\n",
      "The 9049 th iteration gives loss of 0.3089524349837195\n",
      "The 9050 th iteration gives loss of 0.30893177845793846\n",
      "The 9051 th iteration gives loss of 0.30891112478615945\n",
      "The 9052 th iteration gives loss of 0.30889047396798935\n",
      "The 9053 th iteration gives loss of 0.3088698260030464\n",
      "The 9054 th iteration gives loss of 0.30884918089093816\n",
      "The 9055 th iteration gives loss of 0.3088285386312874\n",
      "The 9056 th iteration gives loss of 0.30880789922370505\n",
      "The 9057 th iteration gives loss of 0.3087872626678042\n",
      "The 9058 th iteration gives loss of 0.3087666289631913\n",
      "The 9059 th iteration gives loss of 0.30874599810947906\n",
      "The 9060 th iteration gives loss of 0.30872537010629986\n",
      "The 9061 th iteration gives loss of 0.30870474495325434\n",
      "The 9062 th iteration gives loss of 0.30868412264996786\n",
      "The 9063 th iteration gives loss of 0.3086635031960354\n",
      "The 9064 th iteration gives loss of 0.3086428865910898\n",
      "The 9065 th iteration gives loss of 0.3086222728347317\n",
      "The 9066 th iteration gives loss of 0.30860166192658656\n",
      "The 9067 th iteration gives loss of 0.3085810538662594\n",
      "The 9068 th iteration gives loss of 0.3085604486533668\n",
      "The 9069 th iteration gives loss of 0.30853984628753395\n",
      "The 9070 th iteration gives loss of 0.3085192467683507\n",
      "The 9071 th iteration gives loss of 0.3084986500954545\n",
      "The 9072 th iteration gives loss of 0.3084780562684588\n",
      "The 9073 th iteration gives loss of 0.3084574652869643\n",
      "The 9074 th iteration gives loss of 0.3084368771506006\n",
      "The 9075 th iteration gives loss of 0.30841629185897623\n",
      "The 9076 th iteration gives loss of 0.30839570941170064\n",
      "The 9077 th iteration gives loss of 0.3083751298083963\n",
      "The 9078 th iteration gives loss of 0.30835455304867065\n",
      "The 9079 th iteration gives loss of 0.3083339791321463\n",
      "The 9080 th iteration gives loss of 0.30831340805843643\n",
      "The 9081 th iteration gives loss of 0.30829283982714917\n",
      "The 9082 th iteration gives loss of 0.30827227443790783\n",
      "The 9083 th iteration gives loss of 0.3082517118903303\n",
      "The 9084 th iteration gives loss of 0.30823115218402386\n",
      "The 9085 th iteration gives loss of 0.30821059531860145\n",
      "The 9086 th iteration gives loss of 0.3081900412936888\n",
      "The 9087 th iteration gives loss of 0.30816949010890293\n",
      "The 9088 th iteration gives loss of 0.3081489417638445\n",
      "The 9089 th iteration gives loss of 0.30812839625814503\n",
      "The 9090 th iteration gives loss of 0.308107853591397\n",
      "The 9091 th iteration gives loss of 0.30808731376324605\n",
      "The 9092 th iteration gives loss of 0.3080667767732856\n",
      "The 9093 th iteration gives loss of 0.30804624262114183\n",
      "The 9094 th iteration gives loss of 0.30802571130642453\n",
      "The 9095 th iteration gives loss of 0.3080051828287473\n",
      "The 9096 th iteration gives loss of 0.3079846571877328\n",
      "The 9097 th iteration gives loss of 0.3079641343830046\n",
      "The 9098 th iteration gives loss of 0.30794361441416535\n",
      "The 9099 th iteration gives loss of 0.30792309728083767\n",
      "The 9100 th iteration gives loss of 0.3079025829826272\n",
      "The 9101 th iteration gives loss of 0.30788207151915814\n",
      "The 9102 th iteration gives loss of 0.30786156289004657\n",
      "The 9103 th iteration gives loss of 0.3078410570949181\n",
      "The 9104 th iteration gives loss of 0.3078205541333666\n",
      "The 9105 th iteration gives loss of 0.30780005400502325\n",
      "The 9106 th iteration gives loss of 0.3077795567095033\n",
      "The 9107 th iteration gives loss of 0.3077590622464271\n",
      "The 9108 th iteration gives loss of 0.3077385706154004\n",
      "The 9109 th iteration gives loss of 0.3077180818160516\n",
      "The 9110 th iteration gives loss of 0.30769759584798373\n",
      "The 9111 th iteration gives loss of 0.30767711271082127\n",
      "The 9112 th iteration gives loss of 0.3076566324041853\n",
      "The 9113 th iteration gives loss of 0.30763615492768065\n",
      "The 9114 th iteration gives loss of 0.30761568028093506\n",
      "The 9115 th iteration gives loss of 0.3075952084635641\n",
      "The 9116 th iteration gives loss of 0.3075747394751751\n",
      "The 9117 th iteration gives loss of 0.3075542733154012\n",
      "The 9118 th iteration gives loss of 0.3075338099838483\n",
      "The 9119 th iteration gives loss of 0.3075133494801283\n",
      "The 9120 th iteration gives loss of 0.3074928918038693\n",
      "The 9121 th iteration gives loss of 0.3074724369546741\n",
      "The 9122 th iteration gives loss of 0.3074519849321807\n",
      "The 9123 th iteration gives loss of 0.3074315357359859\n",
      "The 9124 th iteration gives loss of 0.30741108936572376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 9125 th iteration gives loss of 0.30739064582100056\n",
      "The 9126 th iteration gives loss of 0.30737020510144014\n",
      "The 9127 th iteration gives loss of 0.30734976720665524\n",
      "The 9128 th iteration gives loss of 0.30732933213627267\n",
      "The 9129 th iteration gives loss of 0.3073088998898915\n",
      "The 9130 th iteration gives loss of 0.30728847046714897\n",
      "The 9131 th iteration gives loss of 0.30726804386764944\n",
      "The 9132 th iteration gives loss of 0.3072476200910191\n",
      "The 9133 th iteration gives loss of 0.30722719913686286\n",
      "The 9134 th iteration gives loss of 0.30720678100481513\n",
      "The 9135 th iteration gives loss of 0.30718636569448227\n",
      "The 9136 th iteration gives loss of 0.30716595320548673\n",
      "The 9137 th iteration gives loss of 0.30714554353745027\n",
      "The 9138 th iteration gives loss of 0.30712513668998875\n",
      "The 9139 th iteration gives loss of 0.30710473266271\n",
      "The 9140 th iteration gives loss of 0.3070843314552392\n",
      "The 9141 th iteration gives loss of 0.3070639330671962\n",
      "The 9142 th iteration gives loss of 0.30704353749819135\n",
      "The 9143 th iteration gives loss of 0.30702314474785214\n",
      "The 9144 th iteration gives loss of 0.307002754815807\n",
      "The 9145 th iteration gives loss of 0.3069823677016509\n",
      "The 9146 th iteration gives loss of 0.3069619834050108\n",
      "The 9147 th iteration gives loss of 0.3069416019255157\n",
      "The 9148 th iteration gives loss of 0.30692122326277027\n",
      "The 9149 th iteration gives loss of 0.3069008474163961\n",
      "The 9150 th iteration gives loss of 0.3068804743860129\n",
      "The 9151 th iteration gives loss of 0.3068601041712435\n",
      "The 9152 th iteration gives loss of 0.30683973677170495\n",
      "The 9153 th iteration gives loss of 0.30681937218701183\n",
      "The 9154 th iteration gives loss of 0.3067990104167788\n",
      "The 9155 th iteration gives loss of 0.30677865146063593\n",
      "The 9156 th iteration gives loss of 0.3067582953182037\n",
      "The 9157 th iteration gives loss of 0.30673794198908233\n",
      "The 9158 th iteration gives loss of 0.30671759147290967\n",
      "The 9159 th iteration gives loss of 0.3066972437692999\n",
      "The 9160 th iteration gives loss of 0.306676898877868\n",
      "The 9161 th iteration gives loss of 0.3066565567982316\n",
      "The 9162 th iteration gives loss of 0.30663621753002057\n",
      "The 9163 th iteration gives loss of 0.3066158810728488\n",
      "The 9164 th iteration gives loss of 0.30659554742632616\n",
      "The 9165 th iteration gives loss of 0.3065752165900863\n",
      "The 9166 th iteration gives loss of 0.3065548885637336\n",
      "The 9167 th iteration gives loss of 0.3065345633468976\n",
      "The 9168 th iteration gives loss of 0.3065142409391946\n",
      "The 9169 th iteration gives loss of 0.3064939213402551\n",
      "The 9170 th iteration gives loss of 0.3064736045496835\n",
      "The 9171 th iteration gives loss of 0.30645329056710874\n",
      "The 9172 th iteration gives loss of 0.3064329793921412\n",
      "The 9173 th iteration gives loss of 0.30641267102440384\n",
      "The 9174 th iteration gives loss of 0.3063923654635247\n",
      "The 9175 th iteration gives loss of 0.30637206270911216\n",
      "The 9176 th iteration gives loss of 0.30635176276079024\n",
      "The 9177 th iteration gives loss of 0.306331465618191\n",
      "The 9178 th iteration gives loss of 0.3063111712809133\n",
      "The 9179 th iteration gives loss of 0.30629087974859237\n",
      "The 9180 th iteration gives loss of 0.3062705910208317\n",
      "The 9181 th iteration gives loss of 0.3062503050972702\n",
      "The 9182 th iteration gives loss of 0.30623002197752447\n",
      "The 9183 th iteration gives loss of 0.30620974166121223\n",
      "The 9184 th iteration gives loss of 0.3061894641479475\n",
      "The 9185 th iteration gives loss of 0.3061691894373533\n",
      "The 9186 th iteration gives loss of 0.3061489175290482\n",
      "The 9187 th iteration gives loss of 0.3061286484226615\n",
      "The 9188 th iteration gives loss of 0.306108382117817\n",
      "The 9189 th iteration gives loss of 0.3060881186141159\n",
      "The 9190 th iteration gives loss of 0.30606785791118735\n",
      "The 9191 th iteration gives loss of 0.30604760000865716\n",
      "The 9192 th iteration gives loss of 0.3060273449061437\n",
      "The 9193 th iteration gives loss of 0.3060070926032755\n",
      "The 9194 th iteration gives loss of 0.3059868430996547\n",
      "The 9195 th iteration gives loss of 0.30596659639491836\n",
      "The 9196 th iteration gives loss of 0.30594635248867647\n",
      "The 9197 th iteration gives loss of 0.3059261113805573\n",
      "The 9198 th iteration gives loss of 0.30590587307017153\n",
      "The 9199 th iteration gives loss of 0.3058856375571505\n",
      "The 9200 th iteration gives loss of 0.3058654048411067\n",
      "The 9201 th iteration gives loss of 0.3058451749216675\n",
      "The 9202 th iteration gives loss of 0.3058249477984641\n",
      "The 9203 th iteration gives loss of 0.30580472347109505\n",
      "The 9204 th iteration gives loss of 0.3057845019391993\n",
      "The 9205 th iteration gives loss of 0.30576428320239185\n",
      "The 9206 th iteration gives loss of 0.30574406726029557\n",
      "The 9207 th iteration gives loss of 0.3057238541125353\n",
      "The 9208 th iteration gives loss of 0.3057036437587213\n",
      "The 9209 th iteration gives loss of 0.30568343619847504\n",
      "The 9210 th iteration gives loss of 0.30566323143143137\n",
      "The 9211 th iteration gives loss of 0.3056430294572022\n",
      "The 9212 th iteration gives loss of 0.3056228302754149\n",
      "The 9213 th iteration gives loss of 0.3056026338856861\n",
      "The 9214 th iteration gives loss of 0.3055824402876274\n",
      "The 9215 th iteration gives loss of 0.3055622494808833\n",
      "The 9216 th iteration gives loss of 0.3055420614650651\n",
      "The 9217 th iteration gives loss of 0.30552187623979427\n",
      "The 9218 th iteration gives loss of 0.30550169380468867\n",
      "The 9219 th iteration gives loss of 0.30548151415936997\n",
      "The 9220 th iteration gives loss of 0.305461337303478\n",
      "The 9221 th iteration gives loss of 0.30544116323661813\n",
      "The 9222 th iteration gives loss of 0.30542099195840416\n",
      "The 9223 th iteration gives loss of 0.3054008234684775\n",
      "The 9224 th iteration gives loss of 0.3053806577664519\n",
      "The 9225 th iteration gives loss of 0.30536049485194966\n",
      "The 9226 th iteration gives loss of 0.3053403347245953\n",
      "The 9227 th iteration gives loss of 0.305320177384013\n",
      "The 9228 th iteration gives loss of 0.30530002282981467\n",
      "The 9229 th iteration gives loss of 0.30527987106161564\n",
      "The 9230 th iteration gives loss of 0.3052597220790712\n",
      "The 9231 th iteration gives loss of 0.3052395758817763\n",
      "The 9232 th iteration gives loss of 0.30521943246936245\n",
      "The 9233 th iteration gives loss of 0.3051992918414507\n",
      "The 9234 th iteration gives loss of 0.305179153997673\n",
      "The 9235 th iteration gives loss of 0.3051590189376314\n",
      "The 9236 th iteration gives loss of 0.3051388866609758\n",
      "The 9237 th iteration gives loss of 0.30511875716730574\n",
      "The 9238 th iteration gives loss of 0.3050986304562542\n",
      "The 9239 th iteration gives loss of 0.3050785065274443\n",
      "The 9240 th iteration gives loss of 0.30505838538049607\n",
      "The 9241 th iteration gives loss of 0.30503826701503345\n",
      "The 9242 th iteration gives loss of 0.3050181514306712\n",
      "The 9243 th iteration gives loss of 0.3049980386270468\n",
      "The 9244 th iteration gives loss of 0.30497792860377543\n",
      "The 9245 th iteration gives loss of 0.304957821360482\n",
      "The 9246 th iteration gives loss of 0.3049377168967908\n",
      "The 9247 th iteration gives loss of 0.30491761521233096\n",
      "The 9248 th iteration gives loss of 0.30489751630670914\n",
      "The 9249 th iteration gives loss of 0.3048774201795669\n",
      "The 9250 th iteration gives loss of 0.30485732683051414\n",
      "The 9251 th iteration gives loss of 0.30483723625918224\n",
      "The 9252 th iteration gives loss of 0.30481714846519103\n",
      "The 9253 th iteration gives loss of 0.3047970634481618\n",
      "The 9254 th iteration gives loss of 0.30477698120772984\n",
      "The 9255 th iteration gives loss of 0.3047569017435022\n",
      "The 9256 th iteration gives loss of 0.30473682505511496\n",
      "The 9257 th iteration gives loss of 0.3047167511421834\n",
      "The 9258 th iteration gives loss of 0.30469668000434064\n",
      "The 9259 th iteration gives loss of 0.30467661164119825\n",
      "The 9260 th iteration gives loss of 0.30465654605239606\n",
      "The 9261 th iteration gives loss of 0.30463648323754644\n",
      "The 9262 th iteration gives loss of 0.30461642319627197\n",
      "The 9263 th iteration gives loss of 0.3045963659282029\n",
      "The 9264 th iteration gives loss of 0.30457631143296415\n",
      "The 9265 th iteration gives loss of 0.30455625971017425\n",
      "The 9266 th iteration gives loss of 0.3045362107594602\n",
      "The 9267 th iteration gives loss of 0.3045161645804463\n",
      "The 9268 th iteration gives loss of 0.30449612117275004\n",
      "The 9269 th iteration gives loss of 0.30447608053601627\n",
      "The 9270 th iteration gives loss of 0.3044560426698468\n",
      "The 9271 th iteration gives loss of 0.3044360075738718\n",
      "The 9272 th iteration gives loss of 0.304415975247723\n",
      "The 9273 th iteration gives loss of 0.30439594569101963\n",
      "The 9274 th iteration gives loss of 0.30437591890339655\n",
      "The 9275 th iteration gives loss of 0.3043558948844516\n",
      "The 9276 th iteration gives loss of 0.30433587363384346\n",
      "The 9277 th iteration gives loss of 0.30431585515116694\n",
      "The 9278 th iteration gives loss of 0.3042958394360596\n",
      "The 9279 th iteration gives loss of 0.30427582648815327\n",
      "The 9280 th iteration gives loss of 0.3042558163070636\n",
      "The 9281 th iteration gives loss of 0.30423580889241225\n",
      "The 9282 th iteration gives loss of 0.30421580424384015\n",
      "The 9283 th iteration gives loss of 0.30419580236095906\n",
      "The 9284 th iteration gives loss of 0.30417580324339316\n",
      "The 9285 th iteration gives loss of 0.3041558068907697\n",
      "The 9286 th iteration gives loss of 0.3041358133027251\n",
      "The 9287 th iteration gives loss of 0.30411582247887087\n",
      "The 9288 th iteration gives loss of 0.3040958344188306\n",
      "The 9289 th iteration gives loss of 0.30407584912223784\n",
      "The 9290 th iteration gives loss of 0.3040558665887139\n",
      "The 9291 th iteration gives loss of 0.3040358868178823\n",
      "The 9292 th iteration gives loss of 0.3040159098093765\n",
      "The 9293 th iteration gives loss of 0.3039959355628141\n",
      "The 9294 th iteration gives loss of 0.30397596407782396\n",
      "The 9295 th iteration gives loss of 0.3039559953540336\n",
      "The 9296 th iteration gives loss of 0.30393602939106346\n",
      "The 9297 th iteration gives loss of 0.30391606618854655\n",
      "The 9298 th iteration gives loss of 0.30389610574609804\n",
      "The 9299 th iteration gives loss of 0.30387614806335633\n",
      "The 9300 th iteration gives loss of 0.3038561931399324\n",
      "The 9301 th iteration gives loss of 0.30383624097546313\n",
      "The 9302 th iteration gives loss of 0.30381629156956724\n",
      "The 9303 th iteration gives loss of 0.303796344921885\n",
      "The 9304 th iteration gives loss of 0.30377640103202463\n",
      "The 9305 th iteration gives loss of 0.3037564598996116\n",
      "The 9306 th iteration gives loss of 0.3037365215242923\n",
      "The 9307 th iteration gives loss of 0.30371658590567485\n",
      "The 9308 th iteration gives loss of 0.3036966530433926\n",
      "The 9309 th iteration gives loss of 0.3036767229370644\n",
      "The 9310 th iteration gives loss of 0.30365679558633196\n",
      "The 9311 th iteration gives loss of 0.3036368709908063\n",
      "The 9312 th iteration gives loss of 0.30361694915011433\n",
      "The 9313 th iteration gives loss of 0.3035970300638881\n",
      "The 9314 th iteration gives loss of 0.303577113731762\n",
      "The 9315 th iteration gives loss of 0.3035572001533468\n",
      "The 9316 th iteration gives loss of 0.30353728932827856\n",
      "The 9317 th iteration gives loss of 0.3035173812561795\n",
      "The 9318 th iteration gives loss of 0.3034974759366782\n",
      "The 9319 th iteration gives loss of 0.3034775733694001\n",
      "The 9320 th iteration gives loss of 0.3034576735539751\n",
      "The 9321 th iteration gives loss of 0.3034377764900173\n",
      "The 9322 th iteration gives loss of 0.30341788217717336\n",
      "The 9323 th iteration gives loss of 0.3033979906150607\n",
      "The 9324 th iteration gives loss of 0.3033781018032996\n",
      "The 9325 th iteration gives loss of 0.303358215741523\n",
      "The 9326 th iteration gives loss of 0.3033383324293677\n",
      "The 9327 th iteration gives loss of 0.3033184518664479\n",
      "The 9328 th iteration gives loss of 0.303298574052398\n",
      "The 9329 th iteration gives loss of 0.30327869898683874\n",
      "The 9330 th iteration gives loss of 0.30325882666940035\n",
      "The 9331 th iteration gives loss of 0.3032389570996984\n",
      "The 9332 th iteration gives loss of 0.3032190902773699\n",
      "The 9333 th iteration gives loss of 0.3031992262020576\n",
      "The 9334 th iteration gives loss of 0.3031793648733681\n",
      "The 9335 th iteration gives loss of 0.3031595062909414\n",
      "The 9336 th iteration gives loss of 0.30313965045439156\n",
      "The 9337 th iteration gives loss of 0.30311979736335437\n",
      "The 9338 th iteration gives loss of 0.3030999470174571\n",
      "The 9339 th iteration gives loss of 0.3030800994163262\n",
      "The 9340 th iteration gives loss of 0.3030602545595884\n",
      "The 9341 th iteration gives loss of 0.3030404124468752\n",
      "The 9342 th iteration gives loss of 0.30302057307781083\n",
      "The 9343 th iteration gives loss of 0.3030007364520241\n",
      "The 9344 th iteration gives loss of 0.30298090256913796\n",
      "The 9345 th iteration gives loss of 0.30296107142879075\n",
      "The 9346 th iteration gives loss of 0.30294124303060554\n",
      "The 9347 th iteration gives loss of 0.3029214173742112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 9348 th iteration gives loss of 0.30290159445922576\n",
      "The 9349 th iteration gives loss of 0.30288177428529217\n",
      "The 9350 th iteration gives loss of 0.30286195685202777\n",
      "The 9351 th iteration gives loss of 0.30284214215906347\n",
      "The 9352 th iteration gives loss of 0.30282233020602817\n",
      "The 9353 th iteration gives loss of 0.3028025209925553\n",
      "The 9354 th iteration gives loss of 0.3027827145182575\n",
      "The 9355 th iteration gives loss of 0.3027629107827785\n",
      "The 9356 th iteration gives loss of 0.3027431097857407\n",
      "The 9357 th iteration gives loss of 0.3027233115267799\n",
      "The 9358 th iteration gives loss of 0.30270351600551443\n",
      "The 9359 th iteration gives loss of 0.3026837232215811\n",
      "The 9360 th iteration gives loss of 0.3026639331746118\n",
      "The 9361 th iteration gives loss of 0.3026441458642156\n",
      "The 9362 th iteration gives loss of 0.30262436129002274\n",
      "The 9363 th iteration gives loss of 0.302604579451684\n",
      "The 9364 th iteration gives loss of 0.3025848003488187\n",
      "The 9365 th iteration gives loss of 0.30256502398104573\n",
      "The 9366 th iteration gives loss of 0.3025452503480095\n",
      "The 9367 th iteration gives loss of 0.3025254794493335\n",
      "The 9368 th iteration gives loss of 0.30250571128464077\n",
      "The 9369 th iteration gives loss of 0.3024859458535555\n",
      "The 9370 th iteration gives loss of 0.30246618315572166\n",
      "The 9371 th iteration gives loss of 0.30244642319076287\n",
      "The 9372 th iteration gives loss of 0.30242666595829476\n",
      "The 9373 th iteration gives loss of 0.3024069114579604\n",
      "The 9374 th iteration gives loss of 0.3023871596893902\n",
      "The 9375 th iteration gives loss of 0.3023674106522114\n",
      "The 9376 th iteration gives loss of 0.3023476643460494\n",
      "The 9377 th iteration gives loss of 0.3023279207705476\n",
      "The 9378 th iteration gives loss of 0.302308179925312\n",
      "The 9379 th iteration gives loss of 0.3022884418099774\n",
      "The 9380 th iteration gives loss of 0.30226870642418335\n",
      "The 9381 th iteration gives loss of 0.30224897376755916\n",
      "The 9382 th iteration gives loss of 0.30222924383973204\n",
      "The 9383 th iteration gives loss of 0.3022095166403319\n",
      "The 9384 th iteration gives loss of 0.30218979216898095\n",
      "The 9385 th iteration gives loss of 0.30217007042532\n",
      "The 9386 th iteration gives loss of 0.302150351408969\n",
      "The 9387 th iteration gives loss of 0.30213063511956634\n",
      "The 9388 th iteration gives loss of 0.3021109215567312\n",
      "The 9389 th iteration gives loss of 0.302091210720104\n",
      "The 9390 th iteration gives loss of 0.3020715026092994\n",
      "The 9391 th iteration gives loss of 0.30205179722397024\n",
      "The 9392 th iteration gives loss of 0.3020320945637306\n",
      "The 9393 th iteration gives loss of 0.302012394628217\n",
      "The 9394 th iteration gives loss of 0.3019926974170604\n",
      "The 9395 th iteration gives loss of 0.30197300292987656\n",
      "The 9396 th iteration gives loss of 0.3019533111663155\n",
      "The 9397 th iteration gives loss of 0.3019336221259889\n",
      "The 9398 th iteration gives loss of 0.3019139358085423\n",
      "The 9399 th iteration gives loss of 0.30189425221360605\n",
      "The 9400 th iteration gives loss of 0.3018745713407968\n",
      "The 9401 th iteration gives loss of 0.3018548931897572\n",
      "The 9402 th iteration gives loss of 0.3018352177601159\n",
      "The 9403 th iteration gives loss of 0.30181554505149044\n",
      "The 9404 th iteration gives loss of 0.3017958750635339\n",
      "The 9405 th iteration gives loss of 0.30177620779586056\n",
      "The 9406 th iteration gives loss of 0.3017565432481055\n",
      "The 9407 th iteration gives loss of 0.30173688141989713\n",
      "The 9408 th iteration gives loss of 0.3017172223108647\n",
      "The 9409 th iteration gives loss of 0.3016975659206452\n",
      "The 9410 th iteration gives loss of 0.3016779122488737\n",
      "The 9411 th iteration gives loss of 0.3016582612951633\n",
      "The 9412 th iteration gives loss of 0.3016386130591652\n",
      "The 9413 th iteration gives loss of 0.3016189675404955\n",
      "The 9414 th iteration gives loss of 0.3015993247387861\n",
      "The 9415 th iteration gives loss of 0.3015796846536809\n",
      "The 9416 th iteration gives loss of 0.3015600472847971\n",
      "The 9417 th iteration gives loss of 0.3015404126317656\n",
      "The 9418 th iteration gives loss of 0.3015207806942314\n",
      "The 9419 th iteration gives loss of 0.30150115147181195\n",
      "The 9420 th iteration gives loss of 0.30148152496415526\n",
      "The 9421 th iteration gives loss of 0.3014619011708681\n",
      "The 9422 th iteration gives loss of 0.30144228009160823\n",
      "The 9423 th iteration gives loss of 0.30142266172598603\n",
      "The 9424 th iteration gives loss of 0.3014030460736345\n",
      "The 9425 th iteration gives loss of 0.3013834331342041\n",
      "The 9426 th iteration gives loss of 0.30136382290730235\n",
      "The 9427 th iteration gives loss of 0.3013442153925828\n",
      "The 9428 th iteration gives loss of 0.30132461058965293\n",
      "The 9429 th iteration gives loss of 0.3013050084981687\n",
      "The 9430 th iteration gives loss of 0.30128540911775004\n",
      "The 9431 th iteration gives loss of 0.3012658124480271\n",
      "The 9432 th iteration gives loss of 0.3012462184886225\n",
      "The 9433 th iteration gives loss of 0.30122662723919663\n",
      "The 9434 th iteration gives loss of 0.30120703869934556\n",
      "The 9435 th iteration gives loss of 0.30118745286873144\n",
      "The 9436 th iteration gives loss of 0.3011678697469826\n",
      "The 9437 th iteration gives loss of 0.3011482893337075\n",
      "The 9438 th iteration gives loss of 0.3011287116285674\n",
      "The 9439 th iteration gives loss of 0.3011091366311761\n",
      "The 9440 th iteration gives loss of 0.30108956434115935\n",
      "The 9441 th iteration gives loss of 0.3010699947581727\n",
      "The 9442 th iteration gives loss of 0.3010504278818357\n",
      "The 9443 th iteration gives loss of 0.3010308637117818\n",
      "The 9444 th iteration gives loss of 0.30101130224763517\n",
      "The 9445 th iteration gives loss of 0.3009917434890362\n",
      "The 9446 th iteration gives loss of 0.3009721874356236\n",
      "The 9447 th iteration gives loss of 0.30095263408701645\n",
      "The 9448 th iteration gives loss of 0.30093308344286074\n",
      "The 9449 th iteration gives loss of 0.30091353550277233\n",
      "The 9450 th iteration gives loss of 0.3008939902663984\n",
      "The 9451 th iteration gives loss of 0.3008744477333689\n",
      "The 9452 th iteration gives loss of 0.3008549079033104\n",
      "The 9453 th iteration gives loss of 0.300835370775863\n",
      "The 9454 th iteration gives loss of 0.30081583635065884\n",
      "The 9455 th iteration gives loss of 0.30079630462732354\n",
      "The 9456 th iteration gives loss of 0.300776775605484\n",
      "The 9457 th iteration gives loss of 0.30075724928479053\n",
      "The 9458 th iteration gives loss of 0.30073772566487106\n",
      "The 9459 th iteration gives loss of 0.30071820474535477\n",
      "The 9460 th iteration gives loss of 0.3006986865258738\n",
      "The 9461 th iteration gives loss of 0.3006791710060673\n",
      "The 9462 th iteration gives loss of 0.3006596581855671\n",
      "The 9463 th iteration gives loss of 0.3006401480640012\n",
      "The 9464 th iteration gives loss of 0.30062064064101135\n",
      "The 9465 th iteration gives loss of 0.3006011359162212\n",
      "The 9466 th iteration gives loss of 0.30058163388926085\n",
      "The 9467 th iteration gives loss of 0.30056213455976893\n",
      "The 9468 th iteration gives loss of 0.30054263792739\n",
      "The 9469 th iteration gives loss of 0.3005231439917507\n",
      "The 9470 th iteration gives loss of 0.3005036527524696\n",
      "The 9471 th iteration gives loss of 0.30048416420919727\n",
      "The 9472 th iteration gives loss of 0.30046467836156815\n",
      "The 9473 th iteration gives loss of 0.30044519520920376\n",
      "The 9474 th iteration gives loss of 0.3004257147517434\n",
      "The 9475 th iteration gives loss of 0.30040623698882757\n",
      "The 9476 th iteration gives loss of 0.30038676192007213\n",
      "The 9477 th iteration gives loss of 0.3003672895451319\n",
      "The 9478 th iteration gives loss of 0.30034781986363085\n",
      "The 9479 th iteration gives loss of 0.3003283528752017\n",
      "The 9480 th iteration gives loss of 0.3003088885794844\n",
      "The 9481 th iteration gives loss of 0.3002894269761034\n",
      "The 9482 th iteration gives loss of 0.30026996806469064\n",
      "The 9483 th iteration gives loss of 0.3002505118448993\n",
      "The 9484 th iteration gives loss of 0.300231058316345\n",
      "The 9485 th iteration gives loss of 0.30021160747866765\n",
      "The 9486 th iteration gives loss of 0.300192159331507\n",
      "The 9487 th iteration gives loss of 0.3001727138744936\n",
      "The 9488 th iteration gives loss of 0.30015327110725404\n",
      "The 9489 th iteration gives loss of 0.30013383102943786\n",
      "The 9490 th iteration gives loss of 0.30011439364066084\n",
      "The 9491 th iteration gives loss of 0.3000949589405657\n",
      "The 9492 th iteration gives loss of 0.30007552692879713\n",
      "The 9493 th iteration gives loss of 0.3000560976049773\n",
      "The 9494 th iteration gives loss of 0.3000366709687353\n",
      "The 9495 th iteration gives loss of 0.30001724701972876\n",
      "The 9496 th iteration gives loss of 0.29999782575757483\n",
      "The 9497 th iteration gives loss of 0.29997840718189994\n",
      "The 9498 th iteration gives loss of 0.2999589912923582\n",
      "The 9499 th iteration gives loss of 0.29993957808857646\n",
      "The 9500 th iteration gives loss of 0.2999201675701901\n",
      "The 9501 th iteration gives loss of 0.29990075973683367\n",
      "The 9502 th iteration gives loss of 0.2998813545881375\n",
      "The 9503 th iteration gives loss of 0.2998619521237422\n",
      "The 9504 th iteration gives loss of 0.29984255234328555\n",
      "The 9505 th iteration gives loss of 0.2998231552463928\n",
      "The 9506 th iteration gives loss of 0.29980376083270455\n",
      "The 9507 th iteration gives loss of 0.2997843691018617\n",
      "The 9508 th iteration gives loss of 0.2997649800534887\n",
      "The 9509 th iteration gives loss of 0.2997455936872201\n",
      "The 9510 th iteration gives loss of 0.2997262100027012\n",
      "The 9511 th iteration gives loss of 0.2997068289995639\n",
      "The 9512 th iteration gives loss of 0.29968745067743613\n",
      "The 9513 th iteration gives loss of 0.2996680750359663\n",
      "The 9514 th iteration gives loss of 0.2996487020747798\n",
      "The 9515 th iteration gives loss of 0.29962933179351386\n",
      "The 9516 th iteration gives loss of 0.2996099641918043\n",
      "The 9517 th iteration gives loss of 0.2995905992692858\n",
      "The 9518 th iteration gives loss of 0.29957123702559835\n",
      "The 9519 th iteration gives loss of 0.29955187746037787\n",
      "The 9520 th iteration gives loss of 0.29953252057325036\n",
      "The 9521 th iteration gives loss of 0.2995131663638603\n",
      "The 9522 th iteration gives loss of 0.29949381483184423\n",
      "The 9523 th iteration gives loss of 0.29947446597682986\n",
      "The 9524 th iteration gives loss of 0.29945511979846173\n",
      "The 9525 th iteration gives loss of 0.2994357762963678\n",
      "The 9526 th iteration gives loss of 0.2994164354701855\n",
      "The 9527 th iteration gives loss of 0.29939709731955966\n",
      "The 9528 th iteration gives loss of 0.29937776184411286\n",
      "The 9529 th iteration gives loss of 0.2993584290434981\n",
      "The 9530 th iteration gives loss of 0.2993390989173308\n",
      "The 9531 th iteration gives loss of 0.29931977146526645\n",
      "The 9532 th iteration gives loss of 0.2993004466869316\n",
      "The 9533 th iteration gives loss of 0.29928112458196326\n",
      "The 9534 th iteration gives loss of 0.2992618051499959\n",
      "The 9535 th iteration gives loss of 0.2992424883906668\n",
      "The 9536 th iteration gives loss of 0.29922317430361056\n",
      "The 9537 th iteration gives loss of 0.2992038628884719\n",
      "The 9538 th iteration gives loss of 0.2991845541448784\n",
      "The 9539 th iteration gives loss of 0.29916524807247347\n",
      "The 9540 th iteration gives loss of 0.29914594467088695\n",
      "The 9541 th iteration gives loss of 0.29912664393975946\n",
      "The 9542 th iteration gives loss of 0.29910734587872906\n",
      "The 9543 th iteration gives loss of 0.2990880504874269\n",
      "The 9544 th iteration gives loss of 0.29906875776549313\n",
      "The 9545 th iteration gives loss of 0.29904946771256263\n",
      "The 9546 th iteration gives loss of 0.29903018032827244\n",
      "The 9547 th iteration gives loss of 0.2990108956122531\n",
      "The 9548 th iteration gives loss of 0.2989916135641592\n",
      "The 9549 th iteration gives loss of 0.2989723341836163\n",
      "The 9550 th iteration gives loss of 0.29895305747025913\n",
      "The 9551 th iteration gives loss of 0.29893378342373644\n",
      "The 9552 th iteration gives loss of 0.2989145120436642\n",
      "The 9553 th iteration gives loss of 0.2988952433296996\n",
      "The 9554 th iteration gives loss of 0.29887597728147\n",
      "The 9555 th iteration gives loss of 0.29885671389861646\n",
      "The 9556 th iteration gives loss of 0.29883745318076693\n",
      "The 9557 th iteration gives loss of 0.2988181951275789\n",
      "The 9558 th iteration gives loss of 0.2987989397386653\n",
      "The 9559 th iteration gives loss of 0.29877968701367774\n",
      "The 9560 th iteration gives loss of 0.2987604369522539\n",
      "The 9561 th iteration gives loss of 0.29874118955402457\n",
      "The 9562 th iteration gives loss of 0.2987219448186349\n",
      "The 9563 th iteration gives loss of 0.2987027027457094\n",
      "The 9564 th iteration gives loss of 0.2986834633348995\n",
      "The 9565 th iteration gives loss of 0.2986642265858423\n",
      "The 9566 th iteration gives loss of 0.29864499249816695\n",
      "The 9567 th iteration gives loss of 0.2986257610715162\n",
      "The 9568 th iteration gives loss of 0.29860653230552764\n",
      "The 9569 th iteration gives loss of 0.2985873061998348\n",
      "The 9570 th iteration gives loss of 0.29856808275407704\n",
      "The 9571 th iteration gives loss of 0.2985488619678963\n",
      "The 9572 th iteration gives loss of 0.29852964384092895\n",
      "The 9573 th iteration gives loss of 0.2985104283728094\n",
      "The 9574 th iteration gives loss of 0.2984912155631822\n",
      "The 9575 th iteration gives loss of 0.2984720054116815\n",
      "The 9576 th iteration gives loss of 0.29845279791794516\n",
      "The 9577 th iteration gives loss of 0.29843359308160916\n",
      "The 9578 th iteration gives loss of 0.29841439090231303\n",
      "The 9579 th iteration gives loss of 0.29839519137969117\n",
      "The 9580 th iteration gives loss of 0.29837599451339236\n",
      "The 9581 th iteration gives loss of 0.2983568003030513\n",
      "The 9582 th iteration gives loss of 0.2983376087483084\n",
      "The 9583 th iteration gives loss of 0.29831841984878277\n",
      "The 9584 th iteration gives loss of 0.2982992336041378\n",
      "The 9585 th iteration gives loss of 0.2982800500139938\n",
      "The 9586 th iteration gives loss of 0.29826086907800414\n",
      "The 9587 th iteration gives loss of 0.29824169079579466\n",
      "The 9588 th iteration gives loss of 0.2982225151670127\n",
      "The 9589 th iteration gives loss of 0.2982033421913009\n",
      "The 9590 th iteration gives loss of 0.2981841718682799\n",
      "The 9591 th iteration gives loss of 0.2981650041976049\n",
      "The 9592 th iteration gives loss of 0.2981458391789071\n",
      "The 9593 th iteration gives loss of 0.2981266768118241\n",
      "The 9594 th iteration gives loss of 0.29810751709599803\n",
      "The 9595 th iteration gives loss of 0.29808836003107\n",
      "The 9596 th iteration gives loss of 0.29806920561668254\n",
      "The 9597 th iteration gives loss of 0.29805005385246275\n",
      "The 9598 th iteration gives loss of 0.2980309047380533\n",
      "The 9599 th iteration gives loss of 0.298011758273102\n",
      "The 9600 th iteration gives loss of 0.2979926144572324\n",
      "The 9601 th iteration gives loss of 0.2979734732900997\n",
      "The 9602 th iteration gives loss of 0.29795433477133504\n",
      "The 9603 th iteration gives loss of 0.297935198900576\n",
      "The 9604 th iteration gives loss of 0.29791606567745477\n",
      "The 9605 th iteration gives loss of 0.29789693510163456\n",
      "The 9606 th iteration gives loss of 0.2978778071727277\n",
      "The 9607 th iteration gives loss of 0.2978586818903903\n",
      "The 9608 th iteration gives loss of 0.2978395592542661\n",
      "The 9609 th iteration gives loss of 0.2978204392639841\n",
      "The 9610 th iteration gives loss of 0.2978013219191814\n",
      "The 9611 th iteration gives loss of 0.2977822072195113\n",
      "The 9612 th iteration gives loss of 0.2977630951645878\n",
      "The 9613 th iteration gives loss of 0.29774398575407224\n",
      "The 9614 th iteration gives loss of 0.2977248789876009\n",
      "The 9615 th iteration gives loss of 0.2977057748648148\n",
      "The 9616 th iteration gives loss of 0.297686673385349\n",
      "The 9617 th iteration gives loss of 0.29766757454884113\n",
      "The 9618 th iteration gives loss of 0.29764847835493285\n",
      "The 9619 th iteration gives loss of 0.29762938480326656\n",
      "The 9620 th iteration gives loss of 0.297610293893483\n",
      "The 9621 th iteration gives loss of 0.2975912056252183\n",
      "The 9622 th iteration gives loss of 0.2975721199981161\n",
      "The 9623 th iteration gives loss of 0.29755303701181907\n",
      "The 9624 th iteration gives loss of 0.2975339566659565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 9625 th iteration gives loss of 0.2975148789601813\n",
      "The 9626 th iteration gives loss of 0.29749580389411945\n",
      "The 9627 th iteration gives loss of 0.29747673146742265\n",
      "The 9628 th iteration gives loss of 0.2974576616797358\n",
      "The 9629 th iteration gives loss of 0.29743859453068194\n",
      "The 9630 th iteration gives loss of 0.2974195300199075\n",
      "The 9631 th iteration gives loss of 0.2974004681470635\n",
      "The 9632 th iteration gives loss of 0.29738140891177944\n",
      "The 9633 th iteration gives loss of 0.2973623523137006\n",
      "The 9634 th iteration gives loss of 0.2973432983524692\n",
      "The 9635 th iteration gives loss of 0.2973242470277195\n",
      "The 9636 th iteration gives loss of 0.29730519833909164\n",
      "The 9637 th iteration gives loss of 0.2972861522862357\n",
      "The 9638 th iteration gives loss of 0.2972671088687818\n",
      "The 9639 th iteration gives loss of 0.29724806808638315\n",
      "The 9640 th iteration gives loss of 0.29722902993866557\n",
      "The 9641 th iteration gives loss of 0.29720999442528157\n",
      "The 9642 th iteration gives loss of 0.2971909615458583\n",
      "The 9643 th iteration gives loss of 0.2971719313000523\n",
      "The 9644 th iteration gives loss of 0.2971529036874974\n",
      "The 9645 th iteration gives loss of 0.2971338787078408\n",
      "The 9646 th iteration gives loss of 0.29711485636071355\n",
      "The 9647 th iteration gives loss of 0.2970958366457584\n",
      "The 9648 th iteration gives loss of 0.2970768195626199\n",
      "The 9649 th iteration gives loss of 0.2970578051109424\n",
      "The 9650 th iteration gives loss of 0.2970387932903621\n",
      "The 9651 th iteration gives loss of 0.29701978410051666\n",
      "The 9652 th iteration gives loss of 0.29700077754105647\n",
      "The 9653 th iteration gives loss of 0.29698177361161765\n",
      "The 9654 th iteration gives loss of 0.29696277231183926\n",
      "The 9655 th iteration gives loss of 0.29694377364136837\n",
      "The 9656 th iteration gives loss of 0.29692477759984115\n",
      "The 9657 th iteration gives loss of 0.29690578418690194\n",
      "The 9658 th iteration gives loss of 0.2968867934022005\n",
      "The 9659 th iteration gives loss of 0.2968678052453645\n",
      "The 9660 th iteration gives loss of 0.29684881971602883\n",
      "The 9661 th iteration gives loss of 0.2968298368138574\n",
      "The 9662 th iteration gives loss of 0.2968108565384849\n",
      "The 9663 th iteration gives loss of 0.29679187888955394\n",
      "The 9664 th iteration gives loss of 0.29677290386669053\n",
      "The 9665 th iteration gives loss of 0.29675393146955176\n",
      "The 9666 th iteration gives loss of 0.2967349616977768\n",
      "The 9667 th iteration gives loss of 0.29671599455100317\n",
      "The 9668 th iteration gives loss of 0.2966970300288853\n",
      "The 9669 th iteration gives loss of 0.2966780681310494\n",
      "The 9670 th iteration gives loss of 0.2966591088571467\n",
      "The 9671 th iteration gives loss of 0.29664015220681345\n",
      "The 9672 th iteration gives loss of 0.2966211981796967\n",
      "The 9673 th iteration gives loss of 0.296602246775432\n",
      "The 9674 th iteration gives loss of 0.2965832979936684\n",
      "The 9675 th iteration gives loss of 0.2965643518340538\n",
      "The 9676 th iteration gives loss of 0.2965454082962219\n",
      "The 9677 th iteration gives loss of 0.2965264673798142\n",
      "The 9678 th iteration gives loss of 0.29650752908446815\n",
      "The 9679 th iteration gives loss of 0.2964885934098329\n",
      "The 9680 th iteration gives loss of 0.29646966035555833\n",
      "The 9681 th iteration gives loss of 0.2964507299212783\n",
      "The 9682 th iteration gives loss of 0.2964318021066371\n",
      "The 9683 th iteration gives loss of 0.29641287691127205\n",
      "The 9684 th iteration gives loss of 0.29639395433483323\n",
      "The 9685 th iteration gives loss of 0.29637503437695517\n",
      "The 9686 th iteration gives loss of 0.29635611703729553\n",
      "The 9687 th iteration gives loss of 0.2963372023154836\n",
      "The 9688 th iteration gives loss of 0.2963182902111687\n",
      "The 9689 th iteration gives loss of 0.296299380723984\n",
      "The 9690 th iteration gives loss of 0.29628047385358003\n",
      "The 9691 th iteration gives loss of 0.2962615695996007\n",
      "The 9692 th iteration gives loss of 0.2962426679616803\n",
      "The 9693 th iteration gives loss of 0.2962237689394777\n",
      "The 9694 th iteration gives loss of 0.2962048725326257\n",
      "The 9695 th iteration gives loss of 0.2961859787407695\n",
      "The 9696 th iteration gives loss of 0.29616708756354526\n",
      "The 9697 th iteration gives loss of 0.29614819900060496\n",
      "The 9698 th iteration gives loss of 0.2961293130515899\n",
      "The 9699 th iteration gives loss of 0.2961104297161436\n",
      "The 9700 th iteration gives loss of 0.2960915489939038\n",
      "The 9701 th iteration gives loss of 0.2960726708845186\n",
      "The 9702 th iteration gives loss of 0.2960537953876356\n",
      "The 9703 th iteration gives loss of 0.2960349225028869\n",
      "The 9704 th iteration gives loss of 0.29601605222992383\n",
      "The 9705 th iteration gives loss of 0.2959971845683873\n",
      "The 9706 th iteration gives loss of 0.29597831951792264\n",
      "The 9707 th iteration gives loss of 0.2959594570781811\n",
      "The 9708 th iteration gives loss of 0.29594059724878097\n",
      "The 9709 th iteration gives loss of 0.29592174002939586\n",
      "The 9710 th iteration gives loss of 0.2959028854196541\n",
      "The 9711 th iteration gives loss of 0.29588403341919195\n",
      "The 9712 th iteration gives loss of 0.295865184027673\n",
      "The 9713 th iteration gives loss of 0.2958463372447289\n",
      "The 9714 th iteration gives loss of 0.2958274930700017\n",
      "The 9715 th iteration gives loss of 0.2958086515031515\n",
      "The 9716 th iteration gives loss of 0.2957898125438071\n",
      "The 9717 th iteration gives loss of 0.2957709761916056\n",
      "The 9718 th iteration gives loss of 0.295752142446212\n",
      "The 9719 th iteration gives loss of 0.2957333113072443\n",
      "The 9720 th iteration gives loss of 0.29571448277436707\n",
      "The 9721 th iteration gives loss of 0.29569565684721233\n",
      "The 9722 th iteration gives loss of 0.2956768335254383\n",
      "The 9723 th iteration gives loss of 0.29565801280868176\n",
      "The 9724 th iteration gives loss of 0.2956391946965908\n",
      "The 9725 th iteration gives loss of 0.29562037918879613\n",
      "The 9726 th iteration gives loss of 0.2956015662849543\n",
      "The 9727 th iteration gives loss of 0.29558275598470696\n",
      "The 9728 th iteration gives loss of 0.2955639482877034\n",
      "The 9729 th iteration gives loss of 0.29554514319357605\n",
      "The 9730 th iteration gives loss of 0.2955263407019806\n",
      "The 9731 th iteration gives loss of 0.29550754081255654\n",
      "The 9732 th iteration gives loss of 0.29548874352494886\n",
      "The 9733 th iteration gives loss of 0.29546994883879935\n",
      "The 9734 th iteration gives loss of 0.2954511567537642\n",
      "The 9735 th iteration gives loss of 0.29543236726947497\n",
      "The 9736 th iteration gives loss of 0.2954135803855804\n",
      "The 9737 th iteration gives loss of 0.29539479610172764\n",
      "The 9738 th iteration gives loss of 0.2953760144175563\n",
      "The 9739 th iteration gives loss of 0.29535723533272246\n",
      "The 9740 th iteration gives loss of 0.2953384588468584\n",
      "The 9741 th iteration gives loss of 0.2953196849596193\n",
      "The 9742 th iteration gives loss of 0.29530091367063976\n",
      "The 9743 th iteration gives loss of 0.2952821449795722\n",
      "The 9744 th iteration gives loss of 0.29526337888605636\n",
      "The 9745 th iteration gives loss of 0.29524461538974744\n",
      "The 9746 th iteration gives loss of 0.29522585449027944\n",
      "The 9747 th iteration gives loss of 0.2952070961873022\n",
      "The 9748 th iteration gives loss of 0.2951883404804633\n",
      "The 9749 th iteration gives loss of 0.2951695873694106\n",
      "The 9750 th iteration gives loss of 0.2951508368537703\n",
      "The 9751 th iteration gives loss of 0.29513208893321274\n",
      "The 9752 th iteration gives loss of 0.2951133436073698\n",
      "The 9753 th iteration gives loss of 0.2950946008758889\n",
      "The 9754 th iteration gives loss of 0.2950758607384142\n",
      "The 9755 th iteration gives loss of 0.2950571231945932\n",
      "The 9756 th iteration gives loss of 0.2950383882440781\n",
      "The 9757 th iteration gives loss of 0.2950196558864962\n",
      "The 9758 th iteration gives loss of 0.29500092612151113\n",
      "The 9759 th iteration gives loss of 0.2949821989487618\n",
      "The 9760 th iteration gives loss of 0.2949634743678937\n",
      "The 9761 th iteration gives loss of 0.29494475237856094\n",
      "The 9762 th iteration gives loss of 0.29492603298039405\n",
      "The 9763 th iteration gives loss of 0.2949073161730455\n",
      "The 9764 th iteration gives loss of 0.2948886019561635\n",
      "The 9765 th iteration gives loss of 0.2948698903293935\n",
      "The 9766 th iteration gives loss of 0.294851181292386\n",
      "The 9767 th iteration gives loss of 0.29483247484477937\n",
      "The 9768 th iteration gives loss of 0.2948137709862177\n",
      "The 9769 th iteration gives loss of 0.2947950697163525\n",
      "The 9770 th iteration gives loss of 0.294776371034826\n",
      "The 9771 th iteration gives loss of 0.29475767494129346\n",
      "The 9772 th iteration gives loss of 0.2947389814353842\n",
      "The 9773 th iteration gives loss of 0.29472029051676574\n",
      "The 9774 th iteration gives loss of 0.2947016021850678\n",
      "The 9775 th iteration gives loss of 0.29468291643995387\n",
      "The 9776 th iteration gives loss of 0.2946642332810413\n",
      "The 9777 th iteration gives loss of 0.29464555270800785\n",
      "The 9778 th iteration gives loss of 0.29462687472048604\n",
      "The 9779 th iteration gives loss of 0.29460819931811183\n",
      "The 9780 th iteration gives loss of 0.29458952650054726\n",
      "The 9781 th iteration gives loss of 0.2945708562674401\n",
      "The 9782 th iteration gives loss of 0.29455218861842564\n",
      "The 9783 th iteration gives loss of 0.29453352355316265\n",
      "The 9784 th iteration gives loss of 0.2945148610712858\n",
      "The 9785 th iteration gives loss of 0.2944962011724467\n",
      "The 9786 th iteration gives loss of 0.2944775438562944\n",
      "The 9787 th iteration gives loss of 0.2944588891224696\n",
      "The 9788 th iteration gives loss of 0.29444023697062593\n",
      "The 9789 th iteration gives loss of 0.2944215874004156\n",
      "The 9790 th iteration gives loss of 0.29440294041147436\n",
      "The 9791 th iteration gives loss of 0.2943842960034483\n",
      "The 9792 th iteration gives loss of 0.2943656541759922\n",
      "The 9793 th iteration gives loss of 0.2943470149287549\n",
      "The 9794 th iteration gives loss of 0.29432837826136954\n",
      "The 9795 th iteration gives loss of 0.29430974417349776\n",
      "The 9796 th iteration gives loss of 0.2942911126647808\n",
      "The 9797 th iteration gives loss of 0.2942724837348707\n",
      "The 9798 th iteration gives loss of 0.2942538573834026\n",
      "The 9799 th iteration gives loss of 0.2942352336100297\n",
      "The 9800 th iteration gives loss of 0.29421661241440344\n",
      "The 9801 th iteration gives loss of 0.29419799379617956\n",
      "The 9802 th iteration gives loss of 0.2941793777549784\n",
      "The 9803 th iteration gives loss of 0.29416076429046895\n",
      "The 9804 th iteration gives loss of 0.2941421534022971\n",
      "The 9805 th iteration gives loss of 0.29412354509011357\n",
      "The 9806 th iteration gives loss of 0.29410493935356097\n",
      "The 9807 th iteration gives loss of 0.2940863361922652\n",
      "The 9808 th iteration gives loss of 0.2940677356059046\n",
      "The 9809 th iteration gives loss of 0.2940491375941266\n",
      "The 9810 th iteration gives loss of 0.2940305421565643\n",
      "The 9811 th iteration gives loss of 0.2940119492928614\n",
      "The 9812 th iteration gives loss of 0.29399335900268037\n",
      "The 9813 th iteration gives loss of 0.2939747712856694\n",
      "The 9814 th iteration gives loss of 0.29395618614145785\n",
      "The 9815 th iteration gives loss of 0.29393760356971527\n",
      "The 9816 th iteration gives loss of 0.29391902357007027\n",
      "The 9817 th iteration gives loss of 0.2939004461421899\n",
      "The 9818 th iteration gives loss of 0.2938818712857116\n",
      "The 9819 th iteration gives loss of 0.2938632990002851\n",
      "The 9820 th iteration gives loss of 0.29384472928555716\n",
      "The 9821 th iteration gives loss of 0.2938261621411801\n",
      "The 9822 th iteration gives loss of 0.29380759756679226\n",
      "The 9823 th iteration gives loss of 0.2937890355620594\n",
      "The 9824 th iteration gives loss of 0.29377047612661666\n",
      "The 9825 th iteration gives loss of 0.2937519192601147\n",
      "The 9826 th iteration gives loss of 0.293733364962206\n",
      "The 9827 th iteration gives loss of 0.2937148132325275\n",
      "The 9828 th iteration gives loss of 0.2936962640707375\n",
      "The 9829 th iteration gives loss of 0.2936777174764878\n",
      "The 9830 th iteration gives loss of 0.29365917344941433\n",
      "The 9831 th iteration gives loss of 0.2936406319891823\n",
      "The 9832 th iteration gives loss of 0.2936220930954257\n",
      "The 9833 th iteration gives loss of 0.2936035567678019\n",
      "The 9834 th iteration gives loss of 0.29358502300595773\n",
      "The 9835 th iteration gives loss of 0.2935664918095458\n",
      "The 9836 th iteration gives loss of 0.29354796317820175\n",
      "The 9837 th iteration gives loss of 0.2935294371115876\n",
      "The 9838 th iteration gives loss of 0.2935109136093412\n",
      "The 9839 th iteration gives loss of 0.29349239267111993\n",
      "The 9840 th iteration gives loss of 0.29347387429657795\n",
      "The 9841 th iteration gives loss of 0.29345535848534665\n",
      "The 9842 th iteration gives loss of 0.2934368452370902\n",
      "The 9843 th iteration gives loss of 0.2934183345514511\n",
      "The 9844 th iteration gives loss of 0.293399826428079\n",
      "The 9845 th iteration gives loss of 0.29338132086663005\n",
      "The 9846 th iteration gives loss of 0.29336281786674406\n",
      "The 9847 th iteration gives loss of 0.29334431742807093\n",
      "The 9848 th iteration gives loss of 0.2933258195502715\n",
      "The 9849 th iteration gives loss of 0.2933073242329751\n",
      "The 9850 th iteration gives loss of 0.2932888314758556\n",
      "The 9851 th iteration gives loss of 0.2932703412785425\n",
      "The 9852 th iteration gives loss of 0.29325185364068923\n",
      "The 9853 th iteration gives loss of 0.2932333685619442\n",
      "The 9854 th iteration gives loss of 0.293214886041963\n",
      "The 9855 th iteration gives loss of 0.2931964060804019\n",
      "The 9856 th iteration gives loss of 0.2931779286768933\n",
      "The 9857 th iteration gives loss of 0.29315945383110126\n",
      "The 9858 th iteration gives loss of 0.29314098154266605\n",
      "The 9859 th iteration gives loss of 0.29312251181124405\n",
      "The 9860 th iteration gives loss of 0.293104044636477\n",
      "The 9861 th iteration gives loss of 0.2930855800180213\n",
      "The 9862 th iteration gives loss of 0.29306711795552665\n",
      "The 9863 th iteration gives loss of 0.2930486584486369\n",
      "The 9864 th iteration gives loss of 0.2930302014970106\n",
      "The 9865 th iteration gives loss of 0.2930117471002961\n",
      "The 9866 th iteration gives loss of 0.2929932952581318\n",
      "The 9867 th iteration gives loss of 0.2929748459701896\n",
      "The 9868 th iteration gives loss of 0.29295639923609573\n",
      "The 9869 th iteration gives loss of 0.29293795505550385\n",
      "The 9870 th iteration gives loss of 0.29291951342808387\n",
      "The 9871 th iteration gives loss of 0.29290107435347407\n",
      "The 9872 th iteration gives loss of 0.29288263783132\n",
      "The 9873 th iteration gives loss of 0.2928642038612722\n",
      "The 9874 th iteration gives loss of 0.29284577244299476\n",
      "The 9875 th iteration gives loss of 0.2928273435761252\n",
      "The 9876 th iteration gives loss of 0.29280891726031577\n",
      "The 9877 th iteration gives loss of 0.29279049349521563\n",
      "The 9878 th iteration gives loss of 0.29277207228048835\n",
      "The 9879 th iteration gives loss of 0.2927536536157617\n",
      "The 9880 th iteration gives loss of 0.292735237500704\n",
      "The 9881 th iteration gives loss of 0.2927168239349551\n",
      "The 9882 th iteration gives loss of 0.2926984129181787\n",
      "The 9883 th iteration gives loss of 0.29268000445001685\n",
      "The 9884 th iteration gives loss of 0.29266159853011936\n",
      "The 9885 th iteration gives loss of 0.29264319515813264\n",
      "The 9886 th iteration gives loss of 0.2926247943337194\n",
      "The 9887 th iteration gives loss of 0.29260639605652267\n",
      "The 9888 th iteration gives loss of 0.2925880003262008\n",
      "The 9889 th iteration gives loss of 0.2925696071423893\n",
      "The 9890 th iteration gives loss of 0.2925512165047587\n",
      "The 9891 th iteration gives loss of 0.2925328284129457\n",
      "The 9892 th iteration gives loss of 0.2925144428666068\n",
      "The 9893 th iteration gives loss of 0.29249605986539257\n",
      "The 9894 th iteration gives loss of 0.29247767940895586\n",
      "The 9895 th iteration gives loss of 0.2924593014969494\n",
      "The 9896 th iteration gives loss of 0.29244092612901396\n",
      "The 9897 th iteration gives loss of 0.2924225533048093\n",
      "The 9898 th iteration gives loss of 0.29240418302398324\n",
      "The 9899 th iteration gives loss of 0.29238581528618857\n",
      "The 9900 th iteration gives loss of 0.29236745009107773\n",
      "The 9901 th iteration gives loss of 0.2923490874383044\n",
      "The 9902 th iteration gives loss of 0.29233072732751825\n",
      "The 9903 th iteration gives loss of 0.29231236975836805\n",
      "The 9904 th iteration gives loss of 0.2922940147305068\n",
      "The 9905 th iteration gives loss of 0.29227566224358953\n",
      "The 9906 th iteration gives loss of 0.29225731229725965\n",
      "The 9907 th iteration gives loss of 0.2922389648911767\n",
      "The 9908 th iteration gives loss of 0.2922206200249853\n",
      "The 9909 th iteration gives loss of 0.292202277698341\n",
      "The 9910 th iteration gives loss of 0.29218393791089814\n",
      "The 9911 th iteration gives loss of 0.2921656006623094\n",
      "The 9912 th iteration gives loss of 0.2921472659522148\n",
      "The 9913 th iteration gives loss of 0.2921289337802806\n",
      "The 9914 th iteration gives loss of 0.2921106041461534\n",
      "The 9915 th iteration gives loss of 0.29209227704948154\n",
      "The 9916 th iteration gives loss of 0.2920739524899208\n",
      "The 9917 th iteration gives loss of 0.2920556304671233\n",
      "The 9918 th iteration gives loss of 0.2920373109807399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 9919 th iteration gives loss of 0.29201899403042186\n",
      "The 9920 th iteration gives loss of 0.29200067961582793\n",
      "The 9921 th iteration gives loss of 0.2919823677366032\n",
      "The 9922 th iteration gives loss of 0.2919640583923964\n",
      "The 9923 th iteration gives loss of 0.29194575158286573\n",
      "The 9924 th iteration gives loss of 0.29192744730766984\n",
      "The 9925 th iteration gives loss of 0.29190914556645053\n",
      "The 9926 th iteration gives loss of 0.29189084635886486\n",
      "The 9927 th iteration gives loss of 0.29187254968455945\n",
      "The 9928 th iteration gives loss of 0.29185425554319666\n",
      "The 9929 th iteration gives loss of 0.29183596393442257\n",
      "The 9930 th iteration gives loss of 0.2918176748578913\n",
      "The 9931 th iteration gives loss of 0.29179938831325564\n",
      "The 9932 th iteration gives loss of 0.2917811043001637\n",
      "The 9933 th iteration gives loss of 0.2917628228182703\n",
      "The 9934 th iteration gives loss of 0.2917445438672388\n",
      "The 9935 th iteration gives loss of 0.2917262674467053\n",
      "The 9936 th iteration gives loss of 0.29170799355633703\n",
      "The 9937 th iteration gives loss of 0.29168972219577916\n",
      "The 9938 th iteration gives loss of 0.29167145336468653\n",
      "The 9939 th iteration gives loss of 0.29165318706271076\n",
      "The 9940 th iteration gives loss of 0.291634923289501\n",
      "The 9941 th iteration gives loss of 0.29161666204471787\n",
      "The 9942 th iteration gives loss of 0.2915984033280076\n",
      "The 9943 th iteration gives loss of 0.29158014713903024\n",
      "The 9944 th iteration gives loss of 0.2915618934774375\n",
      "The 9945 th iteration gives loss of 0.2915436423428727\n",
      "The 9946 th iteration gives loss of 0.2915253937350014\n",
      "The 9947 th iteration gives loss of 0.2915071476534738\n",
      "The 9948 th iteration gives loss of 0.291488904097938\n",
      "The 9949 th iteration gives loss of 0.2914706630680544\n",
      "The 9950 th iteration gives loss of 0.2914524245634647\n",
      "The 9951 th iteration gives loss of 0.29143418858382975\n",
      "The 9952 th iteration gives loss of 0.291415955128817\n",
      "The 9953 th iteration gives loss of 0.2913977241980575\n",
      "The 9954 th iteration gives loss of 0.2913794957912181\n",
      "The 9955 th iteration gives loss of 0.291361269907941\n",
      "The 9956 th iteration gives loss of 0.29134304654789067\n",
      "The 9957 th iteration gives loss of 0.2913248257107217\n",
      "The 9958 th iteration gives loss of 0.29130660739607656\n",
      "The 9959 th iteration gives loss of 0.2912883916036173\n",
      "The 9960 th iteration gives loss of 0.2912701783329918\n",
      "The 9961 th iteration gives loss of 0.29125196758385385\n",
      "The 9962 th iteration gives loss of 0.2912337593558697\n",
      "The 9963 th iteration gives loss of 0.2912155536486807\n",
      "The 9964 th iteration gives loss of 0.291197350461948\n",
      "The 9965 th iteration gives loss of 0.29117914979531434\n",
      "The 9966 th iteration gives loss of 0.2911609516484529\n",
      "The 9967 th iteration gives loss of 0.29114275602099415\n",
      "The 9968 th iteration gives loss of 0.2911245629126118\n",
      "The 9969 th iteration gives loss of 0.2911063723229444\n",
      "The 9970 th iteration gives loss of 0.29108818425166133\n",
      "The 9971 th iteration gives loss of 0.29106999869840494\n",
      "The 9972 th iteration gives loss of 0.2910518156628359\n",
      "The 9973 th iteration gives loss of 0.29103363514461134\n",
      "The 9974 th iteration gives loss of 0.2910154571433739\n",
      "The 9975 th iteration gives loss of 0.29099728165879485\n",
      "The 9976 th iteration gives loss of 0.2909791086905049\n",
      "The 9977 th iteration gives loss of 0.2909609382381692\n",
      "The 9978 th iteration gives loss of 0.29094277030145965\n",
      "The 9979 th iteration gives loss of 0.29092460488001165\n",
      "The 9980 th iteration gives loss of 0.29090644197347987\n",
      "The 9981 th iteration gives loss of 0.2908882815815224\n",
      "The 9982 th iteration gives loss of 0.29087012370379395\n",
      "The 9983 th iteration gives loss of 0.29085196833995297\n",
      "The 9984 th iteration gives loss of 0.29083381548964793\n",
      "The 9985 th iteration gives loss of 0.2908156651525416\n",
      "The 9986 th iteration gives loss of 0.29079751732827847\n",
      "The 9987 th iteration gives loss of 0.2907793720165228\n",
      "The 9988 th iteration gives loss of 0.29076122921692404\n",
      "The 9989 th iteration gives loss of 0.2907430889291407\n",
      "The 9990 th iteration gives loss of 0.29072495115281544\n",
      "The 9991 th iteration gives loss of 0.2907068158876177\n",
      "The 9992 th iteration gives loss of 0.2906886831331964\n",
      "The 9993 th iteration gives loss of 0.29067055288920696\n",
      "The 9994 th iteration gives loss of 0.2906524251553038\n",
      "The 9995 th iteration gives loss of 0.2906342999311551\n",
      "The 9996 th iteration gives loss of 0.29061617721639565\n",
      "The 9997 th iteration gives loss of 0.2905980570106921\n",
      "The 9998 th iteration gives loss of 0.2905799393136975\n",
      "The 9999 th iteration gives loss of 0.2905618241250615\n",
      "The 10000 th iteration gives loss of 0.29054371144443986\n",
      "The 10001 th iteration gives loss of 0.29052560127150306\n",
      "The 10002 th iteration gives loss of 0.2905074936058949\n",
      "The 10003 th iteration gives loss of 0.29048938844727046\n",
      "The 10004 th iteration gives loss of 0.29047128579528136\n",
      "The 10005 th iteration gives loss of 0.29045318564959266\n",
      "The 10006 th iteration gives loss of 0.2904350880098542\n",
      "The 10007 th iteration gives loss of 0.2904169928757327\n",
      "The 10008 th iteration gives loss of 0.2903989002468629\n",
      "The 10009 th iteration gives loss of 0.29038081012291445\n",
      "The 10010 th iteration gives loss of 0.29036272250353673\n",
      "The 10011 th iteration gives loss of 0.2903446373883917\n",
      "The 10012 th iteration gives loss of 0.29032655477713076\n",
      "The 10013 th iteration gives loss of 0.2903084746694071\n",
      "The 10014 th iteration gives loss of 0.2902903970648885\n",
      "The 10015 th iteration gives loss of 0.2902723219632242\n",
      "The 10016 th iteration gives loss of 0.29025424936406186\n",
      "The 10017 th iteration gives loss of 0.29023617926707124\n",
      "The 10018 th iteration gives loss of 0.2902181116719008\n",
      "The 10019 th iteration gives loss of 0.2902000465782017\n",
      "The 10020 th iteration gives loss of 0.29018198398563444\n",
      "The 10021 th iteration gives loss of 0.29016392389386375\n",
      "The 10022 th iteration gives loss of 0.2901458663025341\n",
      "The 10023 th iteration gives loss of 0.29012781121130543\n",
      "The 10024 th iteration gives loss of 0.29010975861983673\n",
      "The 10025 th iteration gives loss of 0.290091708527788\n",
      "The 10026 th iteration gives loss of 0.29007366093480014\n",
      "The 10027 th iteration gives loss of 0.29005561584053713\n",
      "The 10028 th iteration gives loss of 0.2900375732446596\n",
      "The 10029 th iteration gives loss of 0.2900195331468276\n",
      "The 10030 th iteration gives loss of 0.29000149554669463\n",
      "The 10031 th iteration gives loss of 0.2899834604438978\n",
      "The 10032 th iteration gives loss of 0.28996542783812207\n",
      "The 10033 th iteration gives loss of 0.2899473977290088\n",
      "The 10034 th iteration gives loss of 0.2899293701162221\n",
      "The 10035 th iteration gives loss of 0.289911344999407\n",
      "The 10036 th iteration gives loss of 0.28989332237823295\n",
      "The 10037 th iteration gives loss of 0.2898753022523514\n",
      "The 10038 th iteration gives loss of 0.28985728462141946\n",
      "The 10039 th iteration gives loss of 0.28983926948508865\n",
      "The 10040 th iteration gives loss of 0.28982125684302157\n",
      "The 10041 th iteration gives loss of 0.28980324669487245\n",
      "The 10042 th iteration gives loss of 0.2897852390403064\n",
      "The 10043 th iteration gives loss of 0.28976723387896686\n",
      "The 10044 th iteration gives loss of 0.28974923121052615\n",
      "The 10045 th iteration gives loss of 0.28973123103463255\n",
      "The 10046 th iteration gives loss of 0.2897132333509372\n",
      "The 10047 th iteration gives loss of 0.2896952381591015\n",
      "The 10048 th iteration gives loss of 0.2896772454587953\n",
      "The 10049 th iteration gives loss of 0.2896592552496577\n",
      "The 10050 th iteration gives loss of 0.28964126753135583\n",
      "The 10051 th iteration gives loss of 0.28962328230355283\n",
      "The 10052 th iteration gives loss of 0.28960529956588466\n",
      "The 10053 th iteration gives loss of 0.28958731931802584\n",
      "The 10054 th iteration gives loss of 0.28956934155964115\n",
      "The 10055 th iteration gives loss of 0.2895513662903594\n",
      "The 10056 th iteration gives loss of 0.28953339350986107\n",
      "The 10057 th iteration gives loss of 0.28951542321780005\n",
      "The 10058 th iteration gives loss of 0.2894974554138304\n",
      "The 10059 th iteration gives loss of 0.28947949009760754\n",
      "The 10060 th iteration gives loss of 0.2894615272687992\n",
      "The 10061 th iteration gives loss of 0.28944356692705653\n",
      "The 10062 th iteration gives loss of 0.28942560907203746\n",
      "The 10063 th iteration gives loss of 0.2894076537033953\n",
      "The 10064 th iteration gives loss of 0.2893897008207983\n",
      "The 10065 th iteration gives loss of 0.28937175042389757\n",
      "The 10066 th iteration gives loss of 0.2893538025123455\n",
      "The 10067 th iteration gives loss of 0.2893358570858015\n",
      "The 10068 th iteration gives loss of 0.2893179141439387\n",
      "The 10069 th iteration gives loss of 0.2892999736864033\n",
      "The 10070 th iteration gives loss of 0.2892820357128539\n",
      "The 10071 th iteration gives loss of 0.2892641002229414\n",
      "The 10072 th iteration gives loss of 0.28924616721634383\n",
      "The 10073 th iteration gives loss of 0.28922823669269954\n",
      "The 10074 th iteration gives loss of 0.2892103086516719\n",
      "The 10075 th iteration gives loss of 0.28919238309292694\n",
      "The 10076 th iteration gives loss of 0.2891744600161114\n",
      "The 10077 th iteration gives loss of 0.2891565394208888\n",
      "The 10078 th iteration gives loss of 0.2891386213069264\n",
      "The 10079 th iteration gives loss of 0.2891207056738697\n",
      "The 10080 th iteration gives loss of 0.28910279252138416\n",
      "The 10081 th iteration gives loss of 0.28908488184912984\n",
      "The 10082 th iteration gives loss of 0.2890669736567566\n",
      "The 10083 th iteration gives loss of 0.28904906794392105\n",
      "The 10084 th iteration gives loss of 0.2890311647102939\n",
      "The 10085 th iteration gives loss of 0.28901326395553484\n",
      "The 10086 th iteration gives loss of 0.2889953656792898\n",
      "The 10087 th iteration gives loss of 0.2889774698812212\n",
      "The 10088 th iteration gives loss of 0.2889595765609884\n",
      "The 10089 th iteration gives loss of 0.2889416857182587\n",
      "The 10090 th iteration gives loss of 0.28892379735268287\n",
      "The 10091 th iteration gives loss of 0.2889059114639159\n",
      "The 10092 th iteration gives loss of 0.28888802805162234\n",
      "The 10093 th iteration gives loss of 0.2888701471154613\n",
      "The 10094 th iteration gives loss of 0.2888522686550895\n",
      "The 10095 th iteration gives loss of 0.28883439267017325\n",
      "The 10096 th iteration gives loss of 0.28881651916035667\n",
      "The 10097 th iteration gives loss of 0.28879864812531364\n",
      "The 10098 th iteration gives loss of 0.2887807795646896\n",
      "The 10099 th iteration gives loss of 0.2887629134781612\n",
      "The 10100 th iteration gives loss of 0.2887450498653705\n",
      "The 10101 th iteration gives loss of 0.28872718872598757\n",
      "The 10102 th iteration gives loss of 0.2887093300596673\n",
      "The 10103 th iteration gives loss of 0.2886914738660668\n",
      "The 10104 th iteration gives loss of 0.28867362014485615\n",
      "The 10105 th iteration gives loss of 0.288655768895687\n",
      "The 10106 th iteration gives loss of 0.28863792011820444\n",
      "The 10107 th iteration gives loss of 0.28862007381208743\n",
      "The 10108 th iteration gives loss of 0.2886022299769967\n",
      "The 10109 th iteration gives loss of 0.28858438861258856\n",
      "The 10110 th iteration gives loss of 0.2885665497185152\n",
      "The 10111 th iteration gives loss of 0.28854871329443166\n",
      "The 10112 th iteration gives loss of 0.2885308793400144\n",
      "The 10113 th iteration gives loss of 0.288513047854906\n",
      "The 10114 th iteration gives loss of 0.28849521883878404\n",
      "The 10115 th iteration gives loss of 0.2884773922912961\n",
      "The 10116 th iteration gives loss of 0.2884595682121016\n",
      "The 10117 th iteration gives loss of 0.2884417466008745\n",
      "The 10118 th iteration gives loss of 0.2884239274572518\n",
      "The 10119 th iteration gives loss of 0.2884061107809128\n",
      "The 10120 th iteration gives loss of 0.2883882965715045\n",
      "The 10121 th iteration gives loss of 0.28837048482870264\n",
      "The 10122 th iteration gives loss of 0.28835267555215216\n",
      "The 10123 th iteration gives loss of 0.28833486874151815\n",
      "The 10124 th iteration gives loss of 0.288317064396454\n",
      "The 10125 th iteration gives loss of 0.2882992625166301\n",
      "The 10126 th iteration gives loss of 0.288281463101706\n",
      "The 10127 th iteration gives loss of 0.2882636661513412\n",
      "The 10128 th iteration gives loss of 0.2882458716651879\n",
      "The 10129 th iteration gives loss of 0.2882280796429099\n",
      "The 10130 th iteration gives loss of 0.2882102900841743\n",
      "The 10131 th iteration gives loss of 0.2881925029886421\n",
      "The 10132 th iteration gives loss of 0.2881747183559715\n",
      "The 10133 th iteration gives loss of 0.28815693618581045\n",
      "The 10134 th iteration gives loss of 0.28813915647782745\n",
      "The 10135 th iteration gives loss of 0.2881213792316854\n",
      "The 10136 th iteration gives loss of 0.2881036044470423\n",
      "The 10137 th iteration gives loss of 0.28808583212357025\n",
      "The 10138 th iteration gives loss of 0.2880680622609131\n",
      "The 10139 th iteration gives loss of 0.28805029485873923\n",
      "The 10140 th iteration gives loss of 0.28803252991670486\n",
      "The 10141 th iteration gives loss of 0.2880147674344768\n",
      "The 10142 th iteration gives loss of 0.28799700741170964\n",
      "The 10143 th iteration gives loss of 0.2879792498480837\n",
      "The 10144 th iteration gives loss of 0.2879614947432339\n",
      "The 10145 th iteration gives loss of 0.2879437420968301\n",
      "The 10146 th iteration gives loss of 0.2879259919085368\n",
      "The 10147 th iteration gives loss of 0.28790824417800837\n",
      "The 10148 th iteration gives loss of 0.2878904989049146\n",
      "The 10149 th iteration gives loss of 0.2878727560889142\n",
      "The 10150 th iteration gives loss of 0.2878550157296617\n",
      "The 10151 th iteration gives loss of 0.2878372778268248\n",
      "The 10152 th iteration gives loss of 0.2878195423800564\n",
      "The 10153 th iteration gives loss of 0.28780180938903144\n",
      "The 10154 th iteration gives loss of 0.287784078853401\n",
      "The 10155 th iteration gives loss of 0.28776635077283247\n",
      "The 10156 th iteration gives loss of 0.2877486251469811\n",
      "The 10157 th iteration gives loss of 0.2877309019755205\n",
      "The 10158 th iteration gives loss of 0.28771318125808837\n",
      "The 10159 th iteration gives loss of 0.2876954629943571\n",
      "The 10160 th iteration gives loss of 0.2876777471840015\n",
      "The 10161 th iteration gives loss of 0.28766003382666583\n",
      "The 10162 th iteration gives loss of 0.28764232292202174\n",
      "The 10163 th iteration gives loss of 0.287624614469728\n",
      "The 10164 th iteration gives loss of 0.2876069084694413\n",
      "The 10165 th iteration gives loss of 0.2875892049208369\n",
      "The 10166 th iteration gives loss of 0.2875715038235633\n",
      "The 10167 th iteration gives loss of 0.28755380517728746\n",
      "The 10168 th iteration gives loss of 0.2875361089816624\n",
      "The 10169 th iteration gives loss of 0.2875184152363661\n",
      "The 10170 th iteration gives loss of 0.2875007239410503\n",
      "The 10171 th iteration gives loss of 0.28748303509537715\n",
      "The 10172 th iteration gives loss of 0.28746534869901386\n",
      "The 10173 th iteration gives loss of 0.2874476647516156\n",
      "The 10174 th iteration gives loss of 0.28742998325284813\n",
      "The 10175 th iteration gives loss of 0.2874123042023716\n",
      "The 10176 th iteration gives loss of 0.2873946275998494\n",
      "The 10177 th iteration gives loss of 0.28737695344494807\n",
      "The 10178 th iteration gives loss of 0.2873592817373146\n",
      "The 10179 th iteration gives loss of 0.28734161247662326\n",
      "The 10180 th iteration gives loss of 0.28732394566253816\n",
      "The 10181 th iteration gives loss of 0.28730628129472313\n",
      "The 10182 th iteration gives loss of 0.28728861937283445\n",
      "The 10183 th iteration gives loss of 0.2872709598965284\n",
      "The 10184 th iteration gives loss of 0.28725330286547096\n",
      "The 10185 th iteration gives loss of 0.2872356482793387\n",
      "The 10186 th iteration gives loss of 0.28721799613777765\n",
      "The 10187 th iteration gives loss of 0.2872003464404543\n",
      "The 10188 th iteration gives loss of 0.2871826991870407\n",
      "The 10189 th iteration gives loss of 0.28716505437718215\n",
      "The 10190 th iteration gives loss of 0.28714741201055805\n",
      "The 10191 th iteration gives loss of 0.28712977208682466\n",
      "The 10192 th iteration gives loss of 0.28711213460563295\n",
      "The 10193 th iteration gives loss of 0.2870944995666664\n",
      "The 10194 th iteration gives loss of 0.28707686696957396\n",
      "The 10195 th iteration gives loss of 0.2870592368140243\n",
      "The 10196 th iteration gives loss of 0.287041609099672\n",
      "The 10197 th iteration gives loss of 0.2870239838261967\n",
      "The 10198 th iteration gives loss of 0.28700636099324534\n",
      "The 10199 th iteration gives loss of 0.2869887406004856\n",
      "The 10200 th iteration gives loss of 0.2869711226475794\n",
      "The 10201 th iteration gives loss of 0.2869535071342034\n",
      "The 10202 th iteration gives loss of 0.286935894059996\n",
      "The 10203 th iteration gives loss of 0.28691828342463715\n",
      "The 10204 th iteration gives loss of 0.28690067522778256\n",
      "The 10205 th iteration gives loss of 0.2868830694691001\n",
      "The 10206 th iteration gives loss of 0.28686546614825564\n",
      "The 10207 th iteration gives loss of 0.2868478652649006\n",
      "The 10208 th iteration gives loss of 0.2868302668187074\n",
      "The 10209 th iteration gives loss of 0.28681267080934447\n",
      "The 10210 th iteration gives loss of 0.28679507723646624\n",
      "The 10211 th iteration gives loss of 0.28677748609973824\n",
      "The 10212 th iteration gives loss of 0.2867598973988288\n",
      "The 10213 th iteration gives loss of 0.2867423111333883\n",
      "The 10214 th iteration gives loss of 0.28672472730309834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10215 th iteration gives loss of 0.2867071459076113\n",
      "The 10216 th iteration gives loss of 0.2866895669465869\n",
      "The 10217 th iteration gives loss of 0.2866719904196957\n",
      "The 10218 th iteration gives loss of 0.28665441632660676\n",
      "The 10219 th iteration gives loss of 0.28663684466697226\n",
      "The 10220 th iteration gives loss of 0.2866192754404576\n",
      "The 10221 th iteration gives loss of 0.2866017086467311\n",
      "The 10222 th iteration gives loss of 0.28658414428545737\n",
      "The 10223 th iteration gives loss of 0.2865665823562926\n",
      "The 10224 th iteration gives loss of 0.286549022858917\n",
      "The 10225 th iteration gives loss of 0.28653146579297883\n",
      "The 10226 th iteration gives loss of 0.28651391115814184\n",
      "The 10227 th iteration gives loss of 0.286496358954083\n",
      "The 10228 th iteration gives loss of 0.28647880918045504\n",
      "The 10229 th iteration gives loss of 0.2864612618369258\n",
      "The 10230 th iteration gives loss of 0.286443716923157\n",
      "The 10231 th iteration gives loss of 0.28642617443881874\n",
      "The 10232 th iteration gives loss of 0.286408634383569\n",
      "The 10233 th iteration gives loss of 0.2863910967570809\n",
      "The 10234 th iteration gives loss of 0.28637356155900323\n",
      "The 10235 th iteration gives loss of 0.28635602878900746\n",
      "The 10236 th iteration gives loss of 0.28633849844676634\n",
      "The 10237 th iteration gives loss of 0.2863209705319374\n",
      "The 10238 th iteration gives loss of 0.28630344504418404\n",
      "The 10239 th iteration gives loss of 0.2862859219831719\n",
      "The 10240 th iteration gives loss of 0.2862684013485667\n",
      "The 10241 th iteration gives loss of 0.2862508831400323\n",
      "The 10242 th iteration gives loss of 0.28623336735723054\n",
      "The 10243 th iteration gives loss of 0.2862158539998264\n",
      "The 10244 th iteration gives loss of 0.28619834306749187\n",
      "The 10245 th iteration gives loss of 0.28618083455988436\n",
      "The 10246 th iteration gives loss of 0.2861633284766703\n",
      "The 10247 th iteration gives loss of 0.28614582481751555\n",
      "The 10248 th iteration gives loss of 0.2861283235820838\n",
      "The 10249 th iteration gives loss of 0.28611082477004746\n",
      "The 10250 th iteration gives loss of 0.2860933283810488\n",
      "The 10251 th iteration gives loss of 0.2860758344147833\n",
      "The 10252 th iteration gives loss of 0.28605834287088894\n",
      "The 10253 th iteration gives loss of 0.2860408537490459\n",
      "The 10254 th iteration gives loss of 0.2860233670489177\n",
      "The 10255 th iteration gives loss of 0.28600588277017036\n",
      "The 10256 th iteration gives loss of 0.28598840091245814\n",
      "The 10257 th iteration gives loss of 0.2859709214754602\n",
      "The 10258 th iteration gives loss of 0.28595344445883364\n",
      "The 10259 th iteration gives loss of 0.2859359698622462\n",
      "The 10260 th iteration gives loss of 0.28591849768536176\n",
      "The 10261 th iteration gives loss of 0.285901027927842\n",
      "The 10262 th iteration gives loss of 0.2858835605893681\n",
      "The 10263 th iteration gives loss of 0.2858660956695917\n",
      "The 10264 th iteration gives loss of 0.28584863316817577\n",
      "The 10265 th iteration gives loss of 0.285831173084789\n",
      "The 10266 th iteration gives loss of 0.2858137154191023\n",
      "The 10267 th iteration gives loss of 0.28579626017077936\n",
      "The 10268 th iteration gives loss of 0.28577880733947364\n",
      "The 10269 th iteration gives loss of 0.2857613569248687\n",
      "The 10270 th iteration gives loss of 0.2857439089266202\n",
      "The 10271 th iteration gives loss of 0.2857264633443959\n",
      "The 10272 th iteration gives loss of 0.2857090201778591\n",
      "The 10273 th iteration gives loss of 0.2856915794266866\n",
      "The 10274 th iteration gives loss of 0.2856741410905306\n",
      "The 10275 th iteration gives loss of 0.2856567051690615\n",
      "The 10276 th iteration gives loss of 0.28563927166194536\n",
      "The 10277 th iteration gives loss of 0.2856218405688498\n",
      "The 10278 th iteration gives loss of 0.28560441188944324\n",
      "The 10279 th iteration gives loss of 0.2855869856233775\n",
      "The 10280 th iteration gives loss of 0.2855695617703323\n",
      "The 10281 th iteration gives loss of 0.2855521403299728\n",
      "The 10282 th iteration gives loss of 0.2855347213019611\n",
      "The 10283 th iteration gives loss of 0.285517304685961\n",
      "The 10284 th iteration gives loss of 0.28549989048165036\n",
      "The 10285 th iteration gives loss of 0.28548247868868176\n",
      "The 10286 th iteration gives loss of 0.2854650693067273\n",
      "The 10287 th iteration gives loss of 0.2854476623354488\n",
      "The 10288 th iteration gives loss of 0.28543025777452014\n",
      "The 10289 th iteration gives loss of 0.28541285562361074\n",
      "The 10290 th iteration gives loss of 0.2853954558823719\n",
      "The 10291 th iteration gives loss of 0.2853780585504828\n",
      "The 10292 th iteration gives loss of 0.28536066362760204\n",
      "The 10293 th iteration gives loss of 0.2853432711134051\n",
      "The 10294 th iteration gives loss of 0.2853258810075478\n",
      "The 10295 th iteration gives loss of 0.285308493309701\n",
      "The 10296 th iteration gives loss of 0.2852911080195424\n",
      "The 10297 th iteration gives loss of 0.28527372513671734\n",
      "The 10298 th iteration gives loss of 0.28525634466091515\n",
      "The 10299 th iteration gives loss of 0.2852389665917807\n",
      "The 10300 th iteration gives loss of 0.28522159092899996\n",
      "The 10301 th iteration gives loss of 0.28520421767222853\n",
      "The 10302 th iteration gives loss of 0.285186846821132\n",
      "The 10303 th iteration gives loss of 0.28516947837536943\n",
      "The 10304 th iteration gives loss of 0.28515211233463494\n",
      "The 10305 th iteration gives loss of 0.28513474869858035\n",
      "The 10306 th iteration gives loss of 0.28511738746686777\n",
      "The 10307 th iteration gives loss of 0.2851000286391694\n",
      "The 10308 th iteration gives loss of 0.285082672215154\n",
      "The 10309 th iteration gives loss of 0.28506531819447734\n",
      "The 10310 th iteration gives loss of 0.2850479665768256\n",
      "The 10311 th iteration gives loss of 0.2850306173618552\n",
      "The 10312 th iteration gives loss of 0.2850132705492248\n",
      "The 10313 th iteration gives loss of 0.28499592613861674\n",
      "The 10314 th iteration gives loss of 0.28497858412969396\n",
      "The 10315 th iteration gives loss of 0.28496124452211585\n",
      "The 10316 th iteration gives loss of 0.2849439073155608\n",
      "The 10317 th iteration gives loss of 0.28492657250969355\n",
      "The 10318 th iteration gives loss of 0.28490924010417135\n",
      "The 10319 th iteration gives loss of 0.28489191009867093\n",
      "The 10320 th iteration gives loss of 0.2848745824928677\n",
      "The 10321 th iteration gives loss of 0.2848572572864169\n",
      "The 10322 th iteration gives loss of 0.28483993447898237\n",
      "The 10323 th iteration gives loss of 0.2848226140702476\n",
      "The 10324 th iteration gives loss of 0.2848052960598656\n",
      "The 10325 th iteration gives loss of 0.28478798044751175\n",
      "The 10326 th iteration gives loss of 0.2847706672328497\n",
      "The 10327 th iteration gives loss of 0.2847533564155527\n",
      "The 10328 th iteration gives loss of 0.2847360479952785\n",
      "The 10329 th iteration gives loss of 0.28471874197170655\n",
      "The 10330 th iteration gives loss of 0.28470143834449785\n",
      "The 10331 th iteration gives loss of 0.28468413711332846\n",
      "The 10332 th iteration gives loss of 0.2846668382778549\n",
      "The 10333 th iteration gives loss of 0.2846495418377509\n",
      "The 10334 th iteration gives loss of 0.2846322477926849\n",
      "The 10335 th iteration gives loss of 0.28461495614232024\n",
      "The 10336 th iteration gives loss of 0.2845976668863334\n",
      "The 10337 th iteration gives loss of 0.28458038002438696\n",
      "The 10338 th iteration gives loss of 0.2845630955561446\n",
      "The 10339 th iteration gives loss of 0.2845458134812845\n",
      "The 10340 th iteration gives loss of 0.2845285337994702\n",
      "The 10341 th iteration gives loss of 0.2845112565103739\n",
      "The 10342 th iteration gives loss of 0.2844939816136562\n",
      "The 10343 th iteration gives loss of 0.2844767091089954\n",
      "The 10344 th iteration gives loss of 0.28445943899604365\n",
      "The 10345 th iteration gives loss of 0.2844421712744874\n",
      "The 10346 th iteration gives loss of 0.2844249059439849\n",
      "The 10347 th iteration gives loss of 0.28440764300420923\n",
      "The 10348 th iteration gives loss of 0.2843903824548235\n",
      "The 10349 th iteration gives loss of 0.28437312429549333\n",
      "The 10350 th iteration gives loss of 0.28435586852590616\n",
      "The 10351 th iteration gives loss of 0.2843386151457125\n",
      "The 10352 th iteration gives loss of 0.2843213641545909\n",
      "The 10353 th iteration gives loss of 0.2843041155522028\n",
      "The 10354 th iteration gives loss of 0.2842868693382172\n",
      "The 10355 th iteration gives loss of 0.28426962551231\n",
      "The 10356 th iteration gives loss of 0.28425238407414966\n",
      "The 10357 th iteration gives loss of 0.2842351450234022\n",
      "The 10358 th iteration gives loss of 0.28421790835972893\n",
      "The 10359 th iteration gives loss of 0.28420067408280725\n",
      "The 10360 th iteration gives loss of 0.28418344219230346\n",
      "The 10361 th iteration gives loss of 0.2841662126878842\n",
      "The 10362 th iteration gives loss of 0.2841489855692294\n",
      "The 10363 th iteration gives loss of 0.2841317608359984\n",
      "The 10364 th iteration gives loss of 0.2841145384878691\n",
      "The 10365 th iteration gives loss of 0.28409731852449055\n",
      "The 10366 th iteration gives loss of 0.2840801009455488\n",
      "The 10367 th iteration gives loss of 0.28406288575072164\n",
      "The 10368 th iteration gives loss of 0.2840456729396574\n",
      "The 10369 th iteration gives loss of 0.28402846251203456\n",
      "The 10370 th iteration gives loss of 0.28401125446752173\n",
      "The 10371 th iteration gives loss of 0.2839940488057964\n",
      "The 10372 th iteration gives loss of 0.2839768455265222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10373 th iteration gives loss of 0.283959644629365\n",
      "The 10374 th iteration gives loss of 0.2839424461139943\n",
      "The 10375 th iteration gives loss of 0.2839252499800808\n",
      "The 10376 th iteration gives loss of 0.28390805622729887\n",
      "The 10377 th iteration gives loss of 0.2838908648553097\n",
      "The 10378 th iteration gives loss of 0.28387367586379136\n",
      "The 10379 th iteration gives loss of 0.283856489252406\n",
      "The 10380 th iteration gives loss of 0.2838393050208302\n",
      "The 10381 th iteration gives loss of 0.2838221231687274\n",
      "The 10382 th iteration gives loss of 0.2838049436957744\n",
      "The 10383 th iteration gives loss of 0.28378776660164906\n",
      "The 10384 th iteration gives loss of 0.28377059188599635\n",
      "The 10385 th iteration gives loss of 0.2837534195485031\n",
      "The 10386 th iteration gives loss of 0.28373624958882854\n",
      "The 10387 th iteration gives loss of 0.2837190820066614\n",
      "The 10388 th iteration gives loss of 0.2837019168016582\n",
      "The 10389 th iteration gives loss of 0.28368475397348714\n",
      "The 10390 th iteration gives loss of 0.28366759352182763\n",
      "The 10391 th iteration gives loss of 0.2836504354463377\n",
      "The 10392 th iteration gives loss of 0.2836332797466941\n",
      "The 10393 th iteration gives loss of 0.2836161264225741\n",
      "The 10394 th iteration gives loss of 0.28359897547363117\n",
      "The 10395 th iteration gives loss of 0.2835818268995536\n",
      "The 10396 th iteration gives loss of 0.28356468070000057\n",
      "The 10397 th iteration gives loss of 0.2835475368746514\n",
      "The 10398 th iteration gives loss of 0.2835303954231683\n",
      "The 10399 th iteration gives loss of 0.2835132563452256\n",
      "The 10400 th iteration gives loss of 0.2834961196404907\n",
      "The 10401 th iteration gives loss of 0.2834789853086394\n",
      "The 10402 th iteration gives loss of 0.28346185334933033\n",
      "The 10403 th iteration gives loss of 0.2834447237622465\n",
      "The 10404 th iteration gives loss of 0.2834275965470541\n",
      "The 10405 th iteration gives loss of 0.2834104717034153\n",
      "The 10406 th iteration gives loss of 0.283393349231022\n",
      "The 10407 th iteration gives loss of 0.2833762291295313\n",
      "The 10408 th iteration gives loss of 0.2833591113986172\n",
      "The 10409 th iteration gives loss of 0.2833419960379407\n",
      "The 10410 th iteration gives loss of 0.28332488304718334\n",
      "The 10411 th iteration gives loss of 0.2833077724260167\n",
      "The 10412 th iteration gives loss of 0.28329066417409876\n",
      "The 10413 th iteration gives loss of 0.28327355829112466\n",
      "The 10414 th iteration gives loss of 0.2832564547767495\n",
      "The 10415 th iteration gives loss of 0.2832393536306348\n",
      "The 10416 th iteration gives loss of 0.28322225485247027\n",
      "The 10417 th iteration gives loss of 0.28320515844191385\n",
      "The 10418 th iteration gives loss of 0.28318806439864697\n",
      "The 10419 th iteration gives loss of 0.28317097272233455\n",
      "The 10420 th iteration gives loss of 0.2831538834126435\n",
      "The 10421 th iteration gives loss of 0.28313679646925277\n",
      "The 10422 th iteration gives loss of 0.2831197118918336\n",
      "The 10423 th iteration gives loss of 0.28310262968005556\n",
      "The 10424 th iteration gives loss of 0.2830855498335906\n",
      "The 10425 th iteration gives loss of 0.2830684723521128\n",
      "The 10426 th iteration gives loss of 0.283051397235281\n",
      "The 10427 th iteration gives loss of 0.2830343244827773\n",
      "The 10428 th iteration gives loss of 0.2830172540942804\n",
      "The 10429 th iteration gives loss of 0.28300018606944854\n",
      "The 10430 th iteration gives loss of 0.2829831204079554\n",
      "The 10431 th iteration gives loss of 0.2829660571094811\n",
      "The 10432 th iteration gives loss of 0.2829489961736907\n",
      "The 10433 th iteration gives loss of 0.2829319376002519\n",
      "The 10434 th iteration gives loss of 0.2829148813888433\n",
      "The 10435 th iteration gives loss of 0.2828978275391335\n",
      "The 10436 th iteration gives loss of 0.2828807760507947\n",
      "The 10437 th iteration gives loss of 0.2828637269234983\n",
      "The 10438 th iteration gives loss of 0.2828466801569271\n",
      "The 10439 th iteration gives loss of 0.2828296357507329\n",
      "The 10440 th iteration gives loss of 0.2828125937046016\n",
      "The 10441 th iteration gives loss of 0.28279555401819995\n",
      "The 10442 th iteration gives loss of 0.28277851669119386\n",
      "The 10443 th iteration gives loss of 0.2827614817232747\n",
      "The 10444 th iteration gives loss of 0.2827444491141006\n",
      "The 10445 th iteration gives loss of 0.28272741886335284\n",
      "The 10446 th iteration gives loss of 0.282710390970694\n",
      "The 10447 th iteration gives loss of 0.28269336543578716\n",
      "The 10448 th iteration gives loss of 0.282676342258333\n",
      "The 10449 th iteration gives loss of 0.2826593214379809\n",
      "The 10450 th iteration gives loss of 0.28264230297440895\n",
      "The 10451 th iteration gives loss of 0.2826252868672971\n",
      "The 10452 th iteration gives loss of 0.28260827311629666\n",
      "The 10453 th iteration gives loss of 0.2825912617210962\n",
      "The 10454 th iteration gives loss of 0.28257425268137043\n",
      "The 10455 th iteration gives loss of 0.2825572459967939\n",
      "The 10456 th iteration gives loss of 0.28254024166703384\n",
      "The 10457 th iteration gives loss of 0.28252323969174925\n",
      "The 10458 th iteration gives loss of 0.2825062400706389\n",
      "The 10459 th iteration gives loss of 0.2824892428033547\n",
      "The 10460 th iteration gives loss of 0.2824722478895824\n",
      "The 10461 th iteration gives loss of 0.2824552553289866\n",
      "The 10462 th iteration gives loss of 0.2824382651212419\n",
      "The 10463 th iteration gives loss of 0.2824212772660208\n",
      "The 10464 th iteration gives loss of 0.2824042917630023\n",
      "The 10465 th iteration gives loss of 0.2823873086118483\n",
      "The 10466 th iteration gives loss of 0.28237032781224325\n",
      "The 10467 th iteration gives loss of 0.2823533493638533\n",
      "The 10468 th iteration gives loss of 0.2823363732663486\n",
      "The 10469 th iteration gives loss of 0.28231939951941065\n",
      "The 10470 th iteration gives loss of 0.2823024281227108\n",
      "The 10471 th iteration gives loss of 0.2822854590759092\n",
      "The 10472 th iteration gives loss of 0.2822684923786916\n",
      "The 10473 th iteration gives loss of 0.2822515280307391\n",
      "The 10474 th iteration gives loss of 0.2822345660317134\n",
      "The 10475 th iteration gives loss of 0.28221760638128407\n",
      "The 10476 th iteration gives loss of 0.2822006490791302\n",
      "The 10477 th iteration gives loss of 0.2821836941249159\n",
      "The 10478 th iteration gives loss of 0.28216674151833676\n",
      "The 10479 th iteration gives loss of 0.28214979125904166\n",
      "The 10480 th iteration gives loss of 0.28213284334671856\n",
      "The 10481 th iteration gives loss of 0.28211589778103924\n",
      "The 10482 th iteration gives loss of 0.2820989545616674\n",
      "The 10483 th iteration gives loss of 0.28208201368829017\n",
      "The 10484 th iteration gives loss of 0.2820650751605788\n",
      "The 10485 th iteration gives loss of 0.28204813897820524\n",
      "The 10486 th iteration gives loss of 0.282031205140836\n",
      "The 10487 th iteration gives loss of 0.28201427364815557\n",
      "The 10488 th iteration gives loss of 0.2819973444998271\n",
      "The 10489 th iteration gives loss of 0.2819804176955373\n",
      "The 10490 th iteration gives loss of 0.2819634932349401\n",
      "The 10491 th iteration gives loss of 0.28194657111772436\n",
      "The 10492 th iteration gives loss of 0.28192965134357084\n",
      "The 10493 th iteration gives loss of 0.2819127339121351\n",
      "The 10494 th iteration gives loss of 0.281895818823101\n",
      "The 10495 th iteration gives loss of 0.28187890607614463\n",
      "The 10496 th iteration gives loss of 0.2818619956709267\n",
      "The 10497 th iteration gives loss of 0.28184508760713817\n",
      "The 10498 th iteration gives loss of 0.28182818188444597\n",
      "The 10499 th iteration gives loss of 0.28181127850252946\n",
      "The 10500 th iteration gives loss of 0.28179437746105146\n",
      "The 10501 th iteration gives loss of 0.2817774787596905\n",
      "The 10502 th iteration gives loss of 0.2817605823981255\n",
      "The 10503 th iteration gives loss of 0.2817436883760271\n",
      "The 10504 th iteration gives loss of 0.2817267966930771\n",
      "The 10505 th iteration gives loss of 0.28170990734893425\n",
      "The 10506 th iteration gives loss of 0.2816930203432827\n",
      "The 10507 th iteration gives loss of 0.2816761356758014\n",
      "The 10508 th iteration gives loss of 0.2816592533461605\n",
      "The 10509 th iteration gives loss of 0.2816423733540339\n",
      "The 10510 th iteration gives loss of 0.2816254956990904\n",
      "The 10511 th iteration gives loss of 0.28160862038101303\n",
      "The 10512 th iteration gives loss of 0.28159174739947396\n",
      "The 10513 th iteration gives loss of 0.28157487675414844\n",
      "The 10514 th iteration gives loss of 0.2815580084447096\n",
      "The 10515 th iteration gives loss of 0.2815411424708253\n",
      "The 10516 th iteration gives loss of 0.28152427883218933\n",
      "The 10517 th iteration gives loss of 0.2815074175284575\n",
      "The 10518 th iteration gives loss of 0.2814905585593146\n",
      "The 10519 th iteration gives loss of 0.2814737019244301\n",
      "The 10520 th iteration gives loss of 0.28145684762348494\n",
      "The 10521 th iteration gives loss of 0.28143999565614636\n",
      "The 10522 th iteration gives loss of 0.28142314602209445\n",
      "The 10523 th iteration gives loss of 0.2814062987210127\n",
      "The 10524 th iteration gives loss of 0.28138945375255875\n",
      "The 10525 th iteration gives loss of 0.28137261111642176\n",
      "The 10526 th iteration gives loss of 0.28135577081226393\n",
      "The 10527 th iteration gives loss of 0.28133893283976974\n",
      "The 10528 th iteration gives loss of 0.2813220971986145\n",
      "The 10529 th iteration gives loss of 0.28130526388847077\n",
      "The 10530 th iteration gives loss of 0.2812884329090147\n",
      "The 10531 th iteration gives loss of 0.28127160425991676\n",
      "The 10532 th iteration gives loss of 0.28125477794085607\n",
      "The 10533 th iteration gives loss of 0.2812379539515117\n",
      "The 10534 th iteration gives loss of 0.2812211322915621\n",
      "The 10535 th iteration gives loss of 0.2812043129606718\n",
      "The 10536 th iteration gives loss of 0.281187495958517\n",
      "The 10537 th iteration gives loss of 0.281170681284781\n",
      "The 10538 th iteration gives loss of 0.28115386893913663\n",
      "The 10539 th iteration gives loss of 0.2811370589212597\n",
      "The 10540 th iteration gives loss of 0.2811202512308243\n",
      "The 10541 th iteration gives loss of 0.28110344586750563\n",
      "The 10542 th iteration gives loss of 0.2810866428309806\n",
      "The 10543 th iteration gives loss of 0.28106984212092007\n",
      "The 10544 th iteration gives loss of 0.2810530437370093\n",
      "The 10545 th iteration gives loss of 0.28103624767892016\n",
      "The 10546 th iteration gives loss of 0.28101945394632255\n",
      "The 10547 th iteration gives loss of 0.2810026625388989\n",
      "The 10548 th iteration gives loss of 0.28098587345632925\n",
      "The 10549 th iteration gives loss of 0.2809690866982825\n",
      "The 10550 th iteration gives loss of 0.2809523022644366\n",
      "The 10551 th iteration gives loss of 0.2809355201544687\n",
      "The 10552 th iteration gives loss of 0.2809187403680456\n",
      "The 10553 th iteration gives loss of 0.2809019629048574\n",
      "The 10554 th iteration gives loss of 0.28088518776457233\n",
      "The 10555 th iteration gives loss of 0.28086841494686404\n",
      "The 10556 th iteration gives loss of 0.2808516444514165\n",
      "The 10557 th iteration gives loss of 0.280834876277905\n",
      "The 10558 th iteration gives loss of 0.280818110425999\n",
      "The 10559 th iteration gives loss of 0.2808013468953841\n",
      "The 10560 th iteration gives loss of 0.2807845856857252\n",
      "The 10561 th iteration gives loss of 0.280767826796713\n",
      "The 10562 th iteration gives loss of 0.28075107022801904\n",
      "The 10563 th iteration gives loss of 0.2807343159793111\n",
      "The 10564 th iteration gives loss of 0.2807175640502612\n",
      "The 10565 th iteration gives loss of 0.2807008144405713\n",
      "The 10566 th iteration gives loss of 0.28068406714989425\n",
      "The 10567 th iteration gives loss of 0.2806673221779218\n",
      "The 10568 th iteration gives loss of 0.2806505795243205\n",
      "The 10569 th iteration gives loss of 0.28063383918876933\n",
      "The 10570 th iteration gives loss of 0.2806171011709498\n",
      "The 10571 th iteration gives loss of 0.2806003654705372\n",
      "The 10572 th iteration gives loss of 0.28058363208720283\n",
      "The 10573 th iteration gives loss of 0.2805669010206293\n",
      "The 10574 th iteration gives loss of 0.28055017227048407\n",
      "The 10575 th iteration gives loss of 0.2805334458364586\n",
      "The 10576 th iteration gives loss of 0.28051672171822223\n",
      "The 10577 th iteration gives loss of 0.2804999999154473\n",
      "The 10578 th iteration gives loss of 0.2804832804278241\n",
      "The 10579 th iteration gives loss of 0.2804665632550232\n",
      "The 10580 th iteration gives loss of 0.2804498483967122\n",
      "The 10581 th iteration gives loss of 0.2804331358525824\n",
      "The 10582 th iteration gives loss of 0.28041642562230384\n",
      "The 10583 th iteration gives loss of 0.2803997177055501\n",
      "The 10584 th iteration gives loss of 0.2803830121020031\n",
      "The 10585 th iteration gives loss of 0.2803663088113388\n",
      "The 10586 th iteration gives loss of 0.2803496078332407\n",
      "The 10587 th iteration gives loss of 0.2803329091673753\n",
      "The 10588 th iteration gives loss of 0.2803162128134331\n",
      "The 10589 th iteration gives loss of 0.28029951877108383\n",
      "The 10590 th iteration gives loss of 0.28028282704000246\n",
      "The 10591 th iteration gives loss of 0.2802661376198668\n",
      "The 10592 th iteration gives loss of 0.2802494505103601\n",
      "The 10593 th iteration gives loss of 0.2802327657111541\n",
      "The 10594 th iteration gives loss of 0.280216083221924\n",
      "The 10595 th iteration gives loss of 0.28019940304236807\n",
      "The 10596 th iteration gives loss of 0.2801827251721395\n",
      "The 10597 th iteration gives loss of 0.2801660496109185\n",
      "The 10598 th iteration gives loss of 0.2801493763584025\n",
      "The 10599 th iteration gives loss of 0.2801327054142474\n",
      "The 10600 th iteration gives loss of 0.28011603677813673\n",
      "The 10601 th iteration gives loss of 0.2800993704497609\n",
      "The 10602 th iteration gives loss of 0.2800827064287792\n",
      "The 10603 th iteration gives loss of 0.280066044714891\n",
      "The 10604 th iteration gives loss of 0.2800493853077594\n",
      "The 10605 th iteration gives loss of 0.28003272820705644\n",
      "The 10606 th iteration gives loss of 0.28001607341246576\n",
      "The 10607 th iteration gives loss of 0.27999942092368485\n",
      "The 10608 th iteration gives loss of 0.279982770740366\n",
      "The 10609 th iteration gives loss of 0.2799661228622028\n",
      "The 10610 th iteration gives loss of 0.2799494772888606\n",
      "The 10611 th iteration gives loss of 0.27993283402002817\n",
      "The 10612 th iteration gives loss of 0.27991619305537957\n",
      "The 10613 th iteration gives loss of 0.27989955439458974\n",
      "The 10614 th iteration gives loss of 0.2798829180373431\n",
      "The 10615 th iteration gives loss of 0.27986628398330593\n",
      "The 10616 th iteration gives loss of 0.27984965223218644\n",
      "The 10617 th iteration gives loss of 0.2798330227836256\n",
      "The 10618 th iteration gives loss of 0.27981639563732674\n",
      "The 10619 th iteration gives loss of 0.27979977079296636\n",
      "The 10620 th iteration gives loss of 0.2797831482502137\n",
      "The 10621 th iteration gives loss of 0.2797665280087473\n",
      "The 10622 th iteration gives loss of 0.2797499100682544\n",
      "The 10623 th iteration gives loss of 0.27973329442841016\n",
      "The 10624 th iteration gives loss of 0.27971668108889375\n",
      "The 10625 th iteration gives loss of 0.2797000700493861\n",
      "The 10626 th iteration gives loss of 0.27968346130954974\n",
      "The 10627 th iteration gives loss of 0.27966685486908477\n",
      "The 10628 th iteration gives loss of 0.2796502507276568\n",
      "The 10629 th iteration gives loss of 0.27963364888495124\n",
      "The 10630 th iteration gives loss of 0.27961704934064435\n",
      "The 10631 th iteration gives loss of 0.2796004520944202\n",
      "The 10632 th iteration gives loss of 0.27958385714594663\n",
      "The 10633 th iteration gives loss of 0.2795672644949195\n",
      "The 10634 th iteration gives loss of 0.2795506741410008\n",
      "The 10635 th iteration gives loss of 0.27953408608387564\n",
      "The 10636 th iteration gives loss of 0.27951750032322903\n",
      "The 10637 th iteration gives loss of 0.27950091685872663\n",
      "The 10638 th iteration gives loss of 0.27948433569006115\n",
      "The 10639 th iteration gives loss of 0.27946775681691005\n",
      "The 10640 th iteration gives loss of 0.2794511802389483\n",
      "The 10641 th iteration gives loss of 0.2794346059558583\n",
      "The 10642 th iteration gives loss of 0.27941803396731424\n",
      "The 10643 th iteration gives loss of 0.27940146427299656\n",
      "The 10644 th iteration gives loss of 0.27938489687259555\n",
      "The 10645 th iteration gives loss of 0.2793683317657866\n",
      "The 10646 th iteration gives loss of 0.2793517689522368\n",
      "The 10647 th iteration gives loss of 0.2793352084316291\n",
      "The 10648 th iteration gives loss of 0.2793186502036559\n",
      "The 10649 th iteration gives loss of 0.2793020942679827\n",
      "The 10650 th iteration gives loss of 0.2792855406242937\n",
      "The 10651 th iteration gives loss of 0.27926898927227406\n",
      "The 10652 th iteration gives loss of 0.27925244021159706\n",
      "The 10653 th iteration gives loss of 0.2792358934419525\n",
      "The 10654 th iteration gives loss of 0.27921934896300227\n",
      "The 10655 th iteration gives loss of 0.27920280677444437\n",
      "The 10656 th iteration gives loss of 0.27918626687595083\n",
      "The 10657 th iteration gives loss of 0.2791697292671926\n",
      "The 10658 th iteration gives loss of 0.2791531939478647\n",
      "The 10659 th iteration gives loss of 0.2791366609176414\n",
      "The 10660 th iteration gives loss of 0.27912013017620224\n",
      "The 10661 th iteration gives loss of 0.2791036017232272\n",
      "The 10662 th iteration gives loss of 0.27908707555839135\n",
      "The 10663 th iteration gives loss of 0.2790705516813893\n",
      "The 10664 th iteration gives loss of 0.27905403009187846\n",
      "The 10665 th iteration gives loss of 0.27903751078957056\n",
      "The 10666 th iteration gives loss of 0.2790209937741096\n",
      "The 10667 th iteration gives loss of 0.279004479045201\n",
      "The 10668 th iteration gives loss of 0.278987966602521\n",
      "The 10669 th iteration gives loss of 0.27897145644574434\n",
      "The 10670 th iteration gives loss of 0.2789549485745502\n",
      "The 10671 th iteration gives loss of 0.2789384429886256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10672 th iteration gives loss of 0.2789219396876567\n",
      "The 10673 th iteration gives loss of 0.27890543867130313\n",
      "The 10674 th iteration gives loss of 0.278888939939272\n",
      "The 10675 th iteration gives loss of 0.27887244349122114\n",
      "The 10676 th iteration gives loss of 0.2788559493268326\n",
      "The 10677 th iteration gives loss of 0.2788394574458034\n",
      "The 10678 th iteration gives loss of 0.2788229678477991\n",
      "The 10679 th iteration gives loss of 0.2788064805325099\n",
      "The 10680 th iteration gives loss of 0.2787899954996104\n",
      "The 10681 th iteration gives loss of 0.2787735127487855\n",
      "The 10682 th iteration gives loss of 0.2787570322797059\n",
      "The 10683 th iteration gives loss of 0.2787405540920699\n",
      "The 10684 th iteration gives loss of 0.2787240781855543\n",
      "The 10685 th iteration gives loss of 0.2787076045598325\n",
      "The 10686 th iteration gives loss of 0.27869113321457456\n",
      "The 10687 th iteration gives loss of 0.278674664149478\n",
      "The 10688 th iteration gives loss of 0.2786581973642358\n",
      "The 10689 th iteration gives loss of 0.27864173285850313\n",
      "The 10690 th iteration gives loss of 0.27862527063197595\n",
      "The 10691 th iteration gives loss of 0.2786088106843288\n",
      "The 10692 th iteration gives loss of 0.2785923530152464\n",
      "The 10693 th iteration gives loss of 0.27857589762440776\n",
      "The 10694 th iteration gives loss of 0.2785594445114935\n",
      "The 10695 th iteration gives loss of 0.2785429936761924\n",
      "The 10696 th iteration gives loss of 0.27852654511817754\n",
      "The 10697 th iteration gives loss of 0.27851009883712546\n",
      "The 10698 th iteration gives loss of 0.27849365483273164\n",
      "The 10699 th iteration gives loss of 0.27847721310467427\n",
      "The 10700 th iteration gives loss of 0.27846077365262023\n",
      "The 10701 th iteration gives loss of 0.2784443364762758\n",
      "The 10702 th iteration gives loss of 0.27842790157529446\n",
      "The 10703 th iteration gives loss of 0.2784114689493907\n",
      "The 10704 th iteration gives loss of 0.27839503859822073\n",
      "The 10705 th iteration gives loss of 0.278378610521467\n",
      "The 10706 th iteration gives loss of 0.2783621847188156\n",
      "The 10707 th iteration gives loss of 0.2783457611899501\n",
      "The 10708 th iteration gives loss of 0.2783293399345572\n",
      "The 10709 th iteration gives loss of 0.27831292095231064\n",
      "The 10710 th iteration gives loss of 0.27829650424289704\n",
      "The 10711 th iteration gives loss of 0.27828008980599356\n",
      "The 10712 th iteration gives loss of 0.2782636776412878\n",
      "The 10713 th iteration gives loss of 0.278247267748457\n",
      "The 10714 th iteration gives loss of 0.2782308601271873\n",
      "The 10715 th iteration gives loss of 0.2782144547771586\n",
      "The 10716 th iteration gives loss of 0.27819805169805095\n",
      "The 10717 th iteration gives loss of 0.2781816508895453\n",
      "The 10718 th iteration gives loss of 0.2781652523513238\n",
      "The 10719 th iteration gives loss of 0.27814885608307804\n",
      "The 10720 th iteration gives loss of 0.2781324620844869\n",
      "The 10721 th iteration gives loss of 0.27811607035522107\n",
      "The 10722 th iteration gives loss of 0.2780996808949788\n",
      "The 10723 th iteration gives loss of 0.27808329370343027\n",
      "The 10724 th iteration gives loss of 0.27806690878026075\n",
      "The 10725 th iteration gives loss of 0.27805052612516057\n",
      "The 10726 th iteration gives loss of 0.27803414573779417\n",
      "The 10727 th iteration gives loss of 0.27801776761785635\n",
      "The 10728 th iteration gives loss of 0.2780013917650392\n",
      "The 10729 th iteration gives loss of 0.2779850181790025\n",
      "The 10730 th iteration gives loss of 0.27796864685944606\n",
      "The 10731 th iteration gives loss of 0.2779522778060529\n",
      "The 10732 th iteration gives loss of 0.2779359110184948\n",
      "The 10733 th iteration gives loss of 0.27791954649646333\n",
      "The 10734 th iteration gives loss of 0.277903184239629\n",
      "The 10735 th iteration gives loss of 0.2778868242476903\n",
      "The 10736 th iteration gives loss of 0.2778704665203173\n",
      "The 10737 th iteration gives loss of 0.2778541110572102\n",
      "The 10738 th iteration gives loss of 0.27783775785803266\n",
      "The 10739 th iteration gives loss of 0.2778214069224667\n",
      "The 10740 th iteration gives loss of 0.2778050582502117\n",
      "The 10741 th iteration gives loss of 0.27778871184093584\n",
      "The 10742 th iteration gives loss of 0.2777723676943375\n",
      "The 10743 th iteration gives loss of 0.27775602581008646\n",
      "The 10744 th iteration gives loss of 0.277739686187871\n",
      "The 10745 th iteration gives loss of 0.27772334882737476\n",
      "The 10746 th iteration gives loss of 0.277707013728274\n",
      "The 10747 th iteration gives loss of 0.2776906808902559\n",
      "The 10748 th iteration gives loss of 0.2776743503130021\n",
      "The 10749 th iteration gives loss of 0.27765802199620787\n",
      "The 10750 th iteration gives loss of 0.27764169593954\n",
      "The 10751 th iteration gives loss of 0.27762537214269045\n",
      "The 10752 th iteration gives loss of 0.2776090506053454\n",
      "The 10753 th iteration gives loss of 0.27759273132718404\n",
      "The 10754 th iteration gives loss of 0.2775764143078847\n",
      "The 10755 th iteration gives loss of 0.27756009954714433\n",
      "The 10756 th iteration gives loss of 0.27754378704462185\n",
      "The 10757 th iteration gives loss of 0.2775274768000359\n",
      "The 10758 th iteration gives loss of 0.27751116881303944\n",
      "The 10759 th iteration gives loss of 0.2774948630833292\n",
      "The 10760 th iteration gives loss of 0.2774785596105876\n",
      "The 10761 th iteration gives loss of 0.2774622583944974\n",
      "The 10762 th iteration gives loss of 0.2774459594347413\n",
      "The 10763 th iteration gives loss of 0.2774296627310087\n",
      "The 10764 th iteration gives loss of 0.2774133682829771\n",
      "The 10765 th iteration gives loss of 0.2773970760903311\n",
      "The 10766 th iteration gives loss of 0.2773807861527594\n",
      "The 10767 th iteration gives loss of 0.27736449846994354\n",
      "The 10768 th iteration gives loss of 0.2773482130415635\n",
      "The 10769 th iteration gives loss of 0.2773319298673033\n",
      "The 10770 th iteration gives loss of 0.2773156489468516\n",
      "The 10771 th iteration gives loss of 0.27729937027988905\n",
      "The 10772 th iteration gives loss of 0.27728309386609906\n",
      "The 10773 th iteration gives loss of 0.277266819705172\n",
      "The 10774 th iteration gives loss of 0.27725054779678837\n",
      "The 10775 th iteration gives loss of 0.277234278140629\n",
      "The 10776 th iteration gives loss of 0.2772180107363781\n",
      "The 10777 th iteration gives loss of 0.27720174558371985\n",
      "The 10778 th iteration gives loss of 0.27718548268235293\n",
      "The 10779 th iteration gives loss of 0.27716922203194383\n",
      "The 10780 th iteration gives loss of 0.2771529636321785\n",
      "The 10781 th iteration gives loss of 0.2771367074827538\n",
      "The 10782 th iteration gives loss of 0.2771204535833391\n",
      "The 10783 th iteration gives loss of 0.277104201933634\n",
      "The 10784 th iteration gives loss of 0.27708795253331403\n",
      "The 10785 th iteration gives loss of 0.2770717053820527\n",
      "The 10786 th iteration gives loss of 0.27705546047955154\n",
      "The 10787 th iteration gives loss of 0.2770392178254939\n",
      "The 10788 th iteration gives loss of 0.27702297741956106\n",
      "The 10789 th iteration gives loss of 0.27700673926143654\n",
      "The 10790 th iteration gives loss of 0.27699050335080133\n",
      "The 10791 th iteration gives loss of 0.27697426968734334\n",
      "The 10792 th iteration gives loss of 0.2769580382707596\n",
      "The 10793 th iteration gives loss of 0.2769418091007112\n",
      "The 10794 th iteration gives loss of 0.27692558217690577\n",
      "The 10795 th iteration gives loss of 0.2769093574990184\n",
      "The 10796 th iteration gives loss of 0.27689313506672253\n",
      "The 10797 th iteration gives loss of 0.27687691487972227\n",
      "The 10798 th iteration gives loss of 0.27686069693768933\n",
      "The 10799 th iteration gives loss of 0.27684448124032246\n",
      "The 10800 th iteration gives loss of 0.2768282677872966\n",
      "The 10801 th iteration gives loss of 0.27681205657829255\n",
      "The 10802 th iteration gives loss of 0.2767958476130081\n",
      "The 10803 th iteration gives loss of 0.2767796408911211\n",
      "The 10804 th iteration gives loss of 0.2767634364123149\n",
      "The 10805 th iteration gives loss of 0.27674723417627184\n",
      "The 10806 th iteration gives loss of 0.27673103418268125\n",
      "The 10807 th iteration gives loss of 0.27671483643124045\n",
      "The 10808 th iteration gives loss of 0.276698640921616\n",
      "The 10809 th iteration gives loss of 0.27668244765350763\n",
      "The 10810 th iteration gives loss of 0.2766662566265946\n",
      "The 10811 th iteration gives loss of 0.2766500678405514\n",
      "The 10812 th iteration gives loss of 0.276633881295083\n",
      "The 10813 th iteration gives loss of 0.2766176969898655\n",
      "The 10814 th iteration gives loss of 0.27660151492458784\n",
      "The 10815 th iteration gives loss of 0.2765853350989367\n",
      "The 10816 th iteration gives loss of 0.276569157512586\n",
      "The 10817 th iteration gives loss of 0.2765529821652274\n",
      "The 10818 th iteration gives loss of 0.27653680905655276\n",
      "The 10819 th iteration gives loss of 0.27652063818624867\n",
      "The 10820 th iteration gives loss of 0.2765044695539902\n",
      "The 10821 th iteration gives loss of 0.2764883031594749\n",
      "The 10822 th iteration gives loss of 0.27647213900237433\n",
      "The 10823 th iteration gives loss of 0.27645597708238595\n",
      "The 10824 th iteration gives loss of 0.2764398173991936\n",
      "The 10825 th iteration gives loss of 0.27642365995248125\n",
      "The 10826 th iteration gives loss of 0.2764075047419336\n",
      "The 10827 th iteration gives loss of 0.27639135176724317\n",
      "The 10828 th iteration gives loss of 0.2763752010280846\n",
      "The 10829 th iteration gives loss of 0.27635905252416326\n",
      "The 10830 th iteration gives loss of 0.2763429062551414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10831 th iteration gives loss of 0.276326762220727\n",
      "The 10832 th iteration gives loss of 0.2763106204205865\n",
      "The 10833 th iteration gives loss of 0.2762944808544228\n",
      "The 10834 th iteration gives loss of 0.2762783435219188\n",
      "The 10835 th iteration gives loss of 0.27626220842274946\n",
      "The 10836 th iteration gives loss of 0.27624607555660585\n",
      "The 10837 th iteration gives loss of 0.27622994492317804\n",
      "The 10838 th iteration gives loss of 0.2762138165221584\n",
      "The 10839 th iteration gives loss of 0.2761976903532211\n",
      "The 10840 th iteration gives loss of 0.27618156641606734\n",
      "The 10841 th iteration gives loss of 0.27616544471036\n",
      "The 10842 th iteration gives loss of 0.2761493252358164\n",
      "The 10843 th iteration gives loss of 0.27613320799210384\n",
      "The 10844 th iteration gives loss of 0.2761170929789082\n",
      "The 10845 th iteration gives loss of 0.2761009801959109\n",
      "The 10846 th iteration gives loss of 0.2760848696428218\n",
      "The 10847 th iteration gives loss of 0.2760687613193138\n",
      "The 10848 th iteration gives loss of 0.27605265522506633\n",
      "The 10849 th iteration gives loss of 0.2760365513597724\n",
      "The 10850 th iteration gives loss of 0.2760204497231293\n",
      "The 10851 th iteration gives loss of 0.27600435031481113\n",
      "The 10852 th iteration gives loss of 0.2759882531345038\n",
      "The 10853 th iteration gives loss of 0.27597215818190485\n",
      "The 10854 th iteration gives loss of 0.27595606545668355\n",
      "The 10855 th iteration gives loss of 0.27593997495855094\n",
      "The 10856 th iteration gives loss of 0.27592388668717815\n",
      "The 10857 th iteration gives loss of 0.27590780064224874\n",
      "The 10858 th iteration gives loss of 0.27589171682346286\n",
      "The 10859 th iteration gives loss of 0.27587563523050246\n",
      "The 10860 th iteration gives loss of 0.2758595558630581\n",
      "The 10861 th iteration gives loss of 0.27584347872080384\n",
      "The 10862 th iteration gives loss of 0.27582740380344284\n",
      "The 10863 th iteration gives loss of 0.27581133111065065\n",
      "The 10864 th iteration gives loss of 0.27579526064212007\n",
      "The 10865 th iteration gives loss of 0.2757791923975492\n",
      "The 10866 th iteration gives loss of 0.27576312637660627\n",
      "The 10867 th iteration gives loss of 0.27574706257898257\n",
      "The 10868 th iteration gives loss of 0.27573100100437736\n",
      "The 10869 th iteration gives loss of 0.2757149416524588\n",
      "The 10870 th iteration gives loss of 0.2756988845229348\n",
      "The 10871 th iteration gives loss of 0.2756828296154801\n",
      "The 10872 th iteration gives loss of 0.27566677692978836\n",
      "The 10873 th iteration gives loss of 0.2756507264655475\n",
      "The 10874 th iteration gives loss of 0.2756346782224458\n",
      "The 10875 th iteration gives loss of 0.27561863220016647\n",
      "The 10876 th iteration gives loss of 0.275602588398401\n",
      "The 10877 th iteration gives loss of 0.2755865468168363\n",
      "The 10878 th iteration gives loss of 0.27557050745515643\n",
      "The 10879 th iteration gives loss of 0.2755544703130562\n",
      "The 10880 th iteration gives loss of 0.275538435390212\n",
      "The 10881 th iteration gives loss of 0.2755224026863256\n",
      "The 10882 th iteration gives loss of 0.2755063722010777\n",
      "The 10883 th iteration gives loss of 0.2754903439341527\n",
      "The 10884 th iteration gives loss of 0.2754743178852498\n",
      "The 10885 th iteration gives loss of 0.27545829405404376\n",
      "The 10886 th iteration gives loss of 0.27544227244023967\n",
      "The 10887 th iteration gives loss of 0.2754262530435142\n",
      "The 10888 th iteration gives loss of 0.2754102358635411\n",
      "The 10889 th iteration gives loss of 0.27539422090004184\n",
      "The 10890 th iteration gives loss of 0.27537820815267955\n",
      "The 10891 th iteration gives loss of 0.27536219762115655\n",
      "The 10892 th iteration gives loss of 0.2753461893051482\n",
      "The 10893 th iteration gives loss of 0.2753301832043569\n",
      "The 10894 th iteration gives loss of 0.27531417931845803\n",
      "The 10895 th iteration gives loss of 0.2752981776471491\n",
      "The 10896 th iteration gives loss of 0.2752821781901102\n",
      "The 10897 th iteration gives loss of 0.27526618094703836\n",
      "The 10898 th iteration gives loss of 0.2752501859176211\n",
      "The 10899 th iteration gives loss of 0.2752341931015469\n",
      "The 10900 th iteration gives loss of 0.27521820249850265\n",
      "The 10901 th iteration gives loss of 0.2752022141081717\n",
      "The 10902 th iteration gives loss of 0.2751862279302488\n",
      "The 10903 th iteration gives loss of 0.2751702439644204\n",
      "The 10904 th iteration gives loss of 0.2751542622103857\n",
      "The 10905 th iteration gives loss of 0.27513828266781715\n",
      "The 10906 th iteration gives loss of 0.2751223053364172\n",
      "The 10907 th iteration gives loss of 0.27510633021585806\n",
      "The 10908 th iteration gives loss of 0.2750903573058403\n",
      "The 10909 th iteration gives loss of 0.2750743866060592\n",
      "The 10910 th iteration gives loss of 0.2750584181161908\n",
      "The 10911 th iteration gives loss of 0.27504245183593257\n",
      "The 10912 th iteration gives loss of 0.2750264877649724\n",
      "The 10913 th iteration gives loss of 0.275010525902991\n",
      "The 10914 th iteration gives loss of 0.2749945662496925\n",
      "The 10915 th iteration gives loss of 0.27497860880474667\n",
      "The 10916 th iteration gives loss of 0.27496265356785654\n",
      "The 10917 th iteration gives loss of 0.2749467005387002\n",
      "The 10918 th iteration gives loss of 0.27493074971699233\n",
      "The 10919 th iteration gives loss of 0.2749148011023995\n",
      "The 10920 th iteration gives loss of 0.27489885469461695\n",
      "The 10921 th iteration gives loss of 0.274882910493332\n",
      "The 10922 th iteration gives loss of 0.27486696849823783\n",
      "The 10923 th iteration gives loss of 0.27485102870901995\n",
      "The 10924 th iteration gives loss of 0.27483509112537374\n",
      "The 10925 th iteration gives loss of 0.2748191557469742\n",
      "The 10926 th iteration gives loss of 0.27480322257352413\n",
      "The 10927 th iteration gives loss of 0.27478729160471765\n",
      "The 10928 th iteration gives loss of 0.27477136284023324\n",
      "The 10929 th iteration gives loss of 0.2747554362797733\n",
      "The 10930 th iteration gives loss of 0.2747395119230059\n",
      "The 10931 th iteration gives loss of 0.27472358976963046\n",
      "The 10932 th iteration gives loss of 0.2747076698193559\n",
      "The 10933 th iteration gives loss of 0.2746917520718439\n",
      "The 10934 th iteration gives loss of 0.27467583652679334\n",
      "The 10935 th iteration gives loss of 0.2746599231839074\n",
      "The 10936 th iteration gives loss of 0.27464401204286537\n",
      "The 10937 th iteration gives loss of 0.2746281031033545\n",
      "The 10938 th iteration gives loss of 0.2746121963650672\n",
      "The 10939 th iteration gives loss of 0.27459629182769335\n",
      "The 10940 th iteration gives loss of 0.2745803894909255\n",
      "The 10941 th iteration gives loss of 0.27456448935445976\n",
      "The 10942 th iteration gives loss of 0.2745485914179743\n",
      "The 10943 th iteration gives loss of 0.27453269568115357\n",
      "The 10944 th iteration gives loss of 0.2745168021437051\n",
      "The 10945 th iteration gives loss of 0.2745009108053175\n",
      "The 10946 th iteration gives loss of 0.27448502166567224\n",
      "The 10947 th iteration gives loss of 0.27446913472445134\n",
      "The 10948 th iteration gives loss of 0.2744532499813651\n",
      "The 10949 th iteration gives loss of 0.2744373674360966\n",
      "The 10950 th iteration gives loss of 0.27442148708833725\n",
      "The 10951 th iteration gives loss of 0.27440560893777166\n",
      "The 10952 th iteration gives loss of 0.2743897329840983\n",
      "The 10953 th iteration gives loss of 0.27437385922699914\n",
      "The 10954 th iteration gives loss of 0.2743579876661699\n",
      "The 10955 th iteration gives loss of 0.27434211830130556\n",
      "The 10956 th iteration gives loss of 0.27432625113207965\n",
      "The 10957 th iteration gives loss of 0.2743103861582097\n",
      "The 10958 th iteration gives loss of 0.2742945233793565\n",
      "The 10959 th iteration gives loss of 0.2742786627952343\n",
      "The 10960 th iteration gives loss of 0.27426280440552053\n",
      "The 10961 th iteration gives loss of 0.27424694820991313\n",
      "The 10962 th iteration gives loss of 0.27423109420810365\n",
      "The 10963 th iteration gives loss of 0.2742152423997754\n",
      "The 10964 th iteration gives loss of 0.27419939278462674\n",
      "The 10965 th iteration gives loss of 0.27418354536233985\n",
      "The 10966 th iteration gives loss of 0.2741677001326149\n",
      "The 10967 th iteration gives loss of 0.27415185709514034\n",
      "The 10968 th iteration gives loss of 0.2741360162496025\n",
      "The 10969 th iteration gives loss of 0.27412017759570045\n",
      "The 10970 th iteration gives loss of 0.274104341133121\n",
      "The 10971 th iteration gives loss of 0.2740885068615631\n",
      "The 10972 th iteration gives loss of 0.27407267478070035\n",
      "The 10973 th iteration gives loss of 0.27405684489023774\n",
      "The 10974 th iteration gives loss of 0.2740410171898603\n",
      "The 10975 th iteration gives loss of 0.2740251916792577\n",
      "The 10976 th iteration gives loss of 0.27400936835812945\n",
      "The 10977 th iteration gives loss of 0.27399354722616204\n",
      "The 10978 th iteration gives loss of 0.2739777282830501\n",
      "The 10979 th iteration gives loss of 0.2739619115284798\n",
      "The 10980 th iteration gives loss of 0.2739460969621509\n",
      "The 10981 th iteration gives loss of 0.27393028458374163\n",
      "The 10982 th iteration gives loss of 0.2739144743929591\n",
      "The 10983 th iteration gives loss of 0.27389866638948684\n",
      "The 10984 th iteration gives loss of 0.2738828605730162\n",
      "The 10985 th iteration gives loss of 0.2738670569432329\n",
      "The 10986 th iteration gives loss of 0.27385125549984557\n",
      "The 10987 th iteration gives loss of 0.27383545624253014\n",
      "The 10988 th iteration gives loss of 0.27381965917098205\n",
      "The 10989 th iteration gives loss of 0.2738038642848973\n",
      "The 10990 th iteration gives loss of 0.27378807158396473\n",
      "The 10991 th iteration gives loss of 0.27377228106787754\n",
      "The 10992 th iteration gives loss of 0.2737564927363208\n",
      "The 10993 th iteration gives loss of 0.2737407065890042\n",
      "The 10994 th iteration gives loss of 0.2737249226255993\n",
      "The 10995 th iteration gives loss of 0.2737091408458097\n",
      "The 10996 th iteration gives loss of 0.27369336124932436\n",
      "The 10997 th iteration gives loss of 0.27367758383583535\n",
      "The 10998 th iteration gives loss of 0.27366180860503064\n",
      "The 10999 th iteration gives loss of 0.2736460355566155\n",
      "The 11000 th iteration gives loss of 0.2736302646902737\n",
      "The 11001 th iteration gives loss of 0.2736144960056836\n",
      "The 11002 th iteration gives loss of 0.2735987295025639\n",
      "The 11003 th iteration gives loss of 0.27358296518058606\n",
      "The 11004 th iteration gives loss of 0.2735672030394613\n",
      "The 11005 th iteration gives loss of 0.27355144307886553\n",
      "The 11006 th iteration gives loss of 0.2735356852984931\n",
      "The 11007 th iteration gives loss of 0.27351992969804545\n",
      "The 11008 th iteration gives loss of 0.2735041762772064\n",
      "The 11009 th iteration gives loss of 0.27348842503566906\n",
      "The 11010 th iteration gives loss of 0.27347267597313396\n",
      "The 11011 th iteration gives loss of 0.2734569290892901\n",
      "The 11012 th iteration gives loss of 0.27344118438381987\n",
      "The 11013 th iteration gives loss of 0.27342544185643536\n",
      "The 11014 th iteration gives loss of 0.27340970150680544\n",
      "The 11015 th iteration gives loss of 0.27339396333464944\n",
      "The 11016 th iteration gives loss of 0.2733782273396395\n",
      "The 11017 th iteration gives loss of 0.2733624935214702\n",
      "The 11018 th iteration gives loss of 0.2733467618798432\n",
      "The 11019 th iteration gives loss of 0.2733310324144581\n",
      "The 11020 th iteration gives loss of 0.2733153051249833\n",
      "The 11021 th iteration gives loss of 0.2732995800111336\n",
      "The 11022 th iteration gives loss of 0.2732838570725881\n",
      "The 11023 th iteration gives loss of 0.27326813630904745\n",
      "The 11024 th iteration gives loss of 0.27325241772020464\n",
      "The 11025 th iteration gives loss of 0.273236701305753\n",
      "The 11026 th iteration gives loss of 0.273220987065385\n",
      "The 11027 th iteration gives loss of 0.2732052749987868\n",
      "The 11028 th iteration gives loss of 0.2731895651056549\n",
      "The 11029 th iteration gives loss of 0.273173857385697\n",
      "The 11030 th iteration gives loss of 0.27315815183859005\n",
      "The 11031 th iteration gives loss of 0.2731424484640244\n",
      "The 11032 th iteration gives loss of 0.2731267472617119\n",
      "The 11033 th iteration gives loss of 0.2731110482313261\n",
      "The 11034 th iteration gives loss of 0.2730953513725708\n",
      "The 11035 th iteration gives loss of 0.2730796566851439\n",
      "The 11036 th iteration gives loss of 0.27306396416872275\n",
      "The 11037 th iteration gives loss of 0.27304827382301733\n",
      "The 11038 th iteration gives loss of 0.273032585647708\n",
      "The 11039 th iteration gives loss of 0.2730168996425075\n",
      "The 11040 th iteration gives loss of 0.27300121580708275\n",
      "The 11041 th iteration gives loss of 0.27298553414114285\n",
      "The 11042 th iteration gives loss of 0.2729698546443847\n",
      "The 11043 th iteration gives loss of 0.2729541773165005\n",
      "The 11044 th iteration gives loss of 0.2729385021571753\n",
      "The 11045 th iteration gives loss of 0.27292282916611277\n",
      "The 11046 th iteration gives loss of 0.2729071583430017\n",
      "The 11047 th iteration gives loss of 0.2728914896875332\n",
      "The 11048 th iteration gives loss of 0.27287582319940523\n",
      "The 11049 th iteration gives loss of 0.2728601588783093\n",
      "The 11050 th iteration gives loss of 0.27284449672394334\n",
      "The 11051 th iteration gives loss of 0.27282883673599784\n",
      "The 11052 th iteration gives loss of 0.27281317891416823\n",
      "The 11053 th iteration gives loss of 0.2727975232581563\n",
      "The 11054 th iteration gives loss of 0.2727818697676401\n",
      "The 11055 th iteration gives loss of 0.272766218442323\n",
      "The 11056 th iteration gives loss of 0.27275056928190217\n",
      "The 11057 th iteration gives loss of 0.2727349222860665\n",
      "The 11058 th iteration gives loss of 0.2727192774545061\n",
      "The 11059 th iteration gives loss of 0.2727036347869214\n",
      "The 11060 th iteration gives loss of 0.27268799428301344\n",
      "The 11061 th iteration gives loss of 0.27267235594246425\n",
      "The 11062 th iteration gives loss of 0.27265671976497124\n",
      "The 11063 th iteration gives loss of 0.2726410857502329\n",
      "The 11064 th iteration gives loss of 0.2726254538979398\n",
      "The 11065 th iteration gives loss of 0.27260982420779406\n",
      "The 11066 th iteration gives loss of 0.2725941966794764\n",
      "The 11067 th iteration gives loss of 0.27257857131269453\n",
      "The 11068 th iteration gives loss of 0.27256294810713505\n",
      "The 11069 th iteration gives loss of 0.272547327062493\n",
      "The 11070 th iteration gives loss of 0.2725317081784642\n",
      "The 11071 th iteration gives loss of 0.2725160914547426\n",
      "The 11072 th iteration gives loss of 0.2725004768910341\n",
      "The 11073 th iteration gives loss of 0.2724848644870224\n",
      "The 11074 th iteration gives loss of 0.27246925424239876\n",
      "The 11075 th iteration gives loss of 0.27245364615687373\n",
      "The 11076 th iteration gives loss of 0.27243804023012114\n",
      "The 11077 th iteration gives loss of 0.2724224364618522\n",
      "The 11078 th iteration gives loss of 0.2724068348517583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 11079 th iteration gives loss of 0.27239123539951926\n",
      "The 11080 th iteration gives loss of 0.2723756381048588\n",
      "The 11081 th iteration gives loss of 0.27236004296744437\n",
      "The 11082 th iteration gives loss of 0.2723444499869893\n",
      "The 11083 th iteration gives loss of 0.27232885916318067\n",
      "The 11084 th iteration gives loss of 0.2723132704957104\n",
      "The 11085 th iteration gives loss of 0.27229768398428505\n",
      "The 11086 th iteration gives loss of 0.27228209962859534\n",
      "The 11087 th iteration gives loss of 0.2722665174283329\n",
      "The 11088 th iteration gives loss of 0.272250937383186\n",
      "The 11089 th iteration gives loss of 0.272235359492869\n",
      "The 11090 th iteration gives loss of 0.2722197837570708\n",
      "The 11091 th iteration gives loss of 0.2722042101754759\n",
      "The 11092 th iteration gives loss of 0.2721886387477905\n",
      "The 11093 th iteration gives loss of 0.272173069473702\n",
      "The 11094 th iteration gives loss of 0.27215750235290853\n",
      "The 11095 th iteration gives loss of 0.27214193738511105\n",
      "The 11096 th iteration gives loss of 0.2721263745700059\n",
      "The 11097 th iteration gives loss of 0.2721108139072806\n",
      "The 11098 th iteration gives loss of 0.27209525539663476\n",
      "The 11099 th iteration gives loss of 0.2720796990377635\n",
      "The 11100 th iteration gives loss of 0.2720641448303613\n",
      "The 11101 th iteration gives loss of 0.2720485927741246\n",
      "The 11102 th iteration gives loss of 0.2720330428687594\n",
      "The 11103 th iteration gives loss of 0.27201749511393575\n",
      "The 11104 th iteration gives loss of 0.2720019495093764\n",
      "The 11105 th iteration gives loss of 0.2719864060547638\n",
      "The 11106 th iteration gives loss of 0.2719708647498008\n",
      "The 11107 th iteration gives loss of 0.2719553255941781\n",
      "The 11108 th iteration gives loss of 0.2719397885875975\n",
      "The 11109 th iteration gives loss of 0.2719242537297413\n",
      "The 11110 th iteration gives loss of 0.2719087210203201\n",
      "The 11111 th iteration gives loss of 0.2718931904590259\n",
      "The 11112 th iteration gives loss of 0.2718776620455519\n",
      "The 11113 th iteration gives loss of 0.2718621357795962\n",
      "The 11114 th iteration gives loss of 0.27184661166085305\n",
      "The 11115 th iteration gives loss of 0.2718310896890248\n",
      "The 11116 th iteration gives loss of 0.271815569863799\n",
      "The 11117 th iteration gives loss of 0.2718000521848801\n",
      "The 11118 th iteration gives loss of 0.2717845366519605\n",
      "The 11119 th iteration gives loss of 0.2717690232647314\n",
      "The 11120 th iteration gives loss of 0.27175351202289993\n",
      "The 11121 th iteration gives loss of 0.27173800292615935\n",
      "The 11122 th iteration gives loss of 0.27172249597420295\n",
      "The 11123 th iteration gives loss of 0.2717069911667351\n",
      "The 11124 th iteration gives loss of 0.2716914885034334\n",
      "The 11125 th iteration gives loss of 0.27167598798401227\n",
      "The 11126 th iteration gives loss of 0.2716604896081577\n",
      "The 11127 th iteration gives loss of 0.2716449933755803\n",
      "The 11128 th iteration gives loss of 0.2716294992859663\n",
      "The 11129 th iteration gives loss of 0.2716140073390085\n",
      "The 11130 th iteration gives loss of 0.2715985175344135\n",
      "The 11131 th iteration gives loss of 0.27158302987187233\n",
      "The 11132 th iteration gives loss of 0.27156754435109115\n",
      "The 11133 th iteration gives loss of 0.2715520609717563\n",
      "The 11134 th iteration gives loss of 0.2715365797335603\n",
      "The 11135 th iteration gives loss of 0.2715211006362109\n",
      "The 11136 th iteration gives loss of 0.2715056236793988\n",
      "The 11137 th iteration gives loss of 0.27149014886282397\n",
      "The 11138 th iteration gives loss of 0.2714746761861895\n",
      "The 11139 th iteration gives loss of 0.2714592056491833\n",
      "The 11140 th iteration gives loss of 0.27144373725149573\n",
      "The 11141 th iteration gives loss of 0.2714282709928468\n",
      "The 11142 th iteration gives loss of 0.27141280687291514\n",
      "The 11143 th iteration gives loss of 0.2713973448914061\n",
      "The 11144 th iteration gives loss of 0.27138188504801514\n",
      "The 11145 th iteration gives loss of 0.2713664273424325\n",
      "The 11146 th iteration gives loss of 0.2713509717743694\n",
      "The 11147 th iteration gives loss of 0.2713355183435147\n",
      "The 11148 th iteration gives loss of 0.2713200670495693\n",
      "The 11149 th iteration gives loss of 0.27130461789221444\n",
      "The 11150 th iteration gives loss of 0.27128917087117166\n",
      "The 11151 th iteration gives loss of 0.27127372598612415\n",
      "The 11152 th iteration gives loss of 0.2712582832367783\n",
      "The 11153 th iteration gives loss of 0.271242842622817\n",
      "The 11154 th iteration gives loss of 0.27122740414395385\n",
      "The 11155 th iteration gives loss of 0.27121196779988377\n",
      "The 11156 th iteration gives loss of 0.2711965335902983\n",
      "The 11157 th iteration gives loss of 0.271181101514895\n",
      "The 11158 th iteration gives loss of 0.2711656715733797\n",
      "The 11159 th iteration gives loss of 0.27115024376544233\n",
      "The 11160 th iteration gives loss of 0.2711348180907813\n",
      "The 11161 th iteration gives loss of 0.27111939454909617\n",
      "The 11162 th iteration gives loss of 0.2711039731400875\n",
      "The 11163 th iteration gives loss of 0.271088553863452\n",
      "The 11164 th iteration gives loss of 0.27107313671888655\n",
      "The 11165 th iteration gives loss of 0.27105772170608944\n",
      "The 11166 th iteration gives loss of 0.2710423088247488\n",
      "The 11167 th iteration gives loss of 0.2710268980745778\n",
      "The 11168 th iteration gives loss of 0.2710114894552729\n",
      "The 11169 th iteration gives loss of 0.2709960829665313\n",
      "The 11170 th iteration gives loss of 0.2709806786080404\n",
      "The 11171 th iteration gives loss of 0.2709652763795043\n",
      "The 11172 th iteration gives loss of 0.270949876280631\n",
      "The 11173 th iteration gives loss of 0.27093447831111017\n",
      "The 11174 th iteration gives loss of 0.2709190824706358\n",
      "The 11175 th iteration gives loss of 0.27090368875892074\n",
      "The 11176 th iteration gives loss of 0.27088829717564905\n",
      "The 11177 th iteration gives loss of 0.2708729077205205\n",
      "The 11178 th iteration gives loss of 0.27085752039323857\n",
      "The 11179 th iteration gives loss of 0.27084213519350203\n",
      "The 11180 th iteration gives loss of 0.27082675212100826\n",
      "The 11181 th iteration gives loss of 0.27081137117545406\n",
      "The 11182 th iteration gives loss of 0.2707959923565442\n",
      "The 11183 th iteration gives loss of 0.2707806156639625\n",
      "The 11184 th iteration gives loss of 0.2707652410974217\n",
      "The 11185 th iteration gives loss of 0.2707498686566176\n",
      "The 11186 th iteration gives loss of 0.27073449834125357\n",
      "The 11187 th iteration gives loss of 0.27071913015101234\n",
      "The 11188 th iteration gives loss of 0.27070376408560987\n",
      "The 11189 th iteration gives loss of 0.27068840014473966\n",
      "The 11190 th iteration gives loss of 0.27067303832808953\n",
      "The 11191 th iteration gives loss of 0.27065767863537793\n",
      "The 11192 th iteration gives loss of 0.27064232106628744\n",
      "The 11193 th iteration gives loss of 0.27062696562052796\n",
      "The 11194 th iteration gives loss of 0.27061161229779246\n",
      "The 11195 th iteration gives loss of 0.2705962610977812\n",
      "The 11196 th iteration gives loss of 0.27058091202019297\n",
      "The 11197 th iteration gives loss of 0.27056556506472107\n",
      "The 11198 th iteration gives loss of 0.27055022023107167\n",
      "The 11199 th iteration gives loss of 0.2705348775189547\n",
      "The 11200 th iteration gives loss of 0.2705195369280494\n",
      "The 11201 th iteration gives loss of 0.2705041984580694\n",
      "The 11202 th iteration gives loss of 0.27048886210870315\n",
      "The 11203 th iteration gives loss of 0.27047352787965195\n",
      "The 11204 th iteration gives loss of 0.27045819577061847\n",
      "The 11205 th iteration gives loss of 0.2704428657813099\n",
      "The 11206 th iteration gives loss of 0.27042753791142093\n",
      "The 11207 th iteration gives loss of 0.2704122121606408\n",
      "The 11208 th iteration gives loss of 0.27039688852867555\n",
      "The 11209 th iteration gives loss of 0.2703815670152321\n",
      "The 11210 th iteration gives loss of 0.2703662476199916\n",
      "The 11211 th iteration gives loss of 0.27035093034267427\n",
      "The 11212 th iteration gives loss of 0.27033561518296906\n",
      "The 11213 th iteration gives loss of 0.2703203021405654\n",
      "The 11214 th iteration gives loss of 0.2703049912151916\n",
      "The 11215 th iteration gives loss of 0.27028968240652307\n",
      "The 11216 th iteration gives loss of 0.27027437571427154\n",
      "The 11217 th iteration gives loss of 0.2702590711381288\n",
      "The 11218 th iteration gives loss of 0.2702437686778015\n",
      "The 11219 th iteration gives loss of 0.2702284683329864\n",
      "The 11220 th iteration gives loss of 0.27021317010338447\n",
      "The 11221 th iteration gives loss of 0.27019787398869405\n",
      "The 11222 th iteration gives loss of 0.2701825799886097\n",
      "The 11223 th iteration gives loss of 0.27016728810283724\n",
      "The 11224 th iteration gives loss of 0.2701519983310848\n",
      "The 11225 th iteration gives loss of 0.2701367106730401\n",
      "The 11226 th iteration gives loss of 0.27012142512841225\n",
      "The 11227 th iteration gives loss of 0.2701061416968978\n",
      "The 11228 th iteration gives loss of 0.2700908603781989\n",
      "The 11229 th iteration gives loss of 0.2700755811719998\n",
      "The 11230 th iteration gives loss of 0.27006030407802667\n",
      "The 11231 th iteration gives loss of 0.2700450290959618\n",
      "The 11232 th iteration gives loss of 0.27002975622551134\n",
      "The 11233 th iteration gives loss of 0.27001448546637674\n",
      "The 11234 th iteration gives loss of 0.26999921681825684\n",
      "The 11235 th iteration gives loss of 0.2699839502808542\n",
      "The 11236 th iteration gives loss of 0.26996868585386063\n",
      "The 11237 th iteration gives loss of 0.269953423536993\n",
      "The 11238 th iteration gives loss of 0.2699381633299391\n",
      "The 11239 th iteration gives loss of 0.26992290523240214\n",
      "The 11240 th iteration gives loss of 0.26990764924407984\n",
      "The 11241 th iteration gives loss of 0.2698923953646748\n",
      "The 11242 th iteration gives loss of 0.2698771435939023\n",
      "The 11243 th iteration gives loss of 0.2698618939314403\n",
      "The 11244 th iteration gives loss of 0.2698466463769965\n",
      "The 11245 th iteration gives loss of 0.2698314009302794\n",
      "The 11246 th iteration gives loss of 0.26981615759098554\n",
      "The 11247 th iteration gives loss of 0.26980091635881226\n",
      "The 11248 th iteration gives loss of 0.2697856772334661\n",
      "The 11249 th iteration gives loss of 0.2697704402146512\n",
      "The 11250 th iteration gives loss of 0.2697552053020544\n",
      "The 11251 th iteration gives loss of 0.26973997249538856\n",
      "The 11252 th iteration gives loss of 0.2697247417943535\n",
      "The 11253 th iteration gives loss of 0.2697095131986416\n",
      "The 11254 th iteration gives loss of 0.2696942867079651\n",
      "The 11255 th iteration gives loss of 0.26967906232201855\n",
      "The 11256 th iteration gives loss of 0.2696638400405028\n",
      "The 11257 th iteration gives loss of 0.26964861986312477\n",
      "The 11258 th iteration gives loss of 0.26963340178958234\n",
      "The 11259 th iteration gives loss of 0.26961818581957847\n",
      "The 11260 th iteration gives loss of 0.26960297195281324\n",
      "The 11261 th iteration gives loss of 0.26958776018898817\n",
      "The 11262 th iteration gives loss of 0.26957255052780166\n",
      "The 11263 th iteration gives loss of 0.26955734296895556\n",
      "The 11264 th iteration gives loss of 0.269542137512151\n",
      "The 11265 th iteration gives loss of 0.26952693415709555\n",
      "The 11266 th iteration gives loss of 0.2695117329034829\n",
      "The 11267 th iteration gives loss of 0.2694965337510357\n",
      "The 11268 th iteration gives loss of 0.2694813366994268\n",
      "The 11269 th iteration gives loss of 0.2694661417483577\n",
      "The 11270 th iteration gives loss of 0.26945094889755045\n",
      "The 11271 th iteration gives loss of 0.2694357581467092\n",
      "The 11272 th iteration gives loss of 0.26942056949551557\n",
      "The 11273 th iteration gives loss of 0.26940538294368616\n",
      "The 11274 th iteration gives loss of 0.269390198490908\n",
      "The 11275 th iteration gives loss of 0.26937501613689\n",
      "The 11276 th iteration gives loss of 0.269359835881351\n",
      "The 11277 th iteration gives loss of 0.2693446577239742\n",
      "The 11278 th iteration gives loss of 0.269329481664457\n",
      "The 11279 th iteration gives loss of 0.26931430770251585\n",
      "The 11280 th iteration gives loss of 0.2692991358378464\n",
      "The 11281 th iteration gives loss of 0.2692839660701505\n",
      "The 11282 th iteration gives loss of 0.2692687983991303\n",
      "The 11283 th iteration gives loss of 0.2692536328244871\n",
      "The 11284 th iteration gives loss of 0.2692384693459229\n",
      "The 11285 th iteration gives loss of 0.2692233079631483\n",
      "The 11286 th iteration gives loss of 0.26920814867585424\n",
      "The 11287 th iteration gives loss of 0.2691929914837498\n",
      "The 11288 th iteration gives loss of 0.2691778363865386\n",
      "The 11289 th iteration gives loss of 0.2691626833839122\n",
      "The 11290 th iteration gives loss of 0.2691475324755831\n",
      "The 11291 th iteration gives loss of 0.26913238366124864\n",
      "The 11292 th iteration gives loss of 0.2691172369406164\n",
      "The 11293 th iteration gives loss of 0.26910209231338056\n",
      "The 11294 th iteration gives loss of 0.2690869497792531\n",
      "The 11295 th iteration gives loss of 0.26907180933792996\n",
      "The 11296 th iteration gives loss of 0.2690566709891179\n",
      "The 11297 th iteration gives loss of 0.2690415347325199\n",
      "The 11298 th iteration gives loss of 0.2690264005678332\n",
      "The 11299 th iteration gives loss of 0.2690112684947699\n",
      "The 11300 th iteration gives loss of 0.26899613851301435\n",
      "The 11301 th iteration gives loss of 0.2689810106222903\n",
      "The 11302 th iteration gives loss of 0.26896588482228795\n",
      "The 11303 th iteration gives loss of 0.2689507611127155\n",
      "The 11304 th iteration gives loss of 0.2689356394932758\n",
      "The 11305 th iteration gives loss of 0.2689205199636691\n",
      "The 11306 th iteration gives loss of 0.26890540252359935\n",
      "The 11307 th iteration gives loss of 0.26889028717276864\n",
      "The 11308 th iteration gives loss of 0.2688751739108836\n",
      "The 11309 th iteration gives loss of 0.2688600627376497\n",
      "The 11310 th iteration gives loss of 0.26884495365275407\n",
      "The 11311 th iteration gives loss of 0.2688298466559175\n",
      "The 11312 th iteration gives loss of 0.26881474174682946\n",
      "The 11313 th iteration gives loss of 0.2687996389251993\n",
      "The 11314 th iteration gives loss of 0.268784538190736\n",
      "The 11315 th iteration gives loss of 0.2687694395431404\n",
      "The 11316 th iteration gives loss of 0.268754342982108\n",
      "The 11317 th iteration gives loss of 0.2687392485073585\n",
      "The 11318 th iteration gives loss of 0.2687241561185695\n",
      "The 11319 th iteration gives loss of 0.26870906581546244\n",
      "The 11320 th iteration gives loss of 0.2686939775977383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 11321 th iteration gives loss of 0.2686788914650994\n",
      "The 11322 th iteration gives loss of 0.26866380741725676\n",
      "The 11323 th iteration gives loss of 0.26864872545389706\n",
      "The 11324 th iteration gives loss of 0.2686336455747299\n",
      "The 11325 th iteration gives loss of 0.26861856777945786\n",
      "The 11326 th iteration gives loss of 0.2686034920678047\n",
      "The 11327 th iteration gives loss of 0.2685884184394486\n",
      "The 11328 th iteration gives loss of 0.268573346894112\n",
      "The 11329 th iteration gives loss of 0.26855827743148053\n",
      "The 11330 th iteration gives loss of 0.2685432100512703\n",
      "The 11331 th iteration gives loss of 0.2685281447531717\n",
      "The 11332 th iteration gives loss of 0.2685130815369103\n",
      "The 11333 th iteration gives loss of 0.2684980204021836\n",
      "The 11334 th iteration gives loss of 0.268482961348674\n",
      "The 11335 th iteration gives loss of 0.2684679043761137\n",
      "The 11336 th iteration gives loss of 0.26845284948419024\n",
      "The 11337 th iteration gives loss of 0.2684377966726138\n",
      "The 11338 th iteration gives loss of 0.26842274594108734\n",
      "The 11339 th iteration gives loss of 0.26840769728931396\n",
      "The 11340 th iteration gives loss of 0.26839265071699653\n",
      "The 11341 th iteration gives loss of 0.268377606223838\n",
      "The 11342 th iteration gives loss of 0.2683625638095485\n",
      "The 11343 th iteration gives loss of 0.2683475234738247\n",
      "The 11344 th iteration gives loss of 0.2683324852163747\n",
      "The 11345 th iteration gives loss of 0.26831744903691424\n",
      "The 11346 th iteration gives loss of 0.2683024149351318\n",
      "The 11347 th iteration gives loss of 0.2682873829107331\n",
      "The 11348 th iteration gives loss of 0.2682723529634258\n",
      "The 11349 th iteration gives loss of 0.2682573250929153\n",
      "The 11350 th iteration gives loss of 0.26824229929890087\n",
      "The 11351 th iteration gives loss of 0.2682272755810953\n",
      "The 11352 th iteration gives loss of 0.26821225393920245\n",
      "The 11353 th iteration gives loss of 0.2681972343729217\n",
      "The 11354 th iteration gives loss of 0.2681822168819563\n",
      "The 11355 th iteration gives loss of 0.2681672014660164\n",
      "The 11356 th iteration gives loss of 0.2681521881248084\n",
      "The 11357 th iteration gives loss of 0.2681371768580281\n",
      "The 11358 th iteration gives loss of 0.26812216766538677\n",
      "The 11359 th iteration gives loss of 0.2681071605465893\n",
      "The 11360 th iteration gives loss of 0.26809215550133614\n",
      "The 11361 th iteration gives loss of 0.2680771525293325\n",
      "The 11362 th iteration gives loss of 0.26806215163028896\n",
      "The 11363 th iteration gives loss of 0.26804715280391184\n",
      "The 11364 th iteration gives loss of 0.2680321560498983\n",
      "The 11365 th iteration gives loss of 0.2680171613679517\n",
      "The 11366 th iteration gives loss of 0.26800216875777993\n",
      "The 11367 th iteration gives loss of 0.2679871782190947\n",
      "The 11368 th iteration gives loss of 0.26797218975160425\n",
      "The 11369 th iteration gives loss of 0.267957203354989\n",
      "The 11370 th iteration gives loss of 0.267942219028983\n",
      "The 11371 th iteration gives loss of 0.26792723677327507\n",
      "The 11372 th iteration gives loss of 0.2679122565875716\n",
      "The 11373 th iteration gives loss of 0.2678972784715908\n",
      "The 11374 th iteration gives loss of 0.2678823024250189\n",
      "The 11375 th iteration gives loss of 0.26786732844757605\n",
      "The 11376 th iteration gives loss of 0.2678523565389616\n",
      "The 11377 th iteration gives loss of 0.2678373866988742\n",
      "The 11378 th iteration gives loss of 0.2678224189270245\n",
      "The 11379 th iteration gives loss of 0.26780745322312227\n",
      "The 11380 th iteration gives loss of 0.2677924895868694\n",
      "The 11381 th iteration gives loss of 0.2677775280179803\n",
      "The 11382 th iteration gives loss of 0.26776256851615293\n",
      "The 11383 th iteration gives loss of 0.26774761108108425\n",
      "The 11384 th iteration gives loss of 0.2677326557124913\n",
      "The 11385 th iteration gives loss of 0.26771770241007825\n",
      "The 11386 th iteration gives loss of 0.26770275117354764\n",
      "The 11387 th iteration gives loss of 0.26768780200261366\n",
      "The 11388 th iteration gives loss of 0.26767285489697157\n",
      "The 11389 th iteration gives loss of 0.26765790985632376\n",
      "The 11390 th iteration gives loss of 0.267642966880387\n",
      "The 11391 th iteration gives loss of 0.26762802596886337\n",
      "The 11392 th iteration gives loss of 0.2676130871214567\n",
      "The 11393 th iteration gives loss of 0.2675981503378883\n",
      "The 11394 th iteration gives loss of 0.2675832156178408\n",
      "The 11395 th iteration gives loss of 0.2675682829610283\n",
      "The 11396 th iteration gives loss of 0.26755335236716243\n",
      "The 11397 th iteration gives loss of 0.2675384238359381\n",
      "The 11398 th iteration gives loss of 0.2675234973670801\n",
      "The 11399 th iteration gives loss of 0.2675085729602819\n",
      "The 11400 th iteration gives loss of 0.2674936506152578\n",
      "The 11401 th iteration gives loss of 0.26747873033169317\n",
      "The 11402 th iteration gives loss of 0.2674638121093116\n",
      "The 11403 th iteration gives loss of 0.2674488959478197\n",
      "The 11404 th iteration gives loss of 0.2674339818469224\n",
      "The 11405 th iteration gives loss of 0.26741906980632957\n",
      "The 11406 th iteration gives loss of 0.2674041598257386\n",
      "The 11407 th iteration gives loss of 0.26738925190485247\n",
      "The 11408 th iteration gives loss of 0.2673743460433907\n",
      "The 11409 th iteration gives loss of 0.26735944224104646\n",
      "The 11410 th iteration gives loss of 0.2673445404975385\n",
      "The 11411 th iteration gives loss of 0.2673296408125779\n",
      "The 11412 th iteration gives loss of 0.2673147431858527\n",
      "The 11413 th iteration gives loss of 0.26729984761708175\n",
      "The 11414 th iteration gives loss of 0.26728495410596603\n",
      "The 11415 th iteration gives loss of 0.2672700626522176\n",
      "The 11416 th iteration gives loss of 0.26725517325554\n",
      "The 11417 th iteration gives loss of 0.26724028591564164\n",
      "The 11418 th iteration gives loss of 0.26722540063222594\n",
      "The 11419 th iteration gives loss of 0.26721051740500684\n",
      "The 11420 th iteration gives loss of 0.26719563623368164\n",
      "The 11421 th iteration gives loss of 0.26718075711796463\n",
      "The 11422 th iteration gives loss of 0.2671658800575571\n",
      "The 11423 th iteration gives loss of 0.2671510050521762\n",
      "The 11424 th iteration gives loss of 0.26713613210152\n",
      "The 11425 th iteration gives loss of 0.2671212612052909\n",
      "The 11426 th iteration gives loss of 0.2671063923632066\n",
      "The 11427 th iteration gives loss of 0.2670915255749707\n",
      "The 11428 th iteration gives loss of 0.2670766608402876\n",
      "The 11429 th iteration gives loss of 0.2670617981588757\n",
      "The 11430 th iteration gives loss of 0.267046937530427\n",
      "The 11431 th iteration gives loss of 0.2670320789546615\n",
      "The 11432 th iteration gives loss of 0.26701722243127307\n",
      "The 11433 th iteration gives loss of 0.2670023679599698\n",
      "The 11434 th iteration gives loss of 0.26698751554048006\n",
      "The 11435 th iteration gives loss of 0.2669726651724917\n",
      "The 11436 th iteration gives loss of 0.26695781685571385\n",
      "The 11437 th iteration gives loss of 0.26694297058985417\n",
      "The 11438 th iteration gives loss of 0.26692812637462776\n",
      "The 11439 th iteration gives loss of 0.26691328420974214\n",
      "The 11440 th iteration gives loss of 0.2668984440948996\n",
      "The 11441 th iteration gives loss of 0.2668836060297999\n",
      "The 11442 th iteration gives loss of 0.2668687700141635\n",
      "The 11443 th iteration gives loss of 0.2668539360476945\n",
      "The 11444 th iteration gives loss of 0.26683910413010703\n",
      "The 11445 th iteration gives loss of 0.2668242742610958\n",
      "The 11446 th iteration gives loss of 0.2668094464403752\n",
      "The 11447 th iteration gives loss of 0.26679462066764714\n",
      "The 11448 th iteration gives loss of 0.2667797969426366\n",
      "The 11449 th iteration gives loss of 0.26676497526502385\n",
      "The 11450 th iteration gives loss of 0.2667501556345446\n",
      "The 11451 th iteration gives loss of 0.2667353380508914\n",
      "The 11452 th iteration gives loss of 0.2667205225137787\n",
      "The 11453 th iteration gives loss of 0.2667057090229041\n",
      "The 11454 th iteration gives loss of 0.266690897577993\n",
      "The 11455 th iteration gives loss of 0.26667608817873384\n",
      "The 11456 th iteration gives loss of 0.26666128082485285\n",
      "The 11457 th iteration gives loss of 0.2666464755160391\n",
      "The 11458 th iteration gives loss of 0.2666316722520167\n",
      "The 11459 th iteration gives loss of 0.26661687103249204\n",
      "The 11460 th iteration gives loss of 0.2666020718571638\n",
      "The 11461 th iteration gives loss of 0.2665872747257526\n",
      "The 11462 th iteration gives loss of 0.26657247963796293\n",
      "The 11463 th iteration gives loss of 0.2665576865934945\n",
      "The 11464 th iteration gives loss of 0.266542895592067\n",
      "The 11465 th iteration gives loss of 0.26652810663338095\n",
      "The 11466 th iteration gives loss of 0.26651331971715186\n",
      "The 11467 th iteration gives loss of 0.2664985348430864\n",
      "The 11468 th iteration gives loss of 0.266483752010885\n",
      "The 11469 th iteration gives loss of 0.26646897122026375\n",
      "The 11470 th iteration gives loss of 0.2664541924709315\n",
      "The 11471 th iteration gives loss of 0.26643941576258817\n",
      "The 11472 th iteration gives loss of 0.26642464109495095\n",
      "The 11473 th iteration gives loss of 0.2664098684677341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 11474 th iteration gives loss of 0.26639509788063254\n",
      "The 11475 th iteration gives loss of 0.26638032933336486\n",
      "The 11476 th iteration gives loss of 0.26636556282563884\n",
      "The 11477 th iteration gives loss of 0.26635079835715747\n",
      "The 11478 th iteration gives loss of 0.26633603592763794\n",
      "The 11479 th iteration gives loss of 0.26632127553678697\n",
      "The 11480 th iteration gives loss of 0.26630651718430737\n",
      "The 11481 th iteration gives loss of 0.2662917608699098\n",
      "The 11482 th iteration gives loss of 0.26627700659330594\n",
      "The 11483 th iteration gives loss of 0.26626225435420237\n",
      "The 11484 th iteration gives loss of 0.26624750415231396\n",
      "The 11485 th iteration gives loss of 0.2662327559873469\n",
      "The 11486 th iteration gives loss of 0.2662180098590078\n",
      "The 11487 th iteration gives loss of 0.2662032657670086\n",
      "The 11488 th iteration gives loss of 0.2661885237110641\n",
      "The 11489 th iteration gives loss of 0.2661737836908704\n",
      "The 11490 th iteration gives loss of 0.26615904570614\n",
      "The 11491 th iteration gives loss of 0.26614430975659203\n",
      "The 11492 th iteration gives loss of 0.26612957584192803\n",
      "The 11493 th iteration gives loss of 0.26611484396185664\n",
      "The 11494 th iteration gives loss of 0.2661001141160945\n",
      "The 11495 th iteration gives loss of 0.26608538630433753\n",
      "The 11496 th iteration gives loss of 0.266070660526312\n",
      "The 11497 th iteration gives loss of 0.26605593678170636\n",
      "The 11498 th iteration gives loss of 0.26604121507025624\n",
      "The 11499 th iteration gives loss of 0.2660264953916585\n",
      "The 11500 th iteration gives loss of 0.2660117777456261\n",
      "The 11501 th iteration gives loss of 0.2659970621318549\n",
      "The 11502 th iteration gives loss of 0.2659823485500687\n",
      "The 11503 th iteration gives loss of 0.26596763699997183\n",
      "The 11504 th iteration gives loss of 0.2659529274812766\n",
      "The 11505 th iteration gives loss of 0.26593821999369155\n",
      "The 11506 th iteration gives loss of 0.26592351453692703\n",
      "The 11507 th iteration gives loss of 0.26590881111069165\n",
      "The 11508 th iteration gives loss of 0.2658941097146972\n",
      "The 11509 th iteration gives loss of 0.2658794103486523\n",
      "The 11510 th iteration gives loss of 0.26586471301227427\n",
      "The 11511 th iteration gives loss of 0.2658500177052592\n",
      "The 11512 th iteration gives loss of 0.2658353244273219\n",
      "The 11513 th iteration gives loss of 0.2658206331781814\n",
      "The 11514 th iteration gives loss of 0.2658059439575341\n",
      "The 11515 th iteration gives loss of 0.26579125676510085\n",
      "The 11516 th iteration gives loss of 0.2657765716005895\n",
      "The 11517 th iteration gives loss of 0.2657618884637097\n",
      "The 11518 th iteration gives loss of 0.2657472073541689\n",
      "The 11519 th iteration gives loss of 0.2657325282716854\n",
      "The 11520 th iteration gives loss of 0.2657178512159635\n",
      "The 11521 th iteration gives loss of 0.26570317618670897\n",
      "The 11522 th iteration gives loss of 0.265688503183638\n",
      "The 11523 th iteration gives loss of 0.26567383220645713\n",
      "The 11524 th iteration gives loss of 0.265659163254881\n",
      "The 11525 th iteration gives loss of 0.265644496328615\n",
      "The 11526 th iteration gives loss of 0.2656298314273812\n",
      "The 11527 th iteration gives loss of 0.2656151685508857\n",
      "The 11528 th iteration gives loss of 0.26560050769882737\n",
      "The 11529 th iteration gives loss of 0.26558584887092795\n",
      "The 11530 th iteration gives loss of 0.2655711920669014\n",
      "The 11531 th iteration gives loss of 0.2655565372864406\n",
      "The 11532 th iteration gives loss of 0.2655418845292804\n",
      "The 11533 th iteration gives loss of 0.26552723379511034\n",
      "The 11534 th iteration gives loss of 0.2655125850836601\n",
      "The 11535 th iteration gives loss of 0.2654979383946248\n",
      "The 11536 th iteration gives loss of 0.26548329372772034\n",
      "The 11537 th iteration gives loss of 0.26546865108266493\n",
      "The 11538 th iteration gives loss of 0.26545401045916184\n",
      "The 11539 th iteration gives loss of 0.26543937185692096\n",
      "The 11540 th iteration gives loss of 0.2654247352756593\n",
      "The 11541 th iteration gives loss of 0.2654101007150757\n",
      "The 11542 th iteration gives loss of 0.26539546817489595\n",
      "The 11543 th iteration gives loss of 0.26538083765482817\n",
      "The 11544 th iteration gives loss of 0.2653662091545753\n",
      "The 11545 th iteration gives loss of 0.26535158267385617\n",
      "The 11546 th iteration gives loss of 0.26533695821238157\n",
      "The 11547 th iteration gives loss of 0.2653223357698556\n",
      "The 11548 th iteration gives loss of 0.26530771534600367\n",
      "The 11549 th iteration gives loss of 0.2652930969405195\n",
      "The 11550 th iteration gives loss of 0.26527848055312314\n",
      "The 11551 th iteration gives loss of 0.26526386618353553\n",
      "The 11552 th iteration gives loss of 0.26524925383145576\n",
      "The 11553 th iteration gives loss of 0.2652346434965928\n",
      "The 11554 th iteration gives loss of 0.26522003517867204\n",
      "The 11555 th iteration gives loss of 0.26520542887739046\n",
      "The 11556 th iteration gives loss of 0.2651908245924603\n",
      "The 11557 th iteration gives loss of 0.2651762223236075\n",
      "The 11558 th iteration gives loss of 0.2651616220705331\n",
      "The 11559 th iteration gives loss of 0.2651470238329468\n",
      "The 11560 th iteration gives loss of 0.26513242761056516\n",
      "The 11561 th iteration gives loss of 0.26511783340310213\n",
      "The 11562 th iteration gives loss of 0.2651032412102628\n",
      "The 11563 th iteration gives loss of 0.2650886510317605\n",
      "The 11564 th iteration gives loss of 0.2650740628673131\n",
      "The 11565 th iteration gives loss of 0.26505947671662944\n",
      "The 11566 th iteration gives loss of 0.26504489257942077\n",
      "The 11567 th iteration gives loss of 0.2650303104553889\n",
      "The 11568 th iteration gives loss of 0.26501573034426645\n",
      "The 11569 th iteration gives loss of 0.2650011522457487\n",
      "The 11570 th iteration gives loss of 0.2649865761595542\n",
      "The 11571 th iteration gives loss of 0.2649720020853958\n",
      "The 11572 th iteration gives loss of 0.2649574300229797\n",
      "The 11573 th iteration gives loss of 0.2649428599720299\n",
      "The 11574 th iteration gives loss of 0.26492829193224593\n",
      "The 11575 th iteration gives loss of 0.2649137259033519\n",
      "The 11576 th iteration gives loss of 0.26489916188504836\n",
      "The 11577 th iteration gives loss of 0.26488459987704693\n",
      "The 11578 th iteration gives loss of 0.2648700398790662\n",
      "The 11579 th iteration gives loss of 0.2648554818908293\n",
      "The 11580 th iteration gives loss of 0.26484092591202724\n",
      "The 11581 th iteration gives loss of 0.26482637194238823\n",
      "The 11582 th iteration gives loss of 0.264811819981619\n",
      "The 11583 th iteration gives loss of 0.26479727002943126\n",
      "The 11584 th iteration gives loss of 0.2647827220855327\n",
      "The 11585 th iteration gives loss of 0.26476817614964787\n",
      "The 11586 th iteration gives loss of 0.2647536322214835\n",
      "The 11587 th iteration gives loss of 0.2647390903007489\n",
      "The 11588 th iteration gives loss of 0.2647245503871651\n",
      "The 11589 th iteration gives loss of 0.26471001248043385\n",
      "The 11590 th iteration gives loss of 0.2646954765802741\n",
      "The 11591 th iteration gives loss of 0.26468094268639697\n",
      "The 11592 th iteration gives loss of 0.26466641079851233\n",
      "The 11593 th iteration gives loss of 0.2646518809163458\n",
      "The 11594 th iteration gives loss of 0.264637353039598\n",
      "The 11595 th iteration gives loss of 0.26462282716798446\n",
      "The 11596 th iteration gives loss of 0.26460830330122265\n",
      "The 11597 th iteration gives loss of 0.2645937814390118\n",
      "The 11598 th iteration gives loss of 0.26457926158107864\n",
      "The 11599 th iteration gives loss of 0.26456474372713296\n",
      "The 11600 th iteration gives loss of 0.2645502278768894\n",
      "The 11601 th iteration gives loss of 0.2645357140300598\n",
      "The 11602 th iteration gives loss of 0.26452120218635883\n",
      "The 11603 th iteration gives loss of 0.2645066923454955\n",
      "The 11604 th iteration gives loss of 0.26449218450718354\n",
      "The 11605 th iteration gives loss of 0.26447767867113425\n",
      "The 11606 th iteration gives loss of 0.2644631748370639\n",
      "The 11607 th iteration gives loss of 0.2644486730046893\n",
      "The 11608 th iteration gives loss of 0.26443417317372425\n",
      "The 11609 th iteration gives loss of 0.26441967534387556\n",
      "The 11610 th iteration gives loss of 0.2644051795148546\n",
      "The 11611 th iteration gives loss of 0.2643906856863824\n",
      "The 11612 th iteration gives loss of 0.2643761938581761\n",
      "The 11613 th iteration gives loss of 0.2643617040299406\n",
      "The 11614 th iteration gives loss of 0.26434721620138396\n",
      "The 11615 th iteration gives loss of 0.2643327303722323\n",
      "The 11616 th iteration gives loss of 0.26431824654219677\n",
      "The 11617 th iteration gives loss of 0.2643037647109905\n",
      "The 11618 th iteration gives loss of 0.2642892848783224\n",
      "The 11619 th iteration gives loss of 0.264274807043914\n",
      "The 11620 th iteration gives loss of 0.26426033120746883\n",
      "The 11621 th iteration gives loss of 0.26424585736869993\n",
      "The 11622 th iteration gives loss of 0.26423138552733877\n",
      "The 11623 th iteration gives loss of 0.2642169156830892\n",
      "The 11624 th iteration gives loss of 0.26420244783565827\n",
      "The 11625 th iteration gives loss of 0.2641879819847708\n",
      "The 11626 th iteration gives loss of 0.2641735181301314\n",
      "The 11627 th iteration gives loss of 0.2641590562714502\n",
      "The 11628 th iteration gives loss of 0.2641445964084647\n",
      "The 11629 th iteration gives loss of 0.26413013854086176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 11630 th iteration gives loss of 0.2641156826683711\n",
      "The 11631 th iteration gives loss of 0.26410122879070685\n",
      "The 11632 th iteration gives loss of 0.26408677690757754\n",
      "The 11633 th iteration gives loss of 0.26407232701869715\n",
      "The 11634 th iteration gives loss of 0.26405787912378054\n",
      "The 11635 th iteration gives loss of 0.2640434332225402\n",
      "The 11636 th iteration gives loss of 0.2640289893146969\n",
      "The 11637 th iteration gives loss of 0.26401454739996544\n",
      "The 11638 th iteration gives loss of 0.2640001074780515\n",
      "The 11639 th iteration gives loss of 0.2639856695486756\n",
      "The 11640 th iteration gives loss of 0.2639712336115541\n",
      "The 11641 th iteration gives loss of 0.2639567996664012\n",
      "The 11642 th iteration gives loss of 0.263942367712918\n",
      "The 11643 th iteration gives loss of 0.2639279377508438\n",
      "The 11644 th iteration gives loss of 0.2639135097798702\n",
      "The 11645 th iteration gives loss of 0.26389908379972427\n",
      "The 11646 th iteration gives loss of 0.26388465981011083\n",
      "The 11647 th iteration gives loss of 0.2638702378107565\n",
      "The 11648 th iteration gives loss of 0.2638558178013722\n",
      "The 11649 th iteration gives loss of 0.26384139978166454\n",
      "The 11650 th iteration gives loss of 0.263826983751353\n",
      "The 11651 th iteration gives loss of 0.2638125697101602\n",
      "The 11652 th iteration gives loss of 0.263798157657801\n",
      "The 11653 th iteration gives loss of 0.2637837475939742\n",
      "The 11654 th iteration gives loss of 0.26376933951840215\n",
      "The 11655 th iteration gives loss of 0.2637549334307968\n",
      "The 11656 th iteration gives loss of 0.2637405293308985\n",
      "The 11657 th iteration gives loss of 0.2637261272183889\n",
      "The 11658 th iteration gives loss of 0.2637117270930071\n",
      "The 11659 th iteration gives loss of 0.2636973289544462\n",
      "The 11660 th iteration gives loss of 0.26368293280243726\n",
      "The 11661 th iteration gives loss of 0.2636685386366935\n",
      "The 11662 th iteration gives loss of 0.2636541464569218\n",
      "The 11663 th iteration gives loss of 0.26363975626284974\n",
      "The 11664 th iteration gives loss of 0.26362536805418096\n",
      "The 11665 th iteration gives loss of 0.2636109818306412\n",
      "The 11666 th iteration gives loss of 0.2635965975919361\n",
      "The 11667 th iteration gives loss of 0.2635822153377836\n",
      "The 11668 th iteration gives loss of 0.26356783506790843\n",
      "The 11669 th iteration gives loss of 0.26355345678201075\n",
      "The 11670 th iteration gives loss of 0.26353908047981367\n",
      "The 11671 th iteration gives loss of 0.26352470616103696\n",
      "The 11672 th iteration gives loss of 0.26351033382539624\n",
      "The 11673 th iteration gives loss of 0.26349596347259935\n",
      "The 11674 th iteration gives loss of 0.2634815951023647\n",
      "The 11675 th iteration gives loss of 0.26346722871440736\n",
      "The 11676 th iteration gives loss of 0.26345286430844767\n",
      "The 11677 th iteration gives loss of 0.26343850188418944\n",
      "The 11678 th iteration gives loss of 0.26342414144137\n",
      "The 11679 th iteration gives loss of 0.2634097829796845\n",
      "The 11680 th iteration gives loss of 0.2633954264988546\n",
      "The 11681 th iteration gives loss of 0.2633810719986007\n",
      "The 11682 th iteration gives loss of 0.2633667194786356\n",
      "The 11683 th iteration gives loss of 0.263352368938676\n",
      "The 11684 th iteration gives loss of 0.2633380203784396\n",
      "The 11685 th iteration gives loss of 0.26332367379764016\n",
      "The 11686 th iteration gives loss of 0.2633093291959854\n",
      "The 11687 th iteration gives loss of 0.26329498657320494\n",
      "The 11688 th iteration gives loss of 0.2632806459290111\n",
      "The 11689 th iteration gives loss of 0.2632663072631129\n",
      "The 11690 th iteration gives loss of 0.2632519705752362\n",
      "The 11691 th iteration gives loss of 0.26323763586509635\n",
      "The 11692 th iteration gives loss of 0.2632233031323978\n",
      "The 11693 th iteration gives loss of 0.2632089723768768\n",
      "The 11694 th iteration gives loss of 0.263194643598233\n",
      "The 11695 th iteration gives loss of 0.26318031679618575\n",
      "The 11696 th iteration gives loss of 0.26316599197045804\n",
      "The 11697 th iteration gives loss of 0.2631516691207575\n",
      "The 11698 th iteration gives loss of 0.26313734824680146\n",
      "The 11699 th iteration gives loss of 0.26312302934831555\n",
      "The 11700 th iteration gives loss of 0.2631087124250063\n",
      "The 11701 th iteration gives loss of 0.2630943974765972\n",
      "The 11702 th iteration gives loss of 0.26308008450279896\n",
      "The 11703 th iteration gives loss of 0.2630657735033325\n",
      "The 11704 th iteration gives loss of 0.2630514644779186\n",
      "The 11705 th iteration gives loss of 0.26303715742626826\n",
      "The 11706 th iteration gives loss of 0.26302285234809614\n",
      "The 11707 th iteration gives loss of 0.2630085492431162\n",
      "The 11708 th iteration gives loss of 0.26299424811105987\n",
      "The 11709 th iteration gives loss of 0.26297994895162446\n",
      "The 11710 th iteration gives loss of 0.2629656517645386\n",
      "The 11711 th iteration gives loss of 0.2629513565495175\n",
      "The 11712 th iteration gives loss of 0.26293706330628897\n",
      "The 11713 th iteration gives loss of 0.26292277203454634\n",
      "The 11714 th iteration gives loss of 0.26290848273402523\n",
      "The 11715 th iteration gives loss of 0.26289419540443254\n",
      "The 11716 th iteration gives loss of 0.26287991004548866\n",
      "The 11717 th iteration gives loss of 0.2628656266569145\n",
      "The 11718 th iteration gives loss of 0.2628513452384268\n",
      "The 11719 th iteration gives loss of 0.2628370657897383\n",
      "The 11720 th iteration gives loss of 0.2628227883105643\n",
      "The 11721 th iteration gives loss of 0.26280851280062995\n",
      "The 11722 th iteration gives loss of 0.26279423925964\n",
      "The 11723 th iteration gives loss of 0.26277996768732276\n",
      "The 11724 th iteration gives loss of 0.26276569808339695\n",
      "The 11725 th iteration gives loss of 0.26275143044756877\n",
      "The 11726 th iteration gives loss of 0.26273716477956366\n",
      "The 11727 th iteration gives loss of 0.2627229010791018\n",
      "The 11728 th iteration gives loss of 0.2627086393458912\n",
      "The 11729 th iteration gives loss of 0.2626943795796633\n",
      "The 11730 th iteration gives loss of 0.2626801217801172\n",
      "The 11731 th iteration gives loss of 0.26266586594699515\n",
      "The 11732 th iteration gives loss of 0.26265161207998483\n",
      "The 11733 th iteration gives loss of 0.2626373601788162\n",
      "The 11734 th iteration gives loss of 0.2626231102432158\n",
      "The 11735 th iteration gives loss of 0.262608862272903\n",
      "The 11736 th iteration gives loss of 0.26259461626757735\n",
      "The 11737 th iteration gives loss of 0.2625803722269703\n",
      "The 11738 th iteration gives loss of 0.2625661301508036\n",
      "The 11739 th iteration gives loss of 0.26255189003877194\n",
      "The 11740 th iteration gives loss of 0.2625376518906193\n",
      "The 11741 th iteration gives loss of 0.2625234157060442\n",
      "The 11742 th iteration gives loss of 0.2625091814847825\n",
      "The 11743 th iteration gives loss of 0.2624949492265404\n",
      "The 11744 th iteration gives loss of 0.26248071893103947\n",
      "The 11745 th iteration gives loss of 0.26246649059800375\n",
      "The 11746 th iteration gives loss of 0.26245226422713747\n",
      "The 11747 th iteration gives loss of 0.2624380398181655\n",
      "The 11748 th iteration gives loss of 0.2624238173708044\n",
      "The 11749 th iteration gives loss of 0.2624095968847879\n",
      "The 11750 th iteration gives loss of 0.2623953783598003\n",
      "The 11751 th iteration gives loss of 0.2623811617955877\n",
      "The 11752 th iteration gives loss of 0.26236694719186066\n",
      "The 11753 th iteration gives loss of 0.2623527345483387\n",
      "The 11754 th iteration gives loss of 0.26233852386474077\n",
      "The 11755 th iteration gives loss of 0.2623243151407851\n",
      "The 11756 th iteration gives loss of 0.2623101083761824\n",
      "The 11757 th iteration gives loss of 0.2622959035706587\n",
      "The 11758 th iteration gives loss of 0.2622817007239247\n",
      "The 11759 th iteration gives loss of 0.26226749983571407\n",
      "The 11760 th iteration gives loss of 0.26225330090573473\n",
      "The 11761 th iteration gives loss of 0.26223910393370986\n",
      "The 11762 th iteration gives loss of 0.26222490891934863\n",
      "The 11763 th iteration gives loss of 0.26221071586238115\n",
      "The 11764 th iteration gives loss of 0.2621965247625227\n",
      "The 11765 th iteration gives loss of 0.26218233561948784\n",
      "The 11766 th iteration gives loss of 0.2621681484329945\n",
      "The 11767 th iteration gives loss of 0.2621539632027731\n",
      "The 11768 th iteration gives loss of 0.2621397799285244\n",
      "The 11769 th iteration gives loss of 0.2621255986099718\n",
      "The 11770 th iteration gives loss of 0.26211141924684944\n",
      "The 11771 th iteration gives loss of 0.2620972418388622\n",
      "The 11772 th iteration gives loss of 0.26208306638573803\n",
      "The 11773 th iteration gives loss of 0.26206889288718366\n",
      "The 11774 th iteration gives loss of 0.26205472134293856\n",
      "The 11775 th iteration gives loss of 0.26204055175270125\n",
      "The 11776 th iteration gives loss of 0.2620263841161989\n",
      "The 11777 th iteration gives loss of 0.2620122184331432\n",
      "The 11778 th iteration gives loss of 0.2619980547032598\n",
      "The 11779 th iteration gives loss of 0.2619838929262778\n",
      "The 11780 th iteration gives loss of 0.26196973310190136\n",
      "The 11781 th iteration gives loss of 0.26195557522985713\n",
      "The 11782 th iteration gives loss of 0.2619414193098614\n",
      "The 11783 th iteration gives loss of 0.2619272653416367\n",
      "The 11784 th iteration gives loss of 0.2619131133248915\n",
      "The 11785 th iteration gives loss of 0.26189896325936324\n",
      "The 11786 th iteration gives loss of 0.2618848151447579\n",
      "The 11787 th iteration gives loss of 0.26187066898079653\n",
      "The 11788 th iteration gives loss of 0.2618565247671972\n",
      "The 11789 th iteration gives loss of 0.2618423825036885\n",
      "The 11790 th iteration gives loss of 0.26182824218998635\n",
      "The 11791 th iteration gives loss of 0.2618141038258089\n",
      "The 11792 th iteration gives loss of 0.26179996741087525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 11793 th iteration gives loss of 0.26178583294490354\n",
      "The 11794 th iteration gives loss of 0.2617717004276128\n",
      "The 11795 th iteration gives loss of 0.26175756985872495\n",
      "The 11796 th iteration gives loss of 0.26174344123796744\n",
      "The 11797 th iteration gives loss of 0.26172931456504417\n",
      "The 11798 th iteration gives loss of 0.26171518983969444\n",
      "The 11799 th iteration gives loss of 0.26170106706161755\n",
      "The 11800 th iteration gives loss of 0.2616869462305453\n",
      "The 11801 th iteration gives loss of 0.26167282734619085\n",
      "The 11802 th iteration gives loss of 0.2616587104082785\n",
      "The 11803 th iteration gives loss of 0.2616445954165382\n",
      "The 11804 th iteration gives loss of 0.26163048237067527\n",
      "The 11805 th iteration gives loss of 0.26161637127041376\n",
      "The 11806 th iteration gives loss of 0.2616022621154723\n",
      "The 11807 th iteration gives loss of 0.2615881549055779\n",
      "The 11808 th iteration gives loss of 0.2615740496404442\n",
      "The 11809 th iteration gives loss of 0.26155994631979557\n",
      "The 11810 th iteration gives loss of 0.2615458449433468\n",
      "The 11811 th iteration gives loss of 0.2615317455108166\n",
      "The 11812 th iteration gives loss of 0.26151764802194066\n",
      "The 11813 th iteration gives loss of 0.26150355247642104\n",
      "The 11814 th iteration gives loss of 0.2614894588739886\n",
      "The 11815 th iteration gives loss of 0.26147536721435444\n",
      "The 11816 th iteration gives loss of 0.2614612774972531\n",
      "The 11817 th iteration gives loss of 0.2614471897223954\n",
      "The 11818 th iteration gives loss of 0.26143310388950847\n",
      "The 11819 th iteration gives loss of 0.26141901999830874\n",
      "The 11820 th iteration gives loss of 0.26140493804850934\n",
      "The 11821 th iteration gives loss of 0.2613908580398356\n",
      "The 11822 th iteration gives loss of 0.26137677997201586\n",
      "The 11823 th iteration gives loss of 0.26136270384475646\n",
      "The 11824 th iteration gives loss of 0.26134862965779576\n",
      "The 11825 th iteration gives loss of 0.2613345574108491\n",
      "The 11826 th iteration gives loss of 0.26132048710361805\n",
      "The 11827 th iteration gives loss of 0.26130641873585303\n",
      "The 11828 th iteration gives loss of 0.26129235230725845\n",
      "The 11829 th iteration gives loss of 0.26127828781755913\n",
      "The 11830 th iteration gives loss of 0.2612642252664783\n",
      "The 11831 th iteration gives loss of 0.26125016465372514\n",
      "The 11832 th iteration gives loss of 0.26123610597903474\n",
      "The 11833 th iteration gives loss of 0.26122204924211984\n",
      "The 11834 th iteration gives loss of 0.2612079944426969\n",
      "The 11835 th iteration gives loss of 0.2611939415804946\n",
      "The 11836 th iteration gives loss of 0.26117989065524\n",
      "The 11837 th iteration gives loss of 0.2611658416666403\n",
      "The 11838 th iteration gives loss of 0.26115179461443366\n",
      "The 11839 th iteration gives loss of 0.26113774949832647\n",
      "The 11840 th iteration gives loss of 0.26112370631804455\n",
      "The 11841 th iteration gives loss of 0.261109665073315\n",
      "The 11842 th iteration gives loss of 0.2610956257638488\n",
      "The 11843 th iteration gives loss of 0.26108158838937273\n",
      "The 11844 th iteration gives loss of 0.2610675529496109\n",
      "The 11845 th iteration gives loss of 0.2610535194442767\n",
      "The 11846 th iteration gives loss of 0.2610394878731108\n",
      "The 11847 th iteration gives loss of 0.261025458235805\n",
      "The 11848 th iteration gives loss of 0.2610114305320971\n",
      "The 11849 th iteration gives loss of 0.2609974047617038\n",
      "The 11850 th iteration gives loss of 0.26098338092436013\n",
      "The 11851 th iteration gives loss of 0.2609693590197738\n",
      "The 11852 th iteration gives loss of 0.26095533904767215\n",
      "The 11853 th iteration gives loss of 0.2609413210077711\n",
      "The 11854 th iteration gives loss of 0.2609273048998036\n",
      "The 11855 th iteration gives loss of 0.26091329072348424\n",
      "The 11856 th iteration gives loss of 0.2608992784785246\n",
      "The 11857 th iteration gives loss of 0.2608852681646644\n",
      "The 11858 th iteration gives loss of 0.2608712597816248\n",
      "The 11859 th iteration gives loss of 0.260857253329117\n",
      "The 11860 th iteration gives loss of 0.26084324880686893\n",
      "The 11861 th iteration gives loss of 0.2608292462145975\n",
      "The 11862 th iteration gives loss of 0.26081524555202673\n",
      "The 11863 th iteration gives loss of 0.26080124681887884\n",
      "The 11864 th iteration gives loss of 0.2607872500148789\n",
      "The 11865 th iteration gives loss of 0.26077325513974986\n",
      "The 11866 th iteration gives loss of 0.2607592621932094\n",
      "The 11867 th iteration gives loss of 0.2607452711749769\n",
      "The 11868 th iteration gives loss of 0.26073128208478624\n",
      "The 11869 th iteration gives loss of 0.2607172949223459\n",
      "The 11870 th iteration gives loss of 0.2607033096873824\n",
      "The 11871 th iteration gives loss of 0.2606893263796212\n",
      "The 11872 th iteration gives loss of 0.26067534499879363\n",
      "The 11873 th iteration gives loss of 0.2606613655446041\n",
      "The 11874 th iteration gives loss of 0.26064738801678516\n",
      "The 11875 th iteration gives loss of 0.2606334124150531\n",
      "The 11876 th iteration gives loss of 0.2606194387391369\n",
      "The 11877 th iteration gives loss of 0.260605466988753\n",
      "The 11878 th iteration gives loss of 0.2605914971636243\n",
      "The 11879 th iteration gives loss of 0.26057752926347805\n",
      "The 11880 th iteration gives loss of 0.2605635632880397\n",
      "The 11881 th iteration gives loss of 0.26054959923702764\n",
      "The 11882 th iteration gives loss of 0.2605356371101667\n",
      "The 11883 th iteration gives loss of 0.2605216769071755\n",
      "The 11884 th iteration gives loss of 0.2605077186277759\n",
      "The 11885 th iteration gives loss of 0.26049376227169824\n",
      "The 11886 th iteration gives loss of 0.2604798078386501\n",
      "The 11887 th iteration gives loss of 0.2604658553283722\n",
      "The 11888 th iteration gives loss of 0.2604519047405747\n",
      "The 11889 th iteration gives loss of 0.2604379560749882\n",
      "The 11890 th iteration gives loss of 0.260424009331333\n",
      "The 11891 th iteration gives loss of 0.2604100645093363\n",
      "The 11892 th iteration gives loss of 0.2603961216087091\n",
      "The 11893 th iteration gives loss of 0.26038218062918944\n",
      "The 11894 th iteration gives loss of 0.2603682415704929\n",
      "The 11895 th iteration gives loss of 0.2603543044323325\n",
      "The 11896 th iteration gives loss of 0.2603403692144471\n",
      "The 11897 th iteration gives loss of 0.2603264359165537\n",
      "The 11898 th iteration gives loss of 0.2603125045383799\n",
      "The 11899 th iteration gives loss of 0.26029857507964277\n",
      "The 11900 th iteration gives loss of 0.26028464754007025\n",
      "The 11901 th iteration gives loss of 0.2602707219193864\n",
      "The 11902 th iteration gives loss of 0.26025679821730385\n",
      "The 11903 th iteration gives loss of 0.26024287643355937\n",
      "The 11904 th iteration gives loss of 0.260228956567866\n",
      "The 11905 th iteration gives loss of 0.2602150386199555\n",
      "The 11906 th iteration gives loss of 0.2602011225895506\n",
      "The 11907 th iteration gives loss of 0.2601872084763612\n",
      "The 11908 th iteration gives loss of 0.26017329628012825\n",
      "The 11909 th iteration gives loss of 0.26015938600056643\n",
      "The 11910 th iteration gives loss of 0.2601454776374048\n",
      "The 11911 th iteration gives loss of 0.2601315711903661\n",
      "The 11912 th iteration gives loss of 0.26011766665916797\n",
      "The 11913 th iteration gives loss of 0.2601037640435358\n",
      "The 11914 th iteration gives loss of 0.2600898633431959\n",
      "The 11915 th iteration gives loss of 0.26007596455787657\n",
      "The 11916 th iteration gives loss of 0.26006206768729456\n",
      "The 11917 th iteration gives loss of 0.2600481727311824\n",
      "The 11918 th iteration gives loss of 0.260034279689246\n",
      "The 11919 th iteration gives loss of 0.2600203885612264\n",
      "The 11920 th iteration gives loss of 0.26000649934682785\n",
      "The 11921 th iteration gives loss of 0.25999261204580565\n",
      "The 11922 th iteration gives loss of 0.2599787266578608\n",
      "The 11923 th iteration gives loss of 0.2599648431827194\n",
      "The 11924 th iteration gives loss of 0.2599509616201166\n",
      "The 11925 th iteration gives loss of 0.25993708196976484\n",
      "The 11926 th iteration gives loss of 0.25992320423139126\n",
      "The 11927 th iteration gives loss of 0.2599093284047192\n",
      "The 11928 th iteration gives loss of 0.25989545448948054\n",
      "The 11929 th iteration gives loss of 0.25988158248539595\n",
      "The 11930 th iteration gives loss of 0.2598677123921757\n",
      "The 11931 th iteration gives loss of 0.259853844209573\n",
      "The 11932 th iteration gives loss of 0.25983997793728414\n",
      "The 11933 th iteration gives loss of 0.259826113575046\n",
      "The 11934 th iteration gives loss of 0.2598122511225771\n",
      "The 11935 th iteration gives loss of 0.2597983905796109\n",
      "The 11936 th iteration gives loss of 0.2597845319458691\n",
      "The 11937 th iteration gives loss of 0.2597706752210742\n",
      "The 11938 th iteration gives loss of 0.2597568204049449\n",
      "The 11939 th iteration gives loss of 0.2597429674972171\n",
      "The 11940 th iteration gives loss of 0.2597291164976099\n",
      "The 11941 th iteration gives loss of 0.2597152674058492\n",
      "The 11942 th iteration gives loss of 0.2597014202216591\n",
      "The 11943 th iteration gives loss of 0.259687574944755\n",
      "The 11944 th iteration gives loss of 0.25967373157487733\n",
      "The 11945 th iteration gives loss of 0.25965989011174634\n",
      "The 11946 th iteration gives loss of 0.2596460505550835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 11947 th iteration gives loss of 0.2596322129046099\n",
      "The 11948 th iteration gives loss of 0.2596183771600623\n",
      "The 11949 th iteration gives loss of 0.2596045433211644\n",
      "The 11950 th iteration gives loss of 0.2595907113876255\n",
      "The 11951 th iteration gives loss of 0.25957688135917906\n",
      "The 11952 th iteration gives loss of 0.2595630532355582\n",
      "The 11953 th iteration gives loss of 0.2595492270164795\n",
      "The 11954 th iteration gives loss of 0.259535402701666\n",
      "The 11955 th iteration gives loss of 0.25952158029084976\n",
      "The 11956 th iteration gives loss of 0.25950775978374674\n",
      "The 11957 th iteration gives loss of 0.2594939411800926\n",
      "The 11958 th iteration gives loss of 0.25948012447961133\n",
      "The 11959 th iteration gives loss of 0.2594663096820213\n",
      "The 11960 th iteration gives loss of 0.2594524967870519\n",
      "The 11961 th iteration gives loss of 0.25943868579442686\n",
      "The 11962 th iteration gives loss of 0.259424876703887\n",
      "The 11963 th iteration gives loss of 0.2594110695151296\n",
      "The 11964 th iteration gives loss of 0.25939726422789877\n",
      "The 11965 th iteration gives loss of 0.25938346084191294\n",
      "The 11966 th iteration gives loss of 0.25936965935689305\n",
      "The 11967 th iteration gives loss of 0.25935585977257963\n",
      "The 11968 th iteration gives loss of 0.259342062088691\n",
      "The 11969 th iteration gives loss of 0.25932826630494665\n",
      "The 11970 th iteration gives loss of 0.259314472421078\n",
      "The 11971 th iteration gives loss of 0.2593006804368146\n",
      "The 11972 th iteration gives loss of 0.2592868903518792\n",
      "The 11973 th iteration gives loss of 0.25927310216599275\n",
      "The 11974 th iteration gives loss of 0.2592593158788797\n",
      "The 11975 th iteration gives loss of 0.2592455314902735\n",
      "The 11976 th iteration gives loss of 0.25923174899989765\n",
      "The 11977 th iteration gives loss of 0.2592179684074803\n",
      "The 11978 th iteration gives loss of 0.25920418971274295\n",
      "The 11979 th iteration gives loss of 0.2591904129154102\n",
      "The 11980 th iteration gives loss of 0.25917663801521207\n",
      "The 11981 th iteration gives loss of 0.259162865011862\n",
      "The 11982 th iteration gives loss of 0.2591490939051119\n",
      "The 11983 th iteration gives loss of 0.25913532469467226\n",
      "The 11984 th iteration gives loss of 0.25912155738026754\n",
      "The 11985 th iteration gives loss of 0.2591077919616238\n",
      "The 11986 th iteration gives loss of 0.25909402843847346\n",
      "The 11987 th iteration gives loss of 0.2590802668105342\n",
      "The 11988 th iteration gives loss of 0.25906650707754264\n",
      "The 11989 th iteration gives loss of 0.2590527492392135\n",
      "The 11990 th iteration gives loss of 0.25903899329528257\n",
      "The 11991 th iteration gives loss of 0.2590252392454727\n",
      "The 11992 th iteration gives loss of 0.25901148708951066\n",
      "The 11993 th iteration gives loss of 0.2589977368271227\n",
      "The 11994 th iteration gives loss of 0.2589839884580306\n",
      "The 11995 th iteration gives loss of 0.2589702419819654\n",
      "The 11996 th iteration gives loss of 0.2589564973986531\n",
      "The 11997 th iteration gives loss of 0.2589427547078259\n",
      "The 11998 th iteration gives loss of 0.2589290139092047\n",
      "The 11999 th iteration gives loss of 0.25891527500251327\n",
      "The 12000 th iteration gives loss of 0.2589015379874879\n",
      "The 12001 th iteration gives loss of 0.2588878028638395\n",
      "The 12002 th iteration gives loss of 0.25887406963130616\n",
      "The 12003 th iteration gives loss of 0.258860338289618\n",
      "The 12004 th iteration gives loss of 0.258846608838481\n",
      "The 12005 th iteration gives loss of 0.2588328812776541\n",
      "The 12006 th iteration gives loss of 0.2588191556068277\n",
      "The 12007 th iteration gives loss of 0.25880543182576643\n",
      "The 12008 th iteration gives loss of 0.25879170993417805\n",
      "The 12009 th iteration gives loss of 0.2587779899317836\n",
      "The 12010 th iteration gives loss of 0.25876427181831757\n",
      "The 12011 th iteration gives loss of 0.2587505555935102\n",
      "The 12012 th iteration gives loss of 0.2587368412570807\n",
      "The 12013 th iteration gives loss of 0.2587231288087605\n",
      "The 12014 th iteration gives loss of 0.258709418248278\n",
      "The 12015 th iteration gives loss of 0.2586957095753551\n",
      "The 12016 th iteration gives loss of 0.2586820027897182\n",
      "The 12017 th iteration gives loss of 0.25866829789111007\n",
      "The 12018 th iteration gives loss of 0.2586545948792389\n",
      "The 12019 th iteration gives loss of 0.25864089375384364\n",
      "The 12020 th iteration gives loss of 0.25862719451464083\n",
      "The 12021 th iteration gives loss of 0.25861349716137777\n",
      "The 12022 th iteration gives loss of 0.2585998016937531\n",
      "The 12023 th iteration gives loss of 0.2585861081115156\n",
      "The 12024 th iteration gives loss of 0.2585724164143854\n",
      "The 12025 th iteration gives loss of 0.2585587266020973\n",
      "The 12026 th iteration gives loss of 0.25854503867437123\n",
      "The 12027 th iteration gives loss of 0.258531352630941\n",
      "The 12028 th iteration gives loss of 0.25851766847151314\n",
      "The 12029 th iteration gives loss of 0.2585039861958385\n",
      "The 12030 th iteration gives loss of 0.2584903058036433\n",
      "The 12031 th iteration gives loss of 0.2584766272946419\n",
      "The 12032 th iteration gives loss of 0.25846295066857944\n",
      "The 12033 th iteration gives loss of 0.258449275925172\n",
      "The 12034 th iteration gives loss of 0.258435603064141\n",
      "The 12035 th iteration gives loss of 0.2584219320852269\n",
      "The 12036 th iteration gives loss of 0.258408262988154\n",
      "The 12037 th iteration gives loss of 0.2583945957726512\n",
      "The 12038 th iteration gives loss of 0.2583809304384398\n",
      "The 12039 th iteration gives loss of 0.2583672669852574\n",
      "The 12040 th iteration gives loss of 0.25835360541282143\n",
      "The 12041 th iteration gives loss of 0.2583399457208747\n",
      "The 12042 th iteration gives loss of 0.25832628790912654\n",
      "The 12043 th iteration gives loss of 0.2583126319773169\n",
      "The 12044 th iteration gives loss of 0.25829897792517387\n",
      "The 12045 th iteration gives loss of 0.2582853257524275\n",
      "The 12046 th iteration gives loss of 0.25827167545880053\n",
      "The 12047 th iteration gives loss of 0.2582580270440132\n",
      "The 12048 th iteration gives loss of 0.2582443805078174\n",
      "The 12049 th iteration gives loss of 0.2582307358499121\n",
      "The 12050 th iteration gives loss of 0.25821709307005536\n",
      "The 12051 th iteration gives loss of 0.2582034521679444\n",
      "The 12052 th iteration gives loss of 0.25818981314333833\n",
      "The 12053 th iteration gives loss of 0.2581761759959442\n",
      "The 12054 th iteration gives loss of 0.25816254072550004\n",
      "The 12055 th iteration gives loss of 0.2581489073317297\n",
      "The 12056 th iteration gives loss of 0.25813527581435985\n",
      "The 12057 th iteration gives loss of 0.258121646173124\n",
      "The 12058 th iteration gives loss of 0.25810801840775316\n",
      "The 12059 th iteration gives loss of 0.25809439251796823\n",
      "The 12060 th iteration gives loss of 0.25808076850351325\n",
      "The 12061 th iteration gives loss of 0.25806714636409456\n",
      "The 12062 th iteration gives loss of 0.2580535260994547\n",
      "The 12063 th iteration gives loss of 0.2580399077093188\n",
      "The 12064 th iteration gives loss of 0.258026291193415\n",
      "The 12065 th iteration gives loss of 0.2580126765514754\n",
      "The 12066 th iteration gives loss of 0.257999063783225\n",
      "The 12067 th iteration gives loss of 0.25798545288839514\n",
      "The 12068 th iteration gives loss of 0.2579718438667099\n",
      "The 12069 th iteration gives loss of 0.2579582367179094\n",
      "The 12070 th iteration gives loss of 0.2579446314417158\n",
      "The 12071 th iteration gives loss of 0.2579310280378542\n",
      "The 12072 th iteration gives loss of 0.25791742650606403\n",
      "The 12073 th iteration gives loss of 0.25790382684606\n",
      "The 12074 th iteration gives loss of 0.25789022905757913\n",
      "The 12075 th iteration gives loss of 0.25787663314034986\n",
      "The 12076 th iteration gives loss of 0.257863039094103\n",
      "The 12077 th iteration gives loss of 0.2578494469185698\n",
      "The 12078 th iteration gives loss of 0.25783585661348063\n",
      "The 12079 th iteration gives loss of 0.25782226817855874\n",
      "The 12080 th iteration gives loss of 0.25780868161352744\n",
      "The 12081 th iteration gives loss of 0.25779509691813257\n",
      "The 12082 th iteration gives loss of 0.25778151409208794\n",
      "The 12083 th iteration gives loss of 0.2577679331351252\n",
      "The 12084 th iteration gives loss of 0.25775435404698727\n",
      "The 12085 th iteration gives loss of 0.2577407768273928\n",
      "The 12086 th iteration gives loss of 0.2577272014760693\n",
      "The 12087 th iteration gives loss of 0.2577136279927576\n",
      "The 12088 th iteration gives loss of 0.2577000563771744\n",
      "The 12089 th iteration gives loss of 0.2576864866290567\n",
      "The 12090 th iteration gives loss of 0.2576729187481324\n",
      "The 12091 th iteration gives loss of 0.2576593527341277\n",
      "The 12092 th iteration gives loss of 0.25764578858678666\n",
      "The 12093 th iteration gives loss of 0.25763222630581906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 12094 th iteration gives loss of 0.2576186658909608\n",
      "The 12095 th iteration gives loss of 0.25760510734194003\n",
      "The 12096 th iteration gives loss of 0.25759155065850775\n",
      "The 12097 th iteration gives loss of 0.2575779958403558\n",
      "The 12098 th iteration gives loss of 0.25756444288725\n",
      "The 12099 th iteration gives loss of 0.2575508917989075\n",
      "The 12100 th iteration gives loss of 0.25753734257505034\n",
      "The 12101 th iteration gives loss of 0.25752379521541563\n",
      "The 12102 th iteration gives loss of 0.2575102497197376\n",
      "The 12103 th iteration gives loss of 0.2574967060877355\n",
      "The 12104 th iteration gives loss of 0.2574831643191526\n",
      "The 12105 th iteration gives loss of 0.25746962441370164\n",
      "The 12106 th iteration gives loss of 0.2574560863711156\n",
      "The 12107 th iteration gives loss of 0.25744255019114815\n",
      "The 12108 th iteration gives loss of 0.25742901587350214\n",
      "The 12109 th iteration gives loss of 0.25741548341792625\n",
      "The 12110 th iteration gives loss of 0.2574019528241427\n",
      "The 12111 th iteration gives loss of 0.2573884240918797\n",
      "The 12112 th iteration gives loss of 0.2573748972208734\n",
      "The 12113 th iteration gives loss of 0.2573613722108545\n",
      "The 12114 th iteration gives loss of 0.25734784906153896\n",
      "The 12115 th iteration gives loss of 0.25733432777267523\n",
      "The 12116 th iteration gives loss of 0.2573208083439843\n",
      "The 12117 th iteration gives loss of 0.2573072907751925\n",
      "The 12118 th iteration gives loss of 0.25729377506605033\n",
      "The 12119 th iteration gives loss of 0.2572802612162683\n",
      "The 12120 th iteration gives loss of 0.2572667492255869\n",
      "The 12121 th iteration gives loss of 0.2572532390937342\n",
      "The 12122 th iteration gives loss of 0.2572397308204382\n",
      "The 12123 th iteration gives loss of 0.257226224405437\n",
      "The 12124 th iteration gives loss of 0.25721271984845506\n",
      "The 12125 th iteration gives loss of 0.2571992171492186\n",
      "The 12126 th iteration gives loss of 0.2571857163074708\n",
      "The 12127 th iteration gives loss of 0.25717221732293283\n",
      "The 12128 th iteration gives loss of 0.2571587201953407\n",
      "The 12129 th iteration gives loss of 0.25714522492442027\n",
      "The 12130 th iteration gives loss of 0.25713173150991336\n",
      "The 12131 th iteration gives loss of 0.257118239951539\n",
      "The 12132 th iteration gives loss of 0.25710475024903023\n",
      "The 12133 th iteration gives loss of 0.2570912624021261\n",
      "The 12134 th iteration gives loss of 0.25707777641054996\n",
      "The 12135 th iteration gives loss of 0.25706429227402755\n",
      "The 12136 th iteration gives loss of 0.2570508099923114\n",
      "The 12137 th iteration gives loss of 0.25703732956510433\n",
      "The 12138 th iteration gives loss of 0.2570238509921627\n",
      "The 12139 th iteration gives loss of 0.2570103742732074\n",
      "The 12140 th iteration gives loss of 0.2569968994079636\n",
      "The 12141 th iteration gives loss of 0.2569834263961743\n",
      "The 12142 th iteration gives loss of 0.2569699552375612\n",
      "The 12143 th iteration gives loss of 0.2569564859318607\n",
      "The 12144 th iteration gives loss of 0.25694301847880585\n",
      "The 12145 th iteration gives loss of 0.25692955287812236\n",
      "The 12146 th iteration gives loss of 0.25691608912954556\n",
      "The 12147 th iteration gives loss of 0.2569026272328112\n",
      "The 12148 th iteration gives loss of 0.25688916718763805\n",
      "The 12149 th iteration gives loss of 0.25687570899377343\n",
      "The 12150 th iteration gives loss of 0.2568622526509368\n",
      "The 12151 th iteration gives loss of 0.2568487981588665\n",
      "The 12152 th iteration gives loss of 0.2568353455172863\n",
      "The 12153 th iteration gives loss of 0.2568218947259363\n",
      "The 12154 th iteration gives loss of 0.2568084457845434\n",
      "The 12155 th iteration gives loss of 0.25679499869284317\n",
      "The 12156 th iteration gives loss of 0.2567815534505668\n",
      "The 12157 th iteration gives loss of 0.2567681100574514\n",
      "The 12158 th iteration gives loss of 0.2567546685132126\n",
      "The 12159 th iteration gives loss of 0.2567412288175989\n",
      "The 12160 th iteration gives loss of 0.2567277909703305\n",
      "The 12161 th iteration gives loss of 0.25671435497115136\n",
      "The 12162 th iteration gives loss of 0.2567009208197882\n",
      "The 12163 th iteration gives loss of 0.2566874885159554\n",
      "The 12164 th iteration gives loss of 0.2566740580594215\n",
      "The 12165 th iteration gives loss of 0.25666062944988693\n",
      "The 12166 th iteration gives loss of 0.2566472026870987\n",
      "The 12167 th iteration gives loss of 0.25663377777078306\n",
      "The 12168 th iteration gives loss of 0.2566203547006806\n",
      "The 12169 th iteration gives loss of 0.25660693347651353\n",
      "The 12170 th iteration gives loss of 0.2565935140980213\n",
      "The 12171 th iteration gives loss of 0.2565800965649342\n",
      "The 12172 th iteration gives loss of 0.25656668087697665\n",
      "The 12173 th iteration gives loss of 0.25655326703389697\n",
      "The 12174 th iteration gives loss of 0.2565398550354166\n",
      "The 12175 th iteration gives loss of 0.25652644488126597\n",
      "The 12176 th iteration gives loss of 0.2565130365711837\n",
      "The 12177 th iteration gives loss of 0.2564996301049084\n",
      "The 12178 th iteration gives loss of 0.25648622548215055\n",
      "The 12179 th iteration gives loss of 0.25647282270266547\n",
      "The 12180 th iteration gives loss of 0.25645942176617215\n",
      "The 12181 th iteration gives loss of 0.2564460226724114\n",
      "The 12182 th iteration gives loss of 0.2564326254211184\n",
      "The 12183 th iteration gives loss of 0.2564192300120105\n",
      "The 12184 th iteration gives loss of 0.25640583644483217\n",
      "The 12185 th iteration gives loss of 0.2563924447193155\n",
      "The 12186 th iteration gives loss of 0.2563790548351946\n",
      "The 12187 th iteration gives loss of 0.2563656667922017\n",
      "The 12188 th iteration gives loss of 0.2563522805900659\n",
      "The 12189 th iteration gives loss of 0.25633889622851613\n",
      "The 12190 th iteration gives loss of 0.2563255137072993\n",
      "The 12191 th iteration gives loss of 0.2563121330261447\n",
      "The 12192 th iteration gives loss of 0.2562987541847701\n",
      "The 12193 th iteration gives loss of 0.2562853771829255\n",
      "The 12194 th iteration gives loss of 0.2562720020203374\n",
      "The 12195 th iteration gives loss of 0.25625862869673227\n",
      "The 12196 th iteration gives loss of 0.25624525721186237\n",
      "The 12197 th iteration gives loss of 0.2562318875654342\n",
      "The 12198 th iteration gives loss of 0.25621851975720944\n",
      "The 12199 th iteration gives loss of 0.25620515378690084\n",
      "The 12200 th iteration gives loss of 0.25619178965424577\n",
      "The 12201 th iteration gives loss of 0.2561784273589875\n",
      "The 12202 th iteration gives loss of 0.25616506690083973\n",
      "The 12203 th iteration gives loss of 0.2561517082795629\n",
      "The 12204 th iteration gives loss of 0.2561383514948784\n",
      "The 12205 th iteration gives loss of 0.2561249965465057\n",
      "The 12206 th iteration gives loss of 0.2561116434342024\n",
      "The 12207 th iteration gives loss of 0.2560982921576783\n",
      "The 12208 th iteration gives loss of 0.25608494271668264\n",
      "The 12209 th iteration gives loss of 0.25607159511094635\n",
      "The 12210 th iteration gives loss of 0.25605824934019633\n",
      "The 12211 th iteration gives loss of 0.25604490540417124\n",
      "The 12212 th iteration gives loss of 0.2560315633026024\n",
      "The 12213 th iteration gives loss of 0.25601822303522953\n",
      "The 12214 th iteration gives loss of 0.25600488460177967\n",
      "The 12215 th iteration gives loss of 0.25599154800199997\n",
      "The 12216 th iteration gives loss of 0.25597821323560577\n",
      "The 12217 th iteration gives loss of 0.2559648803023387\n",
      "The 12218 th iteration gives loss of 0.25595154920193425\n",
      "The 12219 th iteration gives loss of 0.25593821993412447\n",
      "The 12220 th iteration gives loss of 0.2559248924986495\n",
      "The 12221 th iteration gives loss of 0.2559115668952274\n",
      "The 12222 th iteration gives loss of 0.2558982431235981\n",
      "The 12223 th iteration gives loss of 0.25588492118351275\n",
      "The 12224 th iteration gives loss of 0.25587160107468787\n",
      "The 12225 th iteration gives loss of 0.25585828279686873\n",
      "The 12226 th iteration gives loss of 0.2558449663497725\n",
      "The 12227 th iteration gives loss of 0.2558316517331521\n",
      "The 12228 th iteration gives loss of 0.25581833894673045\n",
      "The 12229 th iteration gives loss of 0.2558050279902406\n",
      "The 12230 th iteration gives loss of 0.2557917188634327\n",
      "The 12231 th iteration gives loss of 0.2557784115660246\n",
      "The 12232 th iteration gives loss of 0.25576510609775704\n",
      "The 12233 th iteration gives loss of 0.25575180245835133\n",
      "The 12234 th iteration gives loss of 0.25573850064757225\n",
      "The 12235 th iteration gives loss of 0.25572520066512644\n",
      "The 12236 th iteration gives loss of 0.25571190251075343\n",
      "The 12237 th iteration gives loss of 0.2556986061841993\n",
      "The 12238 th iteration gives loss of 0.25568531168517405\n",
      "The 12239 th iteration gives loss of 0.2556720190134478\n",
      "The 12240 th iteration gives loss of 0.25565872816873014\n",
      "The 12241 th iteration gives loss of 0.2556454391507609\n",
      "The 12242 th iteration gives loss of 0.255632151959283\n",
      "The 12243 th iteration gives loss of 0.25561886659400984\n",
      "The 12244 th iteration gives loss of 0.2556055830547047\n",
      "The 12245 th iteration gives loss of 0.2555923013410861\n",
      "The 12246 th iteration gives loss of 0.2555790214528859\n",
      "The 12247 th iteration gives loss of 0.2555657433898489\n",
      "The 12248 th iteration gives loss of 0.2555524671517026\n",
      "The 12249 th iteration gives loss of 0.255539192738192\n",
      "The 12250 th iteration gives loss of 0.255525920149036\n",
      "The 12251 th iteration gives loss of 0.2555126493839808\n",
      "The 12252 th iteration gives loss of 0.25549938044275494\n",
      "The 12253 th iteration gives loss of 0.25548611332510146\n",
      "The 12254 th iteration gives loss of 0.25547284803075143\n",
      "The 12255 th iteration gives loss of 0.25545958455943785\n",
      "The 12256 th iteration gives loss of 0.25544632291089897\n",
      "The 12257 th iteration gives loss of 0.2554330630848677\n",
      "The 12258 th iteration gives loss of 0.25541980508108286\n",
      "The 12259 th iteration gives loss of 0.25540654889927106\n",
      "The 12260 th iteration gives loss of 0.2553932945391783\n",
      "The 12261 th iteration gives loss of 0.2553800420005307\n",
      "The 12262 th iteration gives loss of 0.2553667912830723\n",
      "The 12263 th iteration gives loss of 0.2553535423865271\n",
      "The 12264 th iteration gives loss of 0.25534029531064284\n",
      "The 12265 th iteration gives loss of 0.2553270500551507\n",
      "The 12266 th iteration gives loss of 0.255313806619786\n",
      "The 12267 th iteration gives loss of 0.2553005650042828\n",
      "The 12268 th iteration gives loss of 0.2552873252083786\n",
      "The 12269 th iteration gives loss of 0.2552740872318098\n",
      "The 12270 th iteration gives loss of 0.25526085107430396\n",
      "The 12271 th iteration gives loss of 0.2552476167356046\n",
      "The 12272 th iteration gives loss of 0.25523438421544936\n",
      "The 12273 th iteration gives loss of 0.2552211535135664\n",
      "The 12274 th iteration gives loss of 0.25520792462970054\n",
      "The 12275 th iteration gives loss of 0.2551946975635713\n",
      "The 12276 th iteration gives loss of 0.25518147231493554\n",
      "The 12277 th iteration gives loss of 0.2551682488835164\n",
      "The 12278 th iteration gives loss of 0.25515502726905454\n",
      "The 12279 th iteration gives loss of 0.2551418074712773\n",
      "The 12280 th iteration gives loss of 0.2551285894899284\n",
      "The 12281 th iteration gives loss of 0.255115373324742\n",
      "The 12282 th iteration gives loss of 0.25510215897545346\n",
      "The 12283 th iteration gives loss of 0.2550889464417998\n",
      "The 12284 th iteration gives loss of 0.2550757357235226\n",
      "The 12285 th iteration gives loss of 0.25506252682034813\n",
      "The 12286 th iteration gives loss of 0.25504931973201295\n",
      "The 12287 th iteration gives loss of 0.2550361144582661\n",
      "The 12288 th iteration gives loss of 0.25502291099882457\n",
      "The 12289 th iteration gives loss of 0.2550097093534406\n",
      "The 12290 th iteration gives loss of 0.254996509521841\n",
      "The 12291 th iteration gives loss of 0.25498331150376585\n",
      "The 12292 th iteration gives loss of 0.2549701152989538\n",
      "The 12293 th iteration gives loss of 0.25495692090713473\n",
      "The 12294 th iteration gives loss of 0.2549437283280497\n",
      "The 12295 th iteration gives loss of 0.2549305375614441\n",
      "The 12296 th iteration gives loss of 0.2549173486070353\n",
      "The 12297 th iteration gives loss of 0.2549041614645619\n",
      "The 12298 th iteration gives loss of 0.25489097613378353\n",
      "The 12299 th iteration gives loss of 0.25487779261441385\n",
      "The 12300 th iteration gives loss of 0.2548646109061976\n",
      "The 12301 th iteration gives loss of 0.25485143100887014\n",
      "The 12302 th iteration gives loss of 0.25483825292216816\n",
      "The 12303 th iteration gives loss of 0.25482507664582177\n",
      "The 12304 th iteration gives loss of 0.25481190217958827\n",
      "The 12305 th iteration gives loss of 0.25479872952318355\n",
      "The 12306 th iteration gives loss of 0.2547855586763495\n",
      "The 12307 th iteration gives loss of 0.25477238963882054\n",
      "The 12308 th iteration gives loss of 0.2547592224103463\n",
      "The 12309 th iteration gives loss of 0.2547460569906563\n",
      "The 12310 th iteration gives loss of 0.254732893379487\n",
      "The 12311 th iteration gives loss of 0.254719731576562\n",
      "The 12312 th iteration gives loss of 0.2547065715816327\n",
      "The 12313 th iteration gives loss of 0.25469341339444435\n",
      "The 12314 th iteration gives loss of 0.25468025701472274\n",
      "The 12315 th iteration gives loss of 0.25466710244220836\n",
      "The 12316 th iteration gives loss of 0.25465394967662563\n",
      "The 12317 th iteration gives loss of 0.2546407987177269\n",
      "The 12318 th iteration gives loss of 0.25462764956525225\n",
      "The 12319 th iteration gives loss of 0.2546145022189247\n",
      "The 12320 th iteration gives loss of 0.2546013566784883\n",
      "The 12321 th iteration gives loss of 0.25458821294368045\n",
      "The 12322 th iteration gives loss of 0.2545750710142398\n",
      "The 12323 th iteration gives loss of 0.25456193088990153\n",
      "The 12324 th iteration gives loss of 0.254548792570404\n",
      "The 12325 th iteration gives loss of 0.2545356560554824\n",
      "The 12326 th iteration gives loss of 0.2545225213448808\n",
      "The 12327 th iteration gives loss of 0.2545093884383285\n",
      "The 12328 th iteration gives loss of 0.2544962573355645\n",
      "The 12329 th iteration gives loss of 0.2544831280363258\n",
      "The 12330 th iteration gives loss of 0.25447000054035906\n",
      "The 12331 th iteration gives loss of 0.254456874847398\n",
      "The 12332 th iteration gives loss of 0.25444375095716576\n",
      "The 12333 th iteration gives loss of 0.25443062886942064\n",
      "The 12334 th iteration gives loss of 0.2544175085838889\n",
      "The 12335 th iteration gives loss of 0.25440439010031485\n",
      "The 12336 th iteration gives loss of 0.25439127341842843\n",
      "The 12337 th iteration gives loss of 0.2543781585379765\n",
      "The 12338 th iteration gives loss of 0.25436504545868194\n",
      "The 12339 th iteration gives loss of 0.2543519341803014\n",
      "The 12340 th iteration gives loss of 0.2543388247025551\n",
      "The 12341 th iteration gives loss of 0.25432571702519746\n",
      "The 12342 th iteration gives loss of 0.2543126111479513\n",
      "The 12343 th iteration gives loss of 0.25429950707057\n",
      "The 12344 th iteration gives loss of 0.2542864047927849\n",
      "The 12345 th iteration gives loss of 0.25427330431432543\n",
      "The 12346 th iteration gives loss of 0.25426020563493795\n",
      "The 12347 th iteration gives loss of 0.25424710875436096\n",
      "The 12348 th iteration gives loss of 0.2542340136723273\n",
      "The 12349 th iteration gives loss of 0.2542209203885832\n",
      "The 12350 th iteration gives loss of 0.25420782890286314\n",
      "The 12351 th iteration gives loss of 0.25419473921490915\n",
      "The 12352 th iteration gives loss of 0.25418165132445386\n",
      "The 12353 th iteration gives loss of 0.2541685652312332\n",
      "The 12354 th iteration gives loss of 0.25415548093499074\n",
      "The 12355 th iteration gives loss of 0.2541423984354599\n",
      "The 12356 th iteration gives loss of 0.25412931773239095\n",
      "The 12357 th iteration gives loss of 0.2541162388255153\n",
      "The 12358 th iteration gives loss of 0.25410316171455977\n",
      "The 12359 th iteration gives loss of 0.2540900863992861\n",
      "The 12360 th iteration gives loss of 0.2540770128794111\n",
      "The 12361 th iteration gives loss of 0.25406394115468284\n",
      "The 12362 th iteration gives loss of 0.2540508712248432\n",
      "The 12363 th iteration gives loss of 0.25403780308962404\n",
      "The 12364 th iteration gives loss of 0.25402473674877024\n",
      "The 12365 th iteration gives loss of 0.2540116722020195\n",
      "The 12366 th iteration gives loss of 0.2539986094491072\n",
      "The 12367 th iteration gives loss of 0.2539855484897725\n",
      "The 12368 th iteration gives loss of 0.2539724893237631\n",
      "The 12369 th iteration gives loss of 0.2539594319507955\n",
      "The 12370 th iteration gives loss of 0.2539463763706321\n",
      "The 12371 th iteration gives loss of 0.2539333225830034\n",
      "The 12372 th iteration gives loss of 0.25392027058764666\n",
      "The 12373 th iteration gives loss of 0.25390722038430047\n",
      "The 12374 th iteration gives loss of 0.2538941719727023\n",
      "The 12375 th iteration gives loss of 0.25388112535259916\n",
      "The 12376 th iteration gives loss of 0.2538680805237262\n",
      "The 12377 th iteration gives loss of 0.2538550374858265\n",
      "The 12378 th iteration gives loss of 0.2538419962386254\n",
      "The 12379 th iteration gives loss of 0.2538289567818678\n",
      "The 12380 th iteration gives loss of 0.25381591911530765\n",
      "The 12381 th iteration gives loss of 0.25380288323866995\n",
      "The 12382 th iteration gives loss of 0.25378984915169545\n",
      "The 12383 th iteration gives loss of 0.2537768168541262\n",
      "The 12384 th iteration gives loss of 0.25376378634569563\n",
      "The 12385 th iteration gives loss of 0.2537507576261545\n",
      "The 12386 th iteration gives loss of 0.2537377306952277\n",
      "The 12387 th iteration gives loss of 0.25372470555266463\n",
      "The 12388 th iteration gives loss of 0.253711682198202\n",
      "The 12389 th iteration gives loss of 0.25369866063158786\n",
      "The 12390 th iteration gives loss of 0.2536856408525417\n",
      "The 12391 th iteration gives loss of 0.2536726228608247\n",
      "The 12392 th iteration gives loss of 0.25365960665616394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 12393 th iteration gives loss of 0.2536465922383012\n",
      "The 12394 th iteration gives loss of 0.25363357960697264\n",
      "The 12395 th iteration gives loss of 0.25362056876193195\n",
      "The 12396 th iteration gives loss of 0.2536075597029076\n",
      "The 12397 th iteration gives loss of 0.25359455242963375\n",
      "The 12398 th iteration gives loss of 0.25358154694185475\n",
      "The 12399 th iteration gives loss of 0.25356854323932126\n",
      "The 12400 th iteration gives loss of 0.25355554132176655\n",
      "The 12401 th iteration gives loss of 0.2535425411889286\n",
      "The 12402 th iteration gives loss of 0.2535295428405462\n",
      "The 12403 th iteration gives loss of 0.25351654627635645\n",
      "The 12404 th iteration gives loss of 0.25350355149610254\n",
      "The 12405 th iteration gives loss of 0.2534905584995342\n",
      "The 12406 th iteration gives loss of 0.253477567286371\n",
      "The 12407 th iteration gives loss of 0.2534645778563807\n",
      "The 12408 th iteration gives loss of 0.25345159020927743\n",
      "The 12409 th iteration gives loss of 0.2534386043448147\n",
      "The 12410 th iteration gives loss of 0.2534256202627268\n",
      "The 12411 th iteration gives loss of 0.25341263796276375\n",
      "The 12412 th iteration gives loss of 0.2533996574446522\n",
      "The 12413 th iteration gives loss of 0.2533866787081455\n",
      "The 12414 th iteration gives loss of 0.25337370175297136\n",
      "The 12415 th iteration gives loss of 0.2533607265788756\n",
      "The 12416 th iteration gives loss of 0.25334775318560204\n",
      "The 12417 th iteration gives loss of 0.2533347815728895\n",
      "The 12418 th iteration gives loss of 0.25332181174048346\n",
      "The 12419 th iteration gives loss of 0.2533088436881113\n",
      "The 12420 th iteration gives loss of 0.2532958774155212\n",
      "The 12421 th iteration gives loss of 0.25328291292245475\n",
      "The 12422 th iteration gives loss of 0.2532699502086417\n",
      "The 12423 th iteration gives loss of 0.2532569892738448\n",
      "The 12424 th iteration gives loss of 0.2532440301177903\n",
      "The 12425 th iteration gives loss of 0.25323107274020834\n",
      "The 12426 th iteration gives loss of 0.25321811714086595\n",
      "The 12427 th iteration gives loss of 0.2532051633194819\n",
      "The 12428 th iteration gives loss of 0.2531922112758102\n",
      "The 12429 th iteration gives loss of 0.2531792610095803\n",
      "The 12430 th iteration gives loss of 0.2531663125205442\n",
      "The 12431 th iteration gives loss of 0.25315336580843795\n",
      "The 12432 th iteration gives loss of 0.25314042087300453\n",
      "The 12433 th iteration gives loss of 0.25312747771397553\n",
      "The 12434 th iteration gives loss of 0.2531145363310967\n",
      "The 12435 th iteration gives loss of 0.25310159672411925\n",
      "The 12436 th iteration gives loss of 0.2530886588927752\n",
      "The 12437 th iteration gives loss of 0.25307572283679985\n",
      "The 12438 th iteration gives loss of 0.2530627885559406\n",
      "The 12439 th iteration gives loss of 0.2530498560499395\n",
      "The 12440 th iteration gives loss of 0.2530369253185459\n",
      "The 12441 th iteration gives loss of 0.25302399636148265\n",
      "The 12442 th iteration gives loss of 0.25301106917850635\n",
      "The 12443 th iteration gives loss of 0.25299814376935065\n",
      "The 12444 th iteration gives loss of 0.2529852201337605\n",
      "The 12445 th iteration gives loss of 0.2529722982714727\n",
      "The 12446 th iteration gives loss of 0.2529593781822333\n",
      "The 12447 th iteration gives loss of 0.2529464598657824\n",
      "The 12448 th iteration gives loss of 0.2529335433218638\n",
      "The 12449 th iteration gives loss of 0.2529206285502103\n",
      "The 12450 th iteration gives loss of 0.2529077155505714\n",
      "The 12451 th iteration gives loss of 0.25289480432268524\n",
      "The 12452 th iteration gives loss of 0.25288189486629536\n",
      "The 12453 th iteration gives loss of 0.2528689871811399\n",
      "The 12454 th iteration gives loss of 0.25285608126697234\n",
      "The 12455 th iteration gives loss of 0.25284317712352034\n",
      "The 12456 th iteration gives loss of 0.252830274750535\n",
      "The 12457 th iteration gives loss of 0.25281737414775113\n",
      "The 12458 th iteration gives loss of 0.25280447531491207\n",
      "The 12459 th iteration gives loss of 0.25279157825175685\n",
      "The 12460 th iteration gives loss of 0.252778682958025\n",
      "The 12461 th iteration gives loss of 0.2527657894334705\n",
      "The 12462 th iteration gives loss of 0.252752897677835\n",
      "The 12463 th iteration gives loss of 0.2527400076908492\n",
      "The 12464 th iteration gives loss of 0.2527271194722612\n",
      "The 12465 th iteration gives loss of 0.2527142330218096\n",
      "The 12466 th iteration gives loss of 0.2527013483392436\n",
      "The 12467 th iteration gives loss of 0.25268846542430085\n",
      "The 12468 th iteration gives loss of 0.25267558427671616\n",
      "The 12469 th iteration gives loss of 0.2526627048962454\n",
      "The 12470 th iteration gives loss of 0.25264982728261914\n",
      "The 12471 th iteration gives loss of 0.2526369514355944\n",
      "The 12472 th iteration gives loss of 0.25262407735489323\n",
      "The 12473 th iteration gives loss of 0.2526112050402654\n",
      "The 12474 th iteration gives loss of 0.2525983344914641\n",
      "The 12475 th iteration gives loss of 0.252585465708219\n",
      "The 12476 th iteration gives loss of 0.25257259869027987\n",
      "The 12477 th iteration gives loss of 0.252559733437379\n",
      "The 12478 th iteration gives loss of 0.25254686994927095\n",
      "The 12479 th iteration gives loss of 0.2525340082256951\n",
      "The 12480 th iteration gives loss of 0.2525211482663964\n",
      "The 12481 th iteration gives loss of 0.25250829007110603\n",
      "The 12482 th iteration gives loss of 0.2524954336395676\n",
      "The 12483 th iteration gives loss of 0.25248257897153376\n",
      "The 12484 th iteration gives loss of 0.2524697260667458\n",
      "The 12485 th iteration gives loss of 0.2524568749249381\n",
      "The 12486 th iteration gives loss of 0.25244402554586043\n",
      "The 12487 th iteration gives loss of 0.25243117792926745\n",
      "The 12488 th iteration gives loss of 0.2524183320748775\n",
      "The 12489 th iteration gives loss of 0.25240548798244045\n",
      "The 12490 th iteration gives loss of 0.2523926456517107\n",
      "The 12491 th iteration gives loss of 0.2523798050824164\n",
      "The 12492 th iteration gives loss of 0.2523669662742992\n",
      "The 12493 th iteration gives loss of 0.2523541292271234\n",
      "The 12494 th iteration gives loss of 0.2523412939406147\n",
      "The 12495 th iteration gives loss of 0.2523284604145182\n",
      "The 12496 th iteration gives loss of 0.2523156286485856\n",
      "The 12497 th iteration gives loss of 0.2523027986425401\n",
      "The 12498 th iteration gives loss of 0.2522899703961441\n",
      "The 12499 th iteration gives loss of 0.25227714390913075\n",
      "The 12500 th iteration gives loss of 0.2522643191812485\n",
      "The 12501 th iteration gives loss of 0.25225149621224396\n",
      "The 12502 th iteration gives loss of 0.2522386750018516\n",
      "The 12503 th iteration gives loss of 0.25222585554981386\n",
      "The 12504 th iteration gives loss of 0.2522130378558742\n",
      "The 12505 th iteration gives loss of 0.25220022191979785\n",
      "The 12506 th iteration gives loss of 0.2521874077412926\n",
      "The 12507 th iteration gives loss of 0.2521745953201281\n",
      "The 12508 th iteration gives loss of 0.25216178465603867\n",
      "The 12509 th iteration gives loss of 0.2521489757487589\n",
      "The 12510 th iteration gives loss of 0.25213616859805166\n",
      "The 12511 th iteration gives loss of 0.2521233632036462\n",
      "The 12512 th iteration gives loss of 0.25211055956528533\n",
      "The 12513 th iteration gives loss of 0.252097757682721\n",
      "The 12514 th iteration gives loss of 0.2520849575556897\n",
      "The 12515 th iteration gives loss of 0.25207215918394515\n",
      "The 12516 th iteration gives loss of 0.25205936256722533\n",
      "The 12517 th iteration gives loss of 0.2520465677052657\n",
      "The 12518 th iteration gives loss of 0.25203377459781584\n",
      "The 12519 th iteration gives loss of 0.25202098324462635\n",
      "The 12520 th iteration gives loss of 0.25200819364543536\n",
      "The 12521 th iteration gives loss of 0.2519954057999881\n",
      "The 12522 th iteration gives loss of 0.25198261970802177\n",
      "The 12523 th iteration gives loss of 0.25196983536928624\n",
      "The 12524 th iteration gives loss of 0.25195705278352554\n",
      "The 12525 th iteration gives loss of 0.2519442719504875\n",
      "The 12526 th iteration gives loss of 0.25193149286989985\n",
      "The 12527 th iteration gives loss of 0.2519187155415281\n",
      "The 12528 th iteration gives loss of 0.2519059399651005\n",
      "The 12529 th iteration gives loss of 0.25189316614036344\n",
      "The 12530 th iteration gives loss of 0.25188039406707513\n",
      "The 12531 th iteration gives loss of 0.2518676237449656\n",
      "The 12532 th iteration gives loss of 0.25185485517377887\n",
      "The 12533 th iteration gives loss of 0.25184208835327065\n",
      "The 12534 th iteration gives loss of 0.25182932328316515\n",
      "The 12535 th iteration gives loss of 0.25181655996322566\n",
      "The 12536 th iteration gives loss of 0.25180379839319234\n",
      "The 12537 th iteration gives loss of 0.251791038572801\n",
      "The 12538 th iteration gives loss of 0.25177828050179435\n",
      "The 12539 th iteration gives loss of 0.2517655241799407\n",
      "The 12540 th iteration gives loss of 0.25175276960696097\n",
      "The 12541 th iteration gives loss of 0.2517400167826099\n",
      "The 12542 th iteration gives loss of 0.2517272657066249\n",
      "The 12543 th iteration gives loss of 0.2517145163787549\n",
      "The 12544 th iteration gives loss of 0.251701768798739\n",
      "The 12545 th iteration gives loss of 0.2516890229663238\n",
      "The 12546 th iteration gives loss of 0.25167627888127003\n",
      "The 12547 th iteration gives loss of 0.25166353654329665\n",
      "The 12548 th iteration gives loss of 0.25165079595217477\n",
      "The 12549 th iteration gives loss of 0.2516380571076278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 12550 th iteration gives loss of 0.2516253200094046\n",
      "The 12551 th iteration gives loss of 0.251612584657258\n",
      "The 12552 th iteration gives loss of 0.2515998510509217\n",
      "The 12553 th iteration gives loss of 0.25158711919015014\n",
      "The 12554 th iteration gives loss of 0.2515743890746856\n",
      "The 12555 th iteration gives loss of 0.251561660704275\n",
      "The 12556 th iteration gives loss of 0.25154893407865747\n",
      "The 12557 th iteration gives loss of 0.2515362091975798\n",
      "The 12558 th iteration gives loss of 0.25152348606078473\n",
      "The 12559 th iteration gives loss of 0.25151076466802397\n",
      "The 12560 th iteration gives loss of 0.25149804501904827\n",
      "The 12561 th iteration gives loss of 0.25148532711358523\n",
      "The 12562 th iteration gives loss of 0.25147261095138684\n",
      "The 12563 th iteration gives loss of 0.25145989653220213\n",
      "The 12564 th iteration gives loss of 0.2514471838557668\n",
      "The 12565 th iteration gives loss of 0.25143447292184656\n",
      "The 12566 th iteration gives loss of 0.25142176373017555\n",
      "The 12567 th iteration gives loss of 0.25140905628049226\n",
      "The 12568 th iteration gives loss of 0.2513963505725414\n",
      "The 12569 th iteration gives loss of 0.2513836466060731\n",
      "The 12570 th iteration gives loss of 0.2513709443808384\n",
      "The 12571 th iteration gives loss of 0.2513582438965737\n",
      "The 12572 th iteration gives loss of 0.25134554515303803\n",
      "The 12573 th iteration gives loss of 0.2513328481499655\n",
      "The 12574 th iteration gives loss of 0.25132015288709114\n",
      "The 12575 th iteration gives loss of 0.25130745936418\n",
      "The 12576 th iteration gives loss of 0.25129476758097097\n",
      "The 12577 th iteration gives loss of 0.2512820775372152\n",
      "The 12578 th iteration gives loss of 0.25126938923264797\n",
      "The 12579 th iteration gives loss of 0.25125670266701383\n",
      "The 12580 th iteration gives loss of 0.2512440178400637\n",
      "The 12581 th iteration gives loss of 0.25123133475155335\n",
      "The 12582 th iteration gives loss of 0.2512186534012162\n",
      "The 12583 th iteration gives loss of 0.2512059737888022\n",
      "The 12584 th iteration gives loss of 0.2511932959140523\n",
      "The 12585 th iteration gives loss of 0.2511806197767179\n",
      "The 12586 th iteration gives loss of 0.25116794537653947\n",
      "The 12587 th iteration gives loss of 0.25115527271326826\n",
      "The 12588 th iteration gives loss of 0.25114260178665015\n",
      "The 12589 th iteration gives loss of 0.2511299325964336\n",
      "The 12590 th iteration gives loss of 0.25111726514234955\n",
      "The 12591 th iteration gives loss of 0.2511045994241623\n",
      "The 12592 th iteration gives loss of 0.2510919354416082\n",
      "The 12593 th iteration gives loss of 0.25107927319443435\n",
      "The 12594 th iteration gives loss of 0.2510666126823958\n",
      "The 12595 th iteration gives loss of 0.25105395390522667\n",
      "The 12596 th iteration gives loss of 0.25104129686267773\n",
      "The 12597 th iteration gives loss of 0.25102864155449534\n",
      "The 12598 th iteration gives loss of 0.2510159879804234\n",
      "The 12599 th iteration gives loss of 0.2510033361402142\n",
      "The 12600 th iteration gives loss of 0.25099068603361335\n",
      "The 12601 th iteration gives loss of 0.25097803766035937\n",
      "The 12602 th iteration gives loss of 0.25096539102020243\n",
      "The 12603 th iteration gives loss of 0.25095274611289375\n",
      "The 12604 th iteration gives loss of 0.25094010293817587\n",
      "The 12605 th iteration gives loss of 0.2509274614957946\n",
      "The 12606 th iteration gives loss of 0.25091482178550123\n",
      "The 12607 th iteration gives loss of 0.2509021838070397\n",
      "The 12608 th iteration gives loss of 0.2508895475601527\n",
      "The 12609 th iteration gives loss of 0.2508769130445883\n",
      "The 12610 th iteration gives loss of 0.25086428026010216\n",
      "The 12611 th iteration gives loss of 0.25085164920642883\n",
      "The 12612 th iteration gives loss of 0.2508390198833195\n",
      "The 12613 th iteration gives loss of 0.2508263922905212\n",
      "The 12614 th iteration gives loss of 0.25081376642778447\n",
      "The 12615 th iteration gives loss of 0.25080114229484357\n",
      "The 12616 th iteration gives loss of 0.2507885198914588\n",
      "The 12617 th iteration gives loss of 0.2507758992173804\n",
      "The 12618 th iteration gives loss of 0.2507632802723415\n",
      "The 12619 th iteration gives loss of 0.25075066305609695\n",
      "The 12620 th iteration gives loss of 0.2507380475683836\n",
      "The 12621 th iteration gives loss of 0.2507254338089724\n",
      "The 12622 th iteration gives loss of 0.25071282177758564\n",
      "The 12623 th iteration gives loss of 0.25070021147398613\n",
      "The 12624 th iteration gives loss of 0.2506876028979111\n",
      "The 12625 th iteration gives loss of 0.25067499604910665\n",
      "The 12626 th iteration gives loss of 0.2506623909273359\n",
      "The 12627 th iteration gives loss of 0.25064978753232625\n",
      "The 12628 th iteration gives loss of 0.250637185863831\n",
      "The 12629 th iteration gives loss of 0.2506245859216108\n",
      "The 12630 th iteration gives loss of 0.25061198770540005\n",
      "The 12631 th iteration gives loss of 0.25059939121493996\n",
      "The 12632 th iteration gives loss of 0.2505867964499939\n",
      "The 12633 th iteration gives loss of 0.2505742034102982\n",
      "The 12634 th iteration gives loss of 0.2505616120956045\n",
      "The 12635 th iteration gives loss of 0.2505490225056538\n",
      "The 12636 th iteration gives loss of 0.25053643464019626\n",
      "The 12637 th iteration gives loss of 0.2505238484989971\n",
      "The 12638 th iteration gives loss of 0.25051126408178465\n",
      "The 12639 th iteration gives loss of 0.25049868138830766\n",
      "The 12640 th iteration gives loss of 0.25048610041832137\n",
      "The 12641 th iteration gives loss of 0.2504735211715644\n",
      "The 12642 th iteration gives loss of 0.25046094364779914\n",
      "The 12643 th iteration gives loss of 0.2504483678467596\n",
      "The 12644 th iteration gives loss of 0.2504357937681906\n",
      "The 12645 th iteration gives loss of 0.25042322141185175\n",
      "The 12646 th iteration gives loss of 0.25041065077748675\n",
      "The 12647 th iteration gives loss of 0.25039808186484563\n",
      "The 12648 th iteration gives loss of 0.25038551467367093\n",
      "The 12649 th iteration gives loss of 0.25037294920371206\n",
      "The 12650 th iteration gives loss of 0.25036038545471695\n",
      "The 12651 th iteration gives loss of 0.2503478234264398\n",
      "The 12652 th iteration gives loss of 0.2503352631186218\n",
      "The 12653 th iteration gives loss of 0.2503227045310186\n",
      "The 12654 th iteration gives loss of 0.25031014766336146\n",
      "The 12655 th iteration gives loss of 0.2502975925154145\n",
      "The 12656 th iteration gives loss of 0.2502850390869269\n",
      "The 12657 th iteration gives loss of 0.2502724873776311\n",
      "The 12658 th iteration gives loss of 0.2502599373872887\n",
      "The 12659 th iteration gives loss of 0.2502473891156486\n",
      "The 12660 th iteration gives loss of 0.2502348425624552\n",
      "The 12661 th iteration gives loss of 0.2502222977274523\n",
      "The 12662 th iteration gives loss of 0.2502097546103924\n",
      "The 12663 th iteration gives loss of 0.25019721321103117\n",
      "The 12664 th iteration gives loss of 0.25018467352910145\n",
      "The 12665 th iteration gives loss of 0.25017213556436235\n",
      "The 12666 th iteration gives loss of 0.2501595993165597\n",
      "The 12667 th iteration gives loss of 0.25014706478544413\n",
      "The 12668 th iteration gives loss of 0.2501345319707674\n",
      "The 12669 th iteration gives loss of 0.25012200087227343\n",
      "The 12670 th iteration gives loss of 0.2501094714897035\n",
      "The 12671 th iteration gives loss of 0.25009694382281566\n",
      "The 12672 th iteration gives loss of 0.2500844178713522\n",
      "The 12673 th iteration gives loss of 0.2500718936350755\n",
      "The 12674 th iteration gives loss of 0.2500593711137162\n",
      "The 12675 th iteration gives loss of 0.25004685030703244\n",
      "The 12676 th iteration gives loss of 0.25003433121477897\n",
      "The 12677 th iteration gives loss of 0.250021813836702\n",
      "The 12678 th iteration gives loss of 0.2500092981725439\n",
      "The 12679 th iteration gives loss of 0.24999678422204646\n",
      "The 12680 th iteration gives loss of 0.24998427198497453\n",
      "The 12681 th iteration gives loss of 0.24997176146107136\n",
      "The 12682 th iteration gives loss of 0.24995925265008287\n",
      "The 12683 th iteration gives loss of 0.24994674555176283\n",
      "The 12684 th iteration gives loss of 0.24993424016585705\n",
      "The 12685 th iteration gives loss of 0.2499217364921115\n",
      "The 12686 th iteration gives loss of 0.24990923453028446\n",
      "The 12687 th iteration gives loss of 0.24989673428012496\n",
      "The 12688 th iteration gives loss of 0.24988423574136945\n",
      "The 12689 th iteration gives loss of 0.24987173891378142\n",
      "The 12690 th iteration gives loss of 0.24985924379710023\n",
      "The 12691 th iteration gives loss of 0.24984675039107762\n",
      "The 12692 th iteration gives loss of 0.24983425869546555\n",
      "The 12693 th iteration gives loss of 0.24982176871001668\n",
      "The 12694 th iteration gives loss of 0.24980928043447073\n",
      "The 12695 th iteration gives loss of 0.2497967938685836\n",
      "The 12696 th iteration gives loss of 0.24978430901209575\n",
      "The 12697 th iteration gives loss of 0.24977182586478\n",
      "The 12698 th iteration gives loss of 0.2497593444263556\n",
      "The 12699 th iteration gives loss of 0.24974686469658658\n",
      "The 12700 th iteration gives loss of 0.24973438667522757\n",
      "The 12701 th iteration gives loss of 0.24972191036201835\n",
      "The 12702 th iteration gives loss of 0.24970943575672222\n",
      "The 12703 th iteration gives loss of 0.24969696285907683\n",
      "The 12704 th iteration gives loss of 0.24968449166883125\n",
      "The 12705 th iteration gives loss of 0.24967202218573958\n",
      "The 12706 th iteration gives loss of 0.24965955440954912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 12707 th iteration gives loss of 0.2496470883400183\n",
      "The 12708 th iteration gives loss of 0.2496346239768829\n",
      "The 12709 th iteration gives loss of 0.2496221613199069\n",
      "The 12710 th iteration gives loss of 0.24960970036882613\n",
      "The 12711 th iteration gives loss of 0.24959724112340018\n",
      "The 12712 th iteration gives loss of 0.24958478358337613\n",
      "The 12713 th iteration gives loss of 0.24957232774850674\n",
      "The 12714 th iteration gives loss of 0.2495598736185449\n",
      "The 12715 th iteration gives loss of 0.249547421193223\n",
      "The 12716 th iteration gives loss of 0.24953497047231524\n",
      "The 12717 th iteration gives loss of 0.2495225214555508\n",
      "The 12718 th iteration gives loss of 0.24951007414269727\n",
      "The 12719 th iteration gives loss of 0.24949762853349242\n",
      "The 12720 th iteration gives loss of 0.24948518462768987\n",
      "The 12721 th iteration gives loss of 0.24947274242503745\n",
      "The 12722 th iteration gives loss of 0.24946030192530083\n",
      "The 12723 th iteration gives loss of 0.24944786312820802\n",
      "The 12724 th iteration gives loss of 0.2494354260335273\n",
      "The 12725 th iteration gives loss of 0.24942299064099407\n",
      "The 12726 th iteration gives loss of 0.24941055695036315\n",
      "The 12727 th iteration gives loss of 0.2493981249613901\n",
      "The 12728 th iteration gives loss of 0.24938569467382463\n",
      "The 12729 th iteration gives loss of 0.24937326608742072\n",
      "The 12730 th iteration gives loss of 0.24936083920192129\n",
      "The 12731 th iteration gives loss of 0.249348414017076\n",
      "The 12732 th iteration gives loss of 0.24933599053264607\n",
      "The 12733 th iteration gives loss of 0.24932356874836936\n",
      "The 12734 th iteration gives loss of 0.24931114866400442\n",
      "The 12735 th iteration gives loss of 0.24929873027929908\n",
      "The 12736 th iteration gives loss of 0.2492863135940027\n",
      "The 12737 th iteration gives loss of 0.24927389860787375\n",
      "The 12738 th iteration gives loss of 0.24926148532065065\n",
      "The 12739 th iteration gives loss of 0.2492490737320953\n",
      "The 12740 th iteration gives loss of 0.2492366638419533\n",
      "The 12741 th iteration gives loss of 0.24922425564997874\n",
      "The 12742 th iteration gives loss of 0.24921184915591485\n",
      "The 12743 th iteration gives loss of 0.24919944435952385\n",
      "The 12744 th iteration gives loss of 0.2491870412605477\n",
      "The 12745 th iteration gives loss of 0.24917463985874996\n",
      "The 12746 th iteration gives loss of 0.24916224015385705\n",
      "The 12747 th iteration gives loss of 0.24914984214563676\n",
      "The 12748 th iteration gives loss of 0.24913744583385233\n",
      "The 12749 th iteration gives loss of 0.2491250512182345\n",
      "The 12750 th iteration gives loss of 0.2491126582985389\n",
      "The 12751 th iteration gives loss of 0.24910026707451652\n",
      "The 12752 th iteration gives loss of 0.24908787754592904\n",
      "The 12753 th iteration gives loss of 0.24907548971251614\n",
      "The 12754 th iteration gives loss of 0.24906310357402878\n",
      "The 12755 th iteration gives loss of 0.24905071913022406\n",
      "The 12756 th iteration gives loss of 0.24903833638085787\n",
      "The 12757 th iteration gives loss of 0.2490259553256758\n",
      "The 12758 th iteration gives loss of 0.24901357596442825\n",
      "The 12759 th iteration gives loss of 0.24900119829686782\n",
      "The 12760 th iteration gives loss of 0.24898882232274283\n",
      "The 12761 th iteration gives loss of 0.248976448041802\n",
      "The 12762 th iteration gives loss of 0.24896407545380866\n",
      "The 12763 th iteration gives loss of 0.24895170455850915\n",
      "The 12764 th iteration gives loss of 0.2489393353556519\n",
      "The 12765 th iteration gives loss of 0.2489269678449953\n",
      "The 12766 th iteration gives loss of 0.2489146020262852\n",
      "The 12767 th iteration gives loss of 0.24890223789926788\n",
      "The 12768 th iteration gives loss of 0.24888987546370694\n",
      "The 12769 th iteration gives loss of 0.24887751471935038\n",
      "The 12770 th iteration gives loss of 0.24886515566594652\n",
      "The 12771 th iteration gives loss of 0.24885279830325557\n",
      "The 12772 th iteration gives loss of 0.2488404426310115\n",
      "The 12773 th iteration gives loss of 0.24882808864898792\n",
      "The 12774 th iteration gives loss of 0.24881573635692553\n",
      "The 12775 th iteration gives loss of 0.24880338575457497\n",
      "The 12776 th iteration gives loss of 0.24879103684169862\n",
      "The 12777 th iteration gives loss of 0.2487786896180313\n",
      "The 12778 th iteration gives loss of 0.24876634408333373\n",
      "The 12779 th iteration gives loss of 0.24875400023736577\n",
      "The 12780 th iteration gives loss of 0.2487416580798695\n",
      "The 12781 th iteration gives loss of 0.24872931761060607\n",
      "The 12782 th iteration gives loss of 0.24871697882932156\n",
      "The 12783 th iteration gives loss of 0.24870464173576\n",
      "The 12784 th iteration gives loss of 0.2486923063296871\n",
      "The 12785 th iteration gives loss of 0.248679972610847\n",
      "The 12786 th iteration gives loss of 0.24866764057900148\n",
      "The 12787 th iteration gives loss of 0.2486553102338941\n",
      "The 12788 th iteration gives loss of 0.24864298157527434\n",
      "The 12789 th iteration gives loss of 0.2486306546029052\n",
      "The 12790 th iteration gives loss of 0.24861832931653263\n",
      "The 12791 th iteration gives loss of 0.2486060057159248\n",
      "The 12792 th iteration gives loss of 0.2485936838008077\n",
      "The 12793 th iteration gives loss of 0.2485813635709403\n",
      "The 12794 th iteration gives loss of 0.24856904502608348\n",
      "The 12795 th iteration gives loss of 0.24855672816599325\n",
      "The 12796 th iteration gives loss of 0.2485444129904173\n",
      "The 12797 th iteration gives loss of 0.2485320994991045\n",
      "The 12798 th iteration gives loss of 0.24851978769180935\n",
      "The 12799 th iteration gives loss of 0.24850747756828415\n",
      "The 12800 th iteration gives loss of 0.24849516912828615\n",
      "The 12801 th iteration gives loss of 0.24848286237156916\n",
      "The 12802 th iteration gives loss of 0.24847055729787484\n",
      "The 12803 th iteration gives loss of 0.24845825390697032\n",
      "The 12804 th iteration gives loss of 0.24844595219859564\n",
      "The 12805 th iteration gives loss of 0.24843365217251753\n",
      "The 12806 th iteration gives loss of 0.24842135382847702\n",
      "The 12807 th iteration gives loss of 0.248409057166227\n",
      "The 12808 th iteration gives loss of 0.24839676218552736\n",
      "The 12809 th iteration gives loss of 0.24838446888613228\n",
      "The 12810 th iteration gives loss of 0.24837217726778305\n",
      "The 12811 th iteration gives loss of 0.24835988733024753\n",
      "The 12812 th iteration gives loss of 0.24834759907326784\n",
      "The 12813 th iteration gives loss of 0.2483353124966059\n",
      "The 12814 th iteration gives loss of 0.24832302760000943\n",
      "The 12815 th iteration gives loss of 0.24831074438323159\n",
      "The 12816 th iteration gives loss of 0.24829846284602525\n",
      "The 12817 th iteration gives loss of 0.2482861829881445\n",
      "The 12818 th iteration gives loss of 0.248273904809352\n",
      "The 12819 th iteration gives loss of 0.24826162830939116\n",
      "The 12820 th iteration gives loss of 0.24824935348800556\n",
      "The 12821 th iteration gives loss of 0.24823708034497152\n",
      "The 12822 th iteration gives loss of 0.2482248088800275\n",
      "The 12823 th iteration gives loss of 0.248212539092925\n",
      "The 12824 th iteration gives loss of 0.24820027098342817\n",
      "The 12825 th iteration gives loss of 0.24818800455128884\n",
      "The 12826 th iteration gives loss of 0.2481757397962532\n",
      "The 12827 th iteration gives loss of 0.24816347671808245\n",
      "The 12828 th iteration gives loss of 0.2481512153165222\n",
      "The 12829 th iteration gives loss of 0.24813895559133084\n",
      "The 12830 th iteration gives loss of 0.24812669754226602\n",
      "The 12831 th iteration gives loss of 0.24811444116907014\n",
      "The 12832 th iteration gives loss of 0.2481021864715094\n",
      "The 12833 th iteration gives loss of 0.248089933449331\n",
      "The 12834 th iteration gives loss of 0.2480776821022957\n",
      "The 12835 th iteration gives loss of 0.24806543243014328\n",
      "The 12836 th iteration gives loss of 0.2480531844326342\n",
      "The 12837 th iteration gives loss of 0.24804093810953354\n",
      "The 12838 th iteration gives loss of 0.24802869346058032\n",
      "The 12839 th iteration gives loss of 0.2480164504855387\n",
      "The 12840 th iteration gives loss of 0.24800420918415456\n",
      "The 12841 th iteration gives loss of 0.2479919695562009\n",
      "The 12842 th iteration gives loss of 0.24797973160140085\n",
      "The 12843 th iteration gives loss of 0.24796749531952647\n",
      "The 12844 th iteration gives loss of 0.24795526071032448\n",
      "The 12845 th iteration gives loss of 0.24794302777357172\n",
      "The 12846 th iteration gives loss of 0.24793079650899144\n",
      "The 12847 th iteration gives loss of 0.24791856691635716\n",
      "The 12848 th iteration gives loss of 0.24790633899542247\n",
      "The 12849 th iteration gives loss of 0.2478941127459261\n",
      "The 12850 th iteration gives loss of 0.24788188816763973\n",
      "The 12851 th iteration gives loss of 0.24786966526031123\n",
      "The 12852 th iteration gives loss of 0.2478574440236913\n",
      "The 12853 th iteration gives loss of 0.2478452244575418\n",
      "The 12854 th iteration gives loss of 0.24783300656160964\n",
      "The 12855 th iteration gives loss of 0.24782079033565646\n",
      "The 12856 th iteration gives loss of 0.24780857577943535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 12857 th iteration gives loss of 0.2477963628927006\n",
      "The 12858 th iteration gives loss of 0.24778415167520146\n",
      "The 12859 th iteration gives loss of 0.2477719421266981\n",
      "The 12860 th iteration gives loss of 0.2477597342469403\n",
      "The 12861 th iteration gives loss of 0.24774752803568895\n",
      "The 12862 th iteration gives loss of 0.24773532349270275\n",
      "The 12863 th iteration gives loss of 0.2477231206177242\n",
      "The 12864 th iteration gives loss of 0.2477109194105124\n",
      "The 12865 th iteration gives loss of 0.2476987198708256\n",
      "The 12866 th iteration gives loss of 0.24768652199841426\n",
      "The 12867 th iteration gives loss of 0.24767432579304097\n",
      "The 12868 th iteration gives loss of 0.24766213125444503\n",
      "The 12869 th iteration gives loss of 0.24764993838240063\n",
      "The 12870 th iteration gives loss of 0.24763774717664905\n",
      "The 12871 th iteration gives loss of 0.24762555763694022\n",
      "The 12872 th iteration gives loss of 0.24761336976305287\n",
      "The 12873 th iteration gives loss of 0.24760118355472877\n",
      "The 12874 th iteration gives loss of 0.2475889990117161\n",
      "The 12875 th iteration gives loss of 0.2475768161337822\n",
      "The 12876 th iteration gives loss of 0.24756463492067698\n",
      "The 12877 th iteration gives loss of 0.247552455372154\n",
      "The 12878 th iteration gives loss of 0.2475402774879626\n",
      "The 12879 th iteration gives loss of 0.2475281012678706\n",
      "The 12880 th iteration gives loss of 0.24751592671163022\n",
      "The 12881 th iteration gives loss of 0.24750375381899986\n",
      "The 12882 th iteration gives loss of 0.24749158258971982\n",
      "The 12883 th iteration gives loss of 0.24747941302355458\n",
      "The 12884 th iteration gives loss of 0.2474672451202568\n",
      "The 12885 th iteration gives loss of 0.24745507887959306\n",
      "The 12886 th iteration gives loss of 0.24744291430130874\n",
      "The 12887 th iteration gives loss of 0.2474307513851609\n",
      "The 12888 th iteration gives loss of 0.2474185901309093\n",
      "The 12889 th iteration gives loss of 0.2474064305382995\n",
      "The 12890 th iteration gives loss of 0.24739427260709781\n",
      "The 12891 th iteration gives loss of 0.24738211633706012\n",
      "The 12892 th iteration gives loss of 0.2473699617279355\n",
      "The 12893 th iteration gives loss of 0.24735780877947758\n",
      "The 12894 th iteration gives loss of 0.24734565749145304\n",
      "The 12895 th iteration gives loss of 0.247333507863605\n",
      "The 12896 th iteration gives loss of 0.24732135989569592\n",
      "The 12897 th iteration gives loss of 0.2473092135874883\n",
      "The 12898 th iteration gives loss of 0.24729706893872358\n",
      "The 12899 th iteration gives loss of 0.24728492594916526\n",
      "The 12900 th iteration gives loss of 0.24727278461857358\n",
      "The 12901 th iteration gives loss of 0.24726064494669653\n",
      "The 12902 th iteration gives loss of 0.24724850693329423\n",
      "The 12903 th iteration gives loss of 0.24723637057812317\n",
      "The 12904 th iteration gives loss of 0.2472242358809427\n",
      "The 12905 th iteration gives loss of 0.24721210284149478\n",
      "The 12906 th iteration gives loss of 0.24719997145955008\n",
      "The 12907 th iteration gives loss of 0.24718784173486075\n",
      "The 12908 th iteration gives loss of 0.24717571366717836\n",
      "The 12909 th iteration gives loss of 0.24716358725626938\n",
      "The 12910 th iteration gives loss of 0.24715146250187883\n",
      "The 12911 th iteration gives loss of 0.24713933940377272\n",
      "The 12912 th iteration gives loss of 0.247127217961695\n",
      "The 12913 th iteration gives loss of 0.24711509817541186\n",
      "The 12914 th iteration gives loss of 0.24710298004468406\n",
      "The 12915 th iteration gives loss of 0.24709086356924972\n",
      "The 12916 th iteration gives loss of 0.2470787487488845\n",
      "The 12917 th iteration gives loss of 0.24706663558333658\n",
      "The 12918 th iteration gives loss of 0.24705452407236733\n",
      "The 12919 th iteration gives loss of 0.24704241421572504\n",
      "The 12920 th iteration gives loss of 0.2470303060131664\n",
      "The 12921 th iteration gives loss of 0.24701819946445697\n",
      "The 12922 th iteration gives loss of 0.24700609456934203\n",
      "The 12923 th iteration gives loss of 0.24699399132759062\n",
      "The 12924 th iteration gives loss of 0.24698188973895302\n",
      "The 12925 th iteration gives loss of 0.24696978980318426\n",
      "The 12926 th iteration gives loss of 0.2469576915200498\n",
      "The 12927 th iteration gives loss of 0.24694559488929468\n",
      "The 12928 th iteration gives loss of 0.24693349991067848\n",
      "The 12929 th iteration gives loss of 0.24692140658396564\n",
      "The 12930 th iteration gives loss of 0.24690931490890475\n",
      "The 12931 th iteration gives loss of 0.2468972248852574\n",
      "The 12932 th iteration gives loss of 0.2468851365127757\n",
      "The 12933 th iteration gives loss of 0.24687304979122404\n",
      "The 12934 th iteration gives loss of 0.24686096472035612\n",
      "The 12935 th iteration gives loss of 0.2468488812999223\n",
      "The 12936 th iteration gives loss of 0.2468367995296896\n",
      "The 12937 th iteration gives loss of 0.24682471940941353\n",
      "The 12938 th iteration gives loss of 0.24681264093884986\n",
      "The 12939 th iteration gives loss of 0.24680056411774762\n",
      "The 12940 th iteration gives loss of 0.2467884889458679\n",
      "The 12941 th iteration gives loss of 0.24677641542298442\n",
      "The 12942 th iteration gives loss of 0.24676434354883792\n",
      "The 12943 th iteration gives loss of 0.2467522733231853\n",
      "The 12944 th iteration gives loss of 0.2467402047457931\n",
      "The 12945 th iteration gives loss of 0.24672813781640138\n",
      "The 12946 th iteration gives loss of 0.24671607253478403\n",
      "The 12947 th iteration gives loss of 0.2467040089006908\n",
      "The 12948 th iteration gives loss of 0.24669194691388852\n",
      "The 12949 th iteration gives loss of 0.24667988657413012\n",
      "The 12950 th iteration gives loss of 0.24666782788116892\n",
      "The 12951 th iteration gives loss of 0.24665577083476412\n",
      "The 12952 th iteration gives loss of 0.2466437154346746\n",
      "The 12953 th iteration gives loss of 0.24663166168065506\n",
      "The 12954 th iteration gives loss of 0.24661960957245954\n",
      "The 12955 th iteration gives loss of 0.24660755910986978\n",
      "The 12956 th iteration gives loss of 0.24659551029261834\n",
      "The 12957 th iteration gives loss of 0.24658346312046334\n",
      "The 12958 th iteration gives loss of 0.24657141759316878\n",
      "The 12959 th iteration gives loss of 0.24655937371049771\n",
      "The 12960 th iteration gives loss of 0.24654733147220034\n",
      "The 12961 th iteration gives loss of 0.24653529087803572\n",
      "The 12962 th iteration gives loss of 0.24652325192776817\n",
      "The 12963 th iteration gives loss of 0.24651121462114833\n",
      "The 12964 th iteration gives loss of 0.246499178957939\n",
      "The 12965 th iteration gives loss of 0.24648714493789822\n",
      "The 12966 th iteration gives loss of 0.2464751125607766\n",
      "The 12967 th iteration gives loss of 0.24646308182634044\n",
      "The 12968 th iteration gives loss of 0.2464510527343354\n",
      "The 12969 th iteration gives loss of 0.24643902528453596\n",
      "The 12970 th iteration gives loss of 0.2464269994766873\n",
      "The 12971 th iteration gives loss of 0.24641497531056528\n",
      "The 12972 th iteration gives loss of 0.2464029527859123\n",
      "The 12973 th iteration gives loss of 0.24639093190248487\n",
      "The 12974 th iteration gives loss of 0.24637891266004652\n",
      "The 12975 th iteration gives loss of 0.24636689505836032\n",
      "The 12976 th iteration gives loss of 0.24635487909717282\n",
      "The 12977 th iteration gives loss of 0.2463428647762585\n",
      "The 12978 th iteration gives loss of 0.24633085209536296\n",
      "The 12979 th iteration gives loss of 0.24631884105425159\n",
      "The 12980 th iteration gives loss of 0.24630683165268305\n",
      "The 12981 th iteration gives loss of 0.2462948238904036\n",
      "The 12982 th iteration gives loss of 0.24628281776719005\n",
      "The 12983 th iteration gives loss of 0.24627081328279102\n",
      "The 12984 th iteration gives loss of 0.24625881043696587\n",
      "The 12985 th iteration gives loss of 0.24624680922946618\n",
      "The 12986 th iteration gives loss of 0.2462348096600573\n",
      "The 12987 th iteration gives loss of 0.24622281172850594\n",
      "The 12988 th iteration gives loss of 0.24621081543455722\n",
      "The 12989 th iteration gives loss of 0.24619882077798017\n",
      "The 12990 th iteration gives loss of 0.24618682775852474\n",
      "The 12991 th iteration gives loss of 0.2461748363759572\n",
      "The 12992 th iteration gives loss of 0.24616284663003205\n",
      "The 12993 th iteration gives loss of 0.24615085852051372\n",
      "The 12994 th iteration gives loss of 0.2461388720471505\n",
      "The 12995 th iteration gives loss of 0.24612688720971432\n",
      "The 12996 th iteration gives loss of 0.2461149040079496\n",
      "The 12997 th iteration gives loss of 0.24610292244163393\n",
      "The 12998 th iteration gives loss of 0.24609094251051575\n",
      "The 12999 th iteration gives loss of 0.24607896421434358\n",
      "The 13000 th iteration gives loss of 0.24606698755289222\n",
      "The 13001 th iteration gives loss of 0.24605501252591586\n",
      "The 13002 th iteration gives loss of 0.24604303913316938\n",
      "The 13003 th iteration gives loss of 0.2460310673744185\n",
      "The 13004 th iteration gives loss of 0.24601909724942056\n",
      "The 13005 th iteration gives loss of 0.24600712875794065\n",
      "The 13006 th iteration gives loss of 0.24599516189972542\n",
      "The 13007 th iteration gives loss of 0.24598319667453664\n",
      "The 13008 th iteration gives loss of 0.24597123308214586\n",
      "The 13009 th iteration gives loss of 0.24595927112229654\n",
      "The 13010 th iteration gives loss of 0.2459473107947616\n",
      "The 13011 th iteration gives loss of 0.2459353520992814\n",
      "The 13012 th iteration gives loss of 0.2459233950356479\n",
      "The 13013 th iteration gives loss of 0.24591143960358777\n",
      "The 13014 th iteration gives loss of 0.24589948580287843\n",
      "The 13015 th iteration gives loss of 0.24588753363327548\n",
      "The 13016 th iteration gives loss of 0.24587558309453036\n",
      "The 13017 th iteration gives loss of 0.24586363418642038\n",
      "The 13018 th iteration gives loss of 0.24585168690869436\n",
      "The 13019 th iteration gives loss of 0.2458397412611067\n",
      "The 13020 th iteration gives loss of 0.2458277972434259\n",
      "The 13021 th iteration gives loss of 0.24581585485540564\n",
      "The 13022 th iteration gives loss of 0.24580391409681093\n",
      "The 13023 th iteration gives loss of 0.24579197496740277\n",
      "The 13024 th iteration gives loss of 0.24578003746693414\n",
      "The 13025 th iteration gives loss of 0.24576810159517332\n",
      "The 13026 th iteration gives loss of 0.24575616735187036\n",
      "The 13027 th iteration gives loss of 0.24574423473678816\n",
      "The 13028 th iteration gives loss of 0.24573230374969615\n",
      "The 13029 th iteration gives loss of 0.24572037439034813\n",
      "The 13030 th iteration gives loss of 0.24570844665849892\n",
      "The 13031 th iteration gives loss of 0.24569652055390737\n",
      "The 13032 th iteration gives loss of 0.2456845960763433\n",
      "The 13033 th iteration gives loss of 0.24567267322556627\n",
      "The 13034 th iteration gives loss of 0.24566075200133086\n",
      "The 13035 th iteration gives loss of 0.2456488324033943\n",
      "The 13036 th iteration gives loss of 0.24563691443152708\n",
      "The 13037 th iteration gives loss of 0.2456249980854818\n",
      "The 13038 th iteration gives loss of 0.24561308336502236\n",
      "The 13039 th iteration gives loss of 0.24560117026990116\n",
      "The 13040 th iteration gives loss of 0.24558925879988838\n",
      "The 13041 th iteration gives loss of 0.24557734895474997\n",
      "The 13042 th iteration gives loss of 0.24556544073422154\n",
      "The 13043 th iteration gives loss of 0.24555353413808867\n",
      "The 13044 th iteration gives loss of 0.24554162916610014\n",
      "The 13045 th iteration gives loss of 0.24552972581802285\n",
      "The 13046 th iteration gives loss of 0.24551782409361128\n",
      "The 13047 th iteration gives loss of 0.245505923992622\n",
      "The 13048 th iteration gives loss of 0.24549402551482685\n",
      "The 13049 th iteration gives loss of 0.24548212865997815\n",
      "The 13050 th iteration gives loss of 0.2454702334278378\n",
      "The 13051 th iteration gives loss of 0.2454583398181754\n",
      "The 13052 th iteration gives loss of 0.24544644783074152\n",
      "The 13053 th iteration gives loss of 0.24543455746530177\n",
      "The 13054 th iteration gives loss of 0.2454226687216132\n",
      "The 13055 th iteration gives loss of 0.24541078159943422\n",
      "The 13056 th iteration gives loss of 0.24539889609853446\n",
      "The 13057 th iteration gives loss of 0.24538701221866982\n",
      "The 13058 th iteration gives loss of 0.24537512995960148\n",
      "The 13059 th iteration gives loss of 0.24536324932109219\n",
      "The 13060 th iteration gives loss of 0.24535137030289902\n",
      "The 13061 th iteration gives loss of 0.24533949290478696\n",
      "The 13062 th iteration gives loss of 0.24532761712651205\n",
      "The 13063 th iteration gives loss of 0.24531574296783798\n",
      "The 13064 th iteration gives loss of 0.24530387042853055\n",
      "The 13065 th iteration gives loss of 0.24529199950834465\n",
      "The 13066 th iteration gives loss of 0.24528013020704692\n",
      "The 13067 th iteration gives loss of 0.24526826252439574\n",
      "The 13068 th iteration gives loss of 0.24525639646014621\n",
      "The 13069 th iteration gives loss of 0.2452445320140688\n",
      "The 13070 th iteration gives loss of 0.24523266918592473\n",
      "The 13071 th iteration gives loss of 0.24522080797546372\n",
      "The 13072 th iteration gives loss of 0.24520894838245694\n",
      "The 13073 th iteration gives loss of 0.2451970904066653\n",
      "The 13074 th iteration gives loss of 0.2451852340478457\n",
      "The 13075 th iteration gives loss of 0.24517337930577401\n",
      "The 13076 th iteration gives loss of 0.24516152618019088\n",
      "The 13077 th iteration gives loss of 0.24514967467086554\n",
      "The 13078 th iteration gives loss of 0.2451378247775667\n",
      "The 13079 th iteration gives loss of 0.2451259765000484\n",
      "The 13080 th iteration gives loss of 0.24511412983807168\n",
      "The 13081 th iteration gives loss of 0.245102284791403\n",
      "The 13082 th iteration gives loss of 0.24509044135979793\n",
      "The 13083 th iteration gives loss of 0.24507859954302666\n",
      "The 13084 th iteration gives loss of 0.2450667593408542\n",
      "The 13085 th iteration gives loss of 0.24505492075302687\n",
      "The 13086 th iteration gives loss of 0.24504308377930956\n",
      "The 13087 th iteration gives loss of 0.245031248419477\n",
      "The 13088 th iteration gives loss of 0.24501941467327573\n",
      "The 13089 th iteration gives loss of 0.24500758254048438\n",
      "The 13090 th iteration gives loss of 0.24499575202083806\n",
      "The 13091 th iteration gives loss of 0.2449839231141265\n",
      "The 13092 th iteration gives loss of 0.24497209582010177\n",
      "The 13093 th iteration gives loss of 0.2449602701385237\n",
      "The 13094 th iteration gives loss of 0.24494844606915137\n",
      "The 13095 th iteration gives loss of 0.2449366236117555\n",
      "The 13096 th iteration gives loss of 0.24492480276609233\n",
      "The 13097 th iteration gives loss of 0.24491298353192212\n",
      "The 13098 th iteration gives loss of 0.2449011659090172\n",
      "The 13099 th iteration gives loss of 0.24488934989712666\n",
      "The 13100 th iteration gives loss of 0.24487753549602104\n",
      "The 13101 th iteration gives loss of 0.24486572270545995\n",
      "The 13102 th iteration gives loss of 0.24485391152520647\n",
      "The 13103 th iteration gives loss of 0.24484210195502157\n",
      "The 13104 th iteration gives loss of 0.24483029399466463\n",
      "The 13105 th iteration gives loss of 0.2448184876439074\n",
      "The 13106 th iteration gives loss of 0.24480668290250326\n",
      "The 13107 th iteration gives loss of 0.24479487977022116\n",
      "The 13108 th iteration gives loss of 0.24478307824682327\n",
      "The 13109 th iteration gives loss of 0.24477127833206588\n",
      "The 13110 th iteration gives loss of 0.24475948002571107\n",
      "The 13111 th iteration gives loss of 0.2447476833275259\n",
      "The 13112 th iteration gives loss of 0.24473588823727446\n",
      "The 13113 th iteration gives loss of 0.24472409475472548\n",
      "The 13114 th iteration gives loss of 0.24471230287962534\n",
      "The 13115 th iteration gives loss of 0.24470051261174666\n",
      "The 13116 th iteration gives loss of 0.24468872395084731\n",
      "The 13117 th iteration gives loss of 0.2446769368966964\n",
      "The 13118 th iteration gives loss of 0.2446651514490495\n",
      "The 13119 th iteration gives loss of 0.244653367607673\n",
      "The 13120 th iteration gives loss of 0.2446415853723365\n",
      "The 13121 th iteration gives loss of 0.24462980474278848\n",
      "The 13122 th iteration gives loss of 0.24461802571879804\n",
      "The 13123 th iteration gives loss of 0.24460624830014022\n",
      "The 13124 th iteration gives loss of 0.2445944724865618\n",
      "The 13125 th iteration gives loss of 0.24458269827783835\n",
      "The 13126 th iteration gives loss of 0.24457092567371386\n",
      "The 13127 th iteration gives loss of 0.2445591546739699\n",
      "The 13128 th iteration gives loss of 0.24454738527836206\n",
      "The 13129 th iteration gives loss of 0.2445356174866538\n",
      "The 13130 th iteration gives loss of 0.24452385129860746\n",
      "The 13131 th iteration gives loss of 0.24451208671399202\n",
      "The 13132 th iteration gives loss of 0.2445003237325678\n",
      "The 13133 th iteration gives loss of 0.24448856235409105\n",
      "The 13134 th iteration gives loss of 0.24447680257834253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 13135 th iteration gives loss of 0.24446504440506145\n",
      "The 13136 th iteration gives loss of 0.24445328783402073\n",
      "The 13137 th iteration gives loss of 0.24444153286499307\n",
      "The 13138 th iteration gives loss of 0.24442977949773656\n",
      "The 13139 th iteration gives loss of 0.24441802773200902\n",
      "The 13140 th iteration gives loss of 0.24440627756758312\n",
      "The 13141 th iteration gives loss of 0.24439452900421893\n",
      "The 13142 th iteration gives loss of 0.24438278204166983\n",
      "The 13143 th iteration gives loss of 0.24437103667971755\n",
      "The 13144 th iteration gives loss of 0.24435929291810637\n",
      "The 13145 th iteration gives loss of 0.24434755075661313\n",
      "The 13146 th iteration gives loss of 0.24433581019499997\n",
      "The 13147 th iteration gives loss of 0.24432407123303138\n",
      "The 13148 th iteration gives loss of 0.24431233387046258\n",
      "The 13149 th iteration gives loss of 0.24430059810706545\n",
      "The 13150 th iteration gives loss of 0.24428886394259752\n",
      "The 13151 th iteration gives loss of 0.24427713137682777\n",
      "The 13152 th iteration gives loss of 0.24426540040951963\n",
      "The 13153 th iteration gives loss of 0.24425367104044002\n",
      "The 13154 th iteration gives loss of 0.24424194326934245\n",
      "The 13155 th iteration gives loss of 0.24423021709599407\n",
      "The 13156 th iteration gives loss of 0.24421849252017058\n",
      "The 13157 th iteration gives loss of 0.2442067695416239\n",
      "The 13158 th iteration gives loss of 0.24419504816012333\n",
      "The 13159 th iteration gives loss of 0.24418332837543544\n",
      "The 13160 th iteration gives loss of 0.24417161018731\n",
      "The 13161 th iteration gives loss of 0.24415989359552423\n",
      "The 13162 th iteration gives loss of 0.24414817859984195\n",
      "The 13163 th iteration gives loss of 0.24413646520002139\n",
      "The 13164 th iteration gives loss of 0.2441247533958306\n",
      "The 13165 th iteration gives loss of 0.2441130431870272\n",
      "The 13166 th iteration gives loss of 0.24410133457339214\n",
      "The 13167 th iteration gives loss of 0.24408962755466926\n",
      "The 13168 th iteration gives loss of 0.24407792213063853\n",
      "The 13169 th iteration gives loss of 0.24406621830105515\n",
      "The 13170 th iteration gives loss of 0.24405451606568873\n",
      "The 13171 th iteration gives loss of 0.24404281542430434\n",
      "The 13172 th iteration gives loss of 0.24403111637665928\n",
      "The 13173 th iteration gives loss of 0.24401941892251894\n",
      "The 13174 th iteration gives loss of 0.24400772306165724\n",
      "The 13175 th iteration gives loss of 0.24399602879382956\n",
      "The 13176 th iteration gives loss of 0.24398433611880962\n",
      "The 13177 th iteration gives loss of 0.24397264503634622\n",
      "The 13178 th iteration gives loss of 0.24396095554622146\n",
      "The 13179 th iteration gives loss of 0.24394926764818803\n",
      "The 13180 th iteration gives loss of 0.24393758134201257\n",
      "The 13181 th iteration gives loss of 0.2439258966274662\n",
      "The 13182 th iteration gives loss of 0.24391421350431347\n",
      "The 13183 th iteration gives loss of 0.24390253197230027\n",
      "The 13184 th iteration gives loss of 0.24389085203122476\n",
      "The 13185 th iteration gives loss of 0.2438791736808192\n",
      "The 13186 th iteration gives loss of 0.2438674969208721\n",
      "The 13187 th iteration gives loss of 0.24385582175114257\n",
      "The 13188 th iteration gives loss of 0.24384414817137767\n",
      "The 13189 th iteration gives loss of 0.24383247618136902\n",
      "The 13190 th iteration gives loss of 0.2438208057808634\n",
      "The 13191 th iteration gives loss of 0.2438091369696306\n",
      "The 13192 th iteration gives loss of 0.24379746974744063\n",
      "The 13193 th iteration gives loss of 0.24378580411405967\n",
      "The 13194 th iteration gives loss of 0.24377414006924195\n",
      "The 13195 th iteration gives loss of 0.2437624776127557\n",
      "The 13196 th iteration gives loss of 0.24375081674437513\n",
      "The 13197 th iteration gives loss of 0.24373915746385777\n",
      "The 13198 th iteration gives loss of 0.24372749977096686\n",
      "The 13199 th iteration gives loss of 0.24371584366547847\n",
      "The 13200 th iteration gives loss of 0.24370418914714298\n",
      "The 13201 th iteration gives loss of 0.24369253621573692\n",
      "The 13202 th iteration gives loss of 0.24368088487102407\n",
      "The 13203 th iteration gives loss of 0.24366923511276609\n",
      "The 13204 th iteration gives loss of 0.24365758694072806\n",
      "The 13205 th iteration gives loss of 0.24364594035468531\n",
      "The 13206 th iteration gives loss of 0.2436342953543848\n",
      "The 13207 th iteration gives loss of 0.2436226519396075\n",
      "The 13208 th iteration gives loss of 0.2436110101101182\n",
      "The 13209 th iteration gives loss of 0.24359936986567562\n",
      "The 13210 th iteration gives loss of 0.24358773120605212\n",
      "The 13211 th iteration gives loss of 0.24357609413100106\n",
      "The 13212 th iteration gives loss of 0.24356445864029785\n",
      "The 13213 th iteration gives loss of 0.24355282473371292\n",
      "The 13214 th iteration gives loss of 0.24354119241101113\n",
      "The 13215 th iteration gives loss of 0.24352956167195122\n",
      "The 13216 th iteration gives loss of 0.2435179325163062\n",
      "The 13217 th iteration gives loss of 0.24350630494383263\n",
      "The 13218 th iteration gives loss of 0.2434946789542877\n",
      "The 13219 th iteration gives loss of 0.24348305454746605\n",
      "The 13220 th iteration gives loss of 0.24347143172310903\n",
      "The 13221 th iteration gives loss of 0.2434598104809851\n",
      "The 13222 th iteration gives loss of 0.24344819082088262\n",
      "The 13223 th iteration gives loss of 0.24343657274254007\n",
      "The 13224 th iteration gives loss of 0.24342495624574179\n",
      "The 13225 th iteration gives loss of 0.24341334133024423\n",
      "The 13226 th iteration gives loss of 0.24340172799581652\n",
      "The 13227 th iteration gives loss of 0.24339011624222145\n",
      "The 13228 th iteration gives loss of 0.24337850606923286\n",
      "The 13229 th iteration gives loss of 0.243366897476611\n",
      "The 13230 th iteration gives loss of 0.24335529046412616\n",
      "The 13231 th iteration gives loss of 0.24334368503154127\n",
      "The 13232 th iteration gives loss of 0.24333208117862312\n",
      "The 13233 th iteration gives loss of 0.24332047890513545\n",
      "The 13234 th iteration gives loss of 0.2433088782108447\n",
      "The 13235 th iteration gives loss of 0.24329727909552173\n",
      "The 13236 th iteration gives loss of 0.2432856815589358\n",
      "The 13237 th iteration gives loss of 0.24327408560084318\n",
      "The 13238 th iteration gives loss of 0.24326249122102506\n",
      "The 13239 th iteration gives loss of 0.24325089841923198\n",
      "The 13240 th iteration gives loss of 0.24323930719524237\n",
      "The 13241 th iteration gives loss of 0.24322771754881536\n",
      "The 13242 th iteration gives loss of 0.2432161294797118\n",
      "The 13243 th iteration gives loss of 0.24320454298771543\n",
      "The 13244 th iteration gives loss of 0.24319295807258628\n",
      "The 13245 th iteration gives loss of 0.24318137473408402\n",
      "The 13246 th iteration gives loss of 0.24316979297197858\n",
      "The 13247 th iteration gives loss of 0.24315821278603822\n",
      "The 13248 th iteration gives loss of 0.24314663417603874\n",
      "The 13249 th iteration gives loss of 0.24313505714172545\n",
      "The 13250 th iteration gives loss of 0.24312348168287948\n",
      "The 13251 th iteration gives loss of 0.2431119077992709\n",
      "The 13252 th iteration gives loss of 0.2431003354906661\n",
      "The 13253 th iteration gives loss of 0.24308876475682045\n",
      "The 13254 th iteration gives loss of 0.24307719559750354\n",
      "The 13255 th iteration gives loss of 0.24306562801250556\n",
      "The 13256 th iteration gives loss of 0.24305406200156446\n",
      "The 13257 th iteration gives loss of 0.24304249756445367\n",
      "The 13258 th iteration gives loss of 0.2430309347009502\n",
      "The 13259 th iteration gives loss of 0.24301937341081167\n",
      "The 13260 th iteration gives loss of 0.24300781369380808\n",
      "The 13261 th iteration gives loss of 0.2429962555497078\n",
      "The 13262 th iteration gives loss of 0.2429846989782789\n",
      "The 13263 th iteration gives loss of 0.24297314397928718\n",
      "The 13264 th iteration gives loss of 0.24296159055250657\n",
      "The 13265 th iteration gives loss of 0.24295003869769197\n",
      "The 13266 th iteration gives loss of 0.24293848841461907\n",
      "The 13267 th iteration gives loss of 0.2429269397030528\n",
      "The 13268 th iteration gives loss of 0.24291539256275957\n",
      "The 13269 th iteration gives loss of 0.24290384699350528\n",
      "The 13270 th iteration gives loss of 0.24289230299506684\n",
      "The 13271 th iteration gives loss of 0.24288076056719826\n",
      "The 13272 th iteration gives loss of 0.24286921970967884\n",
      "The 13273 th iteration gives loss of 0.24285768042226408\n",
      "The 13274 th iteration gives loss of 0.2428461427047348\n",
      "The 13275 th iteration gives loss of 0.24283460655685046\n",
      "The 13276 th iteration gives loss of 0.2428230719783849\n",
      "The 13277 th iteration gives loss of 0.2428115389690946\n",
      "The 13278 th iteration gives loss of 0.24280000752875788\n",
      "The 13279 th iteration gives loss of 0.24278847765713812\n",
      "The 13280 th iteration gives loss of 0.24277694935400643\n",
      "The 13281 th iteration gives loss of 0.24276542261912532\n",
      "The 13282 th iteration gives loss of 0.24275389745226697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 13283 th iteration gives loss of 0.24274237385319378\n",
      "The 13284 th iteration gives loss of 0.24273085182167747\n",
      "The 13285 th iteration gives loss of 0.24271933135749427\n",
      "The 13286 th iteration gives loss of 0.24270781246040193\n",
      "The 13287 th iteration gives loss of 0.2426962951301692\n",
      "The 13288 th iteration gives loss of 0.24268477936655966\n",
      "The 13289 th iteration gives loss of 0.24267326516934878\n",
      "The 13290 th iteration gives loss of 0.24266175253830755\n",
      "The 13291 th iteration gives loss of 0.24265024147319803\n",
      "The 13292 th iteration gives loss of 0.24263873197378627\n",
      "The 13293 th iteration gives loss of 0.24262722403984374\n",
      "The 13294 th iteration gives loss of 0.24261571767113757\n",
      "The 13295 th iteration gives loss of 0.24260421286743578\n",
      "The 13296 th iteration gives loss of 0.242592709628521\n",
      "The 13297 th iteration gives loss of 0.24258120795413704\n",
      "The 13298 th iteration gives loss of 0.2425697078440711\n",
      "The 13299 th iteration gives loss of 0.24255820929807892\n",
      "The 13300 th iteration gives loss of 0.24254671231593658\n",
      "The 13301 th iteration gives loss of 0.24253521689740837\n",
      "The 13302 th iteration gives loss of 0.2425237230422667\n",
      "The 13303 th iteration gives loss of 0.24251223075027678\n",
      "The 13304 th iteration gives loss of 0.24250074002120253\n",
      "The 13305 th iteration gives loss of 0.2424892508548249\n",
      "The 13306 th iteration gives loss of 0.24247776325090345\n",
      "The 13307 th iteration gives loss of 0.24246627720920913\n",
      "The 13308 th iteration gives loss of 0.2424547927295099\n",
      "The 13309 th iteration gives loss of 0.2424433098115721\n",
      "The 13310 th iteration gives loss of 0.24243182845517133\n",
      "The 13311 th iteration gives loss of 0.2424203486600699\n",
      "The 13312 th iteration gives loss of 0.24240887042604303\n",
      "The 13313 th iteration gives loss of 0.2423973937528443\n",
      "The 13314 th iteration gives loss of 0.24238591864027237\n",
      "The 13315 th iteration gives loss of 0.2423744450880696\n",
      "The 13316 th iteration gives loss of 0.2423629730960112\n",
      "The 13317 th iteration gives loss of 0.24235150266386718\n",
      "The 13318 th iteration gives loss of 0.24234003379140473\n",
      "The 13319 th iteration gives loss of 0.2423285664784006\n",
      "The 13320 th iteration gives loss of 0.24231710072461518\n",
      "The 13321 th iteration gives loss of 0.24230563652980855\n",
      "The 13322 th iteration gives loss of 0.24229417389376992\n",
      "The 13323 th iteration gives loss of 0.2422827128162666\n",
      "The 13324 th iteration gives loss of 0.24227125329705615\n",
      "The 13325 th iteration gives loss of 0.2422597953359164\n",
      "The 13326 th iteration gives loss of 0.24224833893260614\n",
      "The 13327 th iteration gives loss of 0.2422368840869073\n",
      "The 13328 th iteration gives loss of 0.24222543079857653\n",
      "The 13329 th iteration gives loss of 0.2422139790673914\n",
      "The 13330 th iteration gives loss of 0.24220252889312646\n",
      "The 13331 th iteration gives loss of 0.2421910802755412\n",
      "The 13332 th iteration gives loss of 0.24217963321440425\n",
      "The 13333 th iteration gives loss of 0.2421681877094882\n",
      "The 13334 th iteration gives loss of 0.24215674376057056\n",
      "The 13335 th iteration gives loss of 0.24214530136740872\n",
      "The 13336 th iteration gives loss of 0.24213386052976765\n",
      "The 13337 th iteration gives loss of 0.2421224212474377\n",
      "The 13338 th iteration gives loss of 0.24211098352017565\n",
      "The 13339 th iteration gives loss of 0.2420995473477495\n",
      "The 13340 th iteration gives loss of 0.2420881127299331\n",
      "The 13341 th iteration gives loss of 0.24207667966649196\n",
      "The 13342 th iteration gives loss of 0.24206524815719735\n",
      "The 13343 th iteration gives loss of 0.24205381820181932\n",
      "The 13344 th iteration gives loss of 0.242042389800132\n",
      "The 13345 th iteration gives loss of 0.24203096295189722\n",
      "The 13346 th iteration gives loss of 0.24201953765689294\n",
      "The 13347 th iteration gives loss of 0.24200811391487606\n",
      "The 13348 th iteration gives loss of 0.24199669172563248\n",
      "The 13349 th iteration gives loss of 0.24198527108892137\n",
      "The 13350 th iteration gives loss of 0.24197385200451754\n",
      "The 13351 th iteration gives loss of 0.24196243447219537\n",
      "The 13352 th iteration gives loss of 0.24195101849171044\n",
      "The 13353 th iteration gives loss of 0.24193960406284581\n",
      "The 13354 th iteration gives loss of 0.24192819118535663\n",
      "The 13355 th iteration gives loss of 0.2419167798590343\n",
      "The 13356 th iteration gives loss of 0.24190537008363677\n",
      "The 13357 th iteration gives loss of 0.2418939618589335\n",
      "The 13358 th iteration gives loss of 0.24188255518469695\n",
      "The 13359 th iteration gives loss of 0.24187115006069837\n",
      "The 13360 th iteration gives loss of 0.24185974648669936\n",
      "The 13361 th iteration gives loss of 0.241848344462489\n",
      "The 13362 th iteration gives loss of 0.24183694398782046\n",
      "The 13363 th iteration gives loss of 0.2418255450624694\n",
      "The 13364 th iteration gives loss of 0.24181414768619944\n",
      "The 13365 th iteration gives loss of 0.24180275185879135\n",
      "The 13366 th iteration gives loss of 0.24179135758001488\n",
      "The 13367 th iteration gives loss of 0.24177996484962938\n",
      "The 13368 th iteration gives loss of 0.24176857366742255\n",
      "The 13369 th iteration gives loss of 0.2417571840331523\n",
      "The 13370 th iteration gives loss of 0.24174579594659118\n",
      "The 13371 th iteration gives loss of 0.24173440940751634\n",
      "The 13372 th iteration gives loss of 0.241723024415685\n",
      "The 13373 th iteration gives loss of 0.24171164097088021\n",
      "The 13374 th iteration gives loss of 0.2417002590728653\n",
      "The 13375 th iteration gives loss of 0.24168887872141565\n",
      "The 13376 th iteration gives loss of 0.2416774999162979\n",
      "The 13377 th iteration gives loss of 0.24166612265728815\n",
      "The 13378 th iteration gives loss of 0.2416547469441601\n",
      "The 13379 th iteration gives loss of 0.24164337277667497\n",
      "The 13380 th iteration gives loss of 0.2416320001546021\n",
      "The 13381 th iteration gives loss of 0.24162062907772086\n",
      "The 13382 th iteration gives loss of 0.24160925954579715\n",
      "The 13383 th iteration gives loss of 0.24159789155860817\n",
      "The 13384 th iteration gives loss of 0.2415865251159173\n",
      "The 13385 th iteration gives loss of 0.24157516021749809\n",
      "The 13386 th iteration gives loss of 0.2415637968631184\n",
      "The 13387 th iteration gives loss of 0.24155243505255758\n",
      "The 13388 th iteration gives loss of 0.2415410747855833\n",
      "The 13389 th iteration gives loss of 0.24152971606195572\n",
      "The 13390 th iteration gives loss of 0.24151835888146425\n",
      "The 13391 th iteration gives loss of 0.24150700324387234\n",
      "The 13392 th iteration gives loss of 0.2414956491489496\n",
      "The 13393 th iteration gives loss of 0.24148429659645976\n",
      "The 13394 th iteration gives loss of 0.24147294558618107\n",
      "The 13395 th iteration gives loss of 0.24146159611789927\n",
      "The 13396 th iteration gives loss of 0.24145024819135613\n",
      "The 13397 th iteration gives loss of 0.241438901806352\n",
      "The 13398 th iteration gives loss of 0.24142755696264168\n",
      "The 13399 th iteration gives loss of 0.24141621366000127\n",
      "The 13400 th iteration gives loss of 0.24140487189819337\n",
      "The 13401 th iteration gives loss of 0.24139353167700067\n",
      "The 13402 th iteration gives loss of 0.24138219299619296\n",
      "The 13403 th iteration gives loss of 0.24137085585554138\n",
      "The 13404 th iteration gives loss of 0.24135952025481788\n",
      "The 13405 th iteration gives loss of 0.24134818619379234\n",
      "The 13406 th iteration gives loss of 0.241336853672231\n",
      "The 13407 th iteration gives loss of 0.24132552268991905\n",
      "The 13408 th iteration gives loss of 0.24131419324661108\n",
      "The 13409 th iteration gives loss of 0.24130286534208997\n",
      "The 13410 th iteration gives loss of 0.24129153897612268\n",
      "The 13411 th iteration gives loss of 0.24128021414848597\n",
      "The 13412 th iteration gives loss of 0.24126889085894987\n",
      "The 13413 th iteration gives loss of 0.2412575691072828\n",
      "The 13414 th iteration gives loss of 0.24124624889326943\n",
      "The 13415 th iteration gives loss of 0.24123493021666298\n",
      "The 13416 th iteration gives loss of 0.24122361307724385\n",
      "The 13417 th iteration gives loss of 0.24121229747478712\n",
      "The 13418 th iteration gives loss of 0.24120098340906085\n",
      "The 13419 th iteration gives loss of 0.24118967087983934\n",
      "The 13420 th iteration gives loss of 0.24117835988688924\n",
      "The 13421 th iteration gives loss of 0.24116705042999304\n",
      "The 13422 th iteration gives loss of 0.24115574250890512\n",
      "The 13423 th iteration gives loss of 0.24114443612341668\n",
      "The 13424 th iteration gives loss of 0.24113313127328834\n",
      "The 13425 th iteration gives loss of 0.24112182795830164\n",
      "The 13426 th iteration gives loss of 0.24111052617822082\n",
      "The 13427 th iteration gives loss of 0.24109922593281782\n",
      "The 13428 th iteration gives loss of 0.24108792722187125\n",
      "The 13429 th iteration gives loss of 0.24107663004514787\n",
      "The 13430 th iteration gives loss of 0.24106533440242076\n",
      "The 13431 th iteration gives loss of 0.2410540402934688\n",
      "The 13432 th iteration gives loss of 0.24104274771805342\n",
      "The 13433 th iteration gives loss of 0.2410314566759606\n",
      "The 13434 th iteration gives loss of 0.2410201671669503\n",
      "The 13435 th iteration gives loss of 0.24100887919079694\n",
      "The 13436 th iteration gives loss of 0.2409975927472774\n",
      "The 13437 th iteration gives loss of 0.24098630783616462\n",
      "The 13438 th iteration gives loss of 0.24097502445722535\n",
      "The 13439 th iteration gives loss of 0.24096374261023465\n",
      "The 13440 th iteration gives loss of 0.2409524622949668\n",
      "The 13441 th iteration gives loss of 0.24094118351119675\n",
      "The 13442 th iteration gives loss of 0.24092990625869198\n",
      "The 13443 th iteration gives loss of 0.24091863053723675\n",
      "The 13444 th iteration gives loss of 0.24090735634658933\n",
      "The 13445 th iteration gives loss of 0.24089608368652943\n",
      "The 13446 th iteration gives loss of 0.2408848125568277\n",
      "The 13447 th iteration gives loss of 0.24087354295725955\n",
      "The 13448 th iteration gives loss of 0.2408622748875949\n",
      "The 13449 th iteration gives loss of 0.24085100834760406\n",
      "The 13450 th iteration gives loss of 0.24083974333706662\n",
      "The 13451 th iteration gives loss of 0.2408284798557549\n",
      "The 13452 th iteration gives loss of 0.24081721790343763\n",
      "The 13453 th iteration gives loss of 0.24080595747989086\n",
      "The 13454 th iteration gives loss of 0.24079469858488636\n",
      "The 13455 th iteration gives loss of 0.2407834412182034\n",
      "The 13456 th iteration gives loss of 0.2407721853796\n",
      "The 13457 th iteration gives loss of 0.24076093106886803\n",
      "The 13458 th iteration gives loss of 0.2407496782857631\n",
      "The 13459 th iteration gives loss of 0.24073842703007886\n",
      "The 13460 th iteration gives loss of 0.24072717730156706\n",
      "The 13461 th iteration gives loss of 0.24071592910002076\n",
      "The 13462 th iteration gives loss of 0.24070468242519297\n",
      "The 13463 th iteration gives loss of 0.2406934372768607\n",
      "The 13464 th iteration gives loss of 0.2406821936548138\n",
      "The 13465 th iteration gives loss of 0.24067095155881493\n",
      "The 13466 th iteration gives loss of 0.24065971098863725\n",
      "The 13467 th iteration gives loss of 0.24064847194405153\n",
      "The 13468 th iteration gives loss of 0.24063723442484333\n",
      "The 13469 th iteration gives loss of 0.24062599843077265\n",
      "The 13470 th iteration gives loss of 0.24061476396161588\n",
      "The 13471 th iteration gives loss of 0.2406035310171449\n",
      "The 13472 th iteration gives loss of 0.24059229959714196\n",
      "The 13473 th iteration gives loss of 0.24058106970138088\n",
      "The 13474 th iteration gives loss of 0.24056984132962866\n",
      "The 13475 th iteration gives loss of 0.24055861448165639\n",
      "The 13476 th iteration gives loss of 0.24054738915724727\n",
      "The 13477 th iteration gives loss of 0.24053616535616035\n",
      "The 13478 th iteration gives loss of 0.2405249430781906\n",
      "The 13479 th iteration gives loss of 0.24051372232309393\n",
      "The 13480 th iteration gives loss of 0.24050250309065005\n",
      "The 13481 th iteration gives loss of 0.24049128538063785\n",
      "The 13482 th iteration gives loss of 0.24048006919281786\n",
      "The 13483 th iteration gives loss of 0.24046885452698616\n",
      "The 13484 th iteration gives loss of 0.24045764138289646\n",
      "The 13485 th iteration gives loss of 0.24044642976032463\n",
      "The 13486 th iteration gives loss of 0.24043521965905806\n",
      "The 13487 th iteration gives loss of 0.24042401107886435\n",
      "The 13488 th iteration gives loss of 0.24041280401951629\n",
      "The 13489 th iteration gives loss of 0.24040159848078413\n",
      "The 13490 th iteration gives loss of 0.24039039446243737\n",
      "The 13491 th iteration gives loss of 0.24037919196426533\n",
      "The 13492 th iteration gives loss of 0.2403679909860308\n",
      "The 13493 th iteration gives loss of 0.2403567915275239\n",
      "The 13494 th iteration gives loss of 0.24034559358849727\n",
      "The 13495 th iteration gives loss of 0.24033439716873997\n",
      "The 13496 th iteration gives loss of 0.24032320226801465\n",
      "The 13497 th iteration gives loss of 0.2403120088861144\n",
      "The 13498 th iteration gives loss of 0.24030081702279124\n",
      "The 13499 th iteration gives loss of 0.24028962667784096\n",
      "The 13500 th iteration gives loss of 0.2402784378510245\n",
      "The 13501 th iteration gives loss of 0.24026725054211284\n",
      "The 13502 th iteration gives loss of 0.2402560647508905\n",
      "The 13503 th iteration gives loss of 0.2402448804771343\n",
      "The 13504 th iteration gives loss of 0.24023369772060388\n",
      "The 13505 th iteration gives loss of 0.24022251648108686\n",
      "The 13506 th iteration gives loss of 0.24021133675836373\n",
      "The 13507 th iteration gives loss of 0.24020015855219248\n",
      "The 13508 th iteration gives loss of 0.2401889818623447\n",
      "The 13509 th iteration gives loss of 0.24017780668861285\n",
      "The 13510 th iteration gives loss of 0.2401666330307616\n",
      "The 13511 th iteration gives loss of 0.24015546088857198\n",
      "The 13512 th iteration gives loss of 0.2401442902618127\n",
      "The 13513 th iteration gives loss of 0.24013312115026192\n",
      "The 13514 th iteration gives loss of 0.2401219535536932\n",
      "The 13515 th iteration gives loss of 0.24011078747188905\n",
      "The 13516 th iteration gives loss of 0.2400996229046042\n",
      "The 13517 th iteration gives loss of 0.24008845985163477\n",
      "The 13518 th iteration gives loss of 0.24007729831274846\n",
      "The 13519 th iteration gives loss of 0.2400661382877138\n",
      "The 13520 th iteration gives loss of 0.24005497977631438\n",
      "The 13521 th iteration gives loss of 0.24004382277831454\n",
      "The 13522 th iteration gives loss of 0.24003266729350606\n",
      "The 13523 th iteration gives loss of 0.24002151332165433\n",
      "The 13524 th iteration gives loss of 0.24001036086254182\n",
      "The 13525 th iteration gives loss of 0.23999920991593604\n",
      "The 13526 th iteration gives loss of 0.2399880604816046\n",
      "The 13527 th iteration gives loss of 0.23997691255934575\n",
      "The 13528 th iteration gives loss of 0.23996576614890736\n",
      "The 13529 th iteration gives loss of 0.2399546212500752\n",
      "The 13530 th iteration gives loss of 0.2399434778626336\n",
      "The 13531 th iteration gives loss of 0.2399323359863537\n",
      "The 13532 th iteration gives loss of 0.23992119562100991\n",
      "The 13533 th iteration gives loss of 0.23991005676637234\n",
      "The 13534 th iteration gives loss of 0.23989891942222036\n",
      "The 13535 th iteration gives loss of 0.23988778358833898\n",
      "The 13536 th iteration gives loss of 0.23987664926449337\n",
      "The 13537 th iteration gives loss of 0.23986551645045365\n",
      "The 13538 th iteration gives loss of 0.23985438514600355\n",
      "The 13539 th iteration gives loss of 0.23984325535092094\n",
      "The 13540 th iteration gives loss of 0.2398321270649772\n",
      "The 13541 th iteration gives loss of 0.2398210002879489\n",
      "The 13542 th iteration gives loss of 0.23980987501961362\n",
      "The 13543 th iteration gives loss of 0.23979875125974492\n",
      "The 13544 th iteration gives loss of 0.2397876290081153\n",
      "The 13545 th iteration gives loss of 0.23977650826450375\n",
      "The 13546 th iteration gives loss of 0.23976538902868885\n",
      "The 13547 th iteration gives loss of 0.2397542713004468\n",
      "The 13548 th iteration gives loss of 0.23974315507955052\n",
      "The 13549 th iteration gives loss of 0.2397320403657718\n",
      "The 13550 th iteration gives loss of 0.23972092715889642\n",
      "The 13551 th iteration gives loss of 0.23970981545869707\n",
      "The 13552 th iteration gives loss of 0.23969870526494105\n",
      "The 13553 th iteration gives loss of 0.2396875965774175\n",
      "The 13554 th iteration gives loss of 0.23967648939589517\n",
      "The 13555 th iteration gives loss of 0.23966538372015508\n",
      "The 13556 th iteration gives loss of 0.2396542795499603\n",
      "The 13557 th iteration gives loss of 0.23964317688510012\n",
      "The 13558 th iteration gives loss of 0.23963207572534884\n",
      "The 13559 th iteration gives loss of 0.2396209760704766\n",
      "The 13560 th iteration gives loss of 0.2396098779202653\n",
      "The 13561 th iteration gives loss of 0.23959878127448586\n",
      "The 13562 th iteration gives loss of 0.2395876861329246\n",
      "The 13563 th iteration gives loss of 0.23957659249534818\n",
      "The 13564 th iteration gives loss of 0.23956550036154353\n",
      "The 13565 th iteration gives loss of 0.23955440973127445\n",
      "The 13566 th iteration gives loss of 0.23954332060431205\n",
      "The 13567 th iteration gives loss of 0.23953223298046122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 13568 th iteration gives loss of 0.23952114685947815\n",
      "The 13569 th iteration gives loss of 0.2395100622411319\n",
      "The 13570 th iteration gives loss of 0.2394989791252157\n",
      "The 13571 th iteration gives loss of 0.23948789751150243\n",
      "The 13572 th iteration gives loss of 0.23947681739976084\n",
      "The 13573 th iteration gives loss of 0.23946573878977664\n",
      "The 13574 th iteration gives loss of 0.23945466168132512\n",
      "The 13575 th iteration gives loss of 0.23944358607417654\n",
      "The 13576 th iteration gives loss of 0.23943251196811188\n",
      "The 13577 th iteration gives loss of 0.23942143936289947\n",
      "The 13578 th iteration gives loss of 0.23941036825833475\n",
      "The 13579 th iteration gives loss of 0.23939929865418086\n",
      "The 13580 th iteration gives loss of 0.23938823055022415\n",
      "The 13581 th iteration gives loss of 0.2393771639462301\n",
      "The 13582 th iteration gives loss of 0.23936609884197943\n",
      "The 13583 th iteration gives loss of 0.23935503523725493\n",
      "The 13584 th iteration gives loss of 0.23934397313182243\n",
      "The 13585 th iteration gives loss of 0.23933291252546995\n",
      "The 13586 th iteration gives loss of 0.23932185341796702\n",
      "The 13587 th iteration gives loss of 0.23931079580909856\n",
      "The 13588 th iteration gives loss of 0.23929973969863125\n",
      "The 13589 th iteration gives loss of 0.23928868508634865\n",
      "The 13590 th iteration gives loss of 0.23927763197203092\n",
      "The 13591 th iteration gives loss of 0.2392665803554531\n",
      "The 13592 th iteration gives loss of 0.2392555302363837\n",
      "The 13593 th iteration gives loss of 0.23924448161461648\n",
      "The 13594 th iteration gives loss of 0.23923343448990791\n",
      "The 13595 th iteration gives loss of 0.2392223888620575\n",
      "The 13596 th iteration gives loss of 0.2392113447308236\n",
      "The 13597 th iteration gives loss of 0.23920030209600326\n",
      "The 13598 th iteration gives loss of 0.23918926095735285\n",
      "The 13599 th iteration gives loss of 0.23917822131465913\n",
      "The 13600 th iteration gives loss of 0.2391671831677085\n",
      "The 13601 th iteration gives loss of 0.2391561465162579\n",
      "The 13602 th iteration gives loss of 0.23914511136009867\n",
      "The 13603 th iteration gives loss of 0.23913407769901507\n",
      "The 13604 th iteration gives loss of 0.2391230455327746\n",
      "The 13605 th iteration gives loss of 0.23911201486114964\n",
      "The 13606 th iteration gives loss of 0.2391009856839241\n",
      "The 13607 th iteration gives loss of 0.23908995800088725\n",
      "The 13608 th iteration gives loss of 0.23907893181179496\n",
      "The 13609 th iteration gives loss of 0.23906790711643797\n",
      "The 13610 th iteration gives loss of 0.23905688391459712\n",
      "The 13611 th iteration gives loss of 0.23904586220603455\n",
      "The 13612 th iteration gives loss of 0.23903484199054406\n",
      "The 13613 th iteration gives loss of 0.23902382326790178\n",
      "The 13614 th iteration gives loss of 0.23901280603787672\n",
      "The 13615 th iteration gives loss of 0.239001790300248\n",
      "The 13616 th iteration gives loss of 0.2389907760548017\n",
      "The 13617 th iteration gives loss of 0.23897976330130521\n",
      "The 13618 th iteration gives loss of 0.23896875203954457\n",
      "The 13619 th iteration gives loss of 0.23895774226929603\n",
      "The 13620 th iteration gives loss of 0.23894673399034352\n",
      "The 13621 th iteration gives loss of 0.2389357272024581\n",
      "The 13622 th iteration gives loss of 0.23892472190541694\n",
      "The 13623 th iteration gives loss of 0.238913718098998\n",
      "The 13624 th iteration gives loss of 0.23890271578298605\n",
      "The 13625 th iteration gives loss of 0.23889171495714992\n",
      "The 13626 th iteration gives loss of 0.23888071562127913\n",
      "The 13627 th iteration gives loss of 0.2388697177751386\n",
      "The 13628 th iteration gives loss of 0.23885872141851427\n",
      "The 13629 th iteration gives loss of 0.23884772655118403\n",
      "The 13630 th iteration gives loss of 0.23883673317292414\n",
      "The 13631 th iteration gives loss of 0.23882574128351303\n",
      "The 13632 th iteration gives loss of 0.2388147508827337\n",
      "The 13633 th iteration gives loss of 0.23880376197036343\n",
      "The 13634 th iteration gives loss of 0.23879277454617404\n",
      "The 13635 th iteration gives loss of 0.2387817886099506\n",
      "The 13636 th iteration gives loss of 0.238770804161471\n",
      "The 13637 th iteration gives loss of 0.23875982120051154\n",
      "The 13638 th iteration gives loss of 0.2387488397268584\n",
      "The 13639 th iteration gives loss of 0.23873785974028056\n",
      "The 13640 th iteration gives loss of 0.23872688124055955\n",
      "The 13641 th iteration gives loss of 0.23871590422746866\n",
      "The 13642 th iteration gives loss of 0.2387049287007995\n",
      "The 13643 th iteration gives loss of 0.23869395466032153\n",
      "The 13644 th iteration gives loss of 0.2386829821058111\n",
      "The 13645 th iteration gives loss of 0.23867201103705768\n",
      "The 13646 th iteration gives loss of 0.23866104145383676\n",
      "The 13647 th iteration gives loss of 0.23865007335591776\n",
      "The 13648 th iteration gives loss of 0.23863910674308508\n",
      "The 13649 th iteration gives loss of 0.23862814161512771\n",
      "The 13650 th iteration gives loss of 0.23861717797181353\n",
      "The 13651 th iteration gives loss of 0.23860621581290956\n",
      "The 13652 th iteration gives loss of 0.2385952551382248\n",
      "The 13653 th iteration gives loss of 0.23858429594751576\n",
      "The 13654 th iteration gives loss of 0.2385733382405653\n",
      "The 13655 th iteration gives loss of 0.2385623820171613\n",
      "The 13656 th iteration gives loss of 0.2385514272770769\n",
      "The 13657 th iteration gives loss of 0.23854047402009068\n",
      "The 13658 th iteration gives loss of 0.23852952224597804\n",
      "The 13659 th iteration gives loss of 0.23851857195452728\n",
      "The 13660 th iteration gives loss of 0.2385076231455034\n",
      "The 13661 th iteration gives loss of 0.23849667581870304\n",
      "The 13662 th iteration gives loss of 0.23848572997389625\n",
      "The 13663 th iteration gives loss of 0.2384747856108751\n",
      "The 13664 th iteration gives loss of 0.23846384272939536\n",
      "The 13665 th iteration gives loss of 0.2384529013292546\n",
      "The 13666 th iteration gives loss of 0.23844196141021673\n",
      "The 13667 th iteration gives loss of 0.23843102297208174\n",
      "The 13668 th iteration gives loss of 0.23842008601461454\n",
      "The 13669 th iteration gives loss of 0.23840915053759373\n",
      "The 13670 th iteration gives loss of 0.23839821654080842\n",
      "The 13671 th iteration gives loss of 0.2383872840240274\n",
      "The 13672 th iteration gives loss of 0.23837635298704604\n",
      "The 13673 th iteration gives loss of 0.23836542342962702\n",
      "The 13674 th iteration gives loss of 0.23835449535155542\n",
      "The 13675 th iteration gives loss of 0.23834356875261967\n",
      "The 13676 th iteration gives loss of 0.23833264363258336\n",
      "The 13677 th iteration gives loss of 0.23832171999123794\n",
      "The 13678 th iteration gives loss of 0.2383107978283634\n",
      "The 13679 th iteration gives loss of 0.23829987714373485\n",
      "The 13680 th iteration gives loss of 0.2382889579371361\n",
      "The 13681 th iteration gives loss of 0.23827804020834228\n",
      "The 13682 th iteration gives loss of 0.23826712395714061\n",
      "The 13683 th iteration gives loss of 0.23825620918329724\n",
      "The 13684 th iteration gives loss of 0.2382452958866044\n",
      "The 13685 th iteration gives loss of 0.23823438406684638\n",
      "The 13686 th iteration gives loss of 0.23822347372378694\n",
      "The 13687 th iteration gives loss of 0.23821256485722142\n",
      "The 13688 th iteration gives loss of 0.2382016574669125\n",
      "The 13689 th iteration gives loss of 0.23819075155266037\n",
      "The 13690 th iteration gives loss of 0.2381798471142278\n",
      "The 13691 th iteration gives loss of 0.23816894415140835\n",
      "The 13692 th iteration gives loss of 0.23815804266398494\n",
      "The 13693 th iteration gives loss of 0.23814714265171552\n",
      "The 13694 th iteration gives loss of 0.23813624411440137\n",
      "The 13695 th iteration gives loss of 0.23812534705181437\n",
      "The 13696 th iteration gives loss of 0.23811445146373666\n",
      "The 13697 th iteration gives loss of 0.23810355734994004\n",
      "The 13698 th iteration gives loss of 0.23809266471022764\n",
      "The 13699 th iteration gives loss of 0.23808177354436083\n",
      "The 13700 th iteration gives loss of 0.23807088385212666\n",
      "The 13701 th iteration gives loss of 0.23805999563329872\n",
      "The 13702 th iteration gives loss of 0.23804910888766306\n",
      "The 13703 th iteration gives loss of 0.23803822361500243\n",
      "The 13704 th iteration gives loss of 0.238027339815085\n",
      "The 13705 th iteration gives loss of 0.23801645748770353\n",
      "The 13706 th iteration gives loss of 0.23800557663264402\n",
      "The 13707 th iteration gives loss of 0.23799469724967462\n",
      "The 13708 th iteration gives loss of 0.2379838193385786\n",
      "The 13709 th iteration gives loss of 0.23797294289914098\n",
      "The 13710 th iteration gives loss of 0.23796206793113853\n",
      "The 13711 th iteration gives loss of 0.23795119443435236\n",
      "The 13712 th iteration gives loss of 0.23794032240856863\n",
      "The 13713 th iteration gives loss of 0.23792945185355793\n",
      "The 13714 th iteration gives loss of 0.23791858276910585\n",
      "The 13715 th iteration gives loss of 0.237907715154995\n",
      "The 13716 th iteration gives loss of 0.23789684901100527\n",
      "The 13717 th iteration gives loss of 0.2378859843369281\n",
      "The 13718 th iteration gives loss of 0.23787512113252593\n",
      "The 13719 th iteration gives loss of 0.23786425939759256\n",
      "The 13720 th iteration gives loss of 0.23785339913189166\n",
      "The 13721 th iteration gives loss of 0.2378425403352233\n",
      "The 13722 th iteration gives loss of 0.2378316830073686\n",
      "The 13723 th iteration gives loss of 0.2378208271481038\n",
      "The 13724 th iteration gives loss of 0.2378099727572081\n",
      "The 13725 th iteration gives loss of 0.23779911983445737\n",
      "The 13726 th iteration gives loss of 0.237788268379637\n",
      "The 13727 th iteration gives loss of 0.23777741839253894\n",
      "The 13728 th iteration gives loss of 0.23776656987292902\n",
      "The 13729 th iteration gives loss of 0.23775572282059504\n",
      "The 13730 th iteration gives loss of 0.237744877235317\n",
      "The 13731 th iteration gives loss of 0.23773403311688462\n",
      "The 13732 th iteration gives loss of 0.23772319046506218\n",
      "The 13733 th iteration gives loss of 0.2377123492796432\n",
      "The 13734 th iteration gives loss of 0.23770150956041305\n",
      "The 13735 th iteration gives loss of 0.23769067130714547\n",
      "The 13736 th iteration gives loss of 0.2376798345196257\n",
      "The 13737 th iteration gives loss of 0.23766899919762818\n",
      "The 13738 th iteration gives loss of 0.23765816534093973\n",
      "The 13739 th iteration gives loss of 0.23764733294933674\n",
      "The 13740 th iteration gives loss of 0.2376365020226134\n",
      "The 13741 th iteration gives loss of 0.23762567256054848\n",
      "The 13742 th iteration gives loss of 0.2376148445629099\n",
      "The 13743 th iteration gives loss of 0.23760401802949416\n",
      "The 13744 th iteration gives loss of 0.23759319296006975\n",
      "The 13745 th iteration gives loss of 0.23758236935443564\n",
      "The 13746 th iteration gives loss of 0.23757154721236443\n",
      "The 13747 th iteration gives loss of 0.23756072653362845\n",
      "The 13748 th iteration gives loss of 0.2375499073180094\n",
      "The 13749 th iteration gives loss of 0.23753908956531267\n",
      "The 13750 th iteration gives loss of 0.23752827327530193\n",
      "The 13751 th iteration gives loss of 0.23751745844776018\n",
      "The 13752 th iteration gives loss of 0.237506645082475\n",
      "The 13753 th iteration gives loss of 0.2374958331792297\n",
      "The 13754 th iteration gives loss of 0.23748502273780012\n",
      "The 13755 th iteration gives loss of 0.23747421375797506\n",
      "The 13756 th iteration gives loss of 0.23746340623952036\n",
      "The 13757 th iteration gives loss of 0.23745260018223407\n",
      "The 13758 th iteration gives loss of 0.23744179558589792\n",
      "The 13759 th iteration gives loss of 0.23743099245028254\n",
      "The 13760 th iteration gives loss of 0.2374201907751723\n",
      "The 13761 th iteration gives loss of 0.23740939056036098\n",
      "The 13762 th iteration gives loss of 0.23739859180563422\n",
      "The 13763 th iteration gives loss of 0.23738779451075462\n",
      "The 13764 th iteration gives loss of 0.23737699867551657\n",
      "The 13765 th iteration gives loss of 0.23736620429970165\n",
      "The 13766 th iteration gives loss of 0.2373554113830902\n",
      "The 13767 th iteration gives loss of 0.2373446199254657\n",
      "The 13768 th iteration gives loss of 0.2373338299266053\n",
      "The 13769 th iteration gives loss of 0.23732304138630347\n",
      "The 13770 th iteration gives loss of 0.23731225430433267\n",
      "The 13771 th iteration gives loss of 0.23730146868048338\n",
      "The 13772 th iteration gives loss of 0.23729068451453142\n",
      "The 13773 th iteration gives loss of 0.23727990180625902\n",
      "The 13774 th iteration gives loss of 0.23726912055544647\n",
      "The 13775 th iteration gives loss of 0.23725834076189084\n",
      "The 13776 th iteration gives loss of 0.23724756242535502\n",
      "The 13777 th iteration gives loss of 0.23723678554563324\n",
      "The 13778 th iteration gives loss of 0.23722601012251343\n",
      "The 13779 th iteration gives loss of 0.23721523615576587\n",
      "The 13780 th iteration gives loss of 0.23720446364517833\n",
      "The 13781 th iteration gives loss of 0.23719369259052653\n",
      "The 13782 th iteration gives loss of 0.2371829229916151\n",
      "The 13783 th iteration gives loss of 0.23717215484821105\n",
      "The 13784 th iteration gives loss of 0.23716138816009472\n",
      "The 13785 th iteration gives loss of 0.23715062292704844\n",
      "The 13786 th iteration gives loss of 0.23713985914886387\n",
      "The 13787 th iteration gives loss of 0.2371290968253252\n",
      "The 13788 th iteration gives loss of 0.23711833595620443\n",
      "The 13789 th iteration gives loss of 0.23710757654129222\n",
      "The 13790 th iteration gives loss of 0.23709681858036805\n",
      "The 13791 th iteration gives loss of 0.23708606207322125\n",
      "The 13792 th iteration gives loss of 0.23707530701962898\n",
      "The 13793 th iteration gives loss of 0.23706455341938168\n",
      "The 13794 th iteration gives loss of 0.23705380127225348\n",
      "The 13795 th iteration gives loss of 0.23704305057802585\n",
      "The 13796 th iteration gives loss of 0.2370323013364938\n",
      "The 13797 th iteration gives loss of 0.23702155354743362\n",
      "The 13798 th iteration gives loss of 0.23701080721062784\n",
      "The 13799 th iteration gives loss of 0.23700006232585571\n",
      "The 13800 th iteration gives loss of 0.23698931889291378\n",
      "The 13801 th iteration gives loss of 0.2369785769115778\n",
      "The 13802 th iteration gives loss of 0.23696783638162836\n",
      "The 13803 th iteration gives loss of 0.2369570973028477\n",
      "The 13804 th iteration gives loss of 0.23694635967502894\n",
      "The 13805 th iteration gives loss of 0.23693562349794778\n",
      "The 13806 th iteration gives loss of 0.2369248887713949\n",
      "The 13807 th iteration gives loss of 0.23691415549514833\n",
      "The 13808 th iteration gives loss of 0.23690342366898207\n",
      "The 13809 th iteration gives loss of 0.23689269329269858\n",
      "The 13810 th iteration gives loss of 0.2368819643660741\n",
      "The 13811 th iteration gives loss of 0.23687123688888775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 13812 th iteration gives loss of 0.2368605108609265\n",
      "The 13813 th iteration gives loss of 0.2368497862819718\n",
      "The 13814 th iteration gives loss of 0.23683906315181852\n",
      "The 13815 th iteration gives loss of 0.23682834147024218\n",
      "The 13816 th iteration gives loss of 0.2368176212370178\n",
      "The 13817 th iteration gives loss of 0.23680690245193678\n",
      "The 13818 th iteration gives loss of 0.23679618511479866\n",
      "The 13819 th iteration gives loss of 0.23678546922535362\n",
      "The 13820 th iteration gives loss of 0.23677475478341733\n",
      "The 13821 th iteration gives loss of 0.23676404178876156\n",
      "The 13822 th iteration gives loss of 0.23675333024116277\n",
      "The 13823 th iteration gives loss of 0.236742620140426\n",
      "The 13824 th iteration gives loss of 0.23673191148630104\n",
      "The 13825 th iteration gives loss of 0.23672120427860605\n",
      "The 13826 th iteration gives loss of 0.23671049851710743\n",
      "The 13827 th iteration gives loss of 0.23669979420158715\n",
      "The 13828 th iteration gives loss of 0.23668909133184202\n",
      "The 13829 th iteration gives loss of 0.23667838990765677\n",
      "The 13830 th iteration gives loss of 0.236667689928807\n",
      "The 13831 th iteration gives loss of 0.23665699139507448\n",
      "The 13832 th iteration gives loss of 0.23664629430623957\n",
      "The 13833 th iteration gives loss of 0.23663559866210782\n",
      "The 13834 th iteration gives loss of 0.23662490446244752\n",
      "The 13835 th iteration gives loss of 0.23661421170704097\n",
      "The 13836 th iteration gives loss of 0.2366035203956856\n",
      "The 13837 th iteration gives loss of 0.23659283052816268\n",
      "The 13838 th iteration gives loss of 0.23658214210424175\n",
      "The 13839 th iteration gives loss of 0.23657145512372268\n",
      "The 13840 th iteration gives loss of 0.23656076958638322\n",
      "The 13841 th iteration gives loss of 0.23655008549200773\n",
      "The 13842 th iteration gives loss of 0.23653940284039138\n",
      "The 13843 th iteration gives loss of 0.23652872163130273\n",
      "The 13844 th iteration gives loss of 0.23651804186453815\n",
      "The 13845 th iteration gives loss of 0.23650736353987487\n",
      "The 13846 th iteration gives loss of 0.23649668665710072\n",
      "The 13847 th iteration gives loss of 0.23648601121601145\n",
      "The 13848 th iteration gives loss of 0.23647533721636874\n",
      "The 13849 th iteration gives loss of 0.23646466465796523\n",
      "The 13850 th iteration gives loss of 0.23645399354059946\n",
      "The 13851 th iteration gives loss of 0.23644332386404807\n",
      "The 13852 th iteration gives loss of 0.23643265562809182\n",
      "The 13853 th iteration gives loss of 0.23642198883252405\n",
      "The 13854 th iteration gives loss of 0.23641132347711763\n",
      "The 13855 th iteration gives loss of 0.23640065956166412\n",
      "The 13856 th iteration gives loss of 0.23638999708595906\n",
      "The 13857 th iteration gives loss of 0.23637933604977004\n",
      "The 13858 th iteration gives loss of 0.23636867645289036\n",
      "The 13859 th iteration gives loss of 0.23635801829510306\n",
      "The 13860 th iteration gives loss of 0.23634736157619862\n",
      "The 13861 th iteration gives loss of 0.2363367062959538\n",
      "The 13862 th iteration gives loss of 0.23632605245415794\n",
      "The 13863 th iteration gives loss of 0.2363154000505895\n",
      "The 13864 th iteration gives loss of 0.23630474908504917\n",
      "The 13865 th iteration gives loss of 0.23629409955731617\n",
      "The 13866 th iteration gives loss of 0.23628345146716917\n",
      "The 13867 th iteration gives loss of 0.23627280481439933\n",
      "The 13868 th iteration gives loss of 0.23626215959879104\n",
      "The 13869 th iteration gives loss of 0.23625151582012072\n",
      "The 13870 th iteration gives loss of 0.23624087347819137\n",
      "The 13871 th iteration gives loss of 0.2362302325727755\n",
      "The 13872 th iteration gives loss of 0.2362195931036567\n",
      "The 13873 th iteration gives loss of 0.2362089550706363\n",
      "The 13874 th iteration gives loss of 0.23619831847347772\n",
      "The 13875 th iteration gives loss of 0.23618768331198728\n",
      "The 13876 th iteration gives loss of 0.23617704958594346\n",
      "The 13877 th iteration gives loss of 0.2361664172951184\n",
      "The 13878 th iteration gives loss of 0.23615578643931784\n",
      "The 13879 th iteration gives loss of 0.23614515701832625\n",
      "The 13880 th iteration gives loss of 0.23613452903190837\n",
      "The 13881 th iteration gives loss of 0.2361239024798684\n",
      "The 13882 th iteration gives loss of 0.23611327736198381\n",
      "The 13883 th iteration gives loss of 0.23610265367804661\n",
      "The 13884 th iteration gives loss of 0.23609203142785284\n",
      "The 13885 th iteration gives loss of 0.23608141061115853\n",
      "The 13886 th iteration gives loss of 0.23607079122776936\n",
      "The 13887 th iteration gives loss of 0.23606017327746673\n",
      "The 13888 th iteration gives loss of 0.23604955676003814\n",
      "The 13889 th iteration gives loss of 0.23603894167528153\n",
      "The 13890 th iteration gives loss of 0.23602832802296117\n",
      "The 13891 th iteration gives loss of 0.2360177158028783\n",
      "The 13892 th iteration gives loss of 0.23600710501481187\n",
      "The 13893 th iteration gives loss of 0.2359964956585479\n",
      "The 13894 th iteration gives loss of 0.23598588773387016\n",
      "The 13895 th iteration gives loss of 0.2359752812405675\n",
      "The 13896 th iteration gives loss of 0.23596467617843275\n",
      "The 13897 th iteration gives loss of 0.23595407254724649\n",
      "The 13898 th iteration gives loss of 0.23594347034680313\n",
      "The 13899 th iteration gives loss of 0.2359328695768703\n",
      "The 13900 th iteration gives loss of 0.23592227023725373\n",
      "The 13901 th iteration gives loss of 0.2359116723277204\n",
      "The 13902 th iteration gives loss of 0.23590107584807682\n",
      "The 13903 th iteration gives loss of 0.23589048079809416\n",
      "The 13904 th iteration gives loss of 0.23587988717757039\n",
      "The 13905 th iteration gives loss of 0.23586929498628903\n",
      "The 13906 th iteration gives loss of 0.23585870422402494\n",
      "The 13907 th iteration gives loss of 0.23584811489057717\n",
      "The 13908 th iteration gives loss of 0.23583752698572194\n",
      "The 13909 th iteration gives loss of 0.23582694050926728\n",
      "The 13910 th iteration gives loss of 0.23581635546097307\n",
      "The 13911 th iteration gives loss of 0.23580577184063845\n",
      "The 13912 th iteration gives loss of 0.23579518964805918\n",
      "The 13913 th iteration gives loss of 0.23578460888299954\n",
      "The 13914 th iteration gives loss of 0.2357740295452549\n",
      "The 13915 th iteration gives loss of 0.2357634516346289\n",
      "The 13916 th iteration gives loss of 0.23575287515089266\n",
      "The 13917 th iteration gives loss of 0.23574230009383002\n",
      "The 13918 th iteration gives loss of 0.2357317264632444\n",
      "The 13919 th iteration gives loss of 0.23572115425890613\n",
      "The 13920 th iteration gives loss of 0.2357105834806027\n",
      "The 13921 th iteration gives loss of 0.23570001412813038\n",
      "The 13922 th iteration gives loss of 0.2356894462012671\n",
      "The 13923 th iteration gives loss of 0.235678879699807\n",
      "The 13924 th iteration gives loss of 0.23566831462353668\n",
      "The 13925 th iteration gives loss of 0.23565775097224026\n",
      "The 13926 th iteration gives loss of 0.23564718874570578\n",
      "The 13927 th iteration gives loss of 0.2356366279437179\n",
      "The 13928 th iteration gives loss of 0.23562606856607696\n",
      "The 13929 th iteration gives loss of 0.23561551061254285\n",
      "The 13930 th iteration gives loss of 0.23560495408291762\n",
      "The 13931 th iteration gives loss of 0.23559439897700482\n",
      "The 13932 th iteration gives loss of 0.23558384529456872\n",
      "The 13933 th iteration gives loss of 0.2355732930354099\n",
      "The 13934 th iteration gives loss of 0.23556274219930062\n",
      "The 13935 th iteration gives loss of 0.2355521927860447\n",
      "The 13936 th iteration gives loss of 0.2355416447954249\n",
      "The 13937 th iteration gives loss of 0.23553109822721538\n",
      "The 13938 th iteration gives loss of 0.23552055308121894\n",
      "The 13939 th iteration gives loss of 0.23551000935721575\n",
      "The 13940 th iteration gives loss of 0.23549946705500324\n",
      "The 13941 th iteration gives loss of 0.2354889261743584\n",
      "The 13942 th iteration gives loss of 0.23547838671506596\n",
      "The 13943 th iteration gives loss of 0.2354678486769248\n",
      "The 13944 th iteration gives loss of 0.23545731205972273\n",
      "The 13945 th iteration gives loss of 0.2354467768632332\n",
      "The 13946 th iteration gives loss of 0.2354362430872541\n",
      "The 13947 th iteration gives loss of 0.23542571073157229\n",
      "The 13948 th iteration gives loss of 0.23541517979597695\n",
      "The 13949 th iteration gives loss of 0.23540465028025406\n",
      "The 13950 th iteration gives loss of 0.23539412218418493\n",
      "The 13951 th iteration gives loss of 0.23538359550755733\n",
      "The 13952 th iteration gives loss of 0.23537307025017445\n",
      "The 13953 th iteration gives loss of 0.23536254641181384\n",
      "The 13954 th iteration gives loss of 0.23535202399226499\n",
      "The 13955 th iteration gives loss of 0.23534150299131357\n",
      "The 13956 th iteration gives loss of 0.23533098340875486\n",
      "The 13957 th iteration gives loss of 0.2353204652443598\n",
      "The 13958 th iteration gives loss of 0.2353099484979318\n",
      "The 13959 th iteration gives loss of 0.23529943316925162\n",
      "The 13960 th iteration gives loss of 0.23528891925811407\n",
      "The 13961 th iteration gives loss of 0.23527840676430067\n",
      "The 13962 th iteration gives loss of 0.23526789568759976\n",
      "The 13963 th iteration gives loss of 0.2352573860278049\n",
      "The 13964 th iteration gives loss of 0.2352468777847044\n",
      "The 13965 th iteration gives loss of 0.23523637095807612\n",
      "The 13966 th iteration gives loss of 0.2352258655477152\n",
      "The 13967 th iteration gives loss of 0.23521536155341255\n",
      "The 13968 th iteration gives loss of 0.23520485897495444\n",
      "The 13969 th iteration gives loss of 0.2351943578121271\n",
      "The 13970 th iteration gives loss of 0.23518385806472034\n",
      "The 13971 th iteration gives loss of 0.23517335973252476\n",
      "The 13972 th iteration gives loss of 0.23516286281532103\n",
      "The 13973 th iteration gives loss of 0.23515236731291184\n",
      "The 13974 th iteration gives loss of 0.2351418732250658\n",
      "The 13975 th iteration gives loss of 0.2351313805515872\n",
      "The 13976 th iteration gives loss of 0.23512088929226155\n",
      "The 13977 th iteration gives loss of 0.23511039944687523\n",
      "The 13978 th iteration gives loss of 0.23509991101521488\n",
      "The 13979 th iteration gives loss of 0.23508942399707186\n",
      "The 13980 th iteration gives loss of 0.23507893839223992\n",
      "The 13981 th iteration gives loss of 0.23506845420049446\n",
      "The 13982 th iteration gives loss of 0.2350579714216349\n",
      "The 13983 th iteration gives loss of 0.2350474900554365\n",
      "The 13984 th iteration gives loss of 0.23503701010170952\n",
      "The 13985 th iteration gives loss of 0.2350265315602255\n",
      "The 13986 th iteration gives loss of 0.23501605443078208\n",
      "The 13987 th iteration gives loss of 0.23500557871316516\n",
      "The 13988 th iteration gives loss of 0.23499510440716034\n",
      "The 13989 th iteration gives loss of 0.23498463151256285\n",
      "The 13990 th iteration gives loss of 0.23497416002915647\n",
      "The 13991 th iteration gives loss of 0.23496368995673225\n",
      "The 13992 th iteration gives loss of 0.2349532212950774\n",
      "The 13993 th iteration gives loss of 0.23494275404398848\n",
      "The 13994 th iteration gives loss of 0.23493228820323847\n",
      "The 13995 th iteration gives loss of 0.23492182377263549\n",
      "The 13996 th iteration gives loss of 0.23491136075196375\n",
      "The 13997 th iteration gives loss of 0.2349008991409968\n",
      "The 13998 th iteration gives loss of 0.23489043893953965\n",
      "The 13999 th iteration gives loss of 0.23487998014737232\n",
      "The 14000 th iteration gives loss of 0.23486952276429335\n",
      "The 14001 th iteration gives loss of 0.23485906679008536\n",
      "The 14002 th iteration gives loss of 0.234848612224536\n",
      "The 14003 th iteration gives loss of 0.2348381590674447\n",
      "The 14004 th iteration gives loss of 0.23482770731858912\n",
      "The 14005 th iteration gives loss of 0.23481725697776942\n",
      "The 14006 th iteration gives loss of 0.23480680804476703\n",
      "The 14007 th iteration gives loss of 0.23479636051937122\n",
      "The 14008 th iteration gives loss of 0.23478591440136712\n",
      "The 14009 th iteration gives loss of 0.2347754696905624\n",
      "The 14010 th iteration gives loss of 0.23476502638673286\n",
      "The 14011 th iteration gives loss of 0.23475458448965977\n",
      "The 14012 th iteration gives loss of 0.23474414399915103\n",
      "The 14013 th iteration gives loss of 0.23473370491498854\n",
      "The 14014 th iteration gives loss of 0.2347232672369509\n",
      "The 14015 th iteration gives loss of 0.2347128309648447\n",
      "The 14016 th iteration gives loss of 0.23470239609845034\n",
      "The 14017 th iteration gives loss of 0.23469196263757125\n",
      "The 14018 th iteration gives loss of 0.23468153058197813\n",
      "The 14019 th iteration gives loss of 0.23467109993147023\n",
      "The 14020 th iteration gives loss of 0.2346606706858317\n",
      "The 14021 th iteration gives loss of 0.2346502428448506\n",
      "The 14022 th iteration gives loss of 0.23463981640833542\n",
      "The 14023 th iteration gives loss of 0.23462939137605235\n",
      "The 14024 th iteration gives loss of 0.23461896774780522\n",
      "The 14025 th iteration gives loss of 0.2346085455233819\n",
      "The 14026 th iteration gives loss of 0.2345981247025789\n",
      "The 14027 th iteration gives loss of 0.23458770528517434\n",
      "The 14028 th iteration gives loss of 0.23457728727095312\n",
      "The 14029 th iteration gives loss of 0.2345668706597242\n",
      "The 14030 th iteration gives loss of 0.23455645545126227\n",
      "The 14031 th iteration gives loss of 0.2345460416453603\n",
      "The 14032 th iteration gives loss of 0.23453562924181837\n",
      "The 14033 th iteration gives loss of 0.23452521824041633\n",
      "The 14034 th iteration gives loss of 0.2345148086409491\n",
      "The 14035 th iteration gives loss of 0.23450440044319976\n",
      "The 14036 th iteration gives loss of 0.23449399364696605\n",
      "The 14037 th iteration gives loss of 0.23448358825204\n",
      "The 14038 th iteration gives loss of 0.23447318425821118\n",
      "The 14039 th iteration gives loss of 0.23446278166525664\n",
      "The 14040 th iteration gives loss of 0.23445238047297043\n",
      "The 14041 th iteration gives loss of 0.2344419806811678\n",
      "The 14042 th iteration gives loss of 0.23443158228961314\n",
      "The 14043 th iteration gives loss of 0.23442118529809802\n",
      "The 14044 th iteration gives loss of 0.23441078970642526\n",
      "The 14045 th iteration gives loss of 0.234400395514383\n",
      "The 14046 th iteration gives loss of 0.23439000272174987\n",
      "The 14047 th iteration gives loss of 0.2343796113283289\n",
      "The 14048 th iteration gives loss of 0.23436922133390692\n",
      "The 14049 th iteration gives loss of 0.23435883273827754\n",
      "The 14050 th iteration gives loss of 0.23434844554121984\n",
      "The 14051 th iteration gives loss of 0.23433805974253075\n",
      "The 14052 th iteration gives loss of 0.23432767534200732\n",
      "The 14053 th iteration gives loss of 0.23431729233943405\n",
      "The 14054 th iteration gives loss of 0.23430691073459664\n",
      "The 14055 th iteration gives loss of 0.23429653052730368\n",
      "The 14056 th iteration gives loss of 0.23428615171732636\n",
      "The 14057 th iteration gives loss of 0.23427577430447066\n",
      "The 14058 th iteration gives loss of 0.23426539828851778\n",
      "The 14059 th iteration gives loss of 0.23425502366926207\n",
      "The 14060 th iteration gives loss of 0.23424465044649329\n",
      "The 14061 th iteration gives loss of 0.23423427861999088\n",
      "The 14062 th iteration gives loss of 0.2342239081895737\n",
      "The 14063 th iteration gives loss of 0.23421353915501092\n",
      "The 14064 th iteration gives loss of 0.23420317151609676\n",
      "The 14065 th iteration gives loss of 0.2341928052726288\n",
      "The 14066 th iteration gives loss of 0.23418244042438224\n",
      "The 14067 th iteration gives loss of 0.23417207697118264\n",
      "The 14068 th iteration gives loss of 0.23416171491278218\n",
      "The 14069 th iteration gives loss of 0.23415135424899414\n",
      "The 14070 th iteration gives loss of 0.23414099497960253\n",
      "The 14071 th iteration gives loss of 0.234130637104405\n",
      "The 14072 th iteration gives loss of 0.23412028062318158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 14073 th iteration gives loss of 0.23410992553573404\n",
      "The 14074 th iteration gives loss of 0.23409957184185606\n",
      "The 14075 th iteration gives loss of 0.23408921954132195\n",
      "The 14076 th iteration gives loss of 0.23407886863393093\n",
      "The 14077 th iteration gives loss of 0.23406851911947973\n",
      "The 14078 th iteration gives loss of 0.23405817099776186\n",
      "The 14079 th iteration gives loss of 0.23404782426855933\n",
      "The 14080 th iteration gives loss of 0.23403747893166887\n",
      "The 14081 th iteration gives loss of 0.2340271349868886\n",
      "The 14082 th iteration gives loss of 0.23401679243400167\n",
      "The 14083 th iteration gives loss of 0.2340064512727923\n",
      "The 14084 th iteration gives loss of 0.23399611150306152\n",
      "The 14085 th iteration gives loss of 0.2339857731246057\n",
      "The 14086 th iteration gives loss of 0.23397543613720725\n",
      "The 14087 th iteration gives loss of 0.2339651005406615\n",
      "The 14088 th iteration gives loss of 0.23395476633476361\n",
      "The 14089 th iteration gives loss of 0.23394443351929503\n",
      "The 14090 th iteration gives loss of 0.23393410209406204\n",
      "The 14091 th iteration gives loss of 0.23392377205884735\n",
      "The 14092 th iteration gives loss of 0.23391344341343498\n",
      "The 14093 th iteration gives loss of 0.23390311615763704\n",
      "The 14094 th iteration gives loss of 0.23389279029122984\n",
      "The 14095 th iteration gives loss of 0.2338824658140135\n",
      "The 14096 th iteration gives loss of 0.23387214272577442\n",
      "The 14097 th iteration gives loss of 0.23386182102630695\n",
      "The 14098 th iteration gives loss of 0.23385150071540184\n",
      "The 14099 th iteration gives loss of 0.23384118179285213\n",
      "The 14100 th iteration gives loss of 0.23383086425845395\n",
      "The 14101 th iteration gives loss of 0.2338205481119839\n",
      "The 14102 th iteration gives loss of 0.23381023335324755\n",
      "The 14103 th iteration gives loss of 0.23379991998204344\n",
      "The 14104 th iteration gives loss of 0.23378960799814685\n",
      "The 14105 th iteration gives loss of 0.23377929740135647\n",
      "The 14106 th iteration gives loss of 0.23376898819146957\n",
      "The 14107 th iteration gives loss of 0.2337586803682785\n",
      "The 14108 th iteration gives loss of 0.2337483739315653\n",
      "The 14109 th iteration gives loss of 0.2337380688811349\n",
      "The 14110 th iteration gives loss of 0.2337277652167739\n",
      "The 14111 th iteration gives loss of 0.2337174629382699\n",
      "The 14112 th iteration gives loss of 0.2337071620454292\n",
      "The 14113 th iteration gives loss of 0.23369686253802463\n",
      "The 14114 th iteration gives loss of 0.23368656441586338\n",
      "The 14115 th iteration gives loss of 0.23367626767873084\n",
      "The 14116 th iteration gives loss of 0.23366597232642525\n",
      "The 14117 th iteration gives loss of 0.2336556783587297\n",
      "The 14118 th iteration gives loss of 0.2336453857754479\n",
      "The 14119 th iteration gives loss of 0.2336350945763661\n",
      "The 14120 th iteration gives loss of 0.23362480476128278\n",
      "The 14121 th iteration gives loss of 0.23361451632998126\n",
      "The 14122 th iteration gives loss of 0.23360422928226027\n",
      "The 14123 th iteration gives loss of 0.23359394361791985\n",
      "The 14124 th iteration gives loss of 0.23358365933672995\n",
      "The 14125 th iteration gives loss of 0.23357337643850334\n",
      "The 14126 th iteration gives loss of 0.2335630949230282\n",
      "The 14127 th iteration gives loss of 0.23355281479010762\n",
      "The 14128 th iteration gives loss of 0.23354253603951006\n",
      "The 14129 th iteration gives loss of 0.23353225867104344\n",
      "The 14130 th iteration gives loss of 0.23352198268449817\n",
      "The 14131 th iteration gives loss of 0.23351170807966556\n",
      "The 14132 th iteration gives loss of 0.23350143485635216\n",
      "The 14133 th iteration gives loss of 0.23349116301432957\n",
      "The 14134 th iteration gives loss of 0.23348089255340165\n",
      "The 14135 th iteration gives loss of 0.23347062347336614\n",
      "The 14136 th iteration gives loss of 0.23346035577400456\n",
      "The 14137 th iteration gives loss of 0.2334500894551086\n",
      "The 14138 th iteration gives loss of 0.23343982451648748\n",
      "The 14139 th iteration gives loss of 0.23342956095792924\n",
      "The 14140 th iteration gives loss of 0.23341929877921797\n",
      "The 14141 th iteration gives loss of 0.23340903798015827\n",
      "The 14142 th iteration gives loss of 0.2333987785605365\n",
      "The 14143 th iteration gives loss of 0.2333885205201392\n",
      "The 14144 th iteration gives loss of 0.23337826385877455\n",
      "The 14145 th iteration gives loss of 0.23336800857622744\n",
      "The 14146 th iteration gives loss of 0.2333577546722889\n",
      "The 14147 th iteration gives loss of 0.23334750214676103\n",
      "The 14148 th iteration gives loss of 0.23333725099943273\n",
      "The 14149 th iteration gives loss of 0.2333270012300963\n",
      "The 14150 th iteration gives loss of 0.2333167528385472\n",
      "The 14151 th iteration gives loss of 0.2333065058245739\n",
      "The 14152 th iteration gives loss of 0.2332962601879762\n",
      "The 14153 th iteration gives loss of 0.23328601592854492\n",
      "The 14154 th iteration gives loss of 0.23327577304607597\n",
      "The 14155 th iteration gives loss of 0.23326553154035762\n",
      "The 14156 th iteration gives loss of 0.233255291411179\n",
      "The 14157 th iteration gives loss of 0.23324505265835407\n",
      "The 14158 th iteration gives loss of 0.23323481528165585\n",
      "The 14159 th iteration gives loss of 0.23322457928088927\n",
      "The 14160 th iteration gives loss of 0.23321434465585014\n",
      "The 14161 th iteration gives loss of 0.23320411140631744\n",
      "The 14162 th iteration gives loss of 0.23319387953210402\n",
      "The 14163 th iteration gives loss of 0.23318364903299038\n",
      "The 14164 th iteration gives loss of 0.2331734199087782\n",
      "The 14165 th iteration gives loss of 0.23316319215925072\n",
      "The 14166 th iteration gives loss of 0.23315296578420747\n",
      "The 14167 th iteration gives loss of 0.23314274078344488\n",
      "The 14168 th iteration gives loss of 0.23313251715676037\n",
      "The 14169 th iteration gives loss of 0.2331222949039377\n",
      "The 14170 th iteration gives loss of 0.23311207402478404\n",
      "The 14171 th iteration gives loss of 0.23310185451908333\n",
      "The 14172 th iteration gives loss of 0.23309163638663413\n",
      "The 14173 th iteration gives loss of 0.23308141962721934\n",
      "The 14174 th iteration gives loss of 0.23307120424065686\n",
      "The 14175 th iteration gives loss of 0.23306099022671886\n",
      "The 14176 th iteration gives loss of 0.23305077758520523\n",
      "The 14177 th iteration gives loss of 0.2330405663159151\n",
      "The 14178 th iteration gives loss of 0.23303035641863476\n",
      "The 14179 th iteration gives loss of 0.2330201478931674\n",
      "The 14180 th iteration gives loss of 0.2330099407393019\n",
      "The 14181 th iteration gives loss of 0.23299973495682869\n",
      "The 14182 th iteration gives loss of 0.23298953054555768\n",
      "The 14183 th iteration gives loss of 0.23297932750526917\n",
      "The 14184 th iteration gives loss of 0.23296912583576262\n",
      "The 14185 th iteration gives loss of 0.23295892553683004\n",
      "The 14186 th iteration gives loss of 0.23294872660827395\n",
      "The 14187 th iteration gives loss of 0.23293852904987677\n",
      "The 14188 th iteration gives loss of 0.23292833286144088\n",
      "The 14189 th iteration gives loss of 0.23291813804275682\n",
      "The 14190 th iteration gives loss of 0.23290794459362313\n",
      "The 14191 th iteration gives loss of 0.2328977525138271\n",
      "The 14192 th iteration gives loss of 0.23288756180316897\n",
      "The 14193 th iteration gives loss of 0.23287737246145085\n",
      "The 14194 th iteration gives loss of 0.23286718448845406\n",
      "The 14195 th iteration gives loss of 0.232856997883977\n",
      "The 14196 th iteration gives loss of 0.23284681264781412\n",
      "The 14197 th iteration gives loss of 0.23283662877977127\n",
      "The 14198 th iteration gives loss of 0.2328264462796293\n",
      "The 14199 th iteration gives loss of 0.23281626514719359\n",
      "The 14200 th iteration gives loss of 0.2328060853822428\n",
      "The 14201 th iteration gives loss of 0.23279590698458788\n",
      "The 14202 th iteration gives loss of 0.2327857299540284\n",
      "The 14203 th iteration gives loss of 0.23277555429034083\n",
      "The 14204 th iteration gives loss of 0.23276537999332844\n",
      "The 14205 th iteration gives loss of 0.23275520706278816\n",
      "The 14206 th iteration gives loss of 0.2327450354985099\n",
      "The 14207 th iteration gives loss of 0.2327348653002951\n",
      "The 14208 th iteration gives loss of 0.23272469646793906\n",
      "The 14209 th iteration gives loss of 0.2327145290012323\n",
      "The 14210 th iteration gives loss of 0.23270436289997037\n",
      "The 14211 th iteration gives loss of 0.23269419816395004\n",
      "The 14212 th iteration gives loss of 0.23268403479297836\n",
      "The 14213 th iteration gives loss of 0.23267387278682863\n",
      "The 14214 th iteration gives loss of 0.23266371214530165\n",
      "The 14215 th iteration gives loss of 0.23265355286819842\n",
      "The 14216 th iteration gives loss of 0.23264339495531858\n",
      "The 14217 th iteration gives loss of 0.23263323840645514\n",
      "The 14218 th iteration gives loss of 0.23262308322139455\n",
      "The 14219 th iteration gives loss of 0.2326129293999423\n",
      "The 14220 th iteration gives loss of 0.23260277694188408\n",
      "The 14221 th iteration gives loss of 0.23259262584702273\n",
      "The 14222 th iteration gives loss of 0.23258247611514998\n",
      "The 14223 th iteration gives loss of 0.23257232774606326\n",
      "The 14224 th iteration gives loss of 0.23256218073956425\n",
      "The 14225 th iteration gives loss of 0.23255203509543532\n",
      "The 14226 th iteration gives loss of 0.23254189081348092\n",
      "The 14227 th iteration gives loss of 0.23253174789349784\n",
      "The 14228 th iteration gives loss of 0.2325216063352787\n",
      "The 14229 th iteration gives loss of 0.23251146613862003\n",
      "The 14230 th iteration gives loss of 0.23250132730331757\n",
      "The 14231 th iteration gives loss of 0.23249118982916103\n",
      "The 14232 th iteration gives loss of 0.23248105371595798\n",
      "The 14233 th iteration gives loss of 0.2324709189634939\n",
      "The 14234 th iteration gives loss of 0.23246078557157013\n",
      "The 14235 th iteration gives loss of 0.2324506535399814\n",
      "The 14236 th iteration gives loss of 0.23244052286852068\n",
      "The 14237 th iteration gives loss of 0.23243039355699066\n",
      "The 14238 th iteration gives loss of 0.23242026560518259\n",
      "The 14239 th iteration gives loss of 0.23241013901288965\n",
      "The 14240 th iteration gives loss of 0.23240001377991912\n",
      "The 14241 th iteration gives loss of 0.2323898899060534\n",
      "The 14242 th iteration gives loss of 0.23237976739109326\n",
      "The 14243 th iteration gives loss of 0.2323696462348363\n",
      "The 14244 th iteration gives loss of 0.23235952643708122\n",
      "The 14245 th iteration gives loss of 0.2323494079976197\n",
      "The 14246 th iteration gives loss of 0.23233929091625005\n",
      "The 14247 th iteration gives loss of 0.23232917519276622\n",
      "The 14248 th iteration gives loss of 0.2323190608269627\n",
      "The 14249 th iteration gives loss of 0.23230894781864514\n",
      "The 14250 th iteration gives loss of 0.2322988361676013\n",
      "The 14251 th iteration gives loss of 0.23228872587363267\n",
      "The 14252 th iteration gives loss of 0.2322786169365337\n",
      "The 14253 th iteration gives loss of 0.23226850935609294\n",
      "The 14254 th iteration gives loss of 0.23225840313211607\n",
      "The 14255 th iteration gives loss of 0.23224829826439883\n",
      "The 14256 th iteration gives loss of 0.23223819475273122\n",
      "The 14257 th iteration gives loss of 0.23222809259692612\n",
      "The 14258 th iteration gives loss of 0.23221799179676558\n",
      "The 14259 th iteration gives loss of 0.23220789235204603\n",
      "The 14260 th iteration gives loss of 0.232197794262572\n",
      "The 14261 th iteration gives loss of 0.2321876975281354\n",
      "The 14262 th iteration gives loss of 0.2321776021485248\n",
      "The 14263 th iteration gives loss of 0.23216750812354356\n",
      "The 14264 th iteration gives loss of 0.23215741545299828\n",
      "The 14265 th iteration gives loss of 0.23214732413667274\n",
      "The 14266 th iteration gives loss of 0.23213723417437063\n",
      "The 14267 th iteration gives loss of 0.23212714556588773\n",
      "The 14268 th iteration gives loss of 0.23211705831101828\n",
      "The 14269 th iteration gives loss of 0.23210697240955863\n",
      "The 14270 th iteration gives loss of 0.23209688786130273\n",
      "The 14271 th iteration gives loss of 0.23208680466605644\n",
      "The 14272 th iteration gives loss of 0.23207672282360953\n",
      "The 14273 th iteration gives loss of 0.2320666423337676\n",
      "The 14274 th iteration gives loss of 0.23205656319631382\n",
      "The 14275 th iteration gives loss of 0.23204648541106263\n",
      "The 14276 th iteration gives loss of 0.23203640897779915\n",
      "The 14277 th iteration gives loss of 0.23202633389631738\n",
      "The 14278 th iteration gives loss of 0.23201626016642884\n",
      "The 14279 th iteration gives loss of 0.23200618778790785\n",
      "The 14280 th iteration gives loss of 0.23199611676056203\n",
      "The 14281 th iteration gives loss of 0.23198604708420575\n",
      "The 14282 th iteration gives loss of 0.23197597875861667\n",
      "The 14283 th iteration gives loss of 0.23196591178360199\n",
      "The 14284 th iteration gives loss of 0.23195584615894524\n",
      "The 14285 th iteration gives loss of 0.23194578188446285\n",
      "The 14286 th iteration gives loss of 0.2319357189599314\n",
      "The 14287 th iteration gives loss of 0.2319256573851696\n",
      "The 14288 th iteration gives loss of 0.23191559715996177\n",
      "The 14289 th iteration gives loss of 0.23190553828410182\n",
      "The 14290 th iteration gives loss of 0.2318954807573951\n",
      "The 14291 th iteration gives loss of 0.23188542457963554\n",
      "The 14292 th iteration gives loss of 0.23187536975062648\n",
      "The 14293 th iteration gives loss of 0.23186531627015455\n",
      "The 14294 th iteration gives loss of 0.2318552641380306\n",
      "The 14295 th iteration gives loss of 0.2318452133540506\n",
      "The 14296 th iteration gives loss of 0.23183516391800132\n",
      "The 14297 th iteration gives loss of 0.23182511582968585\n",
      "The 14298 th iteration gives loss of 0.23181506908890281\n",
      "The 14299 th iteration gives loss of 0.23180502369544567\n",
      "The 14300 th iteration gives loss of 0.2317949796491121\n",
      "The 14301 th iteration gives loss of 0.23178493694970534\n",
      "The 14302 th iteration gives loss of 0.23177489559702685\n",
      "The 14303 th iteration gives loss of 0.23176485559086538\n",
      "The 14304 th iteration gives loss of 0.23175481693102096\n",
      "The 14305 th iteration gives loss of 0.2317447796172955\n",
      "The 14306 th iteration gives loss of 0.23173474364947982\n",
      "The 14307 th iteration gives loss of 0.23172470902737727\n",
      "The 14308 th iteration gives loss of 0.23171467575078672\n",
      "The 14309 th iteration gives loss of 0.23170464381949998\n",
      "The 14310 th iteration gives loss of 0.23169461323331875\n",
      "The 14311 th iteration gives loss of 0.23168458399204542\n",
      "The 14312 th iteration gives loss of 0.23167455609546989\n",
      "The 14313 th iteration gives loss of 0.23166452954339192\n",
      "The 14314 th iteration gives loss of 0.23165450433561935\n",
      "The 14315 th iteration gives loss of 0.23164448047194452\n",
      "The 14316 th iteration gives loss of 0.231634457952159\n",
      "The 14317 th iteration gives loss of 0.2316244367760587\n",
      "The 14318 th iteration gives loss of 0.2316144169434627\n",
      "The 14319 th iteration gives loss of 0.23160439845413924\n",
      "The 14320 th iteration gives loss of 0.23159438130790286\n",
      "The 14321 th iteration gives loss of 0.23158436550457306\n",
      "The 14322 th iteration gives loss of 0.2315743510439044\n",
      "The 14323 th iteration gives loss of 0.23156433792572467\n",
      "The 14324 th iteration gives loss of 0.23155432614983332\n",
      "The 14325 th iteration gives loss of 0.23154431571600317\n",
      "The 14326 th iteration gives loss of 0.2315343066240622\n",
      "The 14327 th iteration gives loss of 0.23152429887379306\n",
      "The 14328 th iteration gives loss of 0.23151429246499428\n",
      "The 14329 th iteration gives loss of 0.23150428739748116\n",
      "The 14330 th iteration gives loss of 0.23149428367102662\n",
      "The 14331 th iteration gives loss of 0.2314842812854395\n",
      "The 14332 th iteration gives loss of 0.23147428024053326\n",
      "The 14333 th iteration gives loss of 0.23146428053608345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 14334 th iteration gives loss of 0.23145428217190034\n",
      "The 14335 th iteration gives loss of 0.23144428514778562\n",
      "The 14336 th iteration gives loss of 0.23143428946353634\n",
      "The 14337 th iteration gives loss of 0.2314242951189385\n",
      "The 14338 th iteration gives loss of 0.23141430211381103\n",
      "The 14339 th iteration gives loss of 0.23140431044793638\n",
      "The 14340 th iteration gives loss of 0.2313943201211224\n",
      "The 14341 th iteration gives loss of 0.2313843311331543\n",
      "The 14342 th iteration gives loss of 0.23137434348385613\n",
      "The 14343 th iteration gives loss of 0.23136435717300022\n",
      "The 14344 th iteration gives loss of 0.2313543722004057\n",
      "The 14345 th iteration gives loss of 0.23134438856585754\n",
      "The 14346 th iteration gives loss of 0.23133440626916274\n",
      "The 14347 th iteration gives loss of 0.2313244253101317\n",
      "The 14348 th iteration gives loss of 0.23131444568853704\n",
      "The 14349 th iteration gives loss of 0.23130446740419824\n",
      "The 14350 th iteration gives loss of 0.23129449045690645\n",
      "The 14351 th iteration gives loss of 0.23128451484644907\n",
      "The 14352 th iteration gives loss of 0.23127454057264404\n",
      "The 14353 th iteration gives loss of 0.2312645676352847\n",
      "The 14354 th iteration gives loss of 0.23125459603417164\n",
      "The 14355 th iteration gives loss of 0.23124462576909588\n",
      "The 14356 th iteration gives loss of 0.2312346568398723\n",
      "The 14357 th iteration gives loss of 0.2312246892462897\n",
      "The 14358 th iteration gives loss of 0.23121472298813883\n",
      "The 14359 th iteration gives loss of 0.23120475806523713\n",
      "The 14360 th iteration gives loss of 0.23119479447736627\n",
      "The 14361 th iteration gives loss of 0.23118483222434355\n",
      "The 14362 th iteration gives loss of 0.23117487130595868\n",
      "The 14363 th iteration gives loss of 0.23116491172201087\n",
      "The 14364 th iteration gives loss of 0.23115495347230072\n",
      "The 14365 th iteration gives loss of 0.23114499655662524\n",
      "The 14366 th iteration gives loss of 0.2311350409747961\n",
      "The 14367 th iteration gives loss of 0.23112508672659027\n",
      "The 14368 th iteration gives loss of 0.23111513381182355\n",
      "The 14369 th iteration gives loss of 0.23110518223029747\n",
      "The 14370 th iteration gives loss of 0.23109523198179782\n",
      "The 14371 th iteration gives loss of 0.2310852830661532\n",
      "The 14372 th iteration gives loss of 0.2310753354831263\n",
      "The 14373 th iteration gives loss of 0.23106538923253545\n",
      "The 14374 th iteration gives loss of 0.23105544431418432\n",
      "The 14375 th iteration gives loss of 0.23104550072785715\n",
      "The 14376 th iteration gives loss of 0.2310355584733709\n",
      "The 14377 th iteration gives loss of 0.23102561755052292\n",
      "The 14378 th iteration gives loss of 0.23101567795909458\n",
      "The 14379 th iteration gives loss of 0.23100573969890922\n",
      "The 14380 th iteration gives loss of 0.2309958027697616\n",
      "The 14381 th iteration gives loss of 0.23098586717143468\n",
      "The 14382 th iteration gives loss of 0.2309759329037507\n",
      "The 14383 th iteration gives loss of 0.23096599996649475\n",
      "The 14384 th iteration gives loss of 0.23095606835947916\n",
      "The 14385 th iteration gives loss of 0.23094613808248962\n",
      "The 14386 th iteration gives loss of 0.2309362091353306\n",
      "The 14387 th iteration gives loss of 0.23092628151780845\n",
      "The 14388 th iteration gives loss of 0.23091635522972048\n",
      "The 14389 th iteration gives loss of 0.23090643027086433\n",
      "The 14390 th iteration gives loss of 0.2308965066410532\n",
      "The 14391 th iteration gives loss of 0.23088658434005907\n",
      "The 14392 th iteration gives loss of 0.23087666336770835\n",
      "The 14393 th iteration gives loss of 0.23086674372379828\n",
      "The 14394 th iteration gives loss of 0.23085682540811273\n",
      "The 14395 th iteration gives loss of 0.230846908420476\n",
      "The 14396 th iteration gives loss of 0.23083699276066397\n",
      "The 14397 th iteration gives loss of 0.23082707842849212\n",
      "The 14398 th iteration gives loss of 0.23081716542375785\n",
      "The 14399 th iteration gives loss of 0.23080725374625655\n",
      "The 14400 th iteration gives loss of 0.2307973433957935\n",
      "The 14401 th iteration gives loss of 0.23078743437217067\n",
      "The 14402 th iteration gives loss of 0.23077752667519025\n",
      "The 14403 th iteration gives loss of 0.23076762030464956\n",
      "The 14404 th iteration gives loss of 0.23075771526034242\n",
      "The 14405 th iteration gives loss of 0.23074781154207596\n",
      "The 14406 th iteration gives loss of 0.23073790914965359\n",
      "The 14407 th iteration gives loss of 0.23072800808286822\n",
      "The 14408 th iteration gives loss of 0.23071810834152756\n",
      "The 14409 th iteration gives loss of 0.23070820992543747\n",
      "The 14410 th iteration gives loss of 0.23069831283439385\n",
      "The 14411 th iteration gives loss of 0.23068841706818222\n",
      "The 14412 th iteration gives loss of 0.2306785226266172\n",
      "The 14413 th iteration gives loss of 0.23066862950950068\n",
      "The 14414 th iteration gives loss of 0.2306587377166307\n",
      "The 14415 th iteration gives loss of 0.23064884724780807\n",
      "The 14416 th iteration gives loss of 0.23063895810283924\n",
      "The 14417 th iteration gives loss of 0.23062907028152257\n",
      "The 14418 th iteration gives loss of 0.23061918378365598\n",
      "The 14419 th iteration gives loss of 0.23060929860902954\n",
      "The 14420 th iteration gives loss of 0.23059941475746093\n",
      "The 14421 th iteration gives loss of 0.23058953222875656\n",
      "The 14422 th iteration gives loss of 0.23057965102270245\n",
      "The 14423 th iteration gives loss of 0.23056977113910543\n",
      "The 14424 th iteration gives loss of 0.2305598925777682\n",
      "The 14425 th iteration gives loss of 0.23055001533848607\n",
      "The 14426 th iteration gives loss of 0.23054013942105972\n",
      "The 14427 th iteration gives loss of 0.23053026482529795\n",
      "The 14428 th iteration gives loss of 0.230520391551004\n",
      "The 14429 th iteration gives loss of 0.23051051959796653\n",
      "The 14430 th iteration gives loss of 0.2305006489659934\n",
      "The 14431 th iteration gives loss of 0.2304907796548924\n",
      "The 14432 th iteration gives loss of 0.23048091166446244\n",
      "The 14433 th iteration gives loss of 0.23047104499449486\n",
      "The 14434 th iteration gives loss of 0.23046117964479884\n",
      "The 14435 th iteration gives loss of 0.23045131561517526\n",
      "The 14436 th iteration gives loss of 0.23044145290542542\n",
      "The 14437 th iteration gives loss of 0.2304315915153399\n",
      "The 14438 th iteration gives loss of 0.23042173144473768\n",
      "The 14439 th iteration gives loss of 0.23041187269341837\n",
      "The 14440 th iteration gives loss of 0.23040201526117382\n",
      "The 14441 th iteration gives loss of 0.23039215914781552\n",
      "The 14442 th iteration gives loss of 0.2303823043531268\n",
      "The 14443 th iteration gives loss of 0.23037245087693387\n",
      "The 14444 th iteration gives loss of 0.2303625987190308\n",
      "The 14445 th iteration gives loss of 0.23035274787921392\n",
      "The 14446 th iteration gives loss of 0.2303428983572751\n",
      "The 14447 th iteration gives loss of 0.23033305015304423\n",
      "The 14448 th iteration gives loss of 0.23032320326630024\n",
      "The 14449 th iteration gives loss of 0.2303133576968485\n",
      "The 14450 th iteration gives loss of 0.23030351344449695\n",
      "The 14451 th iteration gives loss of 0.2302936705090456\n",
      "The 14452 th iteration gives loss of 0.23028382889028504\n",
      "The 14453 th iteration gives loss of 0.2302739885880392\n",
      "The 14454 th iteration gives loss of 0.2302641496020906\n",
      "The 14455 th iteration gives loss of 0.2302543119322511\n",
      "The 14456 th iteration gives loss of 0.23024447557831784\n",
      "The 14457 th iteration gives loss of 0.23023464054010584\n",
      "The 14458 th iteration gives loss of 0.23022480681739496\n",
      "The 14459 th iteration gives loss of 0.23021497441000643\n",
      "The 14460 th iteration gives loss of 0.23020514331772965\n",
      "The 14461 th iteration gives loss of 0.23019531354036957\n",
      "The 14462 th iteration gives loss of 0.23018548507773937\n",
      "The 14463 th iteration gives loss of 0.23017565792962355\n",
      "The 14464 th iteration gives loss of 0.23016583209584304\n",
      "The 14465 th iteration gives loss of 0.2301560075761863\n",
      "The 14466 th iteration gives loss of 0.23014618437046014\n",
      "The 14467 th iteration gives loss of 0.23013636247846442\n",
      "The 14468 th iteration gives loss of 0.23012654190001375\n",
      "The 14469 th iteration gives loss of 0.2301167226348897\n",
      "The 14470 th iteration gives loss of 0.23010690468291084\n",
      "The 14471 th iteration gives loss of 0.23009708804386947\n",
      "The 14472 th iteration gives loss of 0.23008727271758098\n",
      "The 14473 th iteration gives loss of 0.23007745870383423\n",
      "The 14474 th iteration gives loss of 0.23006764600243879\n",
      "The 14475 th iteration gives loss of 0.23005783461319004\n",
      "The 14476 th iteration gives loss of 0.23004802453590428\n",
      "The 14477 th iteration gives loss of 0.23003821577037628\n",
      "The 14478 th iteration gives loss of 0.23002840831640217\n",
      "The 14479 th iteration gives loss of 0.2300186021737911\n",
      "The 14480 th iteration gives loss of 0.2300087973423519\n",
      "The 14481 th iteration gives loss of 0.22999899382188393\n",
      "The 14482 th iteration gives loss of 0.2299891916121818\n",
      "The 14483 th iteration gives loss of 0.22997939071304985\n",
      "The 14484 th iteration gives loss of 0.22996959112429483\n",
      "The 14485 th iteration gives loss of 0.2299597928457287\n",
      "The 14486 th iteration gives loss of 0.22994999587713805\n",
      "The 14487 th iteration gives loss of 0.22994020021833236\n",
      "The 14488 th iteration gives loss of 0.22993040586911798\n",
      "The 14489 th iteration gives loss of 0.22992061282929\n",
      "The 14490 th iteration gives loss of 0.22991082109865874\n",
      "The 14491 th iteration gives loss of 0.22990103067701873\n",
      "The 14492 th iteration gives loss of 0.2298912415641906\n",
      "The 14493 th iteration gives loss of 0.22988145375995608\n",
      "The 14494 th iteration gives loss of 0.22987166726413782\n",
      "The 14495 th iteration gives loss of 0.22986188207651756\n",
      "The 14496 th iteration gives loss of 0.22985209819691976\n",
      "The 14497 th iteration gives loss of 0.22984231562512966\n",
      "The 14498 th iteration gives loss of 0.22983253436096074\n",
      "The 14499 th iteration gives loss of 0.22982275440421587\n",
      "The 14500 th iteration gives loss of 0.2298129757546917\n",
      "The 14501 th iteration gives loss of 0.22980319841219357\n",
      "The 14502 th iteration gives loss of 0.22979342237653208\n",
      "The 14503 th iteration gives loss of 0.22978364764750545\n",
      "The 14504 th iteration gives loss of 0.22977387422491236\n",
      "The 14505 th iteration gives loss of 0.2297641021085691\n",
      "The 14506 th iteration gives loss of 0.22975433129826858\n",
      "The 14507 th iteration gives loss of 0.22974456179380953\n",
      "The 14508 th iteration gives loss of 0.22973479359500965\n",
      "The 14509 th iteration gives loss of 0.22972502670165995\n",
      "The 14510 th iteration gives loss of 0.22971526111357993\n",
      "The 14511 th iteration gives loss of 0.22970549683055672\n",
      "The 14512 th iteration gives loss of 0.22969573385239714\n",
      "The 14513 th iteration gives loss of 0.2296859721789072\n",
      "The 14514 th iteration gives loss of 0.22967621180989098\n",
      "The 14515 th iteration gives loss of 0.22966645274515154\n",
      "The 14516 th iteration gives loss of 0.22965669498449442\n",
      "The 14517 th iteration gives loss of 0.22964693852772386\n",
      "The 14518 th iteration gives loss of 0.22963718337463182\n",
      "The 14519 th iteration gives loss of 0.22962742952504905\n",
      "The 14520 th iteration gives loss of 0.22961767697874974\n",
      "The 14521 th iteration gives loss of 0.229607925735547\n",
      "The 14522 th iteration gives loss of 0.22959817579525707\n",
      "The 14523 th iteration gives loss of 0.22958842715766595\n",
      "The 14524 th iteration gives loss of 0.2295786798225893\n",
      "The 14525 th iteration gives loss of 0.22956893378982857\n",
      "The 14526 th iteration gives loss of 0.2295591890591822\n",
      "The 14527 th iteration gives loss of 0.2295494456304605\n",
      "The 14528 th iteration gives loss of 0.22953970350346714\n",
      "The 14529 th iteration gives loss of 0.22952996267800954\n",
      "The 14530 th iteration gives loss of 0.22952022315388648\n",
      "The 14531 th iteration gives loss of 0.22951048493089354\n",
      "The 14532 th iteration gives loss of 0.22950074800884446\n",
      "The 14533 th iteration gives loss of 0.22949101238755154\n",
      "The 14534 th iteration gives loss of 0.22948127806680158\n",
      "The 14535 th iteration gives loss of 0.2294715450464118\n",
      "The 14536 th iteration gives loss of 0.22946181332618504\n",
      "The 14537 th iteration gives loss of 0.22945208290591598\n",
      "The 14538 th iteration gives loss of 0.22944235378541397\n",
      "The 14539 th iteration gives loss of 0.22943262596448982\n",
      "The 14540 th iteration gives loss of 0.22942289944294023\n",
      "The 14541 th iteration gives loss of 0.22941317422057023\n",
      "The 14542 th iteration gives loss of 0.22940345029718925\n",
      "The 14543 th iteration gives loss of 0.22939372767260024\n",
      "The 14544 th iteration gives loss of 0.22938400634660402\n",
      "The 14545 th iteration gives loss of 0.22937428631900922\n",
      "The 14546 th iteration gives loss of 0.22936456758961196\n",
      "The 14547 th iteration gives loss of 0.22935485015821835\n",
      "The 14548 th iteration gives loss of 0.2293451340246496\n",
      "The 14549 th iteration gives loss of 0.22933541918869396\n",
      "The 14550 th iteration gives loss of 0.2293257056501575\n",
      "The 14551 th iteration gives loss of 0.22931599340885375\n",
      "The 14552 th iteration gives loss of 0.22930628246457346\n",
      "The 14553 th iteration gives loss of 0.22929657281713056\n",
      "The 14554 th iteration gives loss of 0.2292868644663242\n",
      "The 14555 th iteration gives loss of 0.2292771574119744\n",
      "The 14556 th iteration gives loss of 0.22926745165387197\n",
      "The 14557 th iteration gives loss of 0.22925774719182493\n",
      "The 14558 th iteration gives loss of 0.2292480440256314\n",
      "The 14559 th iteration gives loss of 0.22923834215510386\n",
      "The 14560 th iteration gives loss of 0.22922864158004622\n",
      "The 14561 th iteration gives loss of 0.22921894230026083\n",
      "The 14562 th iteration gives loss of 0.22920924431556094\n",
      "The 14563 th iteration gives loss of 0.22919954762574607\n",
      "The 14564 th iteration gives loss of 0.22918985223061292\n",
      "The 14565 th iteration gives loss of 0.22918015812996695\n",
      "The 14566 th iteration gives loss of 0.22917046532363616\n",
      "The 14567 th iteration gives loss of 0.22916077381140582\n",
      "The 14568 th iteration gives loss of 0.2291510835930805\n",
      "The 14569 th iteration gives loss of 0.22914139466846528\n",
      "The 14570 th iteration gives loss of 0.22913170703737534\n",
      "The 14571 th iteration gives loss of 0.2291220206996087\n",
      "The 14572 th iteration gives loss of 0.229112335654976\n",
      "The 14573 th iteration gives loss of 0.2291026519032748\n",
      "The 14574 th iteration gives loss of 0.2290929694443081\n",
      "The 14575 th iteration gives loss of 0.2290832882778984\n",
      "The 14576 th iteration gives loss of 0.22907360840383523\n",
      "The 14577 th iteration gives loss of 0.22906392982193827\n",
      "The 14578 th iteration gives loss of 0.2290542525319861\n",
      "The 14579 th iteration gives loss of 0.22904457653380936\n",
      "The 14580 th iteration gives loss of 0.22903490182720468\n",
      "The 14581 th iteration gives loss of 0.22902522841197873\n",
      "The 14582 th iteration gives loss of 0.22901555628793777\n",
      "The 14583 th iteration gives loss of 0.22900588545488232\n",
      "The 14584 th iteration gives loss of 0.22899621591262223\n",
      "The 14585 th iteration gives loss of 0.22898654766096427\n",
      "The 14586 th iteration gives loss of 0.22897688069971003\n",
      "The 14587 th iteration gives loss of 0.22896721502865927\n",
      "The 14588 th iteration gives loss of 0.228957550647634\n",
      "The 14589 th iteration gives loss of 0.22894788755642898\n",
      "The 14590 th iteration gives loss of 0.22893822575485545\n",
      "The 14591 th iteration gives loss of 0.2289285652427158\n",
      "The 14592 th iteration gives loss of 0.22891890601981754\n",
      "The 14593 th iteration gives loss of 0.22890924808595992\n",
      "The 14594 th iteration gives loss of 0.2288995914409627\n",
      "The 14595 th iteration gives loss of 0.22888993608461242\n",
      "The 14596 th iteration gives loss of 0.22888028201673408\n",
      "The 14597 th iteration gives loss of 0.22887062923711443\n",
      "The 14598 th iteration gives loss of 0.2288609777455736\n",
      "The 14599 th iteration gives loss of 0.2288513275419165\n",
      "The 14600 th iteration gives loss of 0.22884167862593738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 14601 th iteration gives loss of 0.22883203099745636\n",
      "The 14602 th iteration gives loss of 0.22882238465626803\n",
      "The 14603 th iteration gives loss of 0.22881273960219292\n",
      "The 14604 th iteration gives loss of 0.2288030958350253\n",
      "The 14605 th iteration gives loss of 0.2287934533545723\n",
      "The 14606 th iteration gives loss of 0.22878381216064725\n",
      "The 14607 th iteration gives loss of 0.22877417225305285\n",
      "The 14608 th iteration gives loss of 0.2287645336315897\n",
      "The 14609 th iteration gives loss of 0.22875489629606358\n",
      "The 14610 th iteration gives loss of 0.2287452602462952\n",
      "The 14611 th iteration gives loss of 0.22873562548206924\n",
      "The 14612 th iteration gives loss of 0.22872599200321156\n",
      "The 14613 th iteration gives loss of 0.22871635980952154\n",
      "The 14614 th iteration gives loss of 0.22870672890079854\n",
      "The 14615 th iteration gives loss of 0.2286970992768567\n",
      "The 14616 th iteration gives loss of 0.2286874709375052\n",
      "The 14617 th iteration gives loss of 0.22867784388253762\n",
      "The 14618 th iteration gives loss of 0.2286682181117709\n",
      "The 14619 th iteration gives loss of 0.22865859362501964\n",
      "The 14620 th iteration gives loss of 0.22864897042207324\n",
      "The 14621 th iteration gives loss of 0.2286393485027468\n",
      "The 14622 th iteration gives loss of 0.2286297278668378\n",
      "The 14623 th iteration gives loss of 0.22862010851416614\n",
      "The 14624 th iteration gives loss of 0.22861049044453133\n",
      "The 14625 th iteration gives loss of 0.22860087365773846\n",
      "The 14626 th iteration gives loss of 0.2285912581535978\n",
      "The 14627 th iteration gives loss of 0.22858164393190833\n",
      "The 14628 th iteration gives loss of 0.22857203099249562\n",
      "The 14629 th iteration gives loss of 0.2285624193351458\n",
      "The 14630 th iteration gives loss of 0.22855280895968058\n",
      "The 14631 th iteration gives loss of 0.22854319986590002\n",
      "The 14632 th iteration gives loss of 0.22853359205360876\n",
      "The 14633 th iteration gives loss of 0.2285239855226164\n",
      "The 14634 th iteration gives loss of 0.22851438027272516\n",
      "The 14635 th iteration gives loss of 0.22850477630375146\n",
      "The 14636 th iteration gives loss of 0.2284951736154929\n",
      "The 14637 th iteration gives loss of 0.22848557220776486\n",
      "The 14638 th iteration gives loss of 0.22847597208037917\n",
      "The 14639 th iteration gives loss of 0.22846637323312782\n",
      "The 14640 th iteration gives loss of 0.22845677566581465\n",
      "The 14641 th iteration gives loss of 0.2284471793782559\n",
      "The 14642 th iteration gives loss of 0.2284375843702672\n",
      "The 14643 th iteration gives loss of 0.22842799064163985\n",
      "The 14644 th iteration gives loss of 0.22841839819219592\n",
      "The 14645 th iteration gives loss of 0.22840880702172717\n",
      "The 14646 th iteration gives loss of 0.22839921713005573\n",
      "The 14647 th iteration gives loss of 0.22838962851698172\n",
      "The 14648 th iteration gives loss of 0.2283800411823006\n",
      "The 14649 th iteration gives loss of 0.2283704551258416\n",
      "The 14650 th iteration gives loss of 0.22836087034740324\n",
      "The 14651 th iteration gives loss of 0.228351286846786\n",
      "The 14652 th iteration gives loss of 0.22834170462380596\n",
      "The 14653 th iteration gives loss of 0.22833212367826478\n",
      "The 14654 th iteration gives loss of 0.2283225440099742\n",
      "The 14655 th iteration gives loss of 0.22831296561874126\n",
      "The 14656 th iteration gives loss of 0.22830338850436513\n",
      "The 14657 th iteration gives loss of 0.22829381266666238\n",
      "The 14658 th iteration gives loss of 0.22828423810544646\n",
      "The 14659 th iteration gives loss of 0.22827466482051462\n",
      "The 14660 th iteration gives loss of 0.22826509281166793\n",
      "The 14661 th iteration gives loss of 0.22825552207872685\n",
      "The 14662 th iteration gives loss of 0.22824595262150146\n",
      "The 14663 th iteration gives loss of 0.2282363844397887\n",
      "The 14664 th iteration gives loss of 0.2282268175333955\n",
      "The 14665 th iteration gives loss of 0.22821725190213277\n",
      "The 14666 th iteration gives loss of 0.22820768754581575\n",
      "The 14667 th iteration gives loss of 0.22819812446425103\n",
      "The 14668 th iteration gives loss of 0.22818856265723117\n",
      "The 14669 th iteration gives loss of 0.22817900212458947\n",
      "The 14670 th iteration gives loss of 0.22816944286610819\n",
      "The 14671 th iteration gives loss of 0.22815988488160674\n",
      "The 14672 th iteration gives loss of 0.22815032817090147\n",
      "The 14673 th iteration gives loss of 0.2281407727337825\n",
      "The 14674 th iteration gives loss of 0.22813121857005747\n",
      "The 14675 th iteration gives loss of 0.22812166567956205\n",
      "The 14676 th iteration gives loss of 0.22811211406207588\n",
      "The 14677 th iteration gives loss of 0.22810256371741253\n",
      "The 14678 th iteration gives loss of 0.2280930146453923\n",
      "The 14679 th iteration gives loss of 0.22808346684581346\n",
      "The 14680 th iteration gives loss of 0.22807392031847143\n",
      "The 14681 th iteration gives loss of 0.2280643750632066\n",
      "The 14682 th iteration gives loss of 0.2280548310798039\n",
      "The 14683 th iteration gives loss of 0.22804528836807733\n",
      "The 14684 th iteration gives loss of 0.22803574692783518\n",
      "The 14685 th iteration gives loss of 0.22802620675888988\n",
      "The 14686 th iteration gives loss of 0.22801666786103847\n",
      "The 14687 th iteration gives loss of 0.22800713023409974\n",
      "The 14688 th iteration gives loss of 0.22799759387787896\n",
      "The 14689 th iteration gives loss of 0.22798805879218223\n",
      "The 14690 th iteration gives loss of 0.22797852497681434\n",
      "The 14691 th iteration gives loss of 0.22796899243159885\n",
      "The 14692 th iteration gives loss of 0.22795946115632554\n",
      "The 14693 th iteration gives loss of 0.2279499311508106\n",
      "The 14694 th iteration gives loss of 0.2279404024148623\n",
      "The 14695 th iteration gives loss of 0.22793087494829792\n",
      "The 14696 th iteration gives loss of 0.22792134875091496\n",
      "The 14697 th iteration gives loss of 0.22791182382252403\n",
      "The 14698 th iteration gives loss of 0.22790230016293875\n",
      "The 14699 th iteration gives loss of 0.22789277777195865\n",
      "The 14700 th iteration gives loss of 0.22788325664940393\n",
      "The 14701 th iteration gives loss of 0.2278737367950681\n",
      "The 14702 th iteration gives loss of 0.22786421820877828\n",
      "The 14703 th iteration gives loss of 0.22785470089033621\n",
      "The 14704 th iteration gives loss of 0.22784518483954508\n",
      "The 14705 th iteration gives loss of 0.22783567005621197\n",
      "The 14706 th iteration gives loss of 0.22782615654015512\n",
      "The 14707 th iteration gives loss of 0.22781664429117843\n",
      "The 14708 th iteration gives loss of 0.2278071333090861\n",
      "The 14709 th iteration gives loss of 0.22779762359369807\n",
      "The 14710 th iteration gives loss of 0.22778811514482455\n",
      "The 14711 th iteration gives loss of 0.22777860796225438\n",
      "The 14712 th iteration gives loss of 0.2277691020458177\n",
      "The 14713 th iteration gives loss of 0.22775959739531002\n",
      "The 14714 th iteration gives loss of 0.22775009401054258\n",
      "The 14715 th iteration gives loss of 0.22774059189132712\n",
      "The 14716 th iteration gives loss of 0.22773109103748274\n",
      "The 14717 th iteration gives loss of 0.22772159144879947\n",
      "The 14718 th iteration gives loss of 0.22771209312510624\n",
      "The 14719 th iteration gives loss of 0.22770259606619908\n",
      "The 14720 th iteration gives loss of 0.22769310027188328\n",
      "The 14721 th iteration gives loss of 0.22768360574198207\n",
      "The 14722 th iteration gives loss of 0.22767411247629846\n",
      "The 14723 th iteration gives loss of 0.22766462047463132\n",
      "The 14724 th iteration gives loss of 0.22765512973680666\n",
      "The 14725 th iteration gives loss of 0.22764564026262485\n",
      "The 14726 th iteration gives loss of 0.2276361520518946\n",
      "The 14727 th iteration gives loss of 0.22762666510442392\n",
      "The 14728 th iteration gives loss of 0.2276171794200403\n",
      "The 14729 th iteration gives loss of 0.22760769499852926\n",
      "The 14730 th iteration gives loss of 0.22759821183970938\n",
      "The 14731 th iteration gives loss of 0.2275887299433949\n",
      "The 14732 th iteration gives loss of 0.22757924930939016\n",
      "The 14733 th iteration gives loss of 0.22756976993749262\n",
      "The 14734 th iteration gives loss of 0.22756029182753854\n",
      "The 14735 th iteration gives loss of 0.22755081497932148\n",
      "The 14736 th iteration gives loss of 0.2275413393926595\n",
      "The 14737 th iteration gives loss of 0.22753186506735107\n",
      "The 14738 th iteration gives loss of 0.22752239200321223\n",
      "The 14739 th iteration gives loss of 0.22751292020004446\n",
      "The 14740 th iteration gives loss of 0.22750344965766145\n",
      "The 14741 th iteration gives loss of 0.22749398037589186\n",
      "The 14742 th iteration gives loss of 0.2274845123545212\n",
      "The 14743 th iteration gives loss of 0.22747504559335696\n",
      "The 14744 th iteration gives loss of 0.22746558009222906\n",
      "The 14745 th iteration gives loss of 0.227456115850947\n",
      "The 14746 th iteration gives loss of 0.22744665286930565\n",
      "The 14747 th iteration gives loss of 0.22743719114711283\n",
      "The 14748 th iteration gives loss of 0.22742773068419614\n",
      "The 14749 th iteration gives loss of 0.22741827148034663\n",
      "The 14750 th iteration gives loss of 0.22740881353538933\n",
      "The 14751 th iteration gives loss of 0.22739935684912516\n",
      "The 14752 th iteration gives loss of 0.22738990142137502\n",
      "The 14753 th iteration gives loss of 0.2273804472519423\n",
      "The 14754 th iteration gives loss of 0.2273709943406313\n",
      "The 14755 th iteration gives loss of 0.22736154268724879\n",
      "The 14756 th iteration gives loss of 0.22735209229162195\n",
      "The 14757 th iteration gives loss of 0.22734264315354677\n",
      "The 14758 th iteration gives loss of 0.22733319527284784\n",
      "The 14759 th iteration gives loss of 0.22732374864932087\n",
      "The 14760 th iteration gives loss of 0.22731430328278382\n",
      "The 14761 th iteration gives loss of 0.2273048591730451\n",
      "The 14762 th iteration gives loss of 0.22729541631991493\n",
      "The 14763 th iteration gives loss of 0.22728597472320564\n",
      "The 14764 th iteration gives loss of 0.22727653438272305\n",
      "The 14765 th iteration gives loss of 0.22726709529827835\n",
      "The 14766 th iteration gives loss of 0.2272576574696816\n",
      "The 14767 th iteration gives loss of 0.22724822089674762\n",
      "The 14768 th iteration gives loss of 0.2272387855792894\n",
      "The 14769 th iteration gives loss of 0.22722935151710735\n",
      "The 14770 th iteration gives loss of 0.22721991871001682\n",
      "The 14771 th iteration gives loss of 0.227210487157834\n",
      "The 14772 th iteration gives loss of 0.22720105686036185\n",
      "The 14773 th iteration gives loss of 0.22719162781741403\n",
      "The 14774 th iteration gives loss of 0.2271822000288047\n",
      "The 14775 th iteration gives loss of 0.2271727734943321\n",
      "The 14776 th iteration gives loss of 0.22716334821381753\n",
      "The 14777 th iteration gives loss of 0.2271539241870767\n",
      "The 14778 th iteration gives loss of 0.22714450141389897\n",
      "The 14779 th iteration gives loss of 0.22713507989411844\n",
      "The 14780 th iteration gives loss of 0.22712565962753894\n",
      "The 14781 th iteration gives loss of 0.22711624061397276\n",
      "The 14782 th iteration gives loss of 0.2271068228532161\n",
      "The 14783 th iteration gives loss of 0.22709740634509262\n",
      "The 14784 th iteration gives loss of 0.2270879910894236\n",
      "The 14785 th iteration gives loss of 0.22707857708599125\n",
      "The 14786 th iteration gives loss of 0.22706916433463217\n",
      "The 14787 th iteration gives loss of 0.22705975283514862\n",
      "The 14788 th iteration gives loss of 0.22705034258734205\n",
      "The 14789 th iteration gives loss of 0.227040933591039\n",
      "The 14790 th iteration gives loss of 0.22703152584604447\n",
      "The 14791 th iteration gives loss of 0.2270221193521733\n",
      "The 14792 th iteration gives loss of 0.22701271410922633\n",
      "The 14793 th iteration gives loss of 0.22700331011701694\n",
      "The 14794 th iteration gives loss of 0.22699390737536343\n",
      "The 14795 th iteration gives loss of 0.22698450588408944\n",
      "The 14796 th iteration gives loss of 0.22697510564297285\n",
      "The 14797 th iteration gives loss of 0.226965706651846\n",
      "The 14798 th iteration gives loss of 0.22695630891051638\n",
      "The 14799 th iteration gives loss of 0.22694691241880227\n",
      "The 14800 th iteration gives loss of 0.22693751717650787\n",
      "The 14801 th iteration gives loss of 0.2269281231834392\n",
      "The 14802 th iteration gives loss of 0.22691873043941427\n",
      "The 14803 th iteration gives loss of 0.22690933894425344\n",
      "The 14804 th iteration gives loss of 0.2268999486977403\n",
      "The 14805 th iteration gives loss of 0.2268905596997113\n",
      "The 14806 th iteration gives loss of 0.22688117194997826\n",
      "The 14807 th iteration gives loss of 0.22687178544834763\n",
      "The 14808 th iteration gives loss of 0.22686240019462178\n",
      "The 14809 th iteration gives loss of 0.22685301618862616\n",
      "The 14810 th iteration gives loss of 0.22684363343016153\n",
      "The 14811 th iteration gives loss of 0.22683425191903836\n",
      "The 14812 th iteration gives loss of 0.22682487165508342\n",
      "The 14813 th iteration gives loss of 0.22681549263809314\n",
      "The 14814 th iteration gives loss of 0.22680611486788008\n",
      "The 14815 th iteration gives loss of 0.22679673834426317\n",
      "The 14816 th iteration gives loss of 0.22678736306705446\n",
      "The 14817 th iteration gives loss of 0.22677798903606022\n",
      "The 14818 th iteration gives loss of 0.22676861625110203\n",
      "The 14819 th iteration gives loss of 0.2267592447119753\n",
      "The 14820 th iteration gives loss of 0.2267498744185015\n",
      "The 14821 th iteration gives loss of 0.22674050537049537\n",
      "The 14822 th iteration gives loss of 0.22673113756777133\n",
      "The 14823 th iteration gives loss of 0.22672177101012875\n",
      "The 14824 th iteration gives loss of 0.22671240569739062\n",
      "The 14825 th iteration gives loss of 0.2267030416293526\n",
      "The 14826 th iteration gives loss of 0.2266936788058545\n",
      "The 14827 th iteration gives loss of 0.22668431722668306\n",
      "The 14828 th iteration gives loss of 0.22667495689165568\n",
      "The 14829 th iteration gives loss of 0.22666559780059764\n",
      "The 14830 th iteration gives loss of 0.22665623995331355\n",
      "The 14831 th iteration gives loss of 0.22664688334961725\n",
      "The 14832 th iteration gives loss of 0.22663752798930853\n",
      "The 14833 th iteration gives loss of 0.22662817387221273\n",
      "The 14834 th iteration gives loss of 0.22661882099813518\n",
      "The 14835 th iteration gives loss of 0.22660946936689716\n",
      "The 14836 th iteration gives loss of 0.22660011897830248\n",
      "The 14837 th iteration gives loss of 0.22659076983216156\n",
      "The 14838 th iteration gives loss of 0.2265814219282915\n",
      "The 14839 th iteration gives loss of 0.2265720752665107\n",
      "The 14840 th iteration gives loss of 0.22656272984662051\n",
      "The 14841 th iteration gives loss of 0.22655338566843336\n",
      "The 14842 th iteration gives loss of 0.22654404273177897\n",
      "The 14843 th iteration gives loss of 0.22653470103644943\n",
      "The 14844 th iteration gives loss of 0.22652536058226935\n",
      "The 14845 th iteration gives loss of 0.22651602136903778\n",
      "The 14846 th iteration gives loss of 0.2265066833965803\n",
      "The 14847 th iteration gives loss of 0.22649734666471003\n",
      "The 14848 th iteration gives loss of 0.22648801117323394\n",
      "The 14849 th iteration gives loss of 0.2264786769219586\n",
      "The 14850 th iteration gives loss of 0.2264693439107186\n",
      "The 14851 th iteration gives loss of 0.22646001213929862\n",
      "The 14852 th iteration gives loss of 0.2264506816075328\n",
      "The 14853 th iteration gives loss of 0.2264413523152162\n",
      "The 14854 th iteration gives loss of 0.2264320242621731\n",
      "The 14855 th iteration gives loss of 0.22642269744822532\n",
      "The 14856 th iteration gives loss of 0.22641337187316435\n",
      "The 14857 th iteration gives loss of 0.22640404753681825\n",
      "The 14858 th iteration gives loss of 0.22639472443898967\n",
      "The 14859 th iteration gives loss of 0.2263854025795025\n",
      "The 14860 th iteration gives loss of 0.22637608195815243\n",
      "The 14861 th iteration gives loss of 0.22636676257478364\n",
      "The 14862 th iteration gives loss of 0.22635744442917716\n",
      "The 14863 th iteration gives loss of 0.22634812752115582\n",
      "The 14864 th iteration gives loss of 0.22633881185053817\n",
      "The 14865 th iteration gives loss of 0.22632949741713368\n",
      "The 14866 th iteration gives loss of 0.22632018422076144\n",
      "The 14867 th iteration gives loss of 0.2263108722612276\n",
      "The 14868 th iteration gives loss of 0.22630156153834563\n",
      "The 14869 th iteration gives loss of 0.22629225205192266\n",
      "The 14870 th iteration gives loss of 0.22628294380178635\n",
      "The 14871 th iteration gives loss of 0.2262736367877432\n",
      "The 14872 th iteration gives loss of 0.2262643310096041\n",
      "The 14873 th iteration gives loss of 0.22625502646717824\n",
      "The 14874 th iteration gives loss of 0.22624572316028754\n",
      "The 14875 th iteration gives loss of 0.2262364210887372\n",
      "The 14876 th iteration gives loss of 0.22622712025235495\n",
      "The 14877 th iteration gives loss of 0.22621782065094187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 14878 th iteration gives loss of 0.22620852228432167\n",
      "The 14879 th iteration gives loss of 0.2261992251522932\n",
      "The 14880 th iteration gives loss of 0.22618992925467504\n",
      "The 14881 th iteration gives loss of 0.22618063459128868\n",
      "The 14882 th iteration gives loss of 0.22617134116193552\n",
      "The 14883 th iteration gives loss of 0.22616204896643938\n",
      "The 14884 th iteration gives loss of 0.22615275800460813\n",
      "The 14885 th iteration gives loss of 0.22614346827625428\n",
      "The 14886 th iteration gives loss of 0.22613417978120157\n",
      "The 14887 th iteration gives loss of 0.22612489251924994\n",
      "The 14888 th iteration gives loss of 0.22611560649022383\n",
      "The 14889 th iteration gives loss of 0.22610632169392686\n",
      "The 14890 th iteration gives loss of 0.2260970381301891\n",
      "The 14891 th iteration gives loss of 0.22608775579880253\n",
      "The 14892 th iteration gives loss of 0.22607847469958578\n",
      "The 14893 th iteration gives loss of 0.22606919483236837\n",
      "The 14894 th iteration gives loss of 0.22605991619695473\n",
      "The 14895 th iteration gives loss of 0.22605063879316362\n",
      "The 14896 th iteration gives loss of 0.22604136262079771\n",
      "The 14897 th iteration gives loss of 0.22603208767967886\n",
      "The 14898 th iteration gives loss of 0.22602281396961899\n",
      "The 14899 th iteration gives loss of 0.2260135414904273\n",
      "The 14900 th iteration gives loss of 0.22600427024192202\n",
      "The 14901 th iteration gives loss of 0.22599500022391816\n",
      "The 14902 th iteration gives loss of 0.22598573143623935\n",
      "The 14903 th iteration gives loss of 0.22597646387868187\n",
      "The 14904 th iteration gives loss of 0.22596719755106673\n",
      "The 14905 th iteration gives loss of 0.22595793245321294\n",
      "The 14906 th iteration gives loss of 0.22594866858492454\n",
      "The 14907 th iteration gives loss of 0.22593940594602024\n",
      "The 14908 th iteration gives loss of 0.2259301445363192\n",
      "The 14909 th iteration gives loss of 0.22592088435563215\n",
      "The 14910 th iteration gives loss of 0.22591162540377807\n",
      "The 14911 th iteration gives loss of 0.22590236768055846\n",
      "The 14912 th iteration gives loss of 0.22589311118579938\n",
      "The 14913 th iteration gives loss of 0.22588385591931537\n",
      "The 14914 th iteration gives loss of 0.22587460188091385\n",
      "The 14915 th iteration gives loss of 0.22586534907040806\n",
      "The 14916 th iteration gives loss of 0.22585609748761357\n",
      "The 14917 th iteration gives loss of 0.22584684713235653\n",
      "The 14918 th iteration gives loss of 0.22583759800443182\n",
      "The 14919 th iteration gives loss of 0.22582835010366917\n",
      "The 14920 th iteration gives loss of 0.22581910342987827\n",
      "The 14921 th iteration gives loss of 0.22580985798287576\n",
      "The 14922 th iteration gives loss of 0.22580061376246882\n",
      "The 14923 th iteration gives loss of 0.22579137076848693\n",
      "The 14924 th iteration gives loss of 0.22578212900072625\n",
      "The 14925 th iteration gives loss of 0.2257728884590183\n",
      "The 14926 th iteration gives loss of 0.22576364914316113\n",
      "The 14927 th iteration gives loss of 0.22575441105298769\n",
      "The 14928 th iteration gives loss of 0.22574517418830203\n",
      "The 14929 th iteration gives loss of 0.22573593854891377\n",
      "The 14930 th iteration gives loss of 0.22572670413464263\n",
      "The 14931 th iteration gives loss of 0.22571747094531044\n",
      "The 14932 th iteration gives loss of 0.22570823898071804\n",
      "The 14933 th iteration gives loss of 0.22569900824069267\n",
      "The 14934 th iteration gives loss of 0.22568977872504975\n",
      "The 14935 th iteration gives loss of 0.22568055043359636\n",
      "The 14936 th iteration gives loss of 0.22567132336615484\n",
      "The 14937 th iteration gives loss of 0.22566209752253064\n",
      "The 14938 th iteration gives loss of 0.22565287290253344\n",
      "The 14939 th iteration gives loss of 0.22564364950599897\n",
      "The 14940 th iteration gives loss of 0.22563442733273018\n",
      "The 14941 th iteration gives loss of 0.22562520638253866\n",
      "The 14942 th iteration gives loss of 0.22561598665525062\n",
      "The 14943 th iteration gives loss of 0.22560676815067557\n",
      "The 14944 th iteration gives loss of 0.22559755086862476\n",
      "The 14945 th iteration gives loss of 0.2255883348089255\n",
      "The 14946 th iteration gives loss of 0.22557911997138047\n",
      "The 14947 th iteration gives loss of 0.2255699063558053\n",
      "The 14948 th iteration gives loss of 0.22556069396201803\n",
      "The 14949 th iteration gives loss of 0.22555148278983203\n",
      "The 14950 th iteration gives loss of 0.22554227283906736\n",
      "The 14951 th iteration gives loss of 0.2255330641095382\n",
      "The 14952 th iteration gives loss of 0.2255238566010567\n",
      "The 14953 th iteration gives loss of 0.225514650313442\n",
      "The 14954 th iteration gives loss of 0.22550544524650618\n",
      "The 14955 th iteration gives loss of 0.22549624140006216\n",
      "The 14956 th iteration gives loss of 0.22548703877393295\n",
      "The 14957 th iteration gives loss of 0.22547783736794083\n",
      "The 14958 th iteration gives loss of 0.22546863718187155\n",
      "The 14959 th iteration gives loss of 0.22545943821557587\n",
      "The 14960 th iteration gives loss of 0.2254502404688465\n",
      "The 14961 th iteration gives loss of 0.22544104394150608\n",
      "The 14962 th iteration gives loss of 0.22543184863336538\n",
      "The 14963 th iteration gives loss of 0.2254226545442496\n",
      "The 14964 th iteration gives loss of 0.2254134616739638\n",
      "The 14965 th iteration gives loss of 0.225404270022335\n",
      "The 14966 th iteration gives loss of 0.225395079589175\n",
      "The 14967 th iteration gives loss of 0.2253858903742954\n",
      "The 14968 th iteration gives loss of 0.22537670237751226\n",
      "The 14969 th iteration gives loss of 0.2253675155986461\n",
      "The 14970 th iteration gives loss of 0.22535833003750708\n",
      "The 14971 th iteration gives loss of 0.2253491456939132\n",
      "The 14972 th iteration gives loss of 0.22533996256768984\n",
      "The 14973 th iteration gives loss of 0.22533078065863707\n",
      "The 14974 th iteration gives loss of 0.22532159996657103\n",
      "The 14975 th iteration gives loss of 0.22531242049132633\n",
      "The 14976 th iteration gives loss of 0.22530324223269738\n",
      "The 14977 th iteration gives loss of 0.22529406519051504\n",
      "The 14978 th iteration gives loss of 0.2252848893645891\n",
      "The 14979 th iteration gives loss of 0.22527571475473518\n",
      "The 14980 th iteration gives loss of 0.2252665413607755\n",
      "The 14981 th iteration gives loss of 0.2252573691825166\n",
      "The 14982 th iteration gives loss of 0.2252481982197785\n",
      "The 14983 th iteration gives loss of 0.2252390284723793\n",
      "The 14984 th iteration gives loss of 0.22522985994014194\n",
      "The 14985 th iteration gives loss of 0.2252206926228637\n",
      "The 14986 th iteration gives loss of 0.22521152652037235\n",
      "The 14987 th iteration gives loss of 0.2252023616324918\n",
      "The 14988 th iteration gives loss of 0.2251931979590155\n",
      "The 14989 th iteration gives loss of 0.2251840354997869\n",
      "The 14990 th iteration gives loss of 0.22517487425460464\n",
      "The 14991 th iteration gives loss of 0.22516571422329762\n",
      "The 14992 th iteration gives loss of 0.225156555405669\n",
      "The 14993 th iteration gives loss of 0.22514739780154353\n",
      "The 14994 th iteration gives loss of 0.2251382414107289\n",
      "The 14995 th iteration gives loss of 0.22512908623305336\n",
      "The 14996 th iteration gives loss of 0.22511993226831764\n",
      "The 14997 th iteration gives loss of 0.2251107795163584\n",
      "The 14998 th iteration gives loss of 0.22510162797698643\n",
      "The 14999 th iteration gives loss of 0.2250924776500133\n",
      "The 15000 th iteration gives loss of 0.22508332853524726\n",
      "The 15001 th iteration gives loss of 0.2250741806325163\n",
      "The 15002 th iteration gives loss of 0.22506503394163646\n",
      "The 15003 th iteration gives loss of 0.22505588846242622\n",
      "The 15004 th iteration gives loss of 0.22504674419469425\n",
      "The 15005 th iteration gives loss of 0.22503760113826282\n",
      "The 15006 th iteration gives loss of 0.22502845929294918\n",
      "The 15007 th iteration gives loss of 0.22501931865856625\n",
      "The 15008 th iteration gives loss of 0.22501017923493596\n",
      "The 15009 th iteration gives loss of 0.22500104102186558\n",
      "The 15010 th iteration gives loss of 0.22499190401917837\n",
      "The 15011 th iteration gives loss of 0.22498276822670304\n",
      "The 15012 th iteration gives loss of 0.22497363364423478\n",
      "The 15013 th iteration gives loss of 0.22496450027160028\n",
      "The 15014 th iteration gives loss of 0.22495536810862046\n",
      "The 15015 th iteration gives loss of 0.22494623715511256\n",
      "The 15016 th iteration gives loss of 0.22493710741088266\n",
      "The 15017 th iteration gives loss of 0.2249279788757636\n",
      "The 15018 th iteration gives loss of 0.22491885154955446\n",
      "The 15019 th iteration gives loss of 0.22490972543208568\n",
      "The 15020 th iteration gives loss of 0.22490060052316538\n",
      "The 15021 th iteration gives loss of 0.22489147682261765\n",
      "The 15022 th iteration gives loss of 0.224882354330264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 15023 th iteration gives loss of 0.22487323304591103\n",
      "The 15024 th iteration gives loss of 0.22486411296936504\n",
      "The 15025 th iteration gives loss of 0.22485499410048293\n",
      "The 15026 th iteration gives loss of 0.22484587643904705\n",
      "The 15027 th iteration gives loss of 0.2248367599848823\n",
      "The 15028 th iteration gives loss of 0.22482764473780895\n",
      "The 15029 th iteration gives loss of 0.22481853069764685\n",
      "The 15030 th iteration gives loss of 0.2248094178642052\n",
      "The 15031 th iteration gives loss of 0.22480030623731476\n",
      "The 15032 th iteration gives loss of 0.22479119581678117\n",
      "The 15033 th iteration gives loss of 0.22478208660242385\n",
      "The 15034 th iteration gives loss of 0.22477297859406148\n",
      "The 15035 th iteration gives loss of 0.22476387179151597\n",
      "The 15036 th iteration gives loss of 0.2247547661945955\n",
      "The 15037 th iteration gives loss of 0.22474566180312916\n",
      "The 15038 th iteration gives loss of 0.2247365586169259\n",
      "The 15039 th iteration gives loss of 0.22472745663580193\n",
      "The 15040 th iteration gives loss of 0.22471835585958494\n",
      "The 15041 th iteration gives loss of 0.22470925628808017\n",
      "The 15042 th iteration gives loss of 0.2247001579211102\n",
      "The 15043 th iteration gives loss of 0.22469106075850795\n",
      "The 15044 th iteration gives loss of 0.2246819648000685\n",
      "The 15045 th iteration gives loss of 0.2246728700456146\n",
      "The 15046 th iteration gives loss of 0.22466377649496588\n",
      "The 15047 th iteration gives loss of 0.224654684147951\n",
      "The 15048 th iteration gives loss of 0.22464559300437525\n",
      "The 15049 th iteration gives loss of 0.22463650306406002\n",
      "The 15050 th iteration gives loss of 0.22462741432682284\n",
      "The 15051 th iteration gives loss of 0.2246183267924798\n",
      "The 15052 th iteration gives loss of 0.22460924046084904\n",
      "The 15053 th iteration gives loss of 0.22460015533175665\n",
      "The 15054 th iteration gives loss of 0.22459107140500476\n",
      "The 15055 th iteration gives loss of 0.2245819886804254\n",
      "The 15056 th iteration gives loss of 0.22457290715783573\n",
      "The 15057 th iteration gives loss of 0.22456382683705195\n",
      "The 15058 th iteration gives loss of 0.2245547477178837\n",
      "The 15059 th iteration gives loss of 0.22454566980016014\n",
      "The 15060 th iteration gives loss of 0.22453659308369836\n",
      "The 15061 th iteration gives loss of 0.22452751756830525\n",
      "The 15062 th iteration gives loss of 0.22451844325381404\n",
      "The 15063 th iteration gives loss of 0.22450937014003092\n",
      "The 15064 th iteration gives loss of 0.2245002982267826\n",
      "The 15065 th iteration gives loss of 0.22449122751388897\n",
      "The 15066 th iteration gives loss of 0.2244821580011489\n",
      "The 15067 th iteration gives loss of 0.22447308968840735\n",
      "The 15068 th iteration gives loss of 0.22446402257546866\n",
      "The 15069 th iteration gives loss of 0.22445495666214563\n",
      "The 15070 th iteration gives loss of 0.2244458919482713\n",
      "The 15071 th iteration gives loss of 0.22443682843365753\n",
      "The 15072 th iteration gives loss of 0.2244277661181166\n",
      "The 15073 th iteration gives loss of 0.22441870500147804\n",
      "The 15074 th iteration gives loss of 0.22440964508354785\n",
      "The 15075 th iteration gives loss of 0.2244005863641543\n",
      "The 15076 th iteration gives loss of 0.2243915288431158\n",
      "The 15077 th iteration gives loss of 0.22438247252024285\n",
      "The 15078 th iteration gives loss of 0.2243734173953653\n",
      "The 15079 th iteration gives loss of 0.22436436346829677\n",
      "The 15080 th iteration gives loss of 0.22435531073885048\n",
      "The 15081 th iteration gives loss of 0.22434625920684906\n",
      "The 15082 th iteration gives loss of 0.22433720887210803\n",
      "The 15083 th iteration gives loss of 0.22432815973445694\n",
      "The 15084 th iteration gives loss of 0.22431911179370953\n",
      "The 15085 th iteration gives loss of 0.22431006504967738\n",
      "The 15086 th iteration gives loss of 0.2243010195021875\n",
      "The 15087 th iteration gives loss of 0.2242919751510583\n",
      "The 15088 th iteration gives loss of 0.22428293199610236\n",
      "The 15089 th iteration gives loss of 0.2242738900371362\n",
      "The 15090 th iteration gives loss of 0.22426484927399234\n",
      "The 15091 th iteration gives loss of 0.2242558097064716\n",
      "The 15092 th iteration gives loss of 0.2242467713344088\n",
      "The 15093 th iteration gives loss of 0.22423773415762632\n",
      "The 15094 th iteration gives loss of 0.22422869817592647\n",
      "The 15095 th iteration gives loss of 0.22421966338913923\n",
      "The 15096 th iteration gives loss of 0.2242106297970781\n",
      "The 15097 th iteration gives loss of 0.22420159739956086\n",
      "The 15098 th iteration gives loss of 0.2241925661964239\n",
      "The 15099 th iteration gives loss of 0.22418353618745862\n",
      "The 15100 th iteration gives loss of 0.22417450737250888\n",
      "The 15101 th iteration gives loss of 0.22416547975137552\n",
      "The 15102 th iteration gives loss of 0.22415645332389011\n",
      "The 15103 th iteration gives loss of 0.2241474280898689\n",
      "The 15104 th iteration gives loss of 0.22413840404913038\n",
      "The 15105 th iteration gives loss of 0.22412938120148637\n",
      "The 15106 th iteration gives loss of 0.22412035954676907\n",
      "The 15107 th iteration gives loss of 0.22411133908478986\n",
      "The 15108 th iteration gives loss of 0.22410231981536755\n",
      "The 15109 th iteration gives loss of 0.22409330173832628\n",
      "The 15110 th iteration gives loss of 0.22408428485348159\n",
      "The 15111 th iteration gives loss of 0.22407526916066065\n",
      "The 15112 th iteration gives loss of 0.22406625465967406\n",
      "The 15113 th iteration gives loss of 0.22405724135034483\n",
      "The 15114 th iteration gives loss of 0.22404822923248435\n",
      "The 15115 th iteration gives loss of 0.22403921830592471\n",
      "The 15116 th iteration gives loss of 0.22403020857048192\n",
      "The 15117 th iteration gives loss of 0.22402120002597958\n",
      "The 15118 th iteration gives loss of 0.22401219267221864\n",
      "The 15119 th iteration gives loss of 0.22400318650904014\n",
      "The 15120 th iteration gives loss of 0.2239941815362589\n",
      "The 15121 th iteration gives loss of 0.22398517775368898\n",
      "The 15122 th iteration gives loss of 0.22397617516114934\n",
      "The 15123 th iteration gives loss of 0.2239671737584628\n",
      "The 15124 th iteration gives loss of 0.22395817354545242\n",
      "The 15125 th iteration gives loss of 0.22394917452192797\n",
      "The 15126 th iteration gives loss of 0.2239401766877228\n",
      "The 15127 th iteration gives loss of 0.2239311800426494\n",
      "The 15128 th iteration gives loss of 0.22392218458653418\n",
      "The 15129 th iteration gives loss of 0.22391319031917795\n",
      "The 15130 th iteration gives loss of 0.22390419724041286\n",
      "The 15131 th iteration gives loss of 0.22389520535006416\n",
      "The 15132 th iteration gives loss of 0.2238862146479501\n",
      "The 15133 th iteration gives loss of 0.2238772251338829\n",
      "The 15134 th iteration gives loss of 0.2238682368076936\n",
      "The 15135 th iteration gives loss of 0.22385924966920814\n",
      "The 15136 th iteration gives loss of 0.22385026371821626\n",
      "The 15137 th iteration gives loss of 0.22384127895456646\n",
      "The 15138 th iteration gives loss of 0.22383229537806493\n",
      "The 15139 th iteration gives loss of 0.22382331298854258\n",
      "The 15140 th iteration gives loss of 0.22381433178580346\n",
      "The 15141 th iteration gives loss of 0.22380535176968305\n",
      "The 15142 th iteration gives loss of 0.2237963729399855\n",
      "The 15143 th iteration gives loss of 0.22378739529655445\n",
      "The 15144 th iteration gives loss of 0.2237784188391998\n",
      "The 15145 th iteration gives loss of 0.22376944356772918\n",
      "The 15146 th iteration gives loss of 0.2237604694819797\n",
      "The 15147 th iteration gives loss of 0.22375149658176394\n",
      "The 15148 th iteration gives loss of 0.22374252486690308\n",
      "The 15149 th iteration gives loss of 0.2237335543372152\n",
      "The 15150 th iteration gives loss of 0.2237245849925246\n",
      "The 15151 th iteration gives loss of 0.22371561683265337\n",
      "The 15152 th iteration gives loss of 0.2237066498574208\n",
      "The 15153 th iteration gives loss of 0.22369768406664145\n",
      "The 15154 th iteration gives loss of 0.223688719460142\n",
      "The 15155 th iteration gives loss of 0.22367975603774284\n",
      "The 15156 th iteration gives loss of 0.2236707937992597\n",
      "The 15157 th iteration gives loss of 0.22366183274452034\n",
      "The 15158 th iteration gives loss of 0.22365287287334015\n",
      "The 15159 th iteration gives loss of 0.223643914185547\n",
      "The 15160 th iteration gives loss of 0.22363495668094518\n",
      "The 15161 th iteration gives loss of 0.22362600035937258\n",
      "The 15162 th iteration gives loss of 0.22361704522064016\n",
      "The 15163 th iteration gives loss of 0.22360809126458187\n",
      "The 15164 th iteration gives loss of 0.22359913849099203\n",
      "The 15165 th iteration gives loss of 0.22359018689972265\n",
      "The 15166 th iteration gives loss of 0.22358123649057368\n",
      "The 15167 th iteration gives loss of 0.2235722872633773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 15168 th iteration gives loss of 0.22356333921794813\n",
      "The 15169 th iteration gives loss of 0.22355439235409566\n",
      "The 15170 th iteration gives loss of 0.22354544667166487\n",
      "The 15171 th iteration gives loss of 0.22353650217047202\n",
      "The 15172 th iteration gives loss of 0.22352755885033157\n",
      "The 15173 th iteration gives loss of 0.2235186167110503\n",
      "The 15174 th iteration gives loss of 0.22350967575248046\n",
      "The 15175 th iteration gives loss of 0.22350073597441364\n",
      "The 15176 th iteration gives loss of 0.2234917973766884\n",
      "The 15177 th iteration gives loss of 0.2234828599591263\n",
      "The 15178 th iteration gives loss of 0.2234739237215405\n",
      "The 15179 th iteration gives loss of 0.22346498866374517\n",
      "The 15180 th iteration gives loss of 0.22345605478557992\n",
      "The 15181 th iteration gives loss of 0.22344712208685638\n",
      "The 15182 th iteration gives loss of 0.22343819056740236\n",
      "The 15183 th iteration gives loss of 0.2234292602270206\n",
      "The 15184 th iteration gives loss of 0.22342033106555775\n",
      "The 15185 th iteration gives loss of 0.22341140308281437\n",
      "The 15186 th iteration gives loss of 0.2234024762786241\n",
      "The 15187 th iteration gives loss of 0.2233935506528083\n",
      "The 15188 th iteration gives loss of 0.22338462620517738\n",
      "The 15189 th iteration gives loss of 0.22337570293556772\n",
      "The 15190 th iteration gives loss of 0.22336678084378625\n",
      "The 15191 th iteration gives loss of 0.22335785992966994\n",
      "The 15192 th iteration gives loss of 0.22334894019302606\n",
      "The 15193 th iteration gives loss of 0.2233400216336873\n",
      "The 15194 th iteration gives loss of 0.22333110425146135\n",
      "The 15195 th iteration gives loss of 0.22332218804618165\n",
      "The 15196 th iteration gives loss of 0.2233132730176651\n",
      "The 15197 th iteration gives loss of 0.22330435916573663\n",
      "The 15198 th iteration gives loss of 0.2232954464902222\n",
      "The 15199 th iteration gives loss of 0.22328653499092505\n",
      "The 15200 th iteration gives loss of 0.22327762466768972\n",
      "The 15201 th iteration gives loss of 0.2232687155203207\n",
      "The 15202 th iteration gives loss of 0.2232598075486528\n",
      "The 15203 th iteration gives loss of 0.22325090075248855\n",
      "The 15204 th iteration gives loss of 0.2232419951316663\n",
      "The 15205 th iteration gives loss of 0.22323309068601252\n",
      "The 15206 th iteration gives loss of 0.22322418741533628\n",
      "The 15207 th iteration gives loss of 0.22321528531946933\n",
      "The 15208 th iteration gives loss of 0.2232063843982197\n",
      "The 15209 th iteration gives loss of 0.22319748465142356\n",
      "The 15210 th iteration gives loss of 0.22318858607889933\n",
      "The 15211 th iteration gives loss of 0.22317968868046342\n",
      "The 15212 th iteration gives loss of 0.22317079245593993\n",
      "The 15213 th iteration gives loss of 0.22316189740514955\n",
      "The 15214 th iteration gives loss of 0.2231530035279271\n",
      "The 15215 th iteration gives loss of 0.22314411082407454\n",
      "The 15216 th iteration gives loss of 0.223135219293428\n",
      "The 15217 th iteration gives loss of 0.2231263289358061\n",
      "The 15218 th iteration gives loss of 0.2231174397510287\n",
      "The 15219 th iteration gives loss of 0.22310855173892016\n",
      "The 15220 th iteration gives loss of 0.2230996648993096\n",
      "The 15221 th iteration gives loss of 0.22309077923200982\n",
      "The 15222 th iteration gives loss of 0.22308189473684936\n",
      "The 15223 th iteration gives loss of 0.22307301141363586\n",
      "The 15224 th iteration gives loss of 0.22306412926219818\n",
      "The 15225 th iteration gives loss of 0.22305524828237275\n",
      "The 15226 th iteration gives loss of 0.22304636847397347\n",
      "The 15227 th iteration gives loss of 0.22303748983681734\n",
      "The 15228 th iteration gives loss of 0.22302861237073712\n",
      "The 15229 th iteration gives loss of 0.223019736075549\n",
      "The 15230 th iteration gives loss of 0.22301086095106978\n",
      "The 15231 th iteration gives loss of 0.223001986997129\n",
      "The 15232 th iteration gives loss of 0.2229931142135468\n",
      "The 15233 th iteration gives loss of 0.22298424260015476\n",
      "The 15234 th iteration gives loss of 0.2229753721567638\n",
      "The 15235 th iteration gives loss of 0.22296650288319558\n",
      "The 15236 th iteration gives loss of 0.222957634779283\n",
      "The 15237 th iteration gives loss of 0.22294876784483258\n",
      "The 15238 th iteration gives loss of 0.22293990207968997\n",
      "The 15239 th iteration gives loss of 0.2229310374836596\n",
      "The 15240 th iteration gives loss of 0.22292217405657258\n",
      "The 15241 th iteration gives loss of 0.2229133117982475\n",
      "The 15242 th iteration gives loss of 0.2229044507085117\n",
      "The 15243 th iteration gives loss of 0.2228955907871812\n",
      "The 15244 th iteration gives loss of 0.22288673203408385\n",
      "The 15245 th iteration gives loss of 0.22287787444904472\n",
      "The 15246 th iteration gives loss of 0.2228690180318846\n",
      "The 15247 th iteration gives loss of 0.22286016278241763\n",
      "The 15248 th iteration gives loss of 0.2228513087004764\n",
      "The 15249 th iteration gives loss of 0.22284245578588247\n",
      "The 15250 th iteration gives loss of 0.2228336040384595\n",
      "The 15251 th iteration gives loss of 0.22282475345802957\n",
      "The 15252 th iteration gives loss of 0.22281590404441504\n",
      "The 15253 th iteration gives loss of 0.22280705579743604\n",
      "The 15254 th iteration gives loss of 0.22279820871693143\n",
      "The 15255 th iteration gives loss of 0.22278936280270223\n",
      "The 15256 th iteration gives loss of 0.222780518054582\n",
      "The 15257 th iteration gives loss of 0.22277167447239574\n",
      "The 15258 th iteration gives loss of 0.22276283205596\n",
      "The 15259 th iteration gives loss of 0.2227539908051051\n",
      "The 15260 th iteration gives loss of 0.22274515071965323\n",
      "The 15261 th iteration gives loss of 0.2227363117994214\n",
      "The 15262 th iteration gives loss of 0.22272747404423437\n",
      "The 15263 th iteration gives loss of 0.22271863745391482\n",
      "The 15264 th iteration gives loss of 0.2227098020282992\n",
      "The 15265 th iteration gives loss of 0.22270096776719578\n",
      "The 15266 th iteration gives loss of 0.2226921346704347\n",
      "The 15267 th iteration gives loss of 0.22268330273784548\n",
      "The 15268 th iteration gives loss of 0.22267447196923498\n",
      "The 15269 th iteration gives loss of 0.222665642364443\n",
      "The 15270 th iteration gives loss of 0.22265681392328096\n",
      "The 15271 th iteration gives loss of 0.22264798664557364\n",
      "The 15272 th iteration gives loss of 0.2226391605311521\n",
      "The 15273 th iteration gives loss of 0.2226303355798371\n",
      "The 15274 th iteration gives loss of 0.22262151179145137\n",
      "The 15275 th iteration gives loss of 0.22261268916581528\n",
      "The 15276 th iteration gives loss of 0.22260386770275156\n",
      "The 15277 th iteration gives loss of 0.2225950474020987\n",
      "The 15278 th iteration gives loss of 0.2225862282636601\n",
      "The 15279 th iteration gives loss of 0.22257741028726938\n",
      "The 15280 th iteration gives loss of 0.22256859347276076\n",
      "The 15281 th iteration gives loss of 0.2225597778199381\n",
      "The 15282 th iteration gives loss of 0.22255096332863303\n",
      "The 15283 th iteration gives loss of 0.22254214999866606\n",
      "The 15284 th iteration gives loss of 0.2225333378298748\n",
      "The 15285 th iteration gives loss of 0.22252452682207802\n",
      "The 15286 th iteration gives loss of 0.2225157169750858\n",
      "The 15287 th iteration gives loss of 0.22250690828873762\n",
      "The 15288 th iteration gives loss of 0.22249810076284796\n",
      "The 15289 th iteration gives loss of 0.22248929439724083\n",
      "The 15290 th iteration gives loss of 0.2224804891917504\n",
      "The 15291 th iteration gives loss of 0.2224716851461861\n",
      "The 15292 th iteration gives loss of 0.22246288226038646\n",
      "The 15293 th iteration gives loss of 0.222454080534171\n",
      "The 15294 th iteration gives loss of 0.2224452799673583\n",
      "The 15295 th iteration gives loss of 0.22243648055977572\n",
      "The 15296 th iteration gives loss of 0.22242768231125118\n",
      "The 15297 th iteration gives loss of 0.222418885221604\n",
      "The 15298 th iteration gives loss of 0.22241008929064812\n",
      "The 15299 th iteration gives loss of 0.22240129451822763\n",
      "The 15300 th iteration gives loss of 0.22239250090416332\n",
      "The 15301 th iteration gives loss of 0.22238370844826777\n",
      "The 15302 th iteration gives loss of 0.2223749171503782\n",
      "The 15303 th iteration gives loss of 0.22236612701031055\n",
      "The 15304 th iteration gives loss of 0.22235733802788912\n",
      "The 15305 th iteration gives loss of 0.222348550202942\n",
      "The 15306 th iteration gives loss of 0.22233976353528506\n",
      "The 15307 th iteration gives loss of 0.22233097802476653\n",
      "The 15308 th iteration gives loss of 0.22232219367118616\n",
      "The 15309 th iteration gives loss of 0.22231341047437497\n",
      "The 15310 th iteration gives loss of 0.22230462843415513\n",
      "The 15311 th iteration gives loss of 0.2222958475503656\n",
      "The 15312 th iteration gives loss of 0.22228706782281188\n",
      "The 15313 th iteration gives loss of 0.22227828925132426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 15314 th iteration gives loss of 0.22226951183573854\n",
      "The 15315 th iteration gives loss of 0.22226073557586878\n",
      "The 15316 th iteration gives loss of 0.2222519604715368\n",
      "The 15317 th iteration gives loss of 0.22224318652257824\n",
      "The 15318 th iteration gives loss of 0.22223441372881098\n",
      "The 15319 th iteration gives loss of 0.22222564209005363\n",
      "The 15320 th iteration gives loss of 0.22221687160614947\n",
      "The 15321 th iteration gives loss of 0.22220810227690685\n",
      "The 15322 th iteration gives loss of 0.22219933410214693\n",
      "The 15323 th iteration gives loss of 0.22219056708171597\n",
      "The 15324 th iteration gives loss of 0.2221818012154156\n",
      "The 15325 th iteration gives loss of 0.22217303650308948\n",
      "The 15326 th iteration gives loss of 0.22216427294455435\n",
      "The 15327 th iteration gives loss of 0.22215551053963228\n",
      "The 15328 th iteration gives loss of 0.22214674928814795\n",
      "The 15329 th iteration gives loss of 0.2221379891899298\n",
      "The 15330 th iteration gives loss of 0.22212923024480666\n",
      "The 15331 th iteration gives loss of 0.22212047245259608\n",
      "The 15332 th iteration gives loss of 0.22211171581312264\n",
      "The 15333 th iteration gives loss of 0.2221029603262182\n",
      "The 15334 th iteration gives loss of 0.2220942059917029\n",
      "The 15335 th iteration gives loss of 0.22208545280940167\n",
      "The 15336 th iteration gives loss of 0.22207670077914682\n",
      "The 15337 th iteration gives loss of 0.22206794990075007\n",
      "The 15338 th iteration gives loss of 0.22205920017405198\n",
      "The 15339 th iteration gives loss of 0.22205045159887804\n",
      "The 15340 th iteration gives loss of 0.22204170417502597\n",
      "The 15341 th iteration gives loss of 0.22203295790235372\n",
      "The 15342 th iteration gives loss of 0.22202421278067516\n",
      "The 15343 th iteration gives loss of 0.22201546880981596\n",
      "The 15344 th iteration gives loss of 0.22200672598958593\n",
      "The 15345 th iteration gives loss of 0.22199798431983428\n",
      "The 15346 th iteration gives loss of 0.22198924380037394\n",
      "The 15347 th iteration gives loss of 0.2219805044310442\n",
      "The 15348 th iteration gives loss of 0.22197176621164555\n",
      "The 15349 th iteration gives loss of 0.22196302914202357\n",
      "The 15350 th iteration gives loss of 0.22195429322199733\n",
      "The 15351 th iteration gives loss of 0.22194555845139047\n",
      "The 15352 th iteration gives loss of 0.2219368248300328\n",
      "The 15353 th iteration gives loss of 0.22192809235774374\n",
      "The 15354 th iteration gives loss of 0.22191936103434826\n",
      "The 15355 th iteration gives loss of 0.2219106308596827\n",
      "The 15356 th iteration gives loss of 0.22190190183357042\n",
      "The 15357 th iteration gives loss of 0.22189317395582855\n",
      "The 15358 th iteration gives loss of 0.2218844472262893\n",
      "The 15359 th iteration gives loss of 0.22187572164477376\n",
      "The 15360 th iteration gives loss of 0.22186699721111142\n",
      "The 15361 th iteration gives loss of 0.22185827392512758\n",
      "The 15362 th iteration gives loss of 0.22184955178664806\n",
      "The 15363 th iteration gives loss of 0.22184083079549705\n",
      "The 15364 th iteration gives loss of 0.22183211095150474\n",
      "The 15365 th iteration gives loss of 0.22182339225449274\n",
      "The 15366 th iteration gives loss of 0.2218146747042902\n",
      "The 15367 th iteration gives loss of 0.22180595830071548\n",
      "The 15368 th iteration gives loss of 0.22179724304360154\n",
      "The 15369 th iteration gives loss of 0.22178852893277295\n",
      "The 15370 th iteration gives loss of 0.22177981596804983\n",
      "The 15371 th iteration gives loss of 0.22177110414927606\n",
      "The 15372 th iteration gives loss of 0.22176239347625476\n",
      "The 15373 th iteration gives loss of 0.22175368394882272\n",
      "The 15374 th iteration gives loss of 0.2217449755668138\n",
      "The 15375 th iteration gives loss of 0.22173626833004084\n",
      "The 15376 th iteration gives loss of 0.221727562238337\n",
      "The 15377 th iteration gives loss of 0.22171885729153448\n",
      "The 15378 th iteration gives loss of 0.22171015348944698\n",
      "The 15379 th iteration gives loss of 0.2217014508319029\n",
      "The 15380 th iteration gives loss of 0.22169274931873306\n",
      "The 15381 th iteration gives loss of 0.2216840489497538\n",
      "The 15382 th iteration gives loss of 0.22167534972480796\n",
      "The 15383 th iteration gives loss of 0.22166665164371172\n",
      "The 15384 th iteration gives loss of 0.22165795470629862\n",
      "The 15385 th iteration gives loss of 0.22164925891237988\n",
      "The 15386 th iteration gives loss of 0.22164056426179407\n",
      "The 15387 th iteration gives loss of 0.22163187075436797\n",
      "The 15388 th iteration gives loss of 0.22162317838992956\n",
      "The 15389 th iteration gives loss of 0.2216144871682959\n",
      "The 15390 th iteration gives loss of 0.22160579708929526\n",
      "The 15391 th iteration gives loss of 0.2215971081527645\n",
      "The 15392 th iteration gives loss of 0.22158842035851672\n",
      "The 15393 th iteration gives loss of 0.2215797337063949\n",
      "The 15394 th iteration gives loss of 0.22157104819620793\n",
      "The 15395 th iteration gives loss of 0.2215623638277898\n",
      "The 15396 th iteration gives loss of 0.2215536806009708\n",
      "The 15397 th iteration gives loss of 0.2215449985155665\n",
      "The 15398 th iteration gives loss of 0.22153631757142214\n",
      "The 15399 th iteration gives loss of 0.22152763776835463\n",
      "The 15400 th iteration gives loss of 0.2215189591061848\n",
      "The 15401 th iteration gives loss of 0.2215102815847439\n",
      "The 15402 th iteration gives loss of 0.221501605203857\n",
      "The 15403 th iteration gives loss of 0.22149292996335881\n",
      "The 15404 th iteration gives loss of 0.2214842558630596\n",
      "The 15405 th iteration gives loss of 0.22147558290280864\n",
      "The 15406 th iteration gives loss of 0.2214669110824201\n",
      "The 15407 th iteration gives loss of 0.22145824040171855\n",
      "The 15408 th iteration gives loss of 0.22144957086053643\n",
      "The 15409 th iteration gives loss of 0.2214409024587018\n",
      "The 15410 th iteration gives loss of 0.22143223519603555\n",
      "The 15411 th iteration gives loss of 0.22142356907236696\n",
      "The 15412 th iteration gives loss of 0.22141490408752762\n",
      "The 15413 th iteration gives loss of 0.2214062402413414\n",
      "The 15414 th iteration gives loss of 0.22139757753362382\n",
      "The 15415 th iteration gives loss of 0.2213889159642276\n",
      "The 15416 th iteration gives loss of 0.2213802555329514\n",
      "The 15417 th iteration gives loss of 0.22137159623964342\n",
      "The 15418 th iteration gives loss of 0.22136293808412644\n",
      "The 15419 th iteration gives loss of 0.22135428106622856\n",
      "The 15420 th iteration gives loss of 0.2213456251857685\n",
      "The 15421 th iteration gives loss of 0.221336970442573\n",
      "The 15422 th iteration gives loss of 0.22132831683648418\n",
      "The 15423 th iteration gives loss of 0.2213196643673156\n",
      "The 15424 th iteration gives loss of 0.22131101303489814\n",
      "The 15425 th iteration gives loss of 0.2213023628390585\n",
      "The 15426 th iteration gives loss of 0.22129371377963766\n",
      "The 15427 th iteration gives loss of 0.22128506585644353\n",
      "The 15428 th iteration gives loss of 0.22127641906930712\n",
      "The 15429 th iteration gives loss of 0.221267773418063\n",
      "The 15430 th iteration gives loss of 0.2212591289025436\n",
      "The 15431 th iteration gives loss of 0.2212504855225608\n",
      "The 15432 th iteration gives loss of 0.22124184327795438\n",
      "The 15433 th iteration gives loss of 0.22123320216853956\n",
      "The 15434 th iteration gives loss of 0.22122456219415018\n",
      "The 15435 th iteration gives loss of 0.22121592335462528\n",
      "The 15436 th iteration gives loss of 0.221207285649766\n",
      "The 15437 th iteration gives loss of 0.22119864907943182\n",
      "The 15438 th iteration gives loss of 0.22119001364343602\n",
      "The 15439 th iteration gives loss of 0.22118137934159685\n",
      "The 15440 th iteration gives loss of 0.22117274617375268\n",
      "The 15441 th iteration gives loss of 0.22116411413973697\n",
      "The 15442 th iteration gives loss of 0.22115548323936457\n",
      "The 15443 th iteration gives loss of 0.2211468534724697\n",
      "The 15444 th iteration gives loss of 0.22113822483887607\n",
      "The 15445 th iteration gives loss of 0.22112959733841495\n",
      "The 15446 th iteration gives loss of 0.22112097097091138\n",
      "The 15447 th iteration gives loss of 0.22111234573620614\n",
      "The 15448 th iteration gives loss of 0.2211037216341006\n",
      "The 15449 th iteration gives loss of 0.2210950986644567\n",
      "The 15450 th iteration gives loss of 0.22108647682707844\n",
      "The 15451 th iteration gives loss of 0.2210778561217902\n",
      "The 15452 th iteration gives loss of 0.22106923654843882\n",
      "The 15453 th iteration gives loss of 0.22106061810683683\n",
      "The 15454 th iteration gives loss of 0.22105200079682052\n",
      "The 15455 th iteration gives loss of 0.22104338461822615\n",
      "The 15456 th iteration gives loss of 0.22103476957086277\n",
      "The 15457 th iteration gives loss of 0.22102615565457673\n",
      "The 15458 th iteration gives loss of 0.22101754286917402\n",
      "The 15459 th iteration gives loss of 0.22100893121450926\n",
      "The 15460 th iteration gives loss of 0.22100032069038522\n",
      "The 15461 th iteration gives loss of 0.2209917112966477\n",
      "The 15462 th iteration gives loss of 0.22098310303312457\n",
      "The 15463 th iteration gives loss of 0.2209744958996354\n",
      "The 15464 th iteration gives loss of 0.22096588989601074\n",
      "The 15465 th iteration gives loss of 0.22095728502207718\n",
      "The 15466 th iteration gives loss of 0.22094868127767803\n",
      "The 15467 th iteration gives loss of 0.22094007866261905\n",
      "The 15468 th iteration gives loss of 0.22093147717674627\n",
      "The 15469 th iteration gives loss of 0.22092287681987885\n",
      "The 15470 th iteration gives loss of 0.22091427759184373\n",
      "The 15471 th iteration gives loss of 0.22090567949248047\n",
      "The 15472 th iteration gives loss of 0.22089708252160548\n",
      "The 15473 th iteration gives loss of 0.22088848667906533\n",
      "The 15474 th iteration gives loss of 0.2208798919646684\n",
      "The 15475 th iteration gives loss of 0.22087129837824446\n",
      "The 15476 th iteration gives loss of 0.22086270591963578\n",
      "The 15477 th iteration gives loss of 0.22085411458865337\n",
      "The 15478 th iteration gives loss of 0.22084552438514632\n",
      "The 15479 th iteration gives loss of 0.2208369353089359\n",
      "The 15480 th iteration gives loss of 0.22082834735983983\n",
      "The 15481 th iteration gives loss of 0.2208197605376954\n",
      "The 15482 th iteration gives loss of 0.22081117484233945\n",
      "The 15483 th iteration gives loss of 0.22080259027359045\n",
      "The 15484 th iteration gives loss of 0.2207940068312773\n",
      "The 15485 th iteration gives loss of 0.22078542451522615\n",
      "The 15486 th iteration gives loss of 0.22077684332527503\n",
      "The 15487 th iteration gives loss of 0.22076826326124893\n",
      "The 15488 th iteration gives loss of 0.22075968432298454\n",
      "The 15489 th iteration gives loss of 0.22075110651029362\n",
      "The 15490 th iteration gives loss of 0.22074252982301434\n",
      "The 15491 th iteration gives loss of 0.2207339542609804\n",
      "The 15492 th iteration gives loss of 0.2207253798240073\n",
      "The 15493 th iteration gives loss of 0.220716806511936\n",
      "The 15494 th iteration gives loss of 0.22070823432458597\n",
      "The 15495 th iteration gives loss of 0.22069966326181065\n",
      "The 15496 th iteration gives loss of 0.22069109332341036\n",
      "The 15497 th iteration gives loss of 0.22068252450922893\n",
      "The 15498 th iteration gives loss of 0.22067395681909407\n",
      "The 15499 th iteration gives loss of 0.22066539025282295\n",
      "The 15500 th iteration gives loss of 0.22065682481025514\n",
      "The 15501 th iteration gives loss of 0.22064826049122033\n",
      "The 15502 th iteration gives loss of 0.220639697295544\n",
      "The 15503 th iteration gives loss of 0.22063113522306768\n",
      "The 15504 th iteration gives loss of 0.220622574273614\n",
      "The 15505 th iteration gives loss of 0.22061401444700046\n",
      "The 15506 th iteration gives loss of 0.22060545574306706\n",
      "The 15507 th iteration gives loss of 0.22059689816163616\n",
      "The 15508 th iteration gives loss of 0.22058834170254543\n",
      "The 15509 th iteration gives loss of 0.22057978636562148\n",
      "The 15510 th iteration gives loss of 0.2205712321506978\n",
      "The 15511 th iteration gives loss of 0.22056267905759044\n",
      "The 15512 th iteration gives loss of 0.22055412708614439\n",
      "The 15513 th iteration gives loss of 0.2205455762361904\n",
      "The 15514 th iteration gives loss of 0.2205370265075353\n",
      "The 15515 th iteration gives loss of 0.2205284779000352\n",
      "The 15516 th iteration gives loss of 0.22051993041350593\n",
      "The 15517 th iteration gives loss of 0.22051138404777654\n",
      "The 15518 th iteration gives loss of 0.22050283880268148\n",
      "The 15519 th iteration gives loss of 0.22049429467804646\n",
      "The 15520 th iteration gives loss of 0.22048575167370466\n",
      "The 15521 th iteration gives loss of 0.22047720978949092\n",
      "The 15522 th iteration gives loss of 0.22046866902521842\n",
      "The 15523 th iteration gives loss of 0.2204601293807277\n",
      "The 15524 th iteration gives loss of 0.22045159085586277\n",
      "The 15525 th iteration gives loss of 0.22044305345042883\n",
      "The 15526 th iteration gives loss of 0.2204345171642652\n",
      "The 15527 th iteration gives loss of 0.22042598199720687\n",
      "The 15528 th iteration gives loss of 0.2204174479490726\n",
      "The 15529 th iteration gives loss of 0.22040891501969553\n",
      "The 15530 th iteration gives loss of 0.22040038320890715\n",
      "The 15531 th iteration gives loss of 0.22039185251655263\n",
      "The 15532 th iteration gives loss of 0.22038332294244758\n",
      "The 15533 th iteration gives loss of 0.22037479448642194\n",
      "The 15534 th iteration gives loss of 0.2203662671482999\n",
      "The 15535 th iteration gives loss of 0.22035774092792568\n",
      "The 15536 th iteration gives loss of 0.2203492158251168\n",
      "The 15537 th iteration gives loss of 0.22034069183970745\n",
      "The 15538 th iteration gives loss of 0.22033216897152985\n",
      "The 15539 th iteration gives loss of 0.2203236472204145\n",
      "The 15540 th iteration gives loss of 0.2203151265861916\n",
      "The 15541 th iteration gives loss of 0.2203066070686923\n",
      "The 15542 th iteration gives loss of 0.22029808866774722\n",
      "The 15543 th iteration gives loss of 0.22028957138317654\n",
      "The 15544 th iteration gives loss of 0.2202810552148188\n",
      "The 15545 th iteration gives loss of 0.2202725401625137\n",
      "The 15546 th iteration gives loss of 0.22026402622606955\n",
      "The 15547 th iteration gives loss of 0.22025551340533484\n",
      "The 15548 th iteration gives loss of 0.22024700170013667\n",
      "The 15549 th iteration gives loss of 0.22023849111029845\n",
      "The 15550 th iteration gives loss of 0.2202299816356538\n",
      "The 15551 th iteration gives loss of 0.22022147327603867\n",
      "The 15552 th iteration gives loss of 0.22021296603126864\n",
      "The 15553 th iteration gives loss of 0.22020445990118898\n",
      "The 15554 th iteration gives loss of 0.2201959548856357\n",
      "The 15555 th iteration gives loss of 0.22018745098442216\n",
      "The 15556 th iteration gives loss of 0.22017894819738892\n",
      "The 15557 th iteration gives loss of 0.22017044652436152\n",
      "The 15558 th iteration gives loss of 0.22016194596517188\n",
      "The 15559 th iteration gives loss of 0.22015344651965865\n",
      "The 15560 th iteration gives loss of 0.22014494818764244\n",
      "The 15561 th iteration gives loss of 0.22013645096895257\n",
      "The 15562 th iteration gives loss of 0.2201279548634307\n",
      "The 15563 th iteration gives loss of 0.22011945987089776\n",
      "The 15564 th iteration gives loss of 0.22011096599119304\n",
      "The 15565 th iteration gives loss of 0.22010247322414664\n",
      "The 15566 th iteration gives loss of 0.22009398156957408\n",
      "The 15567 th iteration gives loss of 0.22008549102732483\n",
      "The 15568 th iteration gives loss of 0.22007700159722005\n",
      "The 15569 th iteration gives loss of 0.2200685132790893\n",
      "The 15570 th iteration gives loss of 0.22006002607276393\n",
      "The 15571 th iteration gives loss of 0.2200515399780934\n",
      "The 15572 th iteration gives loss of 0.2200430549948818\n",
      "The 15573 th iteration gives loss of 0.22003457112297606\n",
      "The 15574 th iteration gives loss of 0.22002608836220253\n",
      "The 15575 th iteration gives loss of 0.2200176067123872\n",
      "The 15576 th iteration gives loss of 0.22000912617337237\n",
      "The 15577 th iteration gives loss of 0.2200006467449758\n",
      "The 15578 th iteration gives loss of 0.219992168427041\n",
      "The 15579 th iteration gives loss of 0.21998369121940212\n",
      "The 15580 th iteration gives loss of 0.2199752151218727\n",
      "The 15581 th iteration gives loss of 0.21996674013429474\n",
      "The 15582 th iteration gives loss of 0.219958266256509\n",
      "The 15583 th iteration gives loss of 0.2199497934883316\n",
      "The 15584 th iteration gives loss of 0.21994132182959117\n",
      "The 15585 th iteration gives loss of 0.21993285128012985\n",
      "The 15586 th iteration gives loss of 0.21992438183978238\n",
      "The 15587 th iteration gives loss of 0.2199159135083612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 15588 th iteration gives loss of 0.2199074462857208\n",
      "The 15589 th iteration gives loss of 0.21989898017167628\n",
      "The 15590 th iteration gives loss of 0.21989051516605793\n",
      "The 15591 th iteration gives loss of 0.21988205126870478\n",
      "The 15592 th iteration gives loss of 0.2198735884794489\n",
      "The 15593 th iteration gives loss of 0.21986512679813025\n",
      "The 15594 th iteration gives loss of 0.21985666622455768\n",
      "The 15595 th iteration gives loss of 0.21984820675857702\n",
      "The 15596 th iteration gives loss of 0.21983974840001858\n",
      "The 15597 th iteration gives loss of 0.21983129114871214\n",
      "The 15598 th iteration gives loss of 0.21982283500449765\n",
      "The 15599 th iteration gives loss of 0.21981437996719144\n",
      "The 15600 th iteration gives loss of 0.21980592603663393\n",
      "The 15601 th iteration gives loss of 0.21979747321266177\n",
      "The 15602 th iteration gives loss of 0.21978902149509774\n",
      "The 15603 th iteration gives loss of 0.21978057088377573\n",
      "The 15604 th iteration gives loss of 0.2197721213785192\n",
      "The 15605 th iteration gives loss of 0.21976367297917832\n",
      "The 15606 th iteration gives loss of 0.2197552256855759\n",
      "The 15607 th iteration gives loss of 0.21974677949754198\n",
      "The 15608 th iteration gives loss of 0.21973833441491117\n",
      "The 15609 th iteration gives loss of 0.21972989043751545\n",
      "The 15610 th iteration gives loss of 0.21972144756519396\n",
      "The 15611 th iteration gives loss of 0.2197130057977604\n",
      "The 15612 th iteration gives loss of 0.21970456513505748\n",
      "The 15613 th iteration gives loss of 0.21969612557691326\n",
      "The 15614 th iteration gives loss of 0.21968768712316583\n",
      "The 15615 th iteration gives loss of 0.21967924977364287\n",
      "The 15616 th iteration gives loss of 0.21967081352817638\n",
      "The 15617 th iteration gives loss of 0.21966237838660096\n",
      "The 15618 th iteration gives loss of 0.21965394434874297\n",
      "The 15619 th iteration gives loss of 0.2196455114144491\n",
      "The 15620 th iteration gives loss of 0.21963707958353676\n",
      "The 15621 th iteration gives loss of 0.21962864885584238\n",
      "The 15622 th iteration gives loss of 0.2196202192311936\n",
      "The 15623 th iteration gives loss of 0.2196117907094327\n",
      "The 15624 th iteration gives loss of 0.21960336329038124\n",
      "The 15625 th iteration gives loss of 0.21959493697389054\n",
      "The 15626 th iteration gives loss of 0.2195865117597718\n",
      "The 15627 th iteration gives loss of 0.219578087647866\n",
      "The 15628 th iteration gives loss of 0.2195696646380018\n",
      "The 15629 th iteration gives loss of 0.21956124273001165\n",
      "The 15630 th iteration gives loss of 0.21955282192373798\n",
      "The 15631 th iteration gives loss of 0.21954440221899765\n",
      "The 15632 th iteration gives loss of 0.21953598361563081\n",
      "The 15633 th iteration gives loss of 0.21952756611347338\n",
      "The 15634 th iteration gives loss of 0.2195191497123525\n",
      "The 15635 th iteration gives loss of 0.21951073441210883\n",
      "The 15636 th iteration gives loss of 0.21950232021256486\n",
      "The 15637 th iteration gives loss of 0.21949390711355846\n",
      "The 15638 th iteration gives loss of 0.21948549511491983\n",
      "The 15639 th iteration gives loss of 0.21947708421648454\n",
      "The 15640 th iteration gives loss of 0.21946867441808227\n",
      "The 15641 th iteration gives loss of 0.21946026571954502\n",
      "The 15642 th iteration gives loss of 0.21945185812071433\n",
      "The 15643 th iteration gives loss of 0.21944345162140555\n",
      "The 15644 th iteration gives loss of 0.21943504622147036\n",
      "The 15645 th iteration gives loss of 0.2194266419207294\n",
      "The 15646 th iteration gives loss of 0.21941823871901942\n",
      "The 15647 th iteration gives loss of 0.21940983661617183\n",
      "The 15648 th iteration gives loss of 0.2194014356120152\n",
      "The 15649 th iteration gives loss of 0.21939303570638397\n",
      "The 15650 th iteration gives loss of 0.2193846368991269\n",
      "The 15651 th iteration gives loss of 0.2193762391900597\n",
      "The 15652 th iteration gives loss of 0.21936784257902783\n",
      "The 15653 th iteration gives loss of 0.2193594470658553\n",
      "The 15654 th iteration gives loss of 0.2193510526503635\n",
      "The 15655 th iteration gives loss of 0.2193426593324044\n",
      "The 15656 th iteration gives loss of 0.21933426711179857\n",
      "The 15657 th iteration gives loss of 0.21932587598839412\n",
      "The 15658 th iteration gives loss of 0.21931748596201345\n",
      "The 15659 th iteration gives loss of 0.21930909703249307\n",
      "The 15660 th iteration gives loss of 0.21930070919966727\n",
      "The 15661 th iteration gives loss of 0.21929232246336156\n",
      "The 15662 th iteration gives loss of 0.219283936823411\n",
      "The 15663 th iteration gives loss of 0.21927555227966042\n",
      "The 15664 th iteration gives loss of 0.21926716883192618\n",
      "The 15665 th iteration gives loss of 0.21925878648004649\n",
      "The 15666 th iteration gives loss of 0.21925040522386469\n",
      "The 15667 th iteration gives loss of 0.21924202506320867\n",
      "The 15668 th iteration gives loss of 0.21923364599790834\n",
      "The 15669 th iteration gives loss of 0.2192252680277971\n",
      "The 15670 th iteration gives loss of 0.2192168911527118\n",
      "The 15671 th iteration gives loss of 0.21920851537248026\n",
      "The 15672 th iteration gives loss of 0.21920014068694318\n",
      "The 15673 th iteration gives loss of 0.21919176709592791\n",
      "The 15674 th iteration gives loss of 0.21918339459927244\n",
      "The 15675 th iteration gives loss of 0.2191750231968077\n",
      "The 15676 th iteration gives loss of 0.2191666528883666\n",
      "The 15677 th iteration gives loss of 0.2191582836737802\n",
      "The 15678 th iteration gives loss of 0.21914991555288835\n",
      "The 15679 th iteration gives loss of 0.21914154852552672\n",
      "The 15680 th iteration gives loss of 0.21913318259152545\n",
      "The 15681 th iteration gives loss of 0.21912481775071152\n",
      "The 15682 th iteration gives loss of 0.21911645400292995\n",
      "The 15683 th iteration gives loss of 0.2191080913480034\n",
      "The 15684 th iteration gives loss of 0.21909972978576361\n",
      "The 15685 th iteration gives loss of 0.21909136931606274\n",
      "The 15686 th iteration gives loss of 0.21908300993871163\n",
      "The 15687 th iteration gives loss of 0.21907465165356843\n",
      "The 15688 th iteration gives loss of 0.21906629446044462\n",
      "The 15689 th iteration gives loss of 0.21905793835918266\n",
      "The 15690 th iteration gives loss of 0.2190495833496209\n",
      "The 15691 th iteration gives loss of 0.2190412294315849\n",
      "The 15692 th iteration gives loss of 0.21903287660491771\n",
      "The 15693 th iteration gives loss of 0.21902452486945398\n",
      "The 15694 th iteration gives loss of 0.2190161742250119\n",
      "The 15695 th iteration gives loss of 0.2190078246714418\n",
      "The 15696 th iteration gives loss of 0.21899947620856552\n",
      "The 15697 th iteration gives loss of 0.2189911288362272\n",
      "The 15698 th iteration gives loss of 0.21898278255426026\n",
      "The 15699 th iteration gives loss of 0.21897443736249272\n",
      "The 15700 th iteration gives loss of 0.21896609326075714\n",
      "The 15701 th iteration gives loss of 0.2189577502488874\n",
      "The 15702 th iteration gives loss of 0.21894940832671966\n",
      "The 15703 th iteration gives loss of 0.21894106749410777\n",
      "The 15704 th iteration gives loss of 0.2189327277508576\n",
      "The 15705 th iteration gives loss of 0.21892438909681639\n",
      "The 15706 th iteration gives loss of 0.21891605153181826\n",
      "The 15707 th iteration gives loss of 0.21890771505569162\n",
      "The 15708 th iteration gives loss of 0.21889937966827822\n",
      "The 15709 th iteration gives loss of 0.21889104536940013\n",
      "The 15710 th iteration gives loss of 0.218882712158903\n",
      "The 15711 th iteration gives loss of 0.2188743800366254\n",
      "The 15712 th iteration gives loss of 0.2188660490023937\n",
      "The 15713 th iteration gives loss of 0.21885771905602802\n",
      "The 15714 th iteration gives loss of 0.21884939019738836\n",
      "The 15715 th iteration gives loss of 0.2188410624263017\n",
      "The 15716 th iteration gives loss of 0.218832735742597\n",
      "The 15717 th iteration gives loss of 0.21882441014610815\n",
      "The 15718 th iteration gives loss of 0.21881608563668026\n",
      "The 15719 th iteration gives loss of 0.2188077622141287\n",
      "The 15720 th iteration gives loss of 0.21879943987830414\n",
      "The 15721 th iteration gives loss of 0.2187911186290432\n",
      "The 15722 th iteration gives loss of 0.21878279846616006\n",
      "The 15723 th iteration gives loss of 0.21877447938951625\n",
      "The 15724 th iteration gives loss of 0.21876616139892877\n",
      "The 15725 th iteration gives loss of 0.21875784449423652\n",
      "The 15726 th iteration gives loss of 0.21874952867526684\n",
      "The 15727 th iteration gives loss of 0.21874121394187404\n",
      "The 15728 th iteration gives loss of 0.21873290029387393\n",
      "The 15729 th iteration gives loss of 0.21872458773111583\n",
      "The 15730 th iteration gives loss of 0.2187162762534225\n",
      "The 15731 th iteration gives loss of 0.2187079658606384\n",
      "The 15732 th iteration gives loss of 0.21869965655258725\n",
      "The 15733 th iteration gives loss of 0.21869134832911358\n",
      "The 15734 th iteration gives loss of 0.21868304119004148\n",
      "The 15735 th iteration gives loss of 0.21867473513521682\n",
      "The 15736 th iteration gives loss of 0.2186664301644658\n",
      "The 15737 th iteration gives loss of 0.21865812627763787\n",
      "The 15738 th iteration gives loss of 0.21864982347454728\n",
      "The 15739 th iteration gives loss of 0.21864152175504406\n",
      "The 15740 th iteration gives loss of 0.2186332211189669\n",
      "The 15741 th iteration gives loss of 0.2186249215661386\n",
      "The 15742 th iteration gives loss of 0.2186166230963962\n",
      "The 15743 th iteration gives loss of 0.2186083257095793\n",
      "The 15744 th iteration gives loss of 0.21860002940551942\n",
      "The 15745 th iteration gives loss of 0.2185917341840526\n",
      "The 15746 th iteration gives loss of 0.21858344004502075\n",
      "The 15747 th iteration gives loss of 0.2185751469882516\n",
      "The 15748 th iteration gives loss of 0.2185668550135827\n",
      "The 15749 th iteration gives loss of 0.218558564120847\n",
      "The 15750 th iteration gives loss of 0.21855027430987056\n",
      "The 15751 th iteration gives loss of 0.21854198558051446\n",
      "The 15752 th iteration gives loss of 0.21853369793260183\n",
      "The 15753 th iteration gives loss of 0.2185254113659555\n",
      "The 15754 th iteration gives loss of 0.21851712588042285\n",
      "The 15755 th iteration gives loss of 0.21850884147583102\n",
      "The 15756 th iteration gives loss of 0.21850055815203023\n",
      "The 15757 th iteration gives loss of 0.21849227590883974\n",
      "The 15758 th iteration gives loss of 0.21848399474610397\n",
      "The 15759 th iteration gives loss of 0.2184757146636625\n",
      "The 15760 th iteration gives loss of 0.2184674356613395\n",
      "The 15761 th iteration gives loss of 0.2184591577389828\n",
      "The 15762 th iteration gives loss of 0.21845088089641757\n",
      "The 15763 th iteration gives loss of 0.21844260513348074\n",
      "The 15764 th iteration gives loss of 0.21843433045001093\n",
      "The 15765 th iteration gives loss of 0.21842605684584893\n",
      "The 15766 th iteration gives loss of 0.21841778432081269\n",
      "The 15767 th iteration gives loss of 0.21840951287475976\n",
      "The 15768 th iteration gives loss of 0.21840124250751433\n",
      "The 15769 th iteration gives loss of 0.21839297321890905\n",
      "The 15770 th iteration gives loss of 0.2183847050087794\n",
      "The 15771 th iteration gives loss of 0.2183764378769757\n",
      "The 15772 th iteration gives loss of 0.21836817182332907\n",
      "The 15773 th iteration gives loss of 0.21835990684766504\n",
      "The 15774 th iteration gives loss of 0.21835164294981976\n",
      "The 15775 th iteration gives loss of 0.21834338012963272\n",
      "The 15776 th iteration gives loss of 0.2183351183869431\n",
      "The 15777 th iteration gives loss of 0.21832685772158722\n",
      "The 15778 th iteration gives loss of 0.21831859813339882\n",
      "The 15779 th iteration gives loss of 0.2183103396222202\n",
      "The 15780 th iteration gives loss of 0.21830208218786762\n",
      "The 15781 th iteration gives loss of 0.2182938258301941\n",
      "The 15782 th iteration gives loss of 0.2182855705490355\n",
      "The 15783 th iteration gives loss of 0.21827731634422076\n",
      "The 15784 th iteration gives loss of 0.21826906321559056\n",
      "The 15785 th iteration gives loss of 0.21826081116298082\n",
      "The 15786 th iteration gives loss of 0.21825256018622502\n",
      "The 15787 th iteration gives loss of 0.21824431028516578\n",
      "The 15788 th iteration gives loss of 0.21823606145962818\n",
      "The 15789 th iteration gives loss of 0.21822781370945946\n",
      "The 15790 th iteration gives loss of 0.21821956703448392\n",
      "The 15791 th iteration gives loss of 0.21821132143455146\n",
      "The 15792 th iteration gives loss of 0.21820307690948101\n",
      "The 15793 th iteration gives loss of 0.21819483345913884\n",
      "The 15794 th iteration gives loss of 0.21818659108332628\n",
      "The 15795 th iteration gives loss of 0.21817834978190911\n",
      "The 15796 th iteration gives loss of 0.2181701095546964\n",
      "The 15797 th iteration gives loss of 0.21816187040153726\n",
      "The 15798 th iteration gives loss of 0.2181536323222874\n",
      "The 15799 th iteration gives loss of 0.21814539531675312\n",
      "The 15800 th iteration gives loss of 0.2181371593847818\n",
      "The 15801 th iteration gives loss of 0.21812892452621327\n",
      "The 15802 th iteration gives loss of 0.21812069074087395\n",
      "The 15803 th iteration gives loss of 0.21811245802861629\n",
      "The 15804 th iteration gives loss of 0.218104226389268\n",
      "The 15805 th iteration gives loss of 0.21809599582266628\n",
      "The 15806 th iteration gives loss of 0.21808776632863983\n",
      "The 15807 th iteration gives loss of 0.21807953790703755\n",
      "The 15808 th iteration gives loss of 0.21807131055770046\n",
      "The 15809 th iteration gives loss of 0.21806308428044505\n",
      "The 15810 th iteration gives loss of 0.21805485907511335\n",
      "The 15811 th iteration gives loss of 0.21804663494156445\n",
      "The 15812 th iteration gives loss of 0.21803841187961312\n",
      "The 15813 th iteration gives loss of 0.21803018988910056\n",
      "The 15814 th iteration gives loss of 0.2180219689698604\n",
      "The 15815 th iteration gives loss of 0.21801374912174368\n",
      "The 15816 th iteration gives loss of 0.21800553034457035\n",
      "The 15817 th iteration gives loss of 0.21799731263818248\n",
      "The 15818 th iteration gives loss of 0.2179890960024266\n",
      "The 15819 th iteration gives loss of 0.21798088043712735\n",
      "The 15820 th iteration gives loss of 0.21797266594212159\n",
      "The 15821 th iteration gives loss of 0.2179644525172549\n",
      "The 15822 th iteration gives loss of 0.2179562401623609\n",
      "The 15823 th iteration gives loss of 0.21794802887727383\n",
      "The 15824 th iteration gives loss of 0.21793981866184128\n",
      "The 15825 th iteration gives loss of 0.21793160951588306\n",
      "The 15826 th iteration gives loss of 0.21792340143924444\n",
      "The 15827 th iteration gives loss of 0.21791519443175755\n",
      "The 15828 th iteration gives loss of 0.21790698849327375\n",
      "The 15829 th iteration gives loss of 0.21789878362361775\n",
      "The 15830 th iteration gives loss of 0.21789057982262944\n",
      "The 15831 th iteration gives loss of 0.21788237709015545\n",
      "The 15832 th iteration gives loss of 0.2178741754260135\n",
      "The 15833 th iteration gives loss of 0.2178659748300538\n",
      "The 15834 th iteration gives loss of 0.21785777530211378\n",
      "The 15835 th iteration gives loss of 0.2178495768420278\n",
      "The 15836 th iteration gives loss of 0.21784137944963858\n",
      "The 15837 th iteration gives loss of 0.21783318312477007\n",
      "The 15838 th iteration gives loss of 0.21782498786726964\n",
      "The 15839 th iteration gives loss of 0.21781679367698004\n",
      "The 15840 th iteration gives loss of 0.21780860055372675\n",
      "The 15841 th iteration gives loss of 0.21780040849735316\n",
      "The 15842 th iteration gives loss of 0.21779221750769534\n",
      "The 15843 th iteration gives loss of 0.21778402758459514\n",
      "The 15844 th iteration gives loss of 0.21777583872788323\n",
      "The 15845 th iteration gives loss of 0.21776765093740416\n",
      "The 15846 th iteration gives loss of 0.21775946421298498\n",
      "The 15847 th iteration gives loss of 0.21775127855447385\n",
      "The 15848 th iteration gives loss of 0.2177430939617049\n",
      "The 15849 th iteration gives loss of 0.21773491043451013\n",
      "The 15850 th iteration gives loss of 0.21772672797273862\n",
      "The 15851 th iteration gives loss of 0.21771854657621598\n",
      "The 15852 th iteration gives loss of 0.21771036624479143\n",
      "The 15853 th iteration gives loss of 0.2177021869782915\n",
      "The 15854 th iteration gives loss of 0.2176940087765568\n",
      "The 15855 th iteration gives loss of 0.21768583163943586\n",
      "The 15856 th iteration gives loss of 0.2176776555667562\n",
      "The 15857 th iteration gives loss of 0.21766948055835295\n",
      "The 15858 th iteration gives loss of 0.21766130661406866\n",
      "The 15859 th iteration gives loss of 0.217653133733743\n",
      "The 15860 th iteration gives loss of 0.21764496191721475\n",
      "The 15861 th iteration gives loss of 0.21763679116430934\n",
      "The 15862 th iteration gives loss of 0.21762862147488635\n",
      "The 15863 th iteration gives loss of 0.21762045284876458\n",
      "The 15864 th iteration gives loss of 0.21761228528579143\n",
      "The 15865 th iteration gives loss of 0.2176041187858027\n",
      "The 15866 th iteration gives loss of 0.21759595334863885\n",
      "The 15867 th iteration gives loss of 0.21758778897412576\n",
      "The 15868 th iteration gives loss of 0.21757962566211914\n",
      "The 15869 th iteration gives loss of 0.21757146341244316\n",
      "The 15870 th iteration gives loss of 0.21756330222494866\n",
      "The 15871 th iteration gives loss of 0.21755514209946603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 15872 th iteration gives loss of 0.21754698303582679\n",
      "The 15873 th iteration gives loss of 0.2175388250338793\n",
      "The 15874 th iteration gives loss of 0.21753066809347063\n",
      "The 15875 th iteration gives loss of 0.2175225122144154\n",
      "The 15876 th iteration gives loss of 0.2175143573965598\n",
      "The 15877 th iteration gives loss of 0.21750620363975517\n",
      "The 15878 th iteration gives loss of 0.21749805094382849\n",
      "The 15879 th iteration gives loss of 0.21748989930862517\n",
      "The 15880 th iteration gives loss of 0.2174817487339768\n",
      "The 15881 th iteration gives loss of 0.2174735992197201\n",
      "The 15882 th iteration gives loss of 0.21746545076569415\n",
      "The 15883 th iteration gives loss of 0.2174573033717453\n",
      "The 15884 th iteration gives loss of 0.21744915703770484\n",
      "The 15885 th iteration gives loss of 0.21744101176341463\n",
      "The 15886 th iteration gives loss of 0.2174328675487049\n",
      "The 15887 th iteration gives loss of 0.21742472439342964\n",
      "The 15888 th iteration gives loss of 0.21741658229742022\n",
      "The 15889 th iteration gives loss of 0.2174084412605073\n",
      "The 15890 th iteration gives loss of 0.21740030128253884\n",
      "The 15891 th iteration gives loss of 0.21739216236335757\n",
      "The 15892 th iteration gives loss of 0.21738402450279054\n",
      "The 15893 th iteration gives loss of 0.2173758877006787\n",
      "The 15894 th iteration gives loss of 0.21736775195686225\n",
      "The 15895 th iteration gives loss of 0.21735961727117073\n",
      "The 15896 th iteration gives loss of 0.2173514836434647\n",
      "The 15897 th iteration gives loss of 0.21734335107357342\n",
      "The 15898 th iteration gives loss of 0.2173352195613323\n",
      "The 15899 th iteration gives loss of 0.21732708910657828\n",
      "The 15900 th iteration gives loss of 0.21731895970914966\n",
      "The 15901 th iteration gives loss of 0.21731083136889398\n",
      "The 15902 th iteration gives loss of 0.21730270408564278\n",
      "The 15903 th iteration gives loss of 0.21729457785923548\n",
      "The 15904 th iteration gives loss of 0.21728645268951488\n",
      "The 15905 th iteration gives loss of 0.21727832857632168\n",
      "The 15906 th iteration gives loss of 0.21727020551948112\n",
      "The 15907 th iteration gives loss of 0.21726208351885448\n",
      "The 15908 th iteration gives loss of 0.21725396257425536\n",
      "The 15909 th iteration gives loss of 0.21724584268553876\n",
      "The 15910 th iteration gives loss of 0.2172377238525432\n",
      "The 15911 th iteration gives loss of 0.21722960607509512\n",
      "The 15912 th iteration gives loss of 0.21722148935304858\n",
      "The 15913 th iteration gives loss of 0.2172133736862515\n",
      "The 15914 th iteration gives loss of 0.21720525907451377\n",
      "The 15915 th iteration gives loss of 0.2171971455176876\n",
      "The 15916 th iteration gives loss of 0.2171890330156206\n",
      "The 15917 th iteration gives loss of 0.21718092156814964\n",
      "The 15918 th iteration gives loss of 0.2171728111751072\n",
      "The 15919 th iteration gives loss of 0.21716470183632938\n",
      "The 15920 th iteration gives loss of 0.21715659355167352\n",
      "The 15921 th iteration gives loss of 0.21714848632095862\n",
      "The 15922 th iteration gives loss of 0.21714038014403308\n",
      "The 15923 th iteration gives loss of 0.21713227502073282\n",
      "The 15924 th iteration gives loss of 0.21712417095090072\n",
      "The 15925 th iteration gives loss of 0.21711606793438062\n",
      "The 15926 th iteration gives loss of 0.21710796597100246\n",
      "The 15927 th iteration gives loss of 0.21709986506061327\n",
      "The 15928 th iteration gives loss of 0.2170917652030404\n",
      "The 15929 th iteration gives loss of 0.2170836663981399\n",
      "The 15930 th iteration gives loss of 0.2170755686457395\n",
      "The 15931 th iteration gives loss of 0.21706747194568393\n",
      "The 15932 th iteration gives loss of 0.2170593762978055\n",
      "The 15933 th iteration gives loss of 0.21705128170195717\n",
      "The 15934 th iteration gives loss of 0.2170431881579709\n",
      "The 15935 th iteration gives loss of 0.2170350956656828\n",
      "The 15936 th iteration gives loss of 0.2170270042249386\n",
      "The 15937 th iteration gives loss of 0.21701891383558633\n",
      "The 15938 th iteration gives loss of 0.21701082449744086\n",
      "The 15939 th iteration gives loss of 0.2170027362103604\n",
      "The 15940 th iteration gives loss of 0.21699464897418166\n",
      "The 15941 th iteration gives loss of 0.2169865627887398\n",
      "The 15942 th iteration gives loss of 0.21697847765388006\n",
      "The 15943 th iteration gives loss of 0.216970393569435\n",
      "The 15944 th iteration gives loss of 0.21696231053525594\n",
      "The 15945 th iteration gives loss of 0.2169542285511761\n",
      "The 15946 th iteration gives loss of 0.21694614761703035\n",
      "The 15947 th iteration gives loss of 0.21693806773267135\n",
      "The 15948 th iteration gives loss of 0.2169299888979298\n",
      "The 15949 th iteration gives loss of 0.2169219111126407\n",
      "The 15950 th iteration gives loss of 0.216913834376655\n",
      "The 15951 th iteration gives loss of 0.21690575868980913\n",
      "The 15952 th iteration gives loss of 0.21689768405194973\n",
      "The 15953 th iteration gives loss of 0.21688961046290223\n",
      "The 15954 th iteration gives loss of 0.21688153792251066\n",
      "The 15955 th iteration gives loss of 0.2168734664306215\n",
      "The 15956 th iteration gives loss of 0.21686539598706756\n",
      "The 15957 th iteration gives loss of 0.21685732659170365\n",
      "The 15958 th iteration gives loss of 0.21684925824435491\n",
      "The 15959 th iteration gives loss of 0.21684119094486862\n",
      "The 15960 th iteration gives loss of 0.21683312469307917\n",
      "The 15961 th iteration gives loss of 0.21682505948883116\n",
      "The 15962 th iteration gives loss of 0.21681699533196164\n",
      "The 15963 th iteration gives loss of 0.21680893222230793\n",
      "The 15964 th iteration gives loss of 0.2168008701597323\n",
      "The 15965 th iteration gives loss of 0.21679280914403729\n",
      "The 15966 th iteration gives loss of 0.21678474917510376\n",
      "The 15967 th iteration gives loss of 0.21677669025274085\n",
      "The 15968 th iteration gives loss of 0.21676863237680666\n",
      "The 15969 th iteration gives loss of 0.21676057554713835\n",
      "The 15970 th iteration gives loss of 0.2167525197635596\n",
      "The 15971 th iteration gives loss of 0.21674446502593253\n",
      "The 15972 th iteration gives loss of 0.21673641133409344\n",
      "The 15973 th iteration gives loss of 0.21672835868788373\n",
      "The 15974 th iteration gives loss of 0.21672030708713066\n",
      "The 15975 th iteration gives loss of 0.21671225653168905\n",
      "The 15976 th iteration gives loss of 0.21670420702139134\n",
      "The 15977 th iteration gives loss of 0.21669615855608423\n",
      "The 15978 th iteration gives loss of 0.21668811113560177\n",
      "The 15979 th iteration gives loss of 0.21668006475978716\n",
      "The 15980 th iteration gives loss of 0.216672019428482\n",
      "The 15981 th iteration gives loss of 0.21666397514152927\n",
      "The 15982 th iteration gives loss of 0.2166559318987658\n",
      "The 15983 th iteration gives loss of 0.2166478897000331\n",
      "The 15984 th iteration gives loss of 0.21663984854517185\n",
      "The 15985 th iteration gives loss of 0.2166318084340268\n",
      "The 15986 th iteration gives loss of 0.21662376936644875\n",
      "The 15987 th iteration gives loss of 0.2166157313422495\n",
      "The 15988 th iteration gives loss of 0.2166076943612888\n",
      "The 15989 th iteration gives loss of 0.21659965842339882\n",
      "The 15990 th iteration gives loss of 0.21659162352843384\n",
      "The 15991 th iteration gives loss of 0.21658358967622005\n",
      "The 15992 th iteration gives loss of 0.2165755568666092\n",
      "The 15993 th iteration gives loss of 0.21656752509944066\n",
      "The 15994 th iteration gives loss of 0.21655949437455543\n",
      "The 15995 th iteration gives loss of 0.2165514646917898\n",
      "The 15996 th iteration gives loss of 0.21654343605098594\n",
      "The 15997 th iteration gives loss of 0.216535408451994\n",
      "The 15998 th iteration gives loss of 0.21652738189463772\n",
      "The 15999 th iteration gives loss of 0.21651935637877978\n",
      "The 16000 th iteration gives loss of 0.21651133190424576\n",
      "The 16001 th iteration gives loss of 0.2165033084708766\n",
      "The 16002 th iteration gives loss of 0.21649528607851332\n",
      "The 16003 th iteration gives loss of 0.21648726472700713\n",
      "The 16004 th iteration gives loss of 0.2164792444161935\n",
      "The 16005 th iteration gives loss of 0.21647122514591488\n",
      "The 16006 th iteration gives loss of 0.2164632069160082\n",
      "The 16007 th iteration gives loss of 0.2164551897263232\n",
      "The 16008 th iteration gives loss of 0.21644717357670176\n",
      "The 16009 th iteration gives loss of 0.21643915846696105\n",
      "The 16010 th iteration gives loss of 0.21643114439696723\n",
      "The 16011 th iteration gives loss of 0.21642313136655938\n",
      "The 16012 th iteration gives loss of 0.21641511937556956\n",
      "The 16013 th iteration gives loss of 0.21640710842384858\n",
      "The 16014 th iteration gives loss of 0.2163990985112321\n",
      "The 16015 th iteration gives loss of 0.21639108963756953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 16016 th iteration gives loss of 0.21638308180269325\n",
      "The 16017 th iteration gives loss of 0.21637507500644565\n",
      "The 16018 th iteration gives loss of 0.21636706924867008\n",
      "The 16019 th iteration gives loss of 0.21635906452920872\n",
      "The 16020 th iteration gives loss of 0.2163510608479017\n",
      "The 16021 th iteration gives loss of 0.21634305820459226\n",
      "The 16022 th iteration gives loss of 0.21633505659912053\n",
      "The 16023 th iteration gives loss of 0.2163270560313351\n",
      "The 16024 th iteration gives loss of 0.2163190565010712\n",
      "The 16025 th iteration gives loss of 0.21631105800816833\n",
      "The 16026 th iteration gives loss of 0.21630306055246573\n",
      "The 16027 th iteration gives loss of 0.21629506413381475\n",
      "The 16028 th iteration gives loss of 0.21628706875205764\n",
      "The 16029 th iteration gives loss of 0.21627907440702188\n",
      "The 16030 th iteration gives loss of 0.2162710810985684\n",
      "The 16031 th iteration gives loss of 0.21626308882652545\n",
      "The 16032 th iteration gives loss of 0.21625509759074435\n",
      "The 16033 th iteration gives loss of 0.21624710739105427\n",
      "The 16034 th iteration gives loss of 0.21623911822731198\n",
      "The 16035 th iteration gives loss of 0.21623113009934264\n",
      "The 16036 th iteration gives loss of 0.21622314300700257\n",
      "The 16037 th iteration gives loss of 0.21621515695012591\n",
      "The 16038 th iteration gives loss of 0.2162071719285601\n",
      "The 16039 th iteration gives loss of 0.21619918794214757\n",
      "The 16040 th iteration gives loss of 0.21619120499072342\n",
      "The 16041 th iteration gives loss of 0.21618322307413218\n",
      "The 16042 th iteration gives loss of 0.21617524219222387\n",
      "The 16043 th iteration gives loss of 0.21616726234482808\n",
      "The 16044 th iteration gives loss of 0.21615928353179437\n",
      "The 16045 th iteration gives loss of 0.21615130575296435\n",
      "The 16046 th iteration gives loss of 0.21614332900817668\n",
      "The 16047 th iteration gives loss of 0.2161353532972826\n",
      "The 16048 th iteration gives loss of 0.21612737862011747\n",
      "The 16049 th iteration gives loss of 0.21611940497651858\n",
      "The 16050 th iteration gives loss of 0.21611143236633978\n",
      "The 16051 th iteration gives loss of 0.21610346078942394\n",
      "The 16052 th iteration gives loss of 0.21609549024559324\n",
      "The 16053 th iteration gives loss of 0.21608752073470422\n",
      "The 16054 th iteration gives loss of 0.21607955225660394\n",
      "The 16055 th iteration gives loss of 0.2160715848111354\n",
      "The 16056 th iteration gives loss of 0.21606361839813132\n",
      "The 16057 th iteration gives loss of 0.2160556530174401\n",
      "The 16058 th iteration gives loss of 0.21604768866889898\n",
      "The 16059 th iteration gives loss of 0.2160397253523523\n",
      "The 16060 th iteration gives loss of 0.21603176306764632\n",
      "The 16061 th iteration gives loss of 0.2160238018146226\n",
      "The 16062 th iteration gives loss of 0.21601584159312956\n",
      "The 16063 th iteration gives loss of 0.2160078824029982\n",
      "The 16064 th iteration gives loss of 0.2159999242440719\n",
      "The 16065 th iteration gives loss of 0.21599196711619556\n",
      "The 16066 th iteration gives loss of 0.21598401101920683\n",
      "The 16067 th iteration gives loss of 0.21597605595296435\n",
      "The 16068 th iteration gives loss of 0.2159681019173043\n",
      "The 16069 th iteration gives loss of 0.2159601489120611\n",
      "The 16070 th iteration gives loss of 0.215952196937084\n",
      "The 16071 th iteration gives loss of 0.21594424599221126\n",
      "The 16072 th iteration gives loss of 0.21593629607729867\n",
      "The 16073 th iteration gives loss of 0.21592834719217263\n",
      "The 16074 th iteration gives loss of 0.2159203993366837\n",
      "The 16075 th iteration gives loss of 0.21591245251068025\n",
      "The 16076 th iteration gives loss of 0.21590450671398403\n",
      "The 16077 th iteration gives loss of 0.21589656194645876\n",
      "The 16078 th iteration gives loss of 0.21588861820795144\n",
      "The 16079 th iteration gives loss of 0.2158806754982897\n",
      "The 16080 th iteration gives loss of 0.2158727338173174\n",
      "The 16081 th iteration gives loss of 0.2158647931648875\n",
      "The 16082 th iteration gives loss of 0.21585685354083625\n",
      "The 16083 th iteration gives loss of 0.21584891494500835\n",
      "The 16084 th iteration gives loss of 0.2158409773772368\n",
      "The 16085 th iteration gives loss of 0.21583304083737978\n",
      "The 16086 th iteration gives loss of 0.21582510532527668\n",
      "The 16087 th iteration gives loss of 0.21581717084077706\n",
      "The 16088 th iteration gives loss of 0.21580923738370805\n",
      "The 16089 th iteration gives loss of 0.21580130495392513\n",
      "The 16090 th iteration gives loss of 0.21579337355125836\n",
      "The 16091 th iteration gives loss of 0.21578544317556778\n",
      "The 16092 th iteration gives loss of 0.21577751382667915\n",
      "The 16093 th iteration gives loss of 0.21576958550444686\n",
      "The 16094 th iteration gives loss of 0.2157616582087189\n",
      "The 16095 th iteration gives loss of 0.2157537319393282\n",
      "The 16096 th iteration gives loss of 0.2157458066961266\n",
      "The 16097 th iteration gives loss of 0.21573788247895173\n",
      "The 16098 th iteration gives loss of 0.21572995928764493\n",
      "The 16099 th iteration gives loss of 0.21572203712205612\n",
      "The 16100 th iteration gives loss of 0.2157141159820323\n",
      "The 16101 th iteration gives loss of 0.21570619586739861\n",
      "The 16102 th iteration gives loss of 0.2156982767780186\n",
      "The 16103 th iteration gives loss of 0.21569035871371894\n",
      "The 16104 th iteration gives loss of 0.21568244167436193\n",
      "The 16105 th iteration gives loss of 0.2156745256597671\n",
      "The 16106 th iteration gives loss of 0.2156666106698037\n",
      "The 16107 th iteration gives loss of 0.2156586967042986\n",
      "The 16108 th iteration gives loss of 0.2156507837630994\n",
      "The 16109 th iteration gives loss of 0.21564287184605538\n",
      "The 16110 th iteration gives loss of 0.21563496095299758\n",
      "The 16111 th iteration gives loss of 0.21562705108378274\n",
      "The 16112 th iteration gives loss of 0.2156191422382491\n",
      "The 16113 th iteration gives loss of 0.21561123441624058\n",
      "The 16114 th iteration gives loss of 0.21560332761760428\n",
      "The 16115 th iteration gives loss of 0.21559542184217165\n",
      "The 16116 th iteration gives loss of 0.2155875170898016\n",
      "The 16117 th iteration gives loss of 0.21557961336032197\n",
      "The 16118 th iteration gives loss of 0.21557171065359682\n",
      "The 16119 th iteration gives loss of 0.21556380896946695\n",
      "The 16120 th iteration gives loss of 0.21555590830775023\n",
      "The 16121 th iteration gives loss of 0.21554800866832066\n",
      "The 16122 th iteration gives loss of 0.21554011005101772\n",
      "The 16123 th iteration gives loss of 0.215532212455667\n",
      "The 16124 th iteration gives loss of 0.21552431588212545\n",
      "The 16125 th iteration gives loss of 0.21551642033023868\n",
      "The 16126 th iteration gives loss of 0.21550852579984467\n",
      "The 16127 th iteration gives loss of 0.2155006322907942\n",
      "The 16128 th iteration gives loss of 0.21549273980292752\n",
      "The 16129 th iteration gives loss of 0.2154848483360822\n",
      "The 16130 th iteration gives loss of 0.21547695789010962\n",
      "The 16131 th iteration gives loss of 0.21546906846485614\n",
      "The 16132 th iteration gives loss of 0.2154611800601581\n",
      "The 16133 th iteration gives loss of 0.21545329267588054\n",
      "The 16134 th iteration gives loss of 0.2154454063118469\n",
      "The 16135 th iteration gives loss of 0.21543752096790234\n",
      "The 16136 th iteration gives loss of 0.21542963664388803\n",
      "The 16137 th iteration gives loss of 0.2154217533396603\n",
      "The 16138 th iteration gives loss of 0.2154138710550653\n",
      "The 16139 th iteration gives loss of 0.21540598978993558\n",
      "The 16140 th iteration gives loss of 0.21539810954412575\n",
      "The 16141 th iteration gives loss of 0.21539023031746582\n",
      "The 16142 th iteration gives loss of 0.21538235210981277\n",
      "The 16143 th iteration gives loss of 0.21537447492100897\n",
      "The 16144 th iteration gives loss of 0.2153665987508884\n",
      "The 16145 th iteration gives loss of 0.21535872359931718\n",
      "The 16146 th iteration gives loss of 0.21535084946612099\n",
      "The 16147 th iteration gives loss of 0.21534297635114993\n",
      "The 16148 th iteration gives loss of 0.2153351042542521\n",
      "The 16149 th iteration gives loss of 0.2153272331752681\n",
      "The 16150 th iteration gives loss of 0.21531936311404373\n",
      "The 16151 th iteration gives loss of 0.2153114940704257\n",
      "The 16152 th iteration gives loss of 0.21530362604425568\n",
      "The 16153 th iteration gives loss of 0.21529575903537582\n",
      "The 16154 th iteration gives loss of 0.215287893043636\n",
      "The 16155 th iteration gives loss of 0.21528002806887483\n",
      "The 16156 th iteration gives loss of 0.21527216411094932\n",
      "The 16157 th iteration gives loss of 0.21526430116968745\n",
      "The 16158 th iteration gives loss of 0.215256439244951\n",
      "The 16159 th iteration gives loss of 0.21524857833657338\n",
      "The 16160 th iteration gives loss of 0.21524071844439907\n",
      "The 16161 th iteration gives loss of 0.21523285956827914\n",
      "The 16162 th iteration gives loss of 0.21522500170805006\n",
      "The 16163 th iteration gives loss of 0.2152171448635653\n",
      "The 16164 th iteration gives loss of 0.21520928903467046\n",
      "The 16165 th iteration gives loss of 0.21520143422119886\n",
      "The 16166 th iteration gives loss of 0.2151935804230135\n",
      "The 16167 th iteration gives loss of 0.2151857276399373\n",
      "The 16168 th iteration gives loss of 0.21517787587183332\n",
      "The 16169 th iteration gives loss of 0.2151700251185415\n",
      "The 16170 th iteration gives loss of 0.21516217537990404\n",
      "The 16171 th iteration gives loss of 0.21515432665576809\n",
      "The 16172 th iteration gives loss of 0.2151464789459742\n",
      "The 16173 th iteration gives loss of 0.21513863225037066\n",
      "The 16174 th iteration gives loss of 0.21513078656880533\n",
      "The 16175 th iteration gives loss of 0.21512294190111975\n",
      "The 16176 th iteration gives loss of 0.215115098247167\n",
      "The 16177 th iteration gives loss of 0.21510725560677446\n",
      "The 16178 th iteration gives loss of 0.2150994139798118\n",
      "The 16179 th iteration gives loss of 0.215091573366106\n",
      "The 16180 th iteration gives loss of 0.2150837337655106\n",
      "The 16181 th iteration gives loss of 0.215075895177861\n",
      "The 16182 th iteration gives loss of 0.21506805760301903\n",
      "The 16183 th iteration gives loss of 0.21506022104081968\n",
      "The 16184 th iteration gives loss of 0.21505238549109534\n",
      "The 16185 th iteration gives loss of 0.21504455095371378\n",
      "The 16186 th iteration gives loss of 0.21503671742850833\n",
      "The 16187 th iteration gives loss of 0.215028884915335\n",
      "The 16188 th iteration gives loss of 0.21502105341402675\n",
      "The 16189 th iteration gives loss of 0.21501322292443076\n",
      "The 16190 th iteration gives loss of 0.21500539344640007\n",
      "The 16191 th iteration gives loss of 0.2149975649797779\n",
      "The 16192 th iteration gives loss of 0.21498973752441022\n",
      "The 16193 th iteration gives loss of 0.21498191108013806\n",
      "The 16194 th iteration gives loss of 0.21497408564680895\n",
      "The 16195 th iteration gives loss of 0.21496626122426743\n",
      "The 16196 th iteration gives loss of 0.2149584378123589\n",
      "The 16197 th iteration gives loss of 0.21495061541093657\n",
      "The 16198 th iteration gives loss of 0.21494279401983493\n",
      "The 16199 th iteration gives loss of 0.21493497363890215\n",
      "The 16200 th iteration gives loss of 0.2149271542679854\n",
      "The 16201 th iteration gives loss of 0.21491933590693849\n",
      "The 16202 th iteration gives loss of 0.2149115185556014\n",
      "The 16203 th iteration gives loss of 0.2149037022138154\n",
      "The 16204 th iteration gives loss of 0.21489588688143443\n",
      "The 16205 th iteration gives loss of 0.21488807255829517\n",
      "The 16206 th iteration gives loss of 0.21488025924425008\n",
      "The 16207 th iteration gives loss of 0.21487244693914292\n",
      "The 16208 th iteration gives loss of 0.2148646356428158\n",
      "The 16209 th iteration gives loss of 0.2148568253551266\n",
      "The 16210 th iteration gives loss of 0.21484901607590776\n",
      "The 16211 th iteration gives loss of 0.21484120780501495\n",
      "The 16212 th iteration gives loss of 0.21483340054228392\n",
      "The 16213 th iteration gives loss of 0.21482559428756684\n",
      "The 16214 th iteration gives loss of 0.21481778904071308\n",
      "The 16215 th iteration gives loss of 0.21480998480155483\n",
      "The 16216 th iteration gives loss of 0.21480218156995914\n",
      "The 16217 th iteration gives loss of 0.21479437934575535\n",
      "The 16218 th iteration gives loss of 0.21478657812879598\n",
      "The 16219 th iteration gives loss of 0.2147787779189332\n",
      "The 16220 th iteration gives loss of 0.21477097871600517\n",
      "The 16221 th iteration gives loss of 0.21476318051985785\n",
      "The 16222 th iteration gives loss of 0.21475538333034\n",
      "The 16223 th iteration gives loss of 0.2147475871473003\n",
      "The 16224 th iteration gives loss of 0.21473979197057547\n",
      "The 16225 th iteration gives loss of 0.21473199780002378\n",
      "The 16226 th iteration gives loss of 0.21472420463548128\n",
      "The 16227 th iteration gives loss of 0.21471641247680173\n",
      "The 16228 th iteration gives loss of 0.21470862132382706\n",
      "The 16229 th iteration gives loss of 0.21470083117640193\n",
      "The 16230 th iteration gives loss of 0.21469304203437703\n",
      "The 16231 th iteration gives loss of 0.2146852538976049\n",
      "The 16232 th iteration gives loss of 0.21467746676591873\n",
      "The 16233 th iteration gives loss of 0.21466968063917716\n",
      "The 16234 th iteration gives loss of 0.21466189551721235\n",
      "The 16235 th iteration gives loss of 0.21465411139988425\n",
      "The 16236 th iteration gives loss of 0.21464632828702937\n",
      "The 16237 th iteration gives loss of 0.21463854617849842\n",
      "The 16238 th iteration gives loss of 0.21463076507414697\n",
      "The 16239 th iteration gives loss of 0.2146229849738026\n",
      "The 16240 th iteration gives loss of 0.21461520587733582\n",
      "The 16241 th iteration gives loss of 0.2146074277845763\n",
      "The 16242 th iteration gives loss of 0.214599650695367\n",
      "The 16243 th iteration gives loss of 0.21459187460956697\n",
      "The 16244 th iteration gives loss of 0.21458409952701776\n",
      "The 16245 th iteration gives loss of 0.2145763254475661\n",
      "The 16246 th iteration gives loss of 0.2145685523710599\n",
      "The 16247 th iteration gives loss of 0.21456078029734282\n",
      "The 16248 th iteration gives loss of 0.21455300922626797\n",
      "The 16249 th iteration gives loss of 0.21454523915767043\n",
      "The 16250 th iteration gives loss of 0.2145374700914051\n",
      "The 16251 th iteration gives loss of 0.21452970202732974\n",
      "The 16252 th iteration gives loss of 0.21452193496526717\n",
      "The 16253 th iteration gives loss of 0.21451416890508296\n",
      "The 16254 th iteration gives loss of 0.21450640384661634\n",
      "The 16255 th iteration gives loss of 0.21449863978971925\n",
      "The 16256 th iteration gives loss of 0.21449087673423084\n",
      "The 16257 th iteration gives loss of 0.2144831146800067\n",
      "The 16258 th iteration gives loss of 0.21447535362688022\n",
      "The 16259 th iteration gives loss of 0.21446759357471726\n",
      "The 16260 th iteration gives loss of 0.21445983452336395\n",
      "The 16261 th iteration gives loss of 0.21445207647263442\n",
      "The 16262 th iteration gives loss of 0.2144443194224201\n",
      "The 16263 th iteration gives loss of 0.2144365633725431\n",
      "The 16264 th iteration gives loss of 0.2144288083228586\n",
      "The 16265 th iteration gives loss of 0.21442105427321087\n",
      "The 16266 th iteration gives loss of 0.21441330122343952\n",
      "The 16267 th iteration gives loss of 0.21440554917340476\n",
      "The 16268 th iteration gives loss of 0.21439779812295728\n",
      "The 16269 th iteration gives loss of 0.21439004807192472\n",
      "The 16270 th iteration gives loss of 0.21438229902016745\n",
      "The 16271 th iteration gives loss of 0.21437455096752864\n",
      "The 16272 th iteration gives loss of 0.21436680391386007\n",
      "The 16273 th iteration gives loss of 0.2143590578590059\n",
      "The 16274 th iteration gives loss of 0.21435131280282135\n",
      "The 16275 th iteration gives loss of 0.21434356874513616\n",
      "The 16276 th iteration gives loss of 0.2143358256858098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 16277 th iteration gives loss of 0.21432808362469477\n",
      "The 16278 th iteration gives loss of 0.21432034256162621\n",
      "The 16279 th iteration gives loss of 0.21431260249645848\n",
      "The 16280 th iteration gives loss of 0.2143048634290338\n",
      "The 16281 th iteration gives loss of 0.21429712535921128\n",
      "The 16282 th iteration gives loss of 0.21428938828683247\n",
      "The 16283 th iteration gives loss of 0.21428165221173937\n",
      "The 16284 th iteration gives loss of 0.21427391713378435\n",
      "The 16285 th iteration gives loss of 0.21426618305281284\n",
      "The 16286 th iteration gives loss of 0.21425844996867727\n",
      "The 16287 th iteration gives loss of 0.2142507178812226\n",
      "The 16288 th iteration gives loss of 0.21424298679029394\n",
      "The 16289 th iteration gives loss of 0.2142352566957386\n",
      "The 16290 th iteration gives loss of 0.21422752759741157\n",
      "The 16291 th iteration gives loss of 0.21421979949515735\n",
      "The 16292 th iteration gives loss of 0.21421207238882248\n",
      "The 16293 th iteration gives loss of 0.21420434627825197\n",
      "The 16294 th iteration gives loss of 0.21419662116329757\n",
      "The 16295 th iteration gives loss of 0.21418889704379515\n",
      "The 16296 th iteration gives loss of 0.21418117391961083\n",
      "The 16297 th iteration gives loss of 0.2141734517905855\n",
      "The 16298 th iteration gives loss of 0.21416573065656988\n",
      "The 16299 th iteration gives loss of 0.2141580105174019\n",
      "The 16300 th iteration gives loss of 0.21415029137294272\n",
      "The 16301 th iteration gives loss of 0.21414257322303373\n",
      "The 16302 th iteration gives loss of 0.2141348560675174\n",
      "The 16303 th iteration gives loss of 0.21412713990624427\n",
      "The 16304 th iteration gives loss of 0.21411942473907392\n",
      "The 16305 th iteration gives loss of 0.21411171056584233\n",
      "The 16306 th iteration gives loss of 0.21410399738640454\n",
      "The 16307 th iteration gives loss of 0.21409628520059684\n",
      "The 16308 th iteration gives loss of 0.21408857400828457\n",
      "The 16309 th iteration gives loss of 0.21408086380930177\n",
      "The 16310 th iteration gives loss of 0.21407315460349813\n",
      "The 16311 th iteration gives loss of 0.21406544639073344\n",
      "The 16312 th iteration gives loss of 0.21405773917084375\n",
      "The 16313 th iteration gives loss of 0.21405003294369215\n",
      "The 16314 th iteration gives loss of 0.21404232770910325\n",
      "The 16315 th iteration gives loss of 0.21403462346695218\n",
      "The 16316 th iteration gives loss of 0.21402692021706038\n",
      "The 16317 th iteration gives loss of 0.21401921795929288\n",
      "The 16318 th iteration gives loss of 0.21401151669349208\n",
      "The 16319 th iteration gives loss of 0.21400381641951152\n",
      "The 16320 th iteration gives loss of 0.2139961171371917\n",
      "The 16321 th iteration gives loss of 0.21398841884639613\n",
      "The 16322 th iteration gives loss of 0.21398072154696493\n",
      "The 16323 th iteration gives loss of 0.2139730252387338\n",
      "The 16324 th iteration gives loss of 0.21396532992156983\n",
      "The 16325 th iteration gives loss of 0.21395763559531264\n",
      "The 16326 th iteration gives loss of 0.21394994225981084\n",
      "The 16327 th iteration gives loss of 0.21394224991491695\n",
      "The 16328 th iteration gives loss of 0.213934558560478\n",
      "The 16329 th iteration gives loss of 0.21392686819633977\n",
      "The 16330 th iteration gives loss of 0.2139191788223493\n",
      "The 16331 th iteration gives loss of 0.2139114904383591\n",
      "The 16332 th iteration gives loss of 0.2139038030442198\n",
      "The 16333 th iteration gives loss of 0.21389611663977834\n",
      "The 16334 th iteration gives loss of 0.21388843122488221\n",
      "The 16335 th iteration gives loss of 0.21388074679938368\n",
      "The 16336 th iteration gives loss of 0.21387306336312784\n",
      "The 16337 th iteration gives loss of 0.21386538091596527\n",
      "The 16338 th iteration gives loss of 0.2138576994577386\n",
      "The 16339 th iteration gives loss of 0.21385001898830847\n",
      "The 16340 th iteration gives loss of 0.21384233950751302\n",
      "The 16341 th iteration gives loss of 0.2138346610152021\n",
      "The 16342 th iteration gives loss of 0.21382698351123045\n",
      "The 16343 th iteration gives loss of 0.2138193069954462\n",
      "The 16344 th iteration gives loss of 0.21381163146768392\n",
      "The 16345 th iteration gives loss of 0.21380395692782125\n",
      "The 16346 th iteration gives loss of 0.21379628337568038\n",
      "The 16347 th iteration gives loss of 0.21378861081113085\n",
      "The 16348 th iteration gives loss of 0.21378093923399988\n",
      "The 16349 th iteration gives loss of 0.21377326864415683\n",
      "The 16350 th iteration gives loss of 0.21376559904143552\n",
      "The 16351 th iteration gives loss of 0.2137579304256959\n",
      "The 16352 th iteration gives loss of 0.21375026279678205\n",
      "The 16353 th iteration gives loss of 0.21374259615454727\n",
      "The 16354 th iteration gives loss of 0.21373493049883055\n",
      "The 16355 th iteration gives loss of 0.21372726582949328\n",
      "The 16356 th iteration gives loss of 0.2137196021463764\n",
      "The 16357 th iteration gives loss of 0.21371193944933778\n",
      "The 16358 th iteration gives loss of 0.2137042777382147\n",
      "The 16359 th iteration gives loss of 0.21369661701286805\n",
      "The 16360 th iteration gives loss of 0.2136889572731324\n",
      "The 16361 th iteration gives loss of 0.21368129851887083\n",
      "The 16362 th iteration gives loss of 0.21367364074993492\n",
      "The 16363 th iteration gives loss of 0.2136659839661594\n",
      "The 16364 th iteration gives loss of 0.21365832816739977\n",
      "The 16365 th iteration gives loss of 0.21365067335350862\n",
      "The 16366 th iteration gives loss of 0.21364301952434384\n",
      "The 16367 th iteration gives loss of 0.21363536667973948\n",
      "The 16368 th iteration gives loss of 0.21362771481953996\n",
      "The 16369 th iteration gives loss of 0.21362006394361763\n",
      "The 16370 th iteration gives loss of 0.21361241405180229\n",
      "The 16371 th iteration gives loss of 0.2136047651439605\n",
      "The 16372 th iteration gives loss of 0.2135971172199165\n",
      "The 16373 th iteration gives loss of 0.21358947027954442\n",
      "The 16374 th iteration gives loss of 0.21358182432269124\n",
      "The 16375 th iteration gives loss of 0.2135741793491999\n",
      "The 16376 th iteration gives loss of 0.2135665353589148\n",
      "The 16377 th iteration gives loss of 0.2135588923516917\n",
      "The 16378 th iteration gives loss of 0.2135512503273814\n",
      "The 16379 th iteration gives loss of 0.21354360928582714\n",
      "The 16380 th iteration gives loss of 0.21353596922688614\n",
      "The 16381 th iteration gives loss of 0.21352833015040767\n",
      "The 16382 th iteration gives loss of 0.21352069205624222\n",
      "The 16383 th iteration gives loss of 0.21351305494422906\n",
      "The 16384 th iteration gives loss of 0.21350541881422716\n",
      "The 16385 th iteration gives loss of 0.21349778366609043\n",
      "The 16386 th iteration gives loss of 0.21349014949965806\n",
      "The 16387 th iteration gives loss of 0.21348251631478504\n",
      "The 16388 th iteration gives loss of 0.21347488411131726\n",
      "The 16389 th iteration gives loss of 0.21346725288912574\n",
      "The 16390 th iteration gives loss of 0.2134596226480271\n",
      "The 16391 th iteration gives loss of 0.21345199338789625\n",
      "The 16392 th iteration gives loss of 0.21344436510856962\n",
      "The 16393 th iteration gives loss of 0.21343673780990285\n",
      "The 16394 th iteration gives loss of 0.21342911149174307\n",
      "The 16395 th iteration gives loss of 0.2134214861539395\n",
      "The 16396 th iteration gives loss of 0.2134138617963551\n",
      "The 16397 th iteration gives loss of 0.21340623841882522\n",
      "The 16398 th iteration gives loss of 0.21339861602121543\n",
      "The 16399 th iteration gives loss of 0.21339099460335204\n",
      "The 16400 th iteration gives loss of 0.213383374165104\n",
      "The 16401 th iteration gives loss of 0.21337575470631326\n",
      "The 16402 th iteration gives loss of 0.213368136226825\n",
      "The 16403 th iteration gives loss of 0.21336051872651624\n",
      "The 16404 th iteration gives loss of 0.2133529022052123\n",
      "The 16405 th iteration gives loss of 0.21334528666276278\n",
      "The 16406 th iteration gives loss of 0.21333767209903712\n",
      "The 16407 th iteration gives loss of 0.21333005851386505\n",
      "The 16408 th iteration gives loss of 0.21332244590710184\n",
      "The 16409 th iteration gives loss of 0.2133148342786024\n",
      "The 16410 th iteration gives loss of 0.2133072236282209\n",
      "The 16411 th iteration gives loss of 0.21329961395579397\n",
      "The 16412 th iteration gives loss of 0.21329200526118444\n",
      "The 16413 th iteration gives loss of 0.2132843975442378\n",
      "The 16414 th iteration gives loss of 0.21327679080481163\n",
      "The 16415 th iteration gives loss of 0.21326918504274853\n",
      "The 16416 th iteration gives loss of 0.213261580257903\n",
      "The 16417 th iteration gives loss of 0.21325397645012045\n",
      "The 16418 th iteration gives loss of 0.21324637361925516\n",
      "The 16419 th iteration gives loss of 0.21323877176515613\n",
      "The 16420 th iteration gives loss of 0.21323117088767485\n",
      "The 16421 th iteration gives loss of 0.21322357098666556\n",
      "The 16422 th iteration gives loss of 0.21321597206197684\n",
      "The 16423 th iteration gives loss of 0.21320837411345286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 16424 th iteration gives loss of 0.2132007771409517\n",
      "The 16425 th iteration gives loss of 0.21319318114432048\n",
      "The 16426 th iteration gives loss of 0.21318558612341412\n",
      "The 16427 th iteration gives loss of 0.21317799207807828\n",
      "The 16428 th iteration gives loss of 0.21317039900816728\n",
      "The 16429 th iteration gives loss of 0.21316280691353862\n",
      "The 16430 th iteration gives loss of 0.21315521579402624\n",
      "The 16431 th iteration gives loss of 0.21314762564948775\n",
      "The 16432 th iteration gives loss of 0.21314003647977897\n",
      "The 16433 th iteration gives loss of 0.21313244828475172\n",
      "The 16434 th iteration gives loss of 0.21312486106425627\n",
      "The 16435 th iteration gives loss of 0.21311727481813353\n",
      "The 16436 th iteration gives loss of 0.21310968954624615\n",
      "The 16437 th iteration gives loss of 0.21310210524843323\n",
      "The 16438 th iteration gives loss of 0.21309452192456207\n",
      "The 16439 th iteration gives loss of 0.21308693957447258\n",
      "The 16440 th iteration gives loss of 0.21307935819801474\n",
      "The 16441 th iteration gives loss of 0.21307177779504471\n",
      "The 16442 th iteration gives loss of 0.21306419836540905\n",
      "The 16443 th iteration gives loss of 0.21305661990895924\n",
      "The 16444 th iteration gives loss of 0.21304904242555786\n",
      "The 16445 th iteration gives loss of 0.2130414659150374\n",
      "The 16446 th iteration gives loss of 0.21303389037726822\n",
      "The 16447 th iteration gives loss of 0.21302631581208328\n",
      "The 16448 th iteration gives loss of 0.21301874221934408\n",
      "The 16449 th iteration gives loss of 0.21301116959890565\n",
      "The 16450 th iteration gives loss of 0.21300359795060872\n",
      "The 16451 th iteration gives loss of 0.21299602727431335\n",
      "The 16452 th iteration gives loss of 0.21298845756986917\n",
      "The 16453 th iteration gives loss of 0.2129808888371162\n",
      "The 16454 th iteration gives loss of 0.21297332107592679\n",
      "The 16455 th iteration gives loss of 0.21296575428613504\n",
      "The 16456 th iteration gives loss of 0.21295818846760073\n",
      "The 16457 th iteration gives loss of 0.21295062362016298\n",
      "The 16458 th iteration gives loss of 0.21294305974368907\n",
      "The 16459 th iteration gives loss of 0.21293549683802557\n",
      "The 16460 th iteration gives loss of 0.21292793490302792\n",
      "The 16461 th iteration gives loss of 0.21292037393853133\n",
      "The 16462 th iteration gives loss of 0.2129128139444017\n",
      "The 16463 th iteration gives loss of 0.21290525492049073\n",
      "The 16464 th iteration gives loss of 0.21289769686664026\n",
      "The 16465 th iteration gives loss of 0.21289013978270746\n",
      "The 16466 th iteration gives loss of 0.2128825836685475\n",
      "The 16467 th iteration gives loss of 0.2128750285240196\n",
      "The 16468 th iteration gives loss of 0.2128674743489533\n",
      "The 16469 th iteration gives loss of 0.2128599211432096\n",
      "The 16470 th iteration gives loss of 0.21285236890664247\n",
      "The 16471 th iteration gives loss of 0.21284481763910681\n",
      "The 16472 th iteration gives loss of 0.2128372673404569\n",
      "The 16473 th iteration gives loss of 0.21282971801053627\n",
      "The 16474 th iteration gives loss of 0.21282216964919912\n",
      "The 16475 th iteration gives loss of 0.21281462225629144\n",
      "The 16476 th iteration gives loss of 0.21280707583167674\n",
      "The 16477 th iteration gives loss of 0.2127995303752052\n",
      "The 16478 th iteration gives loss of 0.21279198588671558\n",
      "The 16479 th iteration gives loss of 0.2127844423660713\n",
      "The 16480 th iteration gives loss of 0.21277689981312448\n",
      "The 16481 th iteration gives loss of 0.21276935822772336\n",
      "The 16482 th iteration gives loss of 0.21276181760972737\n",
      "The 16483 th iteration gives loss of 0.21275427795897914\n",
      "The 16484 th iteration gives loss of 0.21274673927532564\n",
      "The 16485 th iteration gives loss of 0.21273920155863071\n",
      "The 16486 th iteration gives loss of 0.212731664808741\n",
      "The 16487 th iteration gives loss of 0.21272412902551277\n",
      "The 16488 th iteration gives loss of 0.2127165942088056\n",
      "The 16489 th iteration gives loss of 0.2127090603584524\n",
      "The 16490 th iteration gives loss of 0.21270152747431018\n",
      "The 16491 th iteration gives loss of 0.21269399555623783\n",
      "The 16492 th iteration gives loss of 0.21268646460408205\n",
      "The 16493 th iteration gives loss of 0.21267893461770593\n",
      "The 16494 th iteration gives loss of 0.21267140559696213\n",
      "The 16495 th iteration gives loss of 0.2126638775416829\n",
      "The 16496 th iteration gives loss of 0.21265635045172201\n",
      "The 16497 th iteration gives loss of 0.2126488243269498\n",
      "The 16498 th iteration gives loss of 0.2126412991672136\n",
      "The 16499 th iteration gives loss of 0.2126337749723683\n",
      "The 16500 th iteration gives loss of 0.21262625174226027\n",
      "The 16501 th iteration gives loss of 0.2126187294767363\n",
      "The 16502 th iteration gives loss of 0.2126112081756533\n",
      "The 16503 th iteration gives loss of 0.21260368783886693\n",
      "The 16504 th iteration gives loss of 0.21259616846623355\n",
      "The 16505 th iteration gives loss of 0.21258865005759298\n",
      "The 16506 th iteration gives loss of 0.212581132612811\n",
      "The 16507 th iteration gives loss of 0.2125736161317342\n",
      "The 16508 th iteration gives loss of 0.2125661006142163\n",
      "The 16509 th iteration gives loss of 0.21255858606009778\n",
      "The 16510 th iteration gives loss of 0.2125510724692541\n",
      "The 16511 th iteration gives loss of 0.21254355984152143\n",
      "The 16512 th iteration gives loss of 0.21253604817675167\n",
      "The 16513 th iteration gives loss of 0.212528537474814\n",
      "The 16514 th iteration gives loss of 0.2125210277355367\n",
      "The 16515 th iteration gives loss of 0.21251351895879847\n",
      "The 16516 th iteration gives loss of 0.21250601114442932\n",
      "The 16517 th iteration gives loss of 0.21249850429228995\n",
      "The 16518 th iteration gives loss of 0.21249099840224536\n",
      "The 16519 th iteration gives loss of 0.21248349347413295\n",
      "The 16520 th iteration gives loss of 0.21247598950781188\n",
      "The 16521 th iteration gives loss of 0.2124684865031232\n",
      "The 16522 th iteration gives loss of 0.21246098445993641\n",
      "The 16523 th iteration gives loss of 0.2124534833781015\n",
      "The 16524 th iteration gives loss of 0.21244598325746253\n",
      "The 16525 th iteration gives loss of 0.21243848409788177\n",
      "The 16526 th iteration gives loss of 0.21243098589921075\n",
      "The 16527 th iteration gives loss of 0.2124234886612873\n",
      "The 16528 th iteration gives loss of 0.2124159923839923\n",
      "The 16529 th iteration gives loss of 0.21240849706714993\n",
      "The 16530 th iteration gives loss of 0.2124010027106392\n",
      "The 16531 th iteration gives loss of 0.21239350931429352\n",
      "The 16532 th iteration gives loss of 0.2123860168779716\n",
      "The 16533 th iteration gives loss of 0.21237852540152638\n",
      "The 16534 th iteration gives loss of 0.21237103488480988\n",
      "The 16535 th iteration gives loss of 0.2123635453276869\n",
      "The 16536 th iteration gives loss of 0.2123560567300009\n",
      "The 16537 th iteration gives loss of 0.2123485690916019\n",
      "The 16538 th iteration gives loss of 0.21234108241234673\n",
      "The 16539 th iteration gives loss of 0.2123335966920829\n",
      "The 16540 th iteration gives loss of 0.2123261119306791\n",
      "The 16541 th iteration gives loss of 0.2123186281279715\n",
      "The 16542 th iteration gives loss of 0.2123111452838191\n",
      "The 16543 th iteration gives loss of 0.21230366339807472\n",
      "The 16544 th iteration gives loss of 0.212296182470602\n",
      "The 16545 th iteration gives loss of 0.21228870250124465\n",
      "The 16546 th iteration gives loss of 0.2122812234898631\n",
      "The 16547 th iteration gives loss of 0.21227374543629676\n",
      "The 16548 th iteration gives loss of 0.21226626834040663\n",
      "The 16549 th iteration gives loss of 0.21225879220204533\n",
      "The 16550 th iteration gives loss of 0.2122513170210722\n",
      "The 16551 th iteration gives loss of 0.2122438427973384\n",
      "The 16552 th iteration gives loss of 0.2122363695306902\n",
      "The 16553 th iteration gives loss of 0.21222889722098548\n",
      "The 16554 th iteration gives loss of 0.21222142586808396\n",
      "The 16555 th iteration gives loss of 0.21221395547183203\n",
      "The 16556 th iteration gives loss of 0.21220648603207876\n",
      "The 16557 th iteration gives loss of 0.21219901754868978\n",
      "The 16558 th iteration gives loss of 0.2121915500215163\n",
      "The 16559 th iteration gives loss of 0.2121840834504099\n",
      "The 16560 th iteration gives loss of 0.2121766178352093\n",
      "The 16561 th iteration gives loss of 0.2121691531757913\n",
      "The 16562 th iteration gives loss of 0.2121616894719989\n",
      "The 16563 th iteration gives loss of 0.2121542267236837\n",
      "The 16564 th iteration gives loss of 0.21214676493070395\n",
      "The 16565 th iteration gives loss of 0.21213930409291792\n",
      "The 16566 th iteration gives loss of 0.21213184421016779\n",
      "The 16567 th iteration gives loss of 0.21212438528231373\n",
      "The 16568 th iteration gives loss of 0.21211692730920903\n",
      "The 16569 th iteration gives loss of 0.2121094702907079\n",
      "The 16570 th iteration gives loss of 0.21210201422666047\n",
      "The 16571 th iteration gives loss of 0.2120945591169308\n",
      "The 16572 th iteration gives loss of 0.21208710496135716\n",
      "The 16573 th iteration gives loss of 0.21207965175980983\n",
      "The 16574 th iteration gives loss of 0.21207219951213135\n",
      "The 16575 th iteration gives loss of 0.2120647482181883\n",
      "The 16576 th iteration gives loss of 0.21205729787781918\n",
      "The 16577 th iteration gives loss of 0.21204984849088437\n",
      "The 16578 th iteration gives loss of 0.21204240005724143\n",
      "The 16579 th iteration gives loss of 0.21203495257674204\n",
      "The 16580 th iteration gives loss of 0.21202750604923187\n",
      "The 16581 th iteration gives loss of 0.21202006047457952\n",
      "The 16582 th iteration gives loss of 0.21201261585263276\n",
      "The 16583 th iteration gives loss of 0.21200517218324474\n",
      "The 16584 th iteration gives loss of 0.21199772946626325\n",
      "The 16585 th iteration gives loss of 0.21199028770155806\n",
      "The 16586 th iteration gives loss of 0.21198284688896912\n",
      "The 16587 th iteration gives loss of 0.21197540702836276\n",
      "The 16588 th iteration gives loss of 0.2119679681195874\n",
      "The 16589 th iteration gives loss of 0.21196053016249397\n",
      "The 16590 th iteration gives loss of 0.21195309315694613\n",
      "The 16591 th iteration gives loss of 0.21194565710277907\n",
      "The 16592 th iteration gives loss of 0.21193822199986548\n",
      "The 16593 th iteration gives loss of 0.21193078784805675\n",
      "The 16594 th iteration gives loss of 0.2119233546472002\n",
      "The 16595 th iteration gives loss of 0.21191592239715903\n",
      "The 16596 th iteration gives loss of 0.21190849109777873\n",
      "The 16597 th iteration gives loss of 0.21190106074892692\n",
      "The 16598 th iteration gives loss of 0.21189363135044006\n",
      "The 16599 th iteration gives loss of 0.21188620290218677\n",
      "The 16600 th iteration gives loss of 0.21187877540401037\n",
      "The 16601 th iteration gives loss of 0.21187134885577727\n",
      "The 16602 th iteration gives loss of 0.21186392325734152\n",
      "The 16603 th iteration gives loss of 0.21185649860854858\n",
      "The 16604 th iteration gives loss of 0.21184907490925403\n",
      "The 16605 th iteration gives loss of 0.21184165215931924\n",
      "The 16606 th iteration gives loss of 0.21183423035860055\n",
      "The 16607 th iteration gives loss of 0.21182680950693528\n",
      "The 16608 th iteration gives loss of 0.2118193896041966\n",
      "The 16609 th iteration gives loss of 0.21181197065022866\n",
      "The 16610 th iteration gives loss of 0.21180455264490136\n",
      "The 16611 th iteration gives loss of 0.21179713558804852\n",
      "The 16612 th iteration gives loss of 0.2117897194795453\n",
      "The 16613 th iteration gives loss of 0.21178230431923184\n",
      "The 16614 th iteration gives loss of 0.2117748901069664\n",
      "The 16615 th iteration gives loss of 0.2117674768426018\n",
      "The 16616 th iteration gives loss of 0.2117600645259951\n",
      "The 16617 th iteration gives loss of 0.21175265315701036\n",
      "The 16618 th iteration gives loss of 0.21174524273548634\n",
      "The 16619 th iteration gives loss of 0.2117378332612906\n",
      "The 16620 th iteration gives loss of 0.21173042473427217\n",
      "The 16621 th iteration gives loss of 0.21172301715427938\n",
      "The 16622 th iteration gives loss of 0.21171561052118784\n",
      "The 16623 th iteration gives loss of 0.2117082048348309\n",
      "The 16624 th iteration gives loss of 0.21170080009506728\n",
      "The 16625 th iteration gives loss of 0.21169339630176784\n",
      "The 16626 th iteration gives loss of 0.21168599345477146\n",
      "The 16627 th iteration gives loss of 0.21167859155394392\n",
      "The 16628 th iteration gives loss of 0.21167119059913098\n",
      "The 16629 th iteration gives loss of 0.21166379059018953\n",
      "The 16630 th iteration gives loss of 0.21165639152697882\n",
      "The 16631 th iteration gives loss of 0.21164899340935533\n",
      "The 16632 th iteration gives loss of 0.21164159623716666\n",
      "The 16633 th iteration gives loss of 0.2116342000102798\n",
      "The 16634 th iteration gives loss of 0.21162680472853965\n",
      "The 16635 th iteration gives loss of 0.21161941039179943\n",
      "The 16636 th iteration gives loss of 0.2116120169999246\n",
      "The 16637 th iteration gives loss of 0.21160462455276127\n",
      "The 16638 th iteration gives loss of 0.21159723305017342\n",
      "The 16639 th iteration gives loss of 0.21158984249201038\n",
      "The 16640 th iteration gives loss of 0.21158245287813196\n",
      "The 16641 th iteration gives loss of 0.2115750642083868\n",
      "The 16642 th iteration gives loss of 0.2115676764826366\n",
      "The 16643 th iteration gives loss of 0.21156028970074212\n",
      "The 16644 th iteration gives loss of 0.2115529038625395\n",
      "The 16645 th iteration gives loss of 0.2115455189678972\n",
      "The 16646 th iteration gives loss of 0.211538135016677\n",
      "The 16647 th iteration gives loss of 0.21153075200871954\n",
      "The 16648 th iteration gives loss of 0.21152336994389556\n",
      "The 16649 th iteration gives loss of 0.21151598882204706\n",
      "The 16650 th iteration gives loss of 0.21150860864303547\n",
      "The 16651 th iteration gives loss of 0.2115012294067166\n",
      "The 16652 th iteration gives loss of 0.2114938511129502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 16653 th iteration gives loss of 0.21148647376158425\n",
      "The 16654 th iteration gives loss of 0.21147909735247925\n",
      "The 16655 th iteration gives loss of 0.21147172188548363\n",
      "The 16656 th iteration gives loss of 0.2114643473604588\n",
      "The 16657 th iteration gives loss of 0.2114569737772594\n",
      "The 16658 th iteration gives loss of 0.21144960113574907\n",
      "The 16659 th iteration gives loss of 0.21144222943577615\n",
      "The 16660 th iteration gives loss of 0.21143485867720005\n",
      "The 16661 th iteration gives loss of 0.2114274888598663\n",
      "The 16662 th iteration gives loss of 0.21142011998364557\n",
      "The 16663 th iteration gives loss of 0.21141275204837726\n",
      "The 16664 th iteration gives loss of 0.2114053850539401\n",
      "The 16665 th iteration gives loss of 0.21139801900016042\n",
      "The 16666 th iteration gives loss of 0.21139065388691933\n",
      "The 16667 th iteration gives loss of 0.2113832897140635\n",
      "The 16668 th iteration gives loss of 0.21137592648144804\n",
      "The 16669 th iteration gives loss of 0.2113685641889173\n",
      "The 16670 th iteration gives loss of 0.21136120283635995\n",
      "The 16671 th iteration gives loss of 0.2113538424236003\n",
      "The 16672 th iteration gives loss of 0.2113464829505001\n",
      "The 16673 th iteration gives loss of 0.2113391244169348\n",
      "The 16674 th iteration gives loss of 0.2113317668227387\n",
      "The 16675 th iteration gives loss of 0.2113244101677774\n",
      "The 16676 th iteration gives loss of 0.21131705445190901\n",
      "The 16677 th iteration gives loss of 0.2113096996749852\n",
      "The 16678 th iteration gives loss of 0.2113023458368652\n",
      "The 16679 th iteration gives loss of 0.21129499293740034\n",
      "The 16680 th iteration gives loss of 0.21128764097644745\n",
      "The 16681 th iteration gives loss of 0.21128028995387477\n",
      "The 16682 th iteration gives loss of 0.21127293986952264\n",
      "The 16683 th iteration gives loss of 0.21126559072325\n",
      "The 16684 th iteration gives loss of 0.2112582425149272\n",
      "The 16685 th iteration gives loss of 0.21125089524439\n",
      "The 16686 th iteration gives loss of 0.21124354891151556\n",
      "The 16687 th iteration gives loss of 0.2112362035161367\n",
      "The 16688 th iteration gives loss of 0.2112288590581383\n",
      "The 16689 th iteration gives loss of 0.21122151553734977\n",
      "The 16690 th iteration gives loss of 0.21121417295364353\n",
      "The 16691 th iteration gives loss of 0.2112068313068708\n",
      "The 16692 th iteration gives loss of 0.21119949059689025\n",
      "The 16693 th iteration gives loss of 0.211192150823554\n",
      "The 16694 th iteration gives loss of 0.21118481198672787\n",
      "The 16695 th iteration gives loss of 0.21117747408625026\n",
      "The 16696 th iteration gives loss of 0.21117013712199617\n",
      "The 16697 th iteration gives loss of 0.21116280109381322\n",
      "The 16698 th iteration gives loss of 0.21115546600156906\n",
      "The 16699 th iteration gives loss of 0.21114813184510928\n",
      "The 16700 th iteration gives loss of 0.21114079862429808\n",
      "The 16701 th iteration gives loss of 0.2111334663389737\n",
      "The 16702 th iteration gives loss of 0.21112613498901237\n",
      "The 16703 th iteration gives loss of 0.21111880457426685\n",
      "The 16704 th iteration gives loss of 0.21111147509459535\n",
      "The 16705 th iteration gives loss of 0.21110414654984175\n",
      "The 16706 th iteration gives loss of 0.21109681893987456\n",
      "The 16707 th iteration gives loss of 0.21108949226455\n",
      "The 16708 th iteration gives loss of 0.2110821665237266\n",
      "The 16709 th iteration gives loss of 0.2110748417172574\n",
      "The 16710 th iteration gives loss of 0.21106751784499445\n",
      "The 16711 th iteration gives loss of 0.21106019490680014\n",
      "The 16712 th iteration gives loss of 0.21105287290252536\n",
      "The 16713 th iteration gives loss of 0.21104555183204177\n",
      "The 16714 th iteration gives loss of 0.21103823169520067\n",
      "The 16715 th iteration gives loss of 0.2110309124918467\n",
      "The 16716 th iteration gives loss of 0.21102359422184466\n",
      "The 16717 th iteration gives loss of 0.21101627688505853\n",
      "The 16718 th iteration gives loss of 0.21100896048133388\n",
      "The 16719 th iteration gives loss of 0.21100164501054575\n",
      "The 16720 th iteration gives loss of 0.21099433047252888\n",
      "The 16721 th iteration gives loss of 0.21098701686714477\n",
      "The 16722 th iteration gives loss of 0.21097970419426584\n",
      "The 16723 th iteration gives loss of 0.2109723924537296\n",
      "The 16724 th iteration gives loss of 0.21096508164540986\n",
      "The 16725 th iteration gives loss of 0.21095777176916336\n",
      "The 16726 th iteration gives loss of 0.210950462824832\n",
      "The 16727 th iteration gives loss of 0.2109431548122829\n",
      "The 16728 th iteration gives loss of 0.21093584773137755\n",
      "The 16729 th iteration gives loss of 0.21092854158195742\n",
      "The 16730 th iteration gives loss of 0.21092123636390075\n",
      "The 16731 th iteration gives loss of 0.2109139320770569\n",
      "The 16732 th iteration gives loss of 0.21090662872126867\n",
      "The 16733 th iteration gives loss of 0.2108993262964119\n",
      "The 16734 th iteration gives loss of 0.21089202480233515\n",
      "The 16735 th iteration gives loss of 0.21088472423890292\n",
      "The 16736 th iteration gives loss of 0.21087742460597458\n",
      "The 16737 th iteration gives loss of 0.2108701259033811\n",
      "The 16738 th iteration gives loss of 0.21086282813101723\n",
      "The 16739 th iteration gives loss of 0.21085553128871165\n",
      "The 16740 th iteration gives loss of 0.21084823537633446\n",
      "The 16741 th iteration gives loss of 0.21084094039374213\n",
      "The 16742 th iteration gives loss of 0.2108336463408024\n",
      "The 16743 th iteration gives loss of 0.21082635321735682\n",
      "The 16744 th iteration gives loss of 0.210819061023266\n",
      "The 16745 th iteration gives loss of 0.21081176975838967\n",
      "The 16746 th iteration gives loss of 0.21080447942258693\n",
      "The 16747 th iteration gives loss of 0.21079719001571615\n",
      "The 16748 th iteration gives loss of 0.21078990153763416\n",
      "The 16749 th iteration gives loss of 0.21078261398819334\n",
      "The 16750 th iteration gives loss of 0.21077532736726098\n",
      "The 16751 th iteration gives loss of 0.2107680416746844\n",
      "The 16752 th iteration gives loss of 0.21076075691032828\n",
      "The 16753 th iteration gives loss of 0.21075347307405098\n",
      "The 16754 th iteration gives loss of 0.21074619016570642\n",
      "The 16755 th iteration gives loss of 0.21073890818515845\n",
      "The 16756 th iteration gives loss of 0.21073162713225393\n",
      "The 16757 th iteration gives loss of 0.21072434700686418\n",
      "The 16758 th iteration gives loss of 0.2107170678088348\n",
      "The 16759 th iteration gives loss of 0.2107097895380279\n",
      "The 16760 th iteration gives loss of 0.21070251219430097\n",
      "The 16761 th iteration gives loss of 0.21069523577752255\n",
      "The 16762 th iteration gives loss of 0.21068796028753908\n",
      "The 16763 th iteration gives loss of 0.2106806857242092\n",
      "The 16764 th iteration gives loss of 0.21067341208739013\n",
      "The 16765 th iteration gives loss of 0.21066613937694934\n",
      "The 16766 th iteration gives loss of 0.2106588675927342\n",
      "The 16767 th iteration gives loss of 0.21065159673459874\n",
      "The 16768 th iteration gives loss of 0.21064432680242412\n",
      "The 16769 th iteration gives loss of 0.2106370577960464\n",
      "The 16770 th iteration gives loss of 0.21062978971533414\n",
      "The 16771 th iteration gives loss of 0.21062252256013553\n",
      "The 16772 th iteration gives loss of 0.21061525633031863\n",
      "The 16773 th iteration gives loss of 0.210607991025735\n",
      "The 16774 th iteration gives loss of 0.21060072664624346\n",
      "The 16775 th iteration gives loss of 0.21059346319170894\n",
      "The 16776 th iteration gives loss of 0.21058620066198802\n",
      "The 16777 th iteration gives loss of 0.21057893905693548\n",
      "The 16778 th iteration gives loss of 0.2105716783764136\n",
      "The 16779 th iteration gives loss of 0.21056441862027644\n",
      "The 16780 th iteration gives loss of 0.21055715978837972\n",
      "The 16781 th iteration gives loss of 0.21054990188058845\n",
      "The 16782 th iteration gives loss of 0.2105426448967585\n",
      "The 16783 th iteration gives loss of 0.21053538883674772\n",
      "The 16784 th iteration gives loss of 0.2105281337004195\n",
      "The 16785 th iteration gives loss of 0.21052087948762618\n",
      "The 16786 th iteration gives loss of 0.21051362619822087\n",
      "The 16787 th iteration gives loss of 0.21050637383207202\n",
      "The 16788 th iteration gives loss of 0.21049912238903323\n",
      "The 16789 th iteration gives loss of 0.2104918718689765\n",
      "The 16790 th iteration gives loss of 0.21048462227173517\n",
      "The 16791 th iteration gives loss of 0.2104773735971945\n",
      "The 16792 th iteration gives loss of 0.21047012584518926\n",
      "The 16793 th iteration gives loss of 0.21046287901559863\n",
      "The 16794 th iteration gives loss of 0.2104556331082599\n",
      "The 16795 th iteration gives loss of 0.210448388123047\n",
      "The 16796 th iteration gives loss of 0.21044114405981673\n",
      "The 16797 th iteration gives loss of 0.21043390091842934\n",
      "The 16798 th iteration gives loss of 0.21042665869873042\n",
      "The 16799 th iteration gives loss of 0.2104194174005966\n",
      "The 16800 th iteration gives loss of 0.2104121770238785\n",
      "The 16801 th iteration gives loss of 0.2104049375684382\n",
      "The 16802 th iteration gives loss of 0.21039769903411953\n",
      "The 16803 th iteration gives loss of 0.21039046142079834\n",
      "The 16804 th iteration gives loss of 0.21038322472833834\n",
      "The 16805 th iteration gives loss of 0.21037598895657794\n",
      "The 16806 th iteration gives loss of 0.21036875410538197\n",
      "The 16807 th iteration gives loss of 0.21036152017462054\n",
      "The 16808 th iteration gives loss of 0.21035428716414573\n",
      "The 16809 th iteration gives loss of 0.21034705507381865\n",
      "The 16810 th iteration gives loss of 0.2103398239034911\n",
      "The 16811 th iteration gives loss of 0.21033259365303458\n",
      "The 16812 th iteration gives loss of 0.210325364322294\n",
      "The 16813 th iteration gives loss of 0.2103181359111377\n",
      "The 16814 th iteration gives loss of 0.21031090841942032\n",
      "The 16815 th iteration gives loss of 0.21030368184700146\n",
      "The 16816 th iteration gives loss of 0.2102964561937379\n",
      "The 16817 th iteration gives loss of 0.21028923145949974\n",
      "The 16818 th iteration gives loss of 0.210282007644137\n",
      "The 16819 th iteration gives loss of 0.21027478474751005\n",
      "The 16820 th iteration gives loss of 0.2102675627694767\n",
      "The 16821 th iteration gives loss of 0.21026034170989444\n",
      "The 16822 th iteration gives loss of 0.21025312156863593\n",
      "The 16823 th iteration gives loss of 0.2102459023455366\n",
      "The 16824 th iteration gives loss of 0.21023868404047558\n",
      "The 16825 th iteration gives loss of 0.21023146665331163\n",
      "The 16826 th iteration gives loss of 0.2102242501838955\n",
      "The 16827 th iteration gives loss of 0.21021703463209335\n",
      "The 16828 th iteration gives loss of 0.21020981999775223\n",
      "The 16829 th iteration gives loss of 0.2102026062807404\n",
      "The 16830 th iteration gives loss of 0.2101953934809296\n",
      "The 16831 th iteration gives loss of 0.21018818159815844\n",
      "The 16832 th iteration gives loss of 0.210180970632287\n",
      "The 16833 th iteration gives loss of 0.2101737605831974\n",
      "The 16834 th iteration gives loss of 0.210166551450722\n",
      "The 16835 th iteration gives loss of 0.2101593432347404\n",
      "The 16836 th iteration gives loss of 0.2101521359351013\n",
      "The 16837 th iteration gives loss of 0.2101449295516686\n",
      "The 16838 th iteration gives loss of 0.21013772408429032\n",
      "The 16839 th iteration gives loss of 0.21013051953283654\n",
      "The 16840 th iteration gives loss of 0.2101233158971756\n",
      "The 16841 th iteration gives loss of 0.21011611317715218\n",
      "The 16842 th iteration gives loss of 0.2101089113726349\n",
      "The 16843 th iteration gives loss of 0.21010171048347973\n",
      "The 16844 th iteration gives loss of 0.21009451050954353\n",
      "The 16845 th iteration gives loss of 0.2100873114506856\n",
      "The 16846 th iteration gives loss of 0.21008011330677526\n",
      "The 16847 th iteration gives loss of 0.21007291607766904\n",
      "The 16848 th iteration gives loss of 0.2100657197632115\n",
      "The 16849 th iteration gives loss of 0.2100585243632832\n",
      "The 16850 th iteration gives loss of 0.2100513298777355\n",
      "The 16851 th iteration gives loss of 0.21004413630642643\n",
      "The 16852 th iteration gives loss of 0.21003694364922182\n",
      "The 16853 th iteration gives loss of 0.21002975190597853\n",
      "The 16854 th iteration gives loss of 0.2100225610765473\n",
      "The 16855 th iteration gives loss of 0.21001537116079472\n",
      "The 16856 th iteration gives loss of 0.2100081821585841\n",
      "The 16857 th iteration gives loss of 0.21000099406977712\n",
      "The 16858 th iteration gives loss of 0.20999380689422237\n",
      "The 16859 th iteration gives loss of 0.20998662063179901\n",
      "The 16860 th iteration gives loss of 0.2099794352823527\n",
      "The 16861 th iteration gives loss of 0.20997225084573978\n",
      "The 16862 th iteration gives loss of 0.20996506732183004\n",
      "The 16863 th iteration gives loss of 0.20995788471047785\n",
      "The 16864 th iteration gives loss of 0.2099507030115449\n",
      "The 16865 th iteration gives loss of 0.20994352222489449\n",
      "The 16866 th iteration gives loss of 0.20993634235038522\n",
      "The 16867 th iteration gives loss of 0.20992916338786993\n",
      "The 16868 th iteration gives loss of 0.2099219853372216\n",
      "The 16869 th iteration gives loss of 0.20991480819829295\n",
      "The 16870 th iteration gives loss of 0.20990763197095133\n",
      "The 16871 th iteration gives loss of 0.20990045665503845\n",
      "The 16872 th iteration gives loss of 0.20989328225043252\n",
      "The 16873 th iteration gives loss of 0.20988610875698982\n",
      "The 16874 th iteration gives loss of 0.20987893617456385\n",
      "The 16875 th iteration gives loss of 0.2098717645030258\n",
      "The 16876 th iteration gives loss of 0.20986459374223343\n",
      "The 16877 th iteration gives loss of 0.2098574238920372\n",
      "The 16878 th iteration gives loss of 0.20985025495230877\n",
      "The 16879 th iteration gives loss of 0.20984308692290257\n",
      "The 16880 th iteration gives loss of 0.2098359198036777\n",
      "The 16881 th iteration gives loss of 0.20982875359450356\n",
      "The 16882 th iteration gives loss of 0.2098215882952363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 16883 th iteration gives loss of 0.20981442390573052\n",
      "The 16884 th iteration gives loss of 0.20980726042584713\n",
      "The 16885 th iteration gives loss of 0.20980009785545178\n",
      "The 16886 th iteration gives loss of 0.209792936194413\n",
      "The 16887 th iteration gives loss of 0.2097857754425709\n",
      "The 16888 th iteration gives loss of 0.2097786155998043\n",
      "The 16889 th iteration gives loss of 0.2097714566659657\n",
      "The 16890 th iteration gives loss of 0.20976429864090984\n",
      "The 16891 th iteration gives loss of 0.20975714152451103\n",
      "The 16892 th iteration gives loss of 0.20974998531661584\n",
      "The 16893 th iteration gives loss of 0.20974283001709124\n",
      "The 16894 th iteration gives loss of 0.20973567562581708\n",
      "The 16895 th iteration gives loss of 0.2097285221426255\n",
      "The 16896 th iteration gives loss of 0.2097213695673871\n",
      "The 16897 th iteration gives loss of 0.20971421789996572\n",
      "The 16898 th iteration gives loss of 0.20970706714021675\n",
      "The 16899 th iteration gives loss of 0.20969991728800577\n",
      "The 16900 th iteration gives loss of 0.2096927683431964\n",
      "The 16901 th iteration gives loss of 0.2096856203056319\n",
      "The 16902 th iteration gives loss of 0.2096784731751998\n",
      "The 16903 th iteration gives loss of 0.20967132695174096\n",
      "The 16904 th iteration gives loss of 0.20966418163512346\n",
      "The 16905 th iteration gives loss of 0.20965703722521545\n",
      "The 16906 th iteration gives loss of 0.20964989372186013\n",
      "The 16907 th iteration gives loss of 0.20964275112493289\n",
      "The 16908 th iteration gives loss of 0.20963560943428286\n",
      "The 16909 th iteration gives loss of 0.20962846864978665\n",
      "The 16910 th iteration gives loss of 0.20962132877129808\n",
      "The 16911 th iteration gives loss of 0.20961418979867172\n",
      "The 16912 th iteration gives loss of 0.2096070517317803\n",
      "The 16913 th iteration gives loss of 0.20959991457047722\n",
      "The 16914 th iteration gives loss of 0.20959277831461326\n",
      "The 16915 th iteration gives loss of 0.20958564296407187\n",
      "The 16916 th iteration gives loss of 0.2095785085187048\n",
      "The 16917 th iteration gives loss of 0.2095713749783703\n",
      "The 16918 th iteration gives loss of 0.20956424234293367\n",
      "The 16919 th iteration gives loss of 0.2095571106122523\n",
      "The 16920 th iteration gives loss of 0.2095499797861991\n",
      "The 16921 th iteration gives loss of 0.20954284986461924\n",
      "The 16922 th iteration gives loss of 0.20953572084737263\n",
      "The 16923 th iteration gives loss of 0.2095285927343335\n",
      "The 16924 th iteration gives loss of 0.2095214655253645\n",
      "The 16925 th iteration gives loss of 0.20951433922031018\n",
      "The 16926 th iteration gives loss of 0.20950721381904336\n",
      "The 16927 th iteration gives loss of 0.20950008932142727\n",
      "The 16928 th iteration gives loss of 0.2094929657273233\n",
      "The 16929 th iteration gives loss of 0.20948584303658524\n",
      "The 16930 th iteration gives loss of 0.20947872124908284\n",
      "The 16931 th iteration gives loss of 0.20947160036466408\n",
      "The 16932 th iteration gives loss of 0.2094644803832108\n",
      "The 16933 th iteration gives loss of 0.2094573613045713\n",
      "The 16934 th iteration gives loss of 0.20945024312861785\n",
      "The 16935 th iteration gives loss of 0.2094431258551862\n",
      "The 16936 th iteration gives loss of 0.209436009484162\n",
      "The 16937 th iteration gives loss of 0.20942889401540352\n",
      "The 16938 th iteration gives loss of 0.20942177944877466\n",
      "The 16939 th iteration gives loss of 0.20941466578412327\n",
      "The 16940 th iteration gives loss of 0.20940755302132577\n",
      "The 16941 th iteration gives loss of 0.20940044116023188\n",
      "The 16942 th iteration gives loss of 0.20939333020071935\n",
      "The 16943 th iteration gives loss of 0.20938622014262948\n",
      "The 16944 th iteration gives loss of 0.20937911098583384\n",
      "The 16945 th iteration gives loss of 0.20937200273020753\n",
      "The 16946 th iteration gives loss of 0.2093648953755895\n",
      "The 16947 th iteration gives loss of 0.20935778892184678\n",
      "The 16948 th iteration gives loss of 0.20935068336885704\n",
      "The 16949 th iteration gives loss of 0.20934357871646772\n",
      "The 16950 th iteration gives loss of 0.20933647496454422\n",
      "The 16951 th iteration gives loss of 0.20932937211294053\n",
      "The 16952 th iteration gives loss of 0.20932227016152896\n",
      "The 16953 th iteration gives loss of 0.20931516911017597\n",
      "The 16954 th iteration gives loss of 0.209308068958726\n",
      "The 16955 th iteration gives loss of 0.20930096970705897\n",
      "The 16956 th iteration gives loss of 0.20929387135502586\n",
      "The 16957 th iteration gives loss of 0.2092867739025\n",
      "The 16958 th iteration gives loss of 0.20927967734932218\n",
      "The 16959 th iteration gives loss of 0.20927258169537347\n",
      "The 16960 th iteration gives loss of 0.20926548694051444\n",
      "The 16961 th iteration gives loss of 0.20925839308459612\n",
      "The 16962 th iteration gives loss of 0.20925130012749366\n",
      "The 16963 th iteration gives loss of 0.20924420806905977\n",
      "The 16964 th iteration gives loss of 0.20923711690916164\n",
      "The 16965 th iteration gives loss of 0.20923002664765353\n",
      "The 16966 th iteration gives loss of 0.20922293728440883\n",
      "The 16967 th iteration gives loss of 0.20921584881928448\n",
      "The 16968 th iteration gives loss of 0.20920876125214588\n",
      "The 16969 th iteration gives loss of 0.20920167458284353\n",
      "The 16970 th iteration gives loss of 0.20919458881124448\n",
      "The 16971 th iteration gives loss of 0.20918750393722818\n",
      "The 16972 th iteration gives loss of 0.20918041996064848\n",
      "The 16973 th iteration gives loss of 0.20917333688134454\n",
      "The 16974 th iteration gives loss of 0.20916625469920835\n",
      "The 16975 th iteration gives loss of 0.20915917341408943\n",
      "The 16976 th iteration gives loss of 0.20915209302585236\n",
      "The 16977 th iteration gives loss of 0.20914501353435958\n",
      "The 16978 th iteration gives loss of 0.20913793493947588\n",
      "The 16979 th iteration gives loss of 0.20913085724105543\n",
      "The 16980 th iteration gives loss of 0.2091237804389664\n",
      "The 16981 th iteration gives loss of 0.20911670453306883\n",
      "The 16982 th iteration gives loss of 0.20910962952323556\n",
      "The 16983 th iteration gives loss of 0.20910255540932066\n",
      "The 16984 th iteration gives loss of 0.20909548219118484\n",
      "The 16985 th iteration gives loss of 0.20908840986868607\n",
      "The 16986 th iteration gives loss of 0.20908133844169874\n",
      "The 16987 th iteration gives loss of 0.20907426791008368\n",
      "The 16988 th iteration gives loss of 0.20906719827369416\n",
      "The 16989 th iteration gives loss of 0.2090601295324028\n",
      "The 16990 th iteration gives loss of 0.20905306168606733\n",
      "The 16991 th iteration gives loss of 0.20904599473455607\n",
      "The 16992 th iteration gives loss of 0.20903892867772444\n",
      "The 16993 th iteration gives loss of 0.20903186351543906\n",
      "The 16994 th iteration gives loss of 0.20902479924756337\n",
      "The 16995 th iteration gives loss of 0.20901773587395966\n",
      "The 16996 th iteration gives loss of 0.2090106733944837\n",
      "The 16997 th iteration gives loss of 0.2090036118090034\n",
      "The 16998 th iteration gives loss of 0.20899655111738744\n",
      "The 16999 th iteration gives loss of 0.20898949131949424\n",
      "The 17000 th iteration gives loss of 0.20898243241518705\n",
      "The 17001 th iteration gives loss of 0.2089753744043233\n",
      "The 17002 th iteration gives loss of 0.20896831728676832\n",
      "The 17003 th iteration gives loss of 0.20896126106239787\n",
      "The 17004 th iteration gives loss of 0.2089542057310617\n",
      "The 17005 th iteration gives loss of 0.20894715129261632\n",
      "The 17006 th iteration gives loss of 0.20894009774694225\n",
      "The 17007 th iteration gives loss of 0.20893304509389138\n",
      "The 17008 th iteration gives loss of 0.20892599333332867\n",
      "The 17009 th iteration gives loss of 0.20891894246512058\n",
      "The 17010 th iteration gives loss of 0.20891189248913072\n",
      "The 17011 th iteration gives loss of 0.2089048434052144\n",
      "The 17012 th iteration gives loss of 0.20889779521324767\n",
      "The 17013 th iteration gives loss of 0.208890747913072\n",
      "The 17014 th iteration gives loss of 0.20888370150457594\n",
      "The 17015 th iteration gives loss of 0.20887665598760213\n",
      "The 17016 th iteration gives loss of 0.20886961136202734\n",
      "The 17017 th iteration gives loss of 0.20886256762770838\n",
      "The 17018 th iteration gives loss of 0.2088555247845073\n",
      "The 17019 th iteration gives loss of 0.2088484828322931\n",
      "The 17020 th iteration gives loss of 0.20884144177092753\n",
      "The 17021 th iteration gives loss of 0.2088344016002766\n",
      "The 17022 th iteration gives loss of 0.20882736232019802\n",
      "The 17023 th iteration gives loss of 0.2088203239305521\n",
      "The 17024 th iteration gives loss of 0.20881328643120783\n",
      "The 17025 th iteration gives loss of 0.20880624982202947\n",
      "The 17026 th iteration gives loss of 0.20879921410287602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 17027 th iteration gives loss of 0.20879217927361163\n",
      "The 17028 th iteration gives loss of 0.2087851453341057\n",
      "The 17029 th iteration gives loss of 0.2087781122842251\n",
      "The 17030 th iteration gives loss of 0.20877108012382545\n",
      "The 17031 th iteration gives loss of 0.20876404885276542\n",
      "The 17032 th iteration gives loss of 0.20875701847091277\n",
      "The 17033 th iteration gives loss of 0.20874998897813013\n",
      "The 17034 th iteration gives loss of 0.2087429603742814\n",
      "The 17035 th iteration gives loss of 0.20873593265923704\n",
      "The 17036 th iteration gives loss of 0.20872890583285061\n",
      "The 17037 th iteration gives loss of 0.20872187989500082\n",
      "The 17038 th iteration gives loss of 0.20871485484553678\n",
      "The 17039 th iteration gives loss of 0.20870783068432414\n",
      "The 17040 th iteration gives loss of 0.20870080741122166\n",
      "The 17041 th iteration gives loss of 0.2086937850261164\n",
      "The 17042 th iteration gives loss of 0.20868676352885396\n",
      "The 17043 th iteration gives loss of 0.20867974291928823\n",
      "The 17044 th iteration gives loss of 0.20867272319730645\n",
      "The 17045 th iteration gives loss of 0.20866570436276055\n",
      "The 17046 th iteration gives loss of 0.2086586864155135\n",
      "The 17047 th iteration gives loss of 0.20865166935542523\n",
      "The 17048 th iteration gives loss of 0.20864465318236558\n",
      "The 17049 th iteration gives loss of 0.20863763789620096\n",
      "The 17050 th iteration gives loss of 0.20863062349679706\n",
      "The 17051 th iteration gives loss of 0.20862360998400936\n",
      "The 17052 th iteration gives loss of 0.20861659735771002\n",
      "The 17053 th iteration gives loss of 0.20860958561774992\n",
      "The 17054 th iteration gives loss of 0.20860257476400323\n",
      "The 17055 th iteration gives loss of 0.20859556479633495\n",
      "The 17056 th iteration gives loss of 0.20858855571460583\n",
      "The 17057 th iteration gives loss of 0.20858154751867744\n",
      "The 17058 th iteration gives loss of 0.20857454020841565\n",
      "The 17059 th iteration gives loss of 0.20856753378368928\n",
      "The 17060 th iteration gives loss of 0.2085605282443635\n",
      "The 17061 th iteration gives loss of 0.2085535235902932\n",
      "The 17062 th iteration gives loss of 0.20854651982135122\n",
      "The 17063 th iteration gives loss of 0.2085395169373927\n",
      "The 17064 th iteration gives loss of 0.20853251493828845\n",
      "The 17065 th iteration gives loss of 0.20852551382390214\n",
      "The 17066 th iteration gives loss of 0.20851851359408918\n",
      "The 17067 th iteration gives loss of 0.20851151424873043\n",
      "The 17068 th iteration gives loss of 0.20850451578768528\n",
      "The 17069 th iteration gives loss of 0.20849751821081014\n",
      "The 17070 th iteration gives loss of 0.20849052151796582\n",
      "The 17071 th iteration gives loss of 0.20848352570903514\n",
      "The 17072 th iteration gives loss of 0.20847653078386047\n",
      "The 17073 th iteration gives loss of 0.20846953674232413\n",
      "The 17074 th iteration gives loss of 0.20846254358428248\n",
      "The 17075 th iteration gives loss of 0.20845555130960938\n",
      "The 17076 th iteration gives loss of 0.20844855991814595\n",
      "The 17077 th iteration gives loss of 0.2084415694097847\n",
      "The 17078 th iteration gives loss of 0.20843457978437402\n",
      "The 17079 th iteration gives loss of 0.2084275910417675\n",
      "The 17080 th iteration gives loss of 0.20842060318185546\n",
      "The 17081 th iteration gives loss of 0.20841361620448864\n",
      "The 17082 th iteration gives loss of 0.20840663010953558\n",
      "The 17083 th iteration gives loss of 0.20839964489686297\n",
      "The 17084 th iteration gives loss of 0.2083926605663204\n",
      "The 17085 th iteration gives loss of 0.20838567711779726\n",
      "The 17086 th iteration gives loss of 0.20837869455113123\n",
      "The 17087 th iteration gives loss of 0.20837171286620312\n",
      "The 17088 th iteration gives loss of 0.20836473206287523\n",
      "The 17089 th iteration gives loss of 0.20835775214101374\n",
      "The 17090 th iteration gives loss of 0.2083507731004806\n",
      "The 17091 th iteration gives loss of 0.20834379494114047\n",
      "The 17092 th iteration gives loss of 0.20833681766286025\n",
      "The 17093 th iteration gives loss of 0.20832984126550141\n",
      "The 17094 th iteration gives loss of 0.20832286574892853\n",
      "The 17095 th iteration gives loss of 0.20831589111300483\n",
      "The 17096 th iteration gives loss of 0.208308917357608\n",
      "The 17097 th iteration gives loss of 0.20830194448259062\n",
      "The 17098 th iteration gives loss of 0.20829497248782117\n",
      "The 17099 th iteration gives loss of 0.20828800137316308\n",
      "The 17100 th iteration gives loss of 0.20828103113848\n",
      "The 17101 th iteration gives loss of 0.20827406178363478\n",
      "The 17102 th iteration gives loss of 0.2082670933085044\n",
      "The 17103 th iteration gives loss of 0.20826012571295305\n",
      "The 17104 th iteration gives loss of 0.20825315899682667\n",
      "The 17105 th iteration gives loss of 0.20824619316000803\n",
      "The 17106 th iteration gives loss of 0.2082392282023491\n",
      "The 17107 th iteration gives loss of 0.20823226412372275\n",
      "The 17108 th iteration gives loss of 0.20822530092399735\n",
      "The 17109 th iteration gives loss of 0.20821833860304131\n",
      "The 17110 th iteration gives loss of 0.20821137716070287\n",
      "The 17111 th iteration gives loss of 0.2082044165968625\n",
      "The 17112 th iteration gives loss of 0.20819745691137845\n",
      "The 17113 th iteration gives loss of 0.20819049810411225\n",
      "The 17114 th iteration gives loss of 0.20818354017495108\n",
      "The 17115 th iteration gives loss of 0.20817658312372678\n",
      "The 17116 th iteration gives loss of 0.20816962695032637\n",
      "The 17117 th iteration gives loss of 0.20816267165461236\n",
      "The 17118 th iteration gives loss of 0.20815571723643883\n",
      "The 17119 th iteration gives loss of 0.2081487636956863\n",
      "The 17120 th iteration gives loss of 0.20814181103221527\n",
      "The 17121 th iteration gives loss of 0.2081348592458794\n",
      "The 17122 th iteration gives loss of 0.2081279083365665\n",
      "The 17123 th iteration gives loss of 0.20812095830411495\n",
      "The 17124 th iteration gives loss of 0.2081140091484143\n",
      "The 17125 th iteration gives loss of 0.20810706086932054\n",
      "The 17126 th iteration gives loss of 0.20810011346669618\n",
      "The 17127 th iteration gives loss of 0.20809316694041607\n",
      "The 17128 th iteration gives loss of 0.208086221290332\n",
      "The 17129 th iteration gives loss of 0.20807927651632446\n",
      "The 17130 th iteration gives loss of 0.20807233261824407\n",
      "The 17131 th iteration gives loss of 0.2080653895959718\n",
      "The 17132 th iteration gives loss of 0.20805844744935117\n",
      "The 17133 th iteration gives loss of 0.2080515061782742\n",
      "The 17134 th iteration gives loss of 0.20804456578258593\n",
      "The 17135 th iteration gives loss of 0.20803762626216088\n",
      "The 17136 th iteration gives loss of 0.20803068761686466\n",
      "The 17137 th iteration gives loss of 0.20802374984656125\n",
      "The 17138 th iteration gives loss of 0.20801681295111032\n",
      "The 17139 th iteration gives loss of 0.2080098769303936\n",
      "The 17140 th iteration gives loss of 0.20800294178427015\n",
      "The 17141 th iteration gives loss of 0.20799600751259198\n",
      "The 17142 th iteration gives loss of 0.20798907411523956\n",
      "The 17143 th iteration gives loss of 0.2079821415920787\n",
      "The 17144 th iteration gives loss of 0.2079752099429738\n",
      "The 17145 th iteration gives loss of 0.20796827916777888\n",
      "The 17146 th iteration gives loss of 0.20796134926636767\n",
      "The 17147 th iteration gives loss of 0.20795442023861888\n",
      "The 17148 th iteration gives loss of 0.20794749208438262\n",
      "The 17149 th iteration gives loss of 0.2079405648035338\n",
      "The 17150 th iteration gives loss of 0.20793363839592754\n",
      "The 17151 th iteration gives loss of 0.20792671286143244\n",
      "The 17152 th iteration gives loss of 0.207919788199923\n",
      "The 17153 th iteration gives loss of 0.2079128644112655\n",
      "The 17154 th iteration gives loss of 0.20790594149531286\n",
      "The 17155 th iteration gives loss of 0.20789901945193973\n",
      "The 17156 th iteration gives loss of 0.20789209828100697\n",
      "The 17157 th iteration gives loss of 0.20788517798239547\n",
      "The 17158 th iteration gives loss of 0.20787825855595457\n",
      "The 17159 th iteration gives loss of 0.20787134000155366\n",
      "The 17160 th iteration gives loss of 0.20786442231907132\n",
      "The 17161 th iteration gives loss of 0.20785750550835996\n",
      "The 17162 th iteration gives loss of 0.20785058956929003\n",
      "The 17163 th iteration gives loss of 0.20784367450172075\n",
      "The 17164 th iteration gives loss of 0.2078367603055364\n",
      "The 17165 th iteration gives loss of 0.2078298469805853\n",
      "The 17166 th iteration gives loss of 0.20782293452674727\n",
      "The 17167 th iteration gives loss of 0.2078160229438653\n",
      "The 17168 th iteration gives loss of 0.20780911223183812\n",
      "The 17169 th iteration gives loss of 0.20780220239051087\n",
      "The 17170 th iteration gives loss of 0.20779529341975678\n",
      "The 17171 th iteration gives loss of 0.20778838531943922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 17172 th iteration gives loss of 0.20778147808943664\n",
      "The 17173 th iteration gives loss of 0.20777457172958547\n",
      "The 17174 th iteration gives loss of 0.20776766623978246\n",
      "The 17175 th iteration gives loss of 0.20776076161987397\n",
      "The 17176 th iteration gives loss of 0.2077538578697457\n",
      "The 17177 th iteration gives loss of 0.20774695498924708\n",
      "The 17178 th iteration gives loss of 0.20774005297825587\n",
      "The 17179 th iteration gives loss of 0.20773315183663035\n",
      "The 17180 th iteration gives loss of 0.20772625156424374\n",
      "The 17181 th iteration gives loss of 0.2077193521609588\n",
      "The 17182 th iteration gives loss of 0.20771245362664392\n",
      "The 17183 th iteration gives loss of 0.20770555596116017\n",
      "The 17184 th iteration gives loss of 0.20769865916437588\n",
      "The 17185 th iteration gives loss of 0.20769176323616628\n",
      "The 17186 th iteration gives loss of 0.20768486817639084\n",
      "The 17187 th iteration gives loss of 0.20767797398492116\n",
      "The 17188 th iteration gives loss of 0.2076710806616144\n",
      "The 17189 th iteration gives loss of 0.20766418820634483\n",
      "The 17190 th iteration gives loss of 0.20765729661898158\n",
      "The 17191 th iteration gives loss of 0.20765040589938122\n",
      "The 17192 th iteration gives loss of 0.20764351604741682\n",
      "The 17193 th iteration gives loss of 0.20763662706295788\n",
      "The 17194 th iteration gives loss of 0.20762973894586165\n",
      "The 17195 th iteration gives loss of 0.20762285169600853\n",
      "The 17196 th iteration gives loss of 0.20761596531325693\n",
      "The 17197 th iteration gives loss of 0.20760907979747603\n",
      "The 17198 th iteration gives loss of 0.20760219514853112\n",
      "The 17199 th iteration gives loss of 0.20759531136628334\n",
      "The 17200 th iteration gives loss of 0.2075884284506129\n",
      "The 17201 th iteration gives loss of 0.20758154640137824\n",
      "The 17202 th iteration gives loss of 0.20757466521844578\n",
      "The 17203 th iteration gives loss of 0.2075677849016837\n",
      "The 17204 th iteration gives loss of 0.20756090545096748\n",
      "The 17205 th iteration gives loss of 0.20755402686614513\n",
      "The 17206 th iteration gives loss of 0.20754714914710345\n",
      "The 17207 th iteration gives loss of 0.2075402722936999\n",
      "The 17208 th iteration gives loss of 0.20753339630580356\n",
      "The 17209 th iteration gives loss of 0.20752652118327283\n",
      "The 17210 th iteration gives loss of 0.20751964692598965\n",
      "The 17211 th iteration gives loss of 0.20751277353381492\n",
      "The 17212 th iteration gives loss of 0.20750590100661553\n",
      "The 17213 th iteration gives loss of 0.2074990293442543\n",
      "The 17214 th iteration gives loss of 0.20749215854660719\n",
      "The 17215 th iteration gives loss of 0.2074852886135409\n",
      "The 17216 th iteration gives loss of 0.2074784195449079\n",
      "The 17217 th iteration gives loss of 0.20747155134058745\n",
      "The 17218 th iteration gives loss of 0.2074646840004523\n",
      "The 17219 th iteration gives loss of 0.20745781752435896\n",
      "The 17220 th iteration gives loss of 0.20745095191217125\n",
      "The 17221 th iteration gives loss of 0.20744408716377083\n",
      "The 17222 th iteration gives loss of 0.20743722327901834\n",
      "The 17223 th iteration gives loss of 0.20743036025777759\n",
      "The 17224 th iteration gives loss of 0.2074234980999246\n",
      "The 17225 th iteration gives loss of 0.2074166368053201\n",
      "The 17226 th iteration gives loss of 0.20740977637382998\n",
      "The 17227 th iteration gives loss of 0.20740291680532522\n",
      "The 17228 th iteration gives loss of 0.2073960580996734\n",
      "The 17229 th iteration gives loss of 0.2073892002567459\n",
      "The 17230 th iteration gives loss of 0.20738234327639865\n",
      "The 17231 th iteration gives loss of 0.20737548715850532\n",
      "The 17232 th iteration gives loss of 0.20736863190294086\n",
      "The 17233 th iteration gives loss of 0.20736177750955606\n",
      "The 17234 th iteration gives loss of 0.2073549239782427\n",
      "The 17235 th iteration gives loss of 0.20734807130884497\n",
      "The 17236 th iteration gives loss of 0.2073412195012452\n",
      "The 17237 th iteration gives loss of 0.2073343685552966\n",
      "The 17238 th iteration gives loss of 0.20732751847088476\n",
      "The 17239 th iteration gives loss of 0.20732066924787174\n",
      "The 17240 th iteration gives loss of 0.20731382088611916\n",
      "The 17241 th iteration gives loss of 0.2073069733854898\n",
      "The 17242 th iteration gives loss of 0.20730012674586631\n",
      "The 17243 th iteration gives loss of 0.20729328096711094\n",
      "The 17244 th iteration gives loss of 0.20728643604908484\n",
      "The 17245 th iteration gives loss of 0.20727959199166027\n",
      "The 17246 th iteration gives loss of 0.20727274879470706\n",
      "The 17247 th iteration gives loss of 0.20726590645809698\n",
      "The 17248 th iteration gives loss of 0.20725906498168958\n",
      "The 17249 th iteration gives loss of 0.20725222436535246\n",
      "The 17250 th iteration gives loss of 0.20724538460896155\n",
      "The 17251 th iteration gives loss of 0.20723854571237788\n",
      "The 17252 th iteration gives loss of 0.20723170767547497\n",
      "The 17253 th iteration gives loss of 0.2072248704981178\n",
      "The 17254 th iteration gives loss of 0.20721803418017165\n",
      "The 17255 th iteration gives loss of 0.20721119872151325\n",
      "The 17256 th iteration gives loss of 0.20720436412199417\n",
      "The 17257 th iteration gives loss of 0.20719753038149435\n",
      "The 17258 th iteration gives loss of 0.20719069749987895\n",
      "The 17259 th iteration gives loss of 0.20718386547701798\n",
      "The 17260 th iteration gives loss of 0.20717703431279202\n",
      "The 17261 th iteration gives loss of 0.20717020400704328\n",
      "The 17262 th iteration gives loss of 0.207163374559653\n",
      "The 17263 th iteration gives loss of 0.20715654597049013\n",
      "The 17264 th iteration gives loss of 0.20714971823942105\n",
      "The 17265 th iteration gives loss of 0.20714289136632266\n",
      "The 17266 th iteration gives loss of 0.20713606535103968\n",
      "The 17267 th iteration gives loss of 0.20712924019346426\n",
      "The 17268 th iteration gives loss of 0.20712241589345567\n",
      "The 17269 th iteration gives loss of 0.20711559245088082\n",
      "The 17270 th iteration gives loss of 0.20710876986561394\n",
      "The 17271 th iteration gives loss of 0.20710194813752453\n",
      "The 17272 th iteration gives loss of 0.20709512726646884\n",
      "The 17273 th iteration gives loss of 0.2070883072523204\n",
      "The 17274 th iteration gives loss of 0.2070814880949604\n",
      "The 17275 th iteration gives loss of 0.207074669794231\n",
      "The 17276 th iteration gives loss of 0.2070678523500237\n",
      "The 17277 th iteration gives loss of 0.2070610357622002\n",
      "The 17278 th iteration gives loss of 0.20705422003062912\n",
      "The 17279 th iteration gives loss of 0.20704740515516987\n",
      "The 17280 th iteration gives loss of 0.2070405911357057\n",
      "The 17281 th iteration gives loss of 0.2070337779720989\n",
      "The 17282 th iteration gives loss of 0.20702696566420736\n",
      "The 17283 th iteration gives loss of 0.20702015421191655\n",
      "The 17284 th iteration gives loss of 0.20701334361508641\n",
      "The 17285 th iteration gives loss of 0.2070065338735855\n",
      "The 17286 th iteration gives loss of 0.20699972498728839\n",
      "The 17287 th iteration gives loss of 0.2069929169560546\n",
      "The 17288 th iteration gives loss of 0.20698610977975998\n",
      "The 17289 th iteration gives loss of 0.20697930345827573\n",
      "The 17290 th iteration gives loss of 0.20697249799145873\n",
      "The 17291 th iteration gives loss of 0.206965693379192\n",
      "The 17292 th iteration gives loss of 0.2069588896213253\n",
      "The 17293 th iteration gives loss of 0.2069520867177512\n",
      "The 17294 th iteration gives loss of 0.20694528466831866\n",
      "The 17295 th iteration gives loss of 0.20693848347291066\n",
      "The 17296 th iteration gives loss of 0.20693168313138469\n",
      "The 17297 th iteration gives loss of 0.20692488364361378\n",
      "The 17298 th iteration gives loss of 0.20691808500947256\n",
      "The 17299 th iteration gives loss of 0.20691128722882066\n",
      "The 17300 th iteration gives loss of 0.20690449030152674\n",
      "The 17301 th iteration gives loss of 0.2068976942274706\n",
      "The 17302 th iteration gives loss of 0.20689089900651148\n",
      "The 17303 th iteration gives loss of 0.2068841046385201\n",
      "The 17304 th iteration gives loss of 0.20687731112336433\n",
      "The 17305 th iteration gives loss of 0.20687051846092822\n",
      "The 17306 th iteration gives loss of 0.20686372665106031\n",
      "The 17307 th iteration gives loss of 0.2068569356936336\n",
      "The 17308 th iteration gives loss of 0.2068501455885238\n",
      "The 17309 th iteration gives loss of 0.20684335633559883\n",
      "The 17310 th iteration gives loss of 0.2068365679347261\n",
      "The 17311 th iteration gives loss of 0.2068297803857722\n",
      "The 17312 th iteration gives loss of 0.2068229936886106\n",
      "The 17313 th iteration gives loss of 0.20681620784311097\n",
      "The 17314 th iteration gives loss of 0.20680942284913667\n",
      "The 17315 th iteration gives loss of 0.2068026387065578\n",
      "The 17316 th iteration gives loss of 0.20679585541524625\n",
      "The 17317 th iteration gives loss of 0.2067890729750808\n",
      "The 17318 th iteration gives loss of 0.20678229138591506\n",
      "The 17319 th iteration gives loss of 0.20677551064762553\n",
      "The 17320 th iteration gives loss of 0.20676873076007798\n",
      "The 17321 th iteration gives loss of 0.20676195172314263\n",
      "The 17322 th iteration gives loss of 0.20675517353669123\n",
      "The 17323 th iteration gives loss of 0.20674839620059188\n",
      "The 17324 th iteration gives loss of 0.2067416197147121\n",
      "The 17325 th iteration gives loss of 0.20673484407892573\n",
      "The 17326 th iteration gives loss of 0.206728069293103\n",
      "The 17327 th iteration gives loss of 0.2067212953571097\n",
      "The 17328 th iteration gives loss of 0.20671452227081308\n",
      "The 17329 th iteration gives loss of 0.20670775003408184\n",
      "The 17330 th iteration gives loss of 0.20670097864678877\n",
      "The 17331 th iteration gives loss of 0.2066942081088075\n",
      "The 17332 th iteration gives loss of 0.2066874384199948\n",
      "The 17333 th iteration gives loss of 0.20668066958024148\n",
      "The 17334 th iteration gives loss of 0.20667390158940366\n",
      "The 17335 th iteration gives loss of 0.20666713444734452\n",
      "The 17336 th iteration gives loss of 0.20666036815394084\n",
      "The 17337 th iteration gives loss of 0.2066536027090707\n",
      "The 17338 th iteration gives loss of 0.20664683811259119\n",
      "The 17339 th iteration gives loss of 0.20664007436437454\n",
      "The 17340 th iteration gives loss of 0.20663331146429434\n",
      "The 17341 th iteration gives loss of 0.20662654941220626\n",
      "The 17342 th iteration gives loss of 0.20661978820799973\n",
      "The 17343 th iteration gives loss of 0.20661302785154131\n",
      "The 17344 th iteration gives loss of 0.20660626834269308\n",
      "The 17345 th iteration gives loss of 0.2065995096813222\n",
      "The 17346 th iteration gives loss of 0.20659275186730322\n",
      "The 17347 th iteration gives loss of 0.20658599490051452\n",
      "The 17348 th iteration gives loss of 0.20657923878081397\n",
      "The 17349 th iteration gives loss of 0.2065724835080709\n",
      "The 17350 th iteration gives loss of 0.2065657290821701\n",
      "The 17351 th iteration gives loss of 0.20655897550296315\n",
      "The 17352 th iteration gives loss of 0.20655222277033064\n",
      "The 17353 th iteration gives loss of 0.20654547088413824\n",
      "The 17354 th iteration gives loss of 0.20653871984425515\n",
      "The 17355 th iteration gives loss of 0.20653196965055604\n",
      "The 17356 th iteration gives loss of 0.20652522030291298\n",
      "The 17357 th iteration gives loss of 0.20651847180119082\n",
      "The 17358 th iteration gives loss of 0.20651172414525282\n",
      "The 17359 th iteration gives loss of 0.20650497733498094\n",
      "The 17360 th iteration gives loss of 0.20649823137023535\n",
      "The 17361 th iteration gives loss of 0.2064914862508942\n",
      "The 17362 th iteration gives loss of 0.20648474197683334\n",
      "The 17363 th iteration gives loss of 0.20647799854790438\n",
      "The 17364 th iteration gives loss of 0.20647125596399538\n",
      "The 17365 th iteration gives loss of 0.2064645142249582\n",
      "The 17366 th iteration gives loss of 0.2064577733306786\n",
      "The 17367 th iteration gives loss of 0.20645103328102446\n",
      "The 17368 th iteration gives loss of 0.20644429407586207\n",
      "The 17369 th iteration gives loss of 0.20643755571505842\n",
      "The 17370 th iteration gives loss of 0.20643081819848996\n",
      "The 17371 th iteration gives loss of 0.20642408152602784\n",
      "The 17372 th iteration gives loss of 0.20641734569753684\n",
      "The 17373 th iteration gives loss of 0.20641061071289155\n",
      "The 17374 th iteration gives loss of 0.20640387657195353\n",
      "The 17375 th iteration gives loss of 0.20639714327461023\n",
      "The 17376 th iteration gives loss of 0.20639041082071893\n",
      "The 17377 th iteration gives loss of 0.20638367921015738\n",
      "The 17378 th iteration gives loss of 0.2063769484427826\n",
      "The 17379 th iteration gives loss of 0.20637021851848678\n",
      "The 17380 th iteration gives loss of 0.20636348943712418\n",
      "The 17381 th iteration gives loss of 0.20635676119856328\n",
      "The 17382 th iteration gives loss of 0.20635003380268177\n",
      "The 17383 th iteration gives loss of 0.20634330724934571\n",
      "The 17384 th iteration gives loss of 0.206336581538431\n",
      "The 17385 th iteration gives loss of 0.2063298566698139\n",
      "The 17386 th iteration gives loss of 0.20632313264334917\n",
      "The 17387 th iteration gives loss of 0.2063164094589176\n",
      "The 17388 th iteration gives loss of 0.20630968711638167\n",
      "The 17389 th iteration gives loss of 0.20630296561562186\n",
      "The 17390 th iteration gives loss of 0.20629624495650575\n",
      "The 17391 th iteration gives loss of 0.2062895251388976\n",
      "The 17392 th iteration gives loss of 0.20628280616267822\n",
      "The 17393 th iteration gives loss of 0.20627608802771694\n",
      "The 17394 th iteration gives loss of 0.20626937073388232\n",
      "The 17395 th iteration gives loss of 0.20626265428103913\n",
      "The 17396 th iteration gives loss of 0.20625593866906478\n",
      "The 17397 th iteration gives loss of 0.2062492238978217\n",
      "The 17398 th iteration gives loss of 0.20624250996718646\n",
      "The 17399 th iteration gives loss of 0.20623579687704144\n",
      "The 17400 th iteration gives loss of 0.20622908462724096\n",
      "The 17401 th iteration gives loss of 0.20622237321766065\n",
      "The 17402 th iteration gives loss of 0.20621566264816651\n",
      "The 17403 th iteration gives loss of 0.20620895291864547\n",
      "The 17404 th iteration gives loss of 0.2062022440289532\n",
      "The 17405 th iteration gives loss of 0.2061955359789636\n",
      "The 17406 th iteration gives loss of 0.20618882876854028\n",
      "The 17407 th iteration gives loss of 0.2061821223975847\n",
      "The 17408 th iteration gives loss of 0.20617541686593432\n",
      "The 17409 th iteration gives loss of 0.2061687121734736\n",
      "The 17410 th iteration gives loss of 0.20616200832007667\n",
      "The 17411 th iteration gives loss of 0.20615530530560924\n",
      "The 17412 th iteration gives loss of 0.20614860312993952\n",
      "The 17413 th iteration gives loss of 0.20614190179295022\n",
      "The 17414 th iteration gives loss of 0.20613520129449703\n",
      "The 17415 th iteration gives loss of 0.20612850163445795\n",
      "The 17416 th iteration gives loss of 0.2061218028127074\n",
      "The 17417 th iteration gives loss of 0.2061151048291101\n",
      "The 17418 th iteration gives loss of 0.20610840768355115\n",
      "The 17419 th iteration gives loss of 0.2061017113758877\n",
      "The 17420 th iteration gives loss of 0.20609501590599374\n",
      "The 17421 th iteration gives loss of 0.20608832127373816\n",
      "The 17422 th iteration gives loss of 0.20608162747900535\n",
      "The 17423 th iteration gives loss of 0.20607493452165598\n",
      "The 17424 th iteration gives loss of 0.20606824240155527\n",
      "The 17425 th iteration gives loss of 0.2060615511185882\n",
      "The 17426 th iteration gives loss of 0.20605486067262543\n",
      "The 17427 th iteration gives loss of 0.2060481710635225\n",
      "The 17428 th iteration gives loss of 0.2060414822911625\n",
      "The 17429 th iteration gives loss of 0.20603479435541272\n",
      "The 17430 th iteration gives loss of 0.20602810725614704\n",
      "The 17431 th iteration gives loss of 0.20602142099323353\n",
      "The 17432 th iteration gives loss of 0.20601473556656583\n",
      "The 17433 th iteration gives loss of 0.20600805097598326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 17434 th iteration gives loss of 0.2060013672213709\n",
      "The 17435 th iteration gives loss of 0.2059946843026037\n",
      "The 17436 th iteration gives loss of 0.20598800221954913\n",
      "The 17437 th iteration gives loss of 0.20598132097207944\n",
      "The 17438 th iteration gives loss of 0.2059746405600674\n",
      "The 17439 th iteration gives loss of 0.2059679609833779\n",
      "The 17440 th iteration gives loss of 0.2059612822418874\n",
      "The 17441 th iteration gives loss of 0.20595460433547363\n",
      "The 17442 th iteration gives loss of 0.20594792726400543\n",
      "The 17443 th iteration gives loss of 0.205941251027346\n",
      "The 17444 th iteration gives loss of 0.20593457562537812\n",
      "The 17445 th iteration gives loss of 0.20592790105796435\n",
      "The 17446 th iteration gives loss of 0.20592122732497561\n",
      "The 17447 th iteration gives loss of 0.20591455442628803\n",
      "The 17448 th iteration gives loss of 0.20590788236178376\n",
      "The 17449 th iteration gives loss of 0.205901211131316\n",
      "The 17450 th iteration gives loss of 0.20589454073477406\n",
      "The 17451 th iteration gives loss of 0.20588787117201285\n",
      "The 17452 th iteration gives loss of 0.20588120244291483\n",
      "The 17453 th iteration gives loss of 0.20587453454734395\n",
      "The 17454 th iteration gives loss of 0.20586786748518296\n",
      "The 17455 th iteration gives loss of 0.2058612012563025\n",
      "The 17456 th iteration gives loss of 0.2058545358605542\n",
      "The 17457 th iteration gives loss of 0.20584787129783802\n",
      "The 17458 th iteration gives loss of 0.20584120756800772\n",
      "The 17459 th iteration gives loss of 0.2058345446709475\n",
      "The 17460 th iteration gives loss of 0.20582788260652213\n",
      "The 17461 th iteration gives loss of 0.20582122137459533\n",
      "The 17462 th iteration gives loss of 0.20581456097505718\n",
      "The 17463 th iteration gives loss of 0.20580790140777103\n",
      "The 17464 th iteration gives loss of 0.20580124267260727\n",
      "The 17465 th iteration gives loss of 0.2057945847694381\n",
      "The 17466 th iteration gives loss of 0.2057879276981352\n",
      "The 17467 th iteration gives loss of 0.20578127145857503\n",
      "The 17468 th iteration gives loss of 0.20577461605062966\n",
      "The 17469 th iteration gives loss of 0.20576796147417067\n",
      "The 17470 th iteration gives loss of 0.20576130772906565\n",
      "The 17471 th iteration gives loss of 0.20575465481518718\n",
      "The 17472 th iteration gives loss of 0.20574800273240793\n",
      "The 17473 th iteration gives loss of 0.2057413514806113\n",
      "The 17474 th iteration gives loss of 0.20573470105965808\n",
      "The 17475 th iteration gives loss of 0.20572805146941486\n",
      "The 17476 th iteration gives loss of 0.20572140270977\n",
      "The 17477 th iteration gives loss of 0.205714754780589\n",
      "The 17478 th iteration gives loss of 0.2057081076817399\n",
      "The 17479 th iteration gives loss of 0.20570146141308487\n",
      "The 17480 th iteration gives loss of 0.20569481597453584\n",
      "The 17481 th iteration gives loss of 0.20568817136592255\n",
      "The 17482 th iteration gives loss of 0.20568152758713887\n",
      "The 17483 th iteration gives loss of 0.20567488463804706\n",
      "The 17484 th iteration gives loss of 0.20566824251852503\n",
      "The 17485 th iteration gives loss of 0.2056616012284473\n",
      "The 17486 th iteration gives loss of 0.2056549607676892\n",
      "The 17487 th iteration gives loss of 0.2056483211361124\n",
      "The 17488 th iteration gives loss of 0.20564168233359625\n",
      "The 17489 th iteration gives loss of 0.2056350443600138\n",
      "The 17490 th iteration gives loss of 0.2056284072152354\n",
      "The 17491 th iteration gives loss of 0.2056217708991306\n",
      "The 17492 th iteration gives loss of 0.20561513541158533\n",
      "The 17493 th iteration gives loss of 0.205608500752454\n",
      "The 17494 th iteration gives loss of 0.20560186692161683\n",
      "The 17495 th iteration gives loss of 0.20559523391894938\n",
      "The 17496 th iteration gives loss of 0.20558860174432522\n",
      "The 17497 th iteration gives loss of 0.20558197039760737\n",
      "The 17498 th iteration gives loss of 0.2055753398786838\n",
      "The 17499 th iteration gives loss of 0.2055687101874129\n",
      "The 17500 th iteration gives loss of 0.20556208132367756\n",
      "The 17501 th iteration gives loss of 0.20555545328733846\n",
      "The 17502 th iteration gives loss of 0.2055488260782792\n",
      "The 17503 th iteration gives loss of 0.20554219969637919\n",
      "The 17504 th iteration gives loss of 0.20553557414149584\n",
      "The 17505 th iteration gives loss of 0.20552894941351052\n",
      "The 17506 th iteration gives loss of 0.20552232551229022\n",
      "The 17507 th iteration gives loss of 0.20551570243770828\n",
      "The 17508 th iteration gives loss of 0.205509080189648\n",
      "The 17509 th iteration gives loss of 0.20550245876796225\n",
      "The 17510 th iteration gives loss of 0.2054958381725459\n",
      "The 17511 th iteration gives loss of 0.20548921840326326\n",
      "The 17512 th iteration gives loss of 0.20548259945998626\n",
      "The 17513 th iteration gives loss of 0.2054759813425837\n",
      "The 17514 th iteration gives loss of 0.20546936405093832\n",
      "The 17515 th iteration gives loss of 0.20546274758490699\n",
      "The 17516 th iteration gives loss of 0.20545613194438878\n",
      "The 17517 th iteration gives loss of 0.20544951712923726\n",
      "The 17518 th iteration gives loss of 0.20544290313933256\n",
      "The 17519 th iteration gives loss of 0.20543628997453467\n",
      "The 17520 th iteration gives loss of 0.20542967763474315\n",
      "The 17521 th iteration gives loss of 0.20542306611980304\n",
      "The 17522 th iteration gives loss of 0.2054164554296024\n",
      "The 17523 th iteration gives loss of 0.20540984556401243\n",
      "The 17524 th iteration gives loss of 0.20540323652291229\n",
      "The 17525 th iteration gives loss of 0.20539662830615496\n",
      "The 17526 th iteration gives loss of 0.20539002091364045\n",
      "The 17527 th iteration gives loss of 0.20538341434522867\n",
      "The 17528 th iteration gives loss of 0.20537680860078272\n",
      "The 17529 th iteration gives loss of 0.2053702036801941\n",
      "The 17530 th iteration gives loss of 0.20536359958332281\n",
      "The 17531 th iteration gives loss of 0.205356996310055\n",
      "The 17532 th iteration gives loss of 0.20535039386025414\n",
      "The 17533 th iteration gives loss of 0.20534379223380234\n",
      "The 17534 th iteration gives loss of 0.20533719143055537\n",
      "The 17535 th iteration gives loss of 0.2053305914504086\n",
      "The 17536 th iteration gives loss of 0.20532399229321632\n",
      "The 17537 th iteration gives loss of 0.2053173939588688\n",
      "The 17538 th iteration gives loss of 0.2053107964472271\n",
      "The 17539 th iteration gives loss of 0.20530419975816705\n",
      "The 17540 th iteration gives loss of 0.2052976038915689\n",
      "The 17541 th iteration gives loss of 0.20529100884730303\n",
      "The 17542 th iteration gives loss of 0.20528441462523828\n",
      "The 17543 th iteration gives loss of 0.2052778212252557\n",
      "The 17544 th iteration gives loss of 0.2052712286472208\n",
      "The 17545 th iteration gives loss of 0.20526463689101598\n",
      "The 17546 th iteration gives loss of 0.20525804595650735\n",
      "The 17547 th iteration gives loss of 0.2052514558435754\n",
      "The 17548 th iteration gives loss of 0.2052448665520856\n",
      "The 17549 th iteration gives loss of 0.20523827808191356\n",
      "The 17550 th iteration gives loss of 0.20523169043293837\n",
      "The 17551 th iteration gives loss of 0.20522510360503077\n",
      "The 17552 th iteration gives loss of 0.205218517598069\n",
      "The 17553 th iteration gives loss of 0.2052119324119131\n",
      "The 17554 th iteration gives loss of 0.2052053480464576\n",
      "The 17555 th iteration gives loss of 0.20519876450154684\n",
      "The 17556 th iteration gives loss of 0.20519218177708795\n",
      "The 17557 th iteration gives loss of 0.2051855998729359\n",
      "The 17558 th iteration gives loss of 0.20517901878896405\n",
      "The 17559 th iteration gives loss of 0.20517243852505868\n",
      "The 17560 th iteration gives loss of 0.20516585908107512\n",
      "The 17561 th iteration gives loss of 0.20515928045689477\n",
      "The 17562 th iteration gives loss of 0.205152702652396\n",
      "The 17563 th iteration gives loss of 0.20514612566746343\n",
      "The 17564 th iteration gives loss of 0.20513954950194926\n",
      "The 17565 th iteration gives loss of 0.2051329741557342\n",
      "The 17566 th iteration gives loss of 0.20512639962870197\n",
      "The 17567 th iteration gives loss of 0.20511982592071973\n",
      "The 17568 th iteration gives loss of 0.20511325303165817\n",
      "The 17569 th iteration gives loss of 0.20510668096139759\n",
      "The 17570 th iteration gives loss of 0.20510010970980802\n",
      "The 17571 th iteration gives loss of 0.2050935392767619\n",
      "The 17572 th iteration gives loss of 0.20508696966213635\n",
      "The 17573 th iteration gives loss of 0.2050804008658088\n",
      "The 17574 th iteration gives loss of 0.20507383288763556\n",
      "The 17575 th iteration gives loss of 0.20506726572751965\n",
      "The 17576 th iteration gives loss of 0.20506069938532218\n",
      "The 17577 th iteration gives loss of 0.20505413386090773\n",
      "The 17578 th iteration gives loss of 0.20504756915415862\n",
      "The 17579 th iteration gives loss of 0.20504100526495275\n",
      "The 17580 th iteration gives loss of 0.20503444219316252\n",
      "The 17581 th iteration gives loss of 0.20502787993866023\n",
      "The 17582 th iteration gives loss of 0.2050213185013175\n",
      "The 17583 th iteration gives loss of 0.2050147578810107\n",
      "The 17584 th iteration gives loss of 0.20500819807761314\n",
      "The 17585 th iteration gives loss of 0.20500163909101343\n",
      "The 17586 th iteration gives loss of 0.2049950809210645\n",
      "The 17587 th iteration gives loss of 0.2049885235676492\n",
      "The 17588 th iteration gives loss of 0.20498196703064328\n",
      "The 17589 th iteration gives loss of 0.2049754113099247\n",
      "The 17590 th iteration gives loss of 0.20496885640535922\n",
      "The 17591 th iteration gives loss of 0.20496230231682816\n",
      "The 17592 th iteration gives loss of 0.2049557490442045\n",
      "The 17593 th iteration gives loss of 0.20494919658736294\n",
      "The 17594 th iteration gives loss of 0.20494264494617237\n",
      "The 17595 th iteration gives loss of 0.2049360941205178\n",
      "The 17596 th iteration gives loss of 0.20492954411026756\n",
      "The 17597 th iteration gives loss of 0.20492299491530044\n",
      "The 17598 th iteration gives loss of 0.20491644653547883\n",
      "The 17599 th iteration gives loss of 0.20490989897068837\n",
      "The 17600 th iteration gives loss of 0.2049033522208103\n",
      "The 17601 th iteration gives loss of 0.204896806285703\n",
      "The 17602 th iteration gives loss of 0.20489026116524656\n",
      "The 17603 th iteration gives loss of 0.2048837168593179\n",
      "The 17604 th iteration gives loss of 0.2048771733677967\n",
      "The 17605 th iteration gives loss of 0.20487063069055045\n",
      "The 17606 th iteration gives loss of 0.20486408882745183\n",
      "The 17607 th iteration gives loss of 0.20485754777838203\n",
      "The 17608 th iteration gives loss of 0.20485100754321164\n",
      "The 17609 th iteration gives loss of 0.2048444681218271\n",
      "The 17610 th iteration gives loss of 0.20483792951408863\n",
      "The 17611 th iteration gives loss of 0.2048313917198735\n",
      "The 17612 th iteration gives loss of 0.20482485473906162\n",
      "The 17613 th iteration gives loss of 0.20481831857152843\n",
      "The 17614 th iteration gives loss of 0.2048117832171363\n",
      "The 17615 th iteration gives loss of 0.2048052486757763\n",
      "The 17616 th iteration gives loss of 0.20479871494731133\n",
      "The 17617 th iteration gives loss of 0.20479218203162414\n",
      "The 17618 th iteration gives loss of 0.20478564992859102\n",
      "The 17619 th iteration gives loss of 0.20477911863808856\n",
      "The 17620 th iteration gives loss of 0.2047725881599758\n",
      "The 17621 th iteration gives loss of 0.20476605849414053\n",
      "The 17622 th iteration gives loss of 0.2047595296404618\n",
      "The 17623 th iteration gives loss of 0.20475300159881502\n",
      "The 17624 th iteration gives loss of 0.2047464743690546\n",
      "The 17625 th iteration gives loss of 0.20473994795108164\n",
      "The 17626 th iteration gives loss of 0.2047334223447448\n",
      "The 17627 th iteration gives loss of 0.2047268975499469\n",
      "The 17628 th iteration gives loss of 0.2047203735665467\n",
      "The 17629 th iteration gives loss of 0.20471385039442463\n",
      "The 17630 th iteration gives loss of 0.20470732803345487\n",
      "The 17631 th iteration gives loss of 0.20470080648351544\n",
      "The 17632 th iteration gives loss of 0.20469428574447526\n",
      "The 17633 th iteration gives loss of 0.20468776581621423\n",
      "The 17634 th iteration gives loss of 0.20468124669860377\n",
      "The 17635 th iteration gives loss of 0.20467472839152034\n",
      "The 17636 th iteration gives loss of 0.20466821089484732\n",
      "The 17637 th iteration gives loss of 0.2046616942084429\n",
      "The 17638 th iteration gives loss of 0.20465517833220473\n",
      "The 17639 th iteration gives loss of 0.20464866326598202\n",
      "The 17640 th iteration gives loss of 0.20464214900967811\n",
      "The 17641 th iteration gives loss of 0.20463563556315487\n",
      "The 17642 th iteration gives loss of 0.20462912292628477\n",
      "The 17643 th iteration gives loss of 0.20462261109893906\n",
      "The 17644 th iteration gives loss of 0.20461610008100228\n",
      "The 17645 th iteration gives loss of 0.20460958987235334\n",
      "The 17646 th iteration gives loss of 0.20460308047286263\n",
      "The 17647 th iteration gives loss of 0.20459657188240765\n",
      "The 17648 th iteration gives loss of 0.20459006410085642\n",
      "The 17649 th iteration gives loss of 0.20458355712808876\n",
      "The 17650 th iteration gives loss of 0.20457705096398354\n",
      "The 17651 th iteration gives loss of 0.2045705456084165\n",
      "The 17652 th iteration gives loss of 0.2045640410612524\n",
      "The 17653 th iteration gives loss of 0.20455753732238824\n",
      "The 17654 th iteration gives loss of 0.20455103439167233\n",
      "The 17655 th iteration gives loss of 0.20454453226901126\n",
      "The 17656 th iteration gives loss of 0.2045380309542592\n",
      "The 17657 th iteration gives loss of 0.204531530447289\n",
      "The 17658 th iteration gives loss of 0.20452503074798725\n",
      "The 17659 th iteration gives loss of 0.20451853185623647\n",
      "The 17660 th iteration gives loss of 0.20451203377190025\n",
      "The 17661 th iteration gives loss of 0.2045055364948474\n",
      "The 17662 th iteration gives loss of 0.2044990400249786\n",
      "The 17663 th iteration gives loss of 0.2044925443621423\n",
      "The 17664 th iteration gives loss of 0.20448604950623006\n",
      "The 17665 th iteration gives loss of 0.2044795554571112\n",
      "The 17666 th iteration gives loss of 0.20447306221467032\n",
      "The 17667 th iteration gives loss of 0.20446656977877326\n",
      "The 17668 th iteration gives loss of 0.20446007814930642\n",
      "The 17669 th iteration gives loss of 0.20445358732614344\n",
      "The 17670 th iteration gives loss of 0.20444709730914537\n",
      "The 17671 th iteration gives loss of 0.2044406080982063\n",
      "The 17672 th iteration gives loss of 0.2044341196931924\n",
      "The 17673 th iteration gives loss of 0.20442763209398704\n",
      "The 17674 th iteration gives loss of 0.20442114530045816\n",
      "The 17675 th iteration gives loss of 0.20441465931248476\n",
      "The 17676 th iteration gives loss of 0.20440817412994636\n",
      "The 17677 th iteration gives loss of 0.2044016897527163\n",
      "The 17678 th iteration gives loss of 0.2043952061806629\n",
      "The 17679 th iteration gives loss of 0.20438872341367495\n",
      "The 17680 th iteration gives loss of 0.20438224145163472\n",
      "The 17681 th iteration gives loss of 0.20437576029439308\n",
      "The 17682 th iteration gives loss of 0.20436927994184872\n",
      "The 17683 th iteration gives loss of 0.2043628003938675\n",
      "The 17684 th iteration gives loss of 0.2043563216503302\n",
      "The 17685 th iteration gives loss of 0.20434984371110593\n",
      "The 17686 th iteration gives loss of 0.20434336657608285\n",
      "The 17687 th iteration gives loss of 0.2043368902451217\n",
      "The 17688 th iteration gives loss of 0.2043304147181171\n",
      "The 17689 th iteration gives loss of 0.20432393999492135\n",
      "The 17690 th iteration gives loss of 0.20431746607544016\n",
      "The 17691 th iteration gives loss of 0.2043109929595345\n",
      "The 17692 th iteration gives loss of 0.20430452064706728\n",
      "The 17693 th iteration gives loss of 0.2042980491379404\n",
      "The 17694 th iteration gives loss of 0.20429157843201545\n",
      "The 17695 th iteration gives loss of 0.20428510852917245\n",
      "The 17696 th iteration gives loss of 0.2042786394292896\n",
      "The 17697 th iteration gives loss of 0.20427217113223475\n",
      "The 17698 th iteration gives loss of 0.20426570363789295\n",
      "The 17699 th iteration gives loss of 0.20425923694614473\n",
      "The 17700 th iteration gives loss of 0.20425277105686054\n",
      "The 17701 th iteration gives loss of 0.20424630596990379\n",
      "The 17702 th iteration gives loss of 0.20423984168517456\n",
      "The 17703 th iteration gives loss of 0.20423337820253737\n",
      "The 17704 th iteration gives loss of 0.20422691552187366\n",
      "The 17705 th iteration gives loss of 0.20422045364305444\n",
      "The 17706 th iteration gives loss of 0.20421399256595538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 17707 th iteration gives loss of 0.20420753229046437\n",
      "The 17708 th iteration gives loss of 0.20420107281643995\n",
      "The 17709 th iteration gives loss of 0.2041946141437799\n",
      "The 17710 th iteration gives loss of 0.20418815627233736\n",
      "The 17711 th iteration gives loss of 0.204181699202007\n",
      "The 17712 th iteration gives loss of 0.20417524293266243\n",
      "The 17713 th iteration gives loss of 0.2041687874641724\n",
      "The 17714 th iteration gives loss of 0.20416233279643206\n",
      "The 17715 th iteration gives loss of 0.20415587892929873\n",
      "The 17716 th iteration gives loss of 0.20414942586266008\n",
      "The 17717 th iteration gives loss of 0.2041429735963761\n",
      "The 17718 th iteration gives loss of 0.20413652213034833\n",
      "The 17719 th iteration gives loss of 0.20413007146443937\n",
      "The 17720 th iteration gives loss of 0.20412362159853464\n",
      "The 17721 th iteration gives loss of 0.20411717253249587\n",
      "The 17722 th iteration gives loss of 0.20411072426622143\n",
      "The 17723 th iteration gives loss of 0.20410427679956358\n",
      "The 17724 th iteration gives loss of 0.2040978301324205\n",
      "The 17725 th iteration gives loss of 0.20409138426465875\n",
      "The 17726 th iteration gives loss of 0.20408493919615983\n",
      "The 17727 th iteration gives loss of 0.20407849492679678\n",
      "The 17728 th iteration gives loss of 0.2040720514564415\n",
      "The 17729 th iteration gives loss of 0.2040656087849878\n",
      "The 17730 th iteration gives loss of 0.20405916691229184\n",
      "The 17731 th iteration gives loss of 0.2040527258382413\n",
      "The 17732 th iteration gives loss of 0.2040462855627125\n",
      "The 17733 th iteration gives loss of 0.2040398460855884\n",
      "The 17734 th iteration gives loss of 0.2040334074067411\n",
      "The 17735 th iteration gives loss of 0.20402696952605182\n",
      "The 17736 th iteration gives loss of 0.20402053244339244\n",
      "The 17737 th iteration gives loss of 0.20401409615863647\n",
      "The 17738 th iteration gives loss of 0.20400766067166917\n",
      "The 17739 th iteration gives loss of 0.2040012259823675\n",
      "The 17740 th iteration gives loss of 0.2039947920906067\n",
      "The 17741 th iteration gives loss of 0.20398835899625437\n",
      "The 17742 th iteration gives loss of 0.20398192669921164\n",
      "The 17743 th iteration gives loss of 0.20397549519932573\n",
      "The 17744 th iteration gives loss of 0.20396906449650315\n",
      "The 17745 th iteration gives loss of 0.2039626345905982\n",
      "The 17746 th iteration gives loss of 0.2039562054814973\n",
      "The 17747 th iteration gives loss of 0.20394977716907867\n",
      "The 17748 th iteration gives loss of 0.20394334965321953\n",
      "The 17749 th iteration gives loss of 0.20393692293380428\n",
      "The 17750 th iteration gives loss of 0.20393049701069627\n",
      "The 17751 th iteration gives loss of 0.20392407188378062\n",
      "The 17752 th iteration gives loss of 0.20391764755293967\n",
      "The 17753 th iteration gives loss of 0.20391122401803852\n",
      "The 17754 th iteration gives loss of 0.20390480127896157\n",
      "The 17755 th iteration gives loss of 0.20389837933558813\n",
      "The 17756 th iteration gives loss of 0.2038919581877976\n",
      "The 17757 th iteration gives loss of 0.2038855378354593\n",
      "The 17758 th iteration gives loss of 0.20387911827845392\n",
      "The 17759 th iteration gives loss of 0.20387269951665887\n",
      "The 17760 th iteration gives loss of 0.20386628154995665\n",
      "The 17761 th iteration gives loss of 0.20385986437822304\n",
      "The 17762 th iteration gives loss of 0.2038534480013383\n",
      "The 17763 th iteration gives loss of 0.2038470324191577\n",
      "The 17764 th iteration gives loss of 0.2038406176315976\n",
      "The 17765 th iteration gives loss of 0.20383420363851001\n",
      "The 17766 th iteration gives loss of 0.20382779043977173\n",
      "The 17767 th iteration gives loss of 0.20382137803527256\n",
      "The 17768 th iteration gives loss of 0.20381496642488736\n",
      "The 17769 th iteration gives loss of 0.20380855560848524\n",
      "The 17770 th iteration gives loss of 0.20380214558596083\n",
      "The 17771 th iteration gives loss of 0.20379573635717602\n",
      "The 17772 th iteration gives loss of 0.20378932792200308\n",
      "The 17773 th iteration gives loss of 0.20378292028033743\n",
      "The 17774 th iteration gives loss of 0.2037765134320553\n",
      "The 17775 th iteration gives loss of 0.20377010737702067\n",
      "The 17776 th iteration gives loss of 0.2037637021151298\n",
      "The 17777 th iteration gives loss of 0.20375729764624642\n",
      "The 17778 th iteration gives loss of 0.20375089397024818\n",
      "The 17779 th iteration gives loss of 0.2037444910870263\n",
      "The 17780 th iteration gives loss of 0.20373808899644855\n",
      "The 17781 th iteration gives loss of 0.20373168769838934\n",
      "The 17782 th iteration gives loss of 0.20372528719273814\n",
      "The 17783 th iteration gives loss of 0.20371888747936792\n",
      "The 17784 th iteration gives loss of 0.20371248855815527\n",
      "The 17785 th iteration gives loss of 0.20370609042898566\n",
      "The 17786 th iteration gives loss of 0.2036996930917176\n",
      "The 17787 th iteration gives loss of 0.2036932965462497\n",
      "The 17788 th iteration gives loss of 0.20368690079244814\n",
      "The 17789 th iteration gives loss of 0.20368050583020067\n",
      "The 17790 th iteration gives loss of 0.20367411165937402\n",
      "The 17791 th iteration gives loss of 0.20366771827985558\n",
      "The 17792 th iteration gives loss of 0.2036613256915291\n",
      "The 17793 th iteration gives loss of 0.20365493389425182\n",
      "The 17794 th iteration gives loss of 0.20364854288792467\n",
      "The 17795 th iteration gives loss of 0.20364215267241625\n",
      "The 17796 th iteration gives loss of 0.20363576324760319\n",
      "The 17797 th iteration gives loss of 0.2036293746133575\n",
      "The 17798 th iteration gives loss of 0.2036229867695751\n",
      "The 17799 th iteration gives loss of 0.20361659971611493\n",
      "The 17800 th iteration gives loss of 0.20361021345287544\n",
      "The 17801 th iteration gives loss of 0.20360382797971355\n",
      "The 17802 th iteration gives loss of 0.20359744329651855\n",
      "The 17803 th iteration gives loss of 0.20359105940317812\n",
      "The 17804 th iteration gives loss of 0.20358467629955504\n",
      "The 17805 th iteration gives loss of 0.2035782939855427\n",
      "The 17806 th iteration gives loss of 0.20357191246101702\n",
      "The 17807 th iteration gives loss of 0.203565531725835\n",
      "The 17808 th iteration gives loss of 0.2035591517798906\n",
      "The 17809 th iteration gives loss of 0.20355277262306723\n",
      "The 17810 th iteration gives loss of 0.20354639425523105\n",
      "The 17811 th iteration gives loss of 0.20354001667628166\n",
      "The 17812 th iteration gives loss of 0.203533639886079\n",
      "The 17813 th iteration gives loss of 0.20352726388451056\n",
      "The 17814 th iteration gives loss of 0.20352088867144902\n",
      "The 17815 th iteration gives loss of 0.20351451424677403\n",
      "The 17816 th iteration gives loss of 0.20350814061036254\n",
      "The 17817 th iteration gives loss of 0.20350176776209788\n",
      "The 17818 th iteration gives loss of 0.20349539570186226\n",
      "The 17819 th iteration gives loss of 0.20348902442952868\n",
      "The 17820 th iteration gives loss of 0.2034826539449736\n",
      "The 17821 th iteration gives loss of 0.2034762842480791\n",
      "The 17822 th iteration gives loss of 0.20346991533872677\n",
      "The 17823 th iteration gives loss of 0.20346354721678642\n",
      "The 17824 th iteration gives loss of 0.20345717988214257\n",
      "The 17825 th iteration gives loss of 0.20345081333467901\n",
      "The 17826 th iteration gives loss of 0.20344444757426422\n",
      "The 17827 th iteration gives loss of 0.20343808260078916\n",
      "The 17828 th iteration gives loss of 0.20343171841412522\n",
      "The 17829 th iteration gives loss of 0.20342535501415066\n",
      "The 17830 th iteration gives loss of 0.20341899240074612\n",
      "The 17831 th iteration gives loss of 0.20341263057378473\n",
      "The 17832 th iteration gives loss of 0.2034062695331569\n",
      "The 17833 th iteration gives loss of 0.20339990927873558\n",
      "The 17834 th iteration gives loss of 0.2033935498104055\n",
      "The 17835 th iteration gives loss of 0.2033871911280308\n",
      "The 17836 th iteration gives loss of 0.20338083323150774\n",
      "The 17837 th iteration gives loss of 0.20337447612070236\n",
      "The 17838 th iteration gives loss of 0.2033681197955045\n",
      "The 17839 th iteration gives loss of 0.20336176425578795\n",
      "The 17840 th iteration gives loss of 0.20335540950141542\n",
      "The 17841 th iteration gives loss of 0.20334905553229765\n",
      "The 17842 th iteration gives loss of 0.20334270234829743\n",
      "The 17843 th iteration gives loss of 0.20333634994929595\n",
      "The 17844 th iteration gives loss of 0.20332999833516852\n",
      "The 17845 th iteration gives loss of 0.20332364750578222\n",
      "The 17846 th iteration gives loss of 0.2033172974610499\n",
      "The 17847 th iteration gives loss of 0.20331094820082476\n",
      "The 17848 th iteration gives loss of 0.20330459972499706\n",
      "The 17849 th iteration gives loss of 0.20329825203344007\n",
      "The 17850 th iteration gives loss of 0.20329190512603407\n",
      "The 17851 th iteration gives loss of 0.20328555900266335\n",
      "The 17852 th iteration gives loss of 0.20327921366319537\n",
      "The 17853 th iteration gives loss of 0.20327286910752884\n",
      "The 17854 th iteration gives loss of 0.20326652533552192\n",
      "The 17855 th iteration gives loss of 0.20326018234706503\n",
      "The 17856 th iteration gives loss of 0.20325384014203557\n",
      "The 17857 th iteration gives loss of 0.20324749872032144\n",
      "The 17858 th iteration gives loss of 0.20324115808179286\n",
      "The 17859 th iteration gives loss of 0.20323481822633127\n",
      "The 17860 th iteration gives loss of 0.20322847915381556\n",
      "The 17861 th iteration gives loss of 0.20322214086412752\n",
      "The 17862 th iteration gives loss of 0.2032158033571384\n",
      "The 17863 th iteration gives loss of 0.20320946663273035\n",
      "The 17864 th iteration gives loss of 0.20320313069079365\n",
      "The 17865 th iteration gives loss of 0.20319679553119083\n",
      "The 17866 th iteration gives loss of 0.20319046115381892\n",
      "The 17867 th iteration gives loss of 0.20318412755855228\n",
      "The 17868 th iteration gives loss of 0.20317779474526423\n",
      "The 17869 th iteration gives loss of 0.20317146271383862\n",
      "The 17870 th iteration gives loss of 0.2031651314641566\n",
      "The 17871 th iteration gives loss of 0.20315880099609387\n",
      "The 17872 th iteration gives loss of 0.2031524713095381\n",
      "The 17873 th iteration gives loss of 0.20314614240435755\n",
      "The 17874 th iteration gives loss of 0.20313981428043834\n",
      "The 17875 th iteration gives loss of 0.20313348693765795\n",
      "The 17876 th iteration gives loss of 0.20312716037590497\n",
      "The 17877 th iteration gives loss of 0.2031208345950464\n",
      "The 17878 th iteration gives loss of 0.2031145095949691\n",
      "The 17879 th iteration gives loss of 0.2031081853755439\n",
      "The 17880 th iteration gives loss of 0.2031018619366594\n",
      "The 17881 th iteration gives loss of 0.2030955392781979\n",
      "The 17882 th iteration gives loss of 0.20308921740003846\n",
      "The 17883 th iteration gives loss of 0.2030828963020624\n",
      "The 17884 th iteration gives loss of 0.20307657598413775\n",
      "The 17885 th iteration gives loss of 0.20307025644614937\n",
      "The 17886 th iteration gives loss of 0.2030639376879725\n",
      "The 17887 th iteration gives loss of 0.20305761970950606\n",
      "The 17888 th iteration gives loss of 0.20305130251061768\n",
      "The 17889 th iteration gives loss of 0.20304498609118535\n",
      "The 17890 th iteration gives loss of 0.20303867045109117\n",
      "The 17891 th iteration gives loss of 0.20303235559021193\n",
      "The 17892 th iteration gives loss of 0.2030260415084351\n",
      "The 17893 th iteration gives loss of 0.20301972820564126\n",
      "The 17894 th iteration gives loss of 0.20301341568170173\n",
      "The 17895 th iteration gives loss of 0.203007103936497\n",
      "The 17896 th iteration gives loss of 0.20300079296991022\n",
      "The 17897 th iteration gives loss of 0.20299448278183443\n",
      "The 17898 th iteration gives loss of 0.20298817337213237\n",
      "The 17899 th iteration gives loss of 0.2029818647406964\n",
      "The 17900 th iteration gives loss of 0.20297555688739063\n",
      "The 17901 th iteration gives loss of 0.20296924981209746\n",
      "The 17902 th iteration gives loss of 0.20296294351471553\n",
      "The 17903 th iteration gives loss of 0.20295663799511562\n",
      "The 17904 th iteration gives loss of 0.20295033325317527\n",
      "The 17905 th iteration gives loss of 0.20294402928877456\n",
      "The 17906 th iteration gives loss of 0.20293772610178873\n",
      "The 17907 th iteration gives loss of 0.2029314236921115\n",
      "The 17908 th iteration gives loss of 0.2029251220596171\n",
      "The 17909 th iteration gives loss of 0.20291882120418578\n",
      "The 17910 th iteration gives loss of 0.20291252112569075\n",
      "The 17911 th iteration gives loss of 0.20290622182402263\n",
      "The 17912 th iteration gives loss of 0.2028999232990523\n",
      "The 17913 th iteration gives loss of 0.2028936255506682\n",
      "The 17914 th iteration gives loss of 0.2028873285787557\n",
      "The 17915 th iteration gives loss of 0.20288103238318034\n",
      "The 17916 th iteration gives loss of 0.20287473696383065\n",
      "The 17917 th iteration gives loss of 0.2028684423205837\n",
      "The 17918 th iteration gives loss of 0.20286214845333114\n",
      "The 17919 th iteration gives loss of 0.20285585536194276\n",
      "The 17920 th iteration gives loss of 0.20284956304630475\n",
      "The 17921 th iteration gives loss of 0.2028432715062933\n",
      "The 17922 th iteration gives loss of 0.2028369807417848\n",
      "The 17923 th iteration gives loss of 0.20283069075266882\n",
      "The 17924 th iteration gives loss of 0.2028244015388227\n",
      "The 17925 th iteration gives loss of 0.20281811310012662\n",
      "The 17926 th iteration gives loss of 0.2028118254364601\n",
      "The 17927 th iteration gives loss of 0.2028055385477086\n",
      "The 17928 th iteration gives loss of 0.20279925243374813\n",
      "The 17929 th iteration gives loss of 0.2027929670944638\n",
      "The 17930 th iteration gives loss of 0.202786682529727\n",
      "The 17931 th iteration gives loss of 0.20278039873943854\n",
      "The 17932 th iteration gives loss of 0.20277411572344617\n",
      "The 17933 th iteration gives loss of 0.20276783348165572\n",
      "The 17934 th iteration gives loss of 0.20276155201394275\n",
      "The 17935 th iteration gives loss of 0.20275527132019172\n",
      "The 17936 th iteration gives loss of 0.20274899140028957\n",
      "The 17937 th iteration gives loss of 0.2027427122540869\n",
      "The 17938 th iteration gives loss of 0.2027364338814933\n",
      "The 17939 th iteration gives loss of 0.2027301562823863\n",
      "The 17940 th iteration gives loss of 0.20272387945664017\n",
      "The 17941 th iteration gives loss of 0.20271760340413392\n",
      "The 17942 th iteration gives loss of 0.20271132812474568\n",
      "The 17943 th iteration gives loss of 0.20270505361836275\n",
      "The 17944 th iteration gives loss of 0.2026987798848716\n",
      "The 17945 th iteration gives loss of 0.2026925069241462\n",
      "The 17946 th iteration gives loss of 0.2026862347360703\n",
      "The 17947 th iteration gives loss of 0.20267996332052005\n",
      "The 17948 th iteration gives loss of 0.2026736926773802\n",
      "The 17949 th iteration gives loss of 0.20266742280653202\n",
      "The 17950 th iteration gives loss of 0.2026611537078644\n",
      "The 17951 th iteration gives loss of 0.20265488538124896\n",
      "The 17952 th iteration gives loss of 0.202648617826562\n",
      "The 17953 th iteration gives loss of 0.2026423510436987\n",
      "The 17954 th iteration gives loss of 0.2026360850325165\n",
      "The 17955 th iteration gives loss of 0.20262981979292055\n",
      "The 17956 th iteration gives loss of 0.20262355532478332\n",
      "The 17957 th iteration gives loss of 0.20261729162798928\n",
      "The 17958 th iteration gives loss of 0.2026110287024154\n",
      "The 17959 th iteration gives loss of 0.20260476654795098\n",
      "The 17960 th iteration gives loss of 0.2025985051644662\n",
      "The 17961 th iteration gives loss of 0.20259224455184352\n",
      "The 17962 th iteration gives loss of 0.2025859847099648\n",
      "The 17963 th iteration gives loss of 0.2025797256387236\n",
      "The 17964 th iteration gives loss of 0.2025734673379851\n",
      "The 17965 th iteration gives loss of 0.20256720980764564\n",
      "The 17966 th iteration gives loss of 0.20256095304757132\n",
      "The 17967 th iteration gives loss of 0.2025546970576527\n",
      "The 17968 th iteration gives loss of 0.20254844183776063\n",
      "The 17969 th iteration gives loss of 0.20254218738780164\n",
      "The 17970 th iteration gives loss of 0.20253593370763393\n",
      "The 17971 th iteration gives loss of 0.20252968079714337\n",
      "The 17972 th iteration gives loss of 0.20252342865622128\n",
      "The 17973 th iteration gives loss of 0.20251717728472762\n",
      "The 17974 th iteration gives loss of 0.20251092668256682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 17975 th iteration gives loss of 0.20250467684961332\n",
      "The 17976 th iteration gives loss of 0.20249842778574184\n",
      "The 17977 th iteration gives loss of 0.20249217949083917\n",
      "The 17978 th iteration gives loss of 0.20248593196479844\n",
      "The 17979 th iteration gives loss of 0.20247968520747975\n",
      "The 17980 th iteration gives loss of 0.20247343921877548\n",
      "The 17981 th iteration gives loss of 0.2024671939985598\n",
      "The 17982 th iteration gives loss of 0.20246094954672933\n",
      "The 17983 th iteration gives loss of 0.20245470586315387\n",
      "The 17984 th iteration gives loss of 0.20244846294771787\n",
      "The 17985 th iteration gives loss of 0.20244222080030908\n",
      "The 17986 th iteration gives loss of 0.20243597942079755\n",
      "The 17987 th iteration gives loss of 0.2024297388090775\n",
      "The 17988 th iteration gives loss of 0.2024234989650199\n",
      "The 17989 th iteration gives loss of 0.2024172598885129\n",
      "The 17990 th iteration gives loss of 0.20241102157943996\n",
      "The 17991 th iteration gives loss of 0.20240478403767945\n",
      "The 17992 th iteration gives loss of 0.20239854726311104\n",
      "The 17993 th iteration gives loss of 0.20239231125562107\n",
      "The 17994 th iteration gives loss of 0.2023860760150826\n",
      "The 17995 th iteration gives loss of 0.20237984154139232\n",
      "The 17996 th iteration gives loss of 0.2023736078344211\n",
      "The 17997 th iteration gives loss of 0.202367374894056\n",
      "The 17998 th iteration gives loss of 0.20236114272017694\n",
      "The 17999 th iteration gives loss of 0.2023549113126579\n",
      "The 18000 th iteration gives loss of 0.20234868067140116\n",
      "The 18001 th iteration gives loss of 0.20234245079626564\n",
      "The 18002 th iteration gives loss of 0.20233622168714402\n",
      "The 18003 th iteration gives loss of 0.20232999334392082\n",
      "The 18004 th iteration gives loss of 0.202323765766479\n",
      "The 18005 th iteration gives loss of 0.20231753895470186\n",
      "The 18006 th iteration gives loss of 0.20231131290846052\n",
      "The 18007 th iteration gives loss of 0.20230508762763788\n",
      "The 18008 th iteration gives loss of 0.2022988631121333\n",
      "The 18009 th iteration gives loss of 0.2022926393618142\n",
      "The 18010 th iteration gives loss of 0.20228641637655678\n",
      "The 18011 th iteration gives loss of 0.20228019415626042\n",
      "The 18012 th iteration gives loss of 0.20227397270079975\n",
      "The 18013 th iteration gives loss of 0.20226775201005626\n",
      "The 18014 th iteration gives loss of 0.2022615320839087\n",
      "The 18015 th iteration gives loss of 0.20225531292224794\n",
      "The 18016 th iteration gives loss of 0.20224909452495002\n",
      "The 18017 th iteration gives loss of 0.20224287689189488\n",
      "The 18018 th iteration gives loss of 0.20223666002297092\n",
      "The 18019 th iteration gives loss of 0.20223044391806352\n",
      "The 18020 th iteration gives loss of 0.20222422857704164\n",
      "The 18021 th iteration gives loss of 0.20221801399980074\n",
      "The 18022 th iteration gives loss of 0.20221180018621354\n",
      "The 18023 th iteration gives loss of 0.2022055871361686\n",
      "The 18024 th iteration gives loss of 0.20219937484954598\n",
      "The 18025 th iteration gives loss of 0.2021931633262311\n",
      "The 18026 th iteration gives loss of 0.20218695256609884\n",
      "The 18027 th iteration gives loss of 0.20218074256904647\n",
      "The 18028 th iteration gives loss of 0.20217453333494095\n",
      "The 18029 th iteration gives loss of 0.20216832486367164\n",
      "The 18030 th iteration gives loss of 0.20216211715511562\n",
      "The 18031 th iteration gives loss of 0.2021559102091733\n",
      "The 18032 th iteration gives loss of 0.2021497040257053\n",
      "The 18033 th iteration gives loss of 0.2021434986046029\n",
      "The 18034 th iteration gives loss of 0.202137293945744\n",
      "The 18035 th iteration gives loss of 0.20213109004903035\n",
      "The 18036 th iteration gives loss of 0.20212488691432032\n",
      "The 18037 th iteration gives loss of 0.20211868454149987\n",
      "The 18038 th iteration gives loss of 0.20211248293046716\n",
      "The 18039 th iteration gives loss of 0.20210628208108852\n",
      "The 18040 th iteration gives loss of 0.20210008199325427\n",
      "The 18041 th iteration gives loss of 0.20209388266685227\n",
      "The 18042 th iteration gives loss of 0.20208768410176203\n",
      "The 18043 th iteration gives loss of 0.20208148629786588\n",
      "The 18044 th iteration gives loss of 0.20207528925503784\n",
      "The 18045 th iteration gives loss of 0.20206909297316944\n",
      "The 18046 th iteration gives loss of 0.20206289745213404\n",
      "The 18047 th iteration gives loss of 0.20205670269183615\n",
      "The 18048 th iteration gives loss of 0.20205050869214064\n",
      "The 18049 th iteration gives loss of 0.20204431545292936\n",
      "The 18050 th iteration gives loss of 0.20203812297409332\n",
      "The 18051 th iteration gives loss of 0.2020319312555188\n",
      "The 18052 th iteration gives loss of 0.20202574029707157\n",
      "The 18053 th iteration gives loss of 0.20201955009864495\n",
      "The 18054 th iteration gives loss of 0.20201336066012826\n",
      "The 18055 th iteration gives loss of 0.2020071719813946\n",
      "The 18056 th iteration gives loss of 0.20200098406234296\n",
      "The 18057 th iteration gives loss of 0.20199479690283637\n",
      "The 18058 th iteration gives loss of 0.2019886105027598\n",
      "The 18059 th iteration gives loss of 0.20198242486200194\n",
      "The 18060 th iteration gives loss of 0.20197623998045414\n",
      "The 18061 th iteration gives loss of 0.20197005585797903\n",
      "The 18062 th iteration gives loss of 0.20196387249448183\n",
      "The 18063 th iteration gives loss of 0.20195768988982948\n",
      "The 18064 th iteration gives loss of 0.20195150804391057\n",
      "The 18065 th iteration gives loss of 0.20194532695661363\n",
      "The 18066 th iteration gives loss of 0.201939146627814\n",
      "The 18067 th iteration gives loss of 0.2019329670574076\n",
      "The 18068 th iteration gives loss of 0.2019267882452664\n",
      "The 18069 th iteration gives loss of 0.2019206101912723\n",
      "The 18070 th iteration gives loss of 0.2019144328953072\n",
      "The 18071 th iteration gives loss of 0.20190825635726886\n",
      "The 18072 th iteration gives loss of 0.2019020805770234\n",
      "The 18073 th iteration gives loss of 0.2018959055544556\n",
      "The 18074 th iteration gives loss of 0.2018897312894599\n",
      "The 18075 th iteration gives loss of 0.20188355778191508\n",
      "The 18076 th iteration gives loss of 0.2018773850317036\n",
      "The 18077 th iteration gives loss of 0.2018712130387065\n",
      "The 18078 th iteration gives loss of 0.20186504180280937\n",
      "The 18079 th iteration gives loss of 0.20185887132389568\n",
      "The 18080 th iteration gives loss of 0.20185270160184982\n",
      "The 18081 th iteration gives loss of 0.2018465326365508\n",
      "The 18082 th iteration gives loss of 0.20184036442788753\n",
      "The 18083 th iteration gives loss of 0.20183419697574384\n",
      "The 18084 th iteration gives loss of 0.20182803027999774\n",
      "The 18085 th iteration gives loss of 0.20182186434053367\n",
      "The 18086 th iteration gives loss of 0.20181569915723807\n",
      "The 18087 th iteration gives loss of 0.20180953472999158\n",
      "The 18088 th iteration gives loss of 0.2018033710586853\n",
      "The 18089 th iteration gives loss of 0.20179720814319507\n",
      "The 18090 th iteration gives loss of 0.20179104598341574\n",
      "The 18091 th iteration gives loss of 0.20178488457920643\n",
      "The 18092 th iteration gives loss of 0.20177872393047142\n",
      "The 18093 th iteration gives loss of 0.20177256403708832\n",
      "The 18094 th iteration gives loss of 0.2017664048989428\n",
      "The 18095 th iteration gives loss of 0.2017602465159252\n",
      "The 18096 th iteration gives loss of 0.20175408888789925\n",
      "The 18097 th iteration gives loss of 0.20174793201476526\n",
      "The 18098 th iteration gives loss of 0.20174177589640352\n",
      "The 18099 th iteration gives loss of 0.20173562053269045\n",
      "The 18100 th iteration gives loss of 0.2017294659235188\n",
      "The 18101 th iteration gives loss of 0.20172331206877525\n",
      "The 18102 th iteration gives loss of 0.20171715896833015\n",
      "The 18103 th iteration gives loss of 0.2017110066220729\n",
      "The 18104 th iteration gives loss of 0.20170485502989527\n",
      "The 18105 th iteration gives loss of 0.2016987041916844\n",
      "The 18106 th iteration gives loss of 0.20169255410729994\n",
      "The 18107 th iteration gives loss of 0.2016864047766533\n",
      "The 18108 th iteration gives loss of 0.20168025619960098\n",
      "The 18109 th iteration gives loss of 0.20167410837605004\n",
      "The 18110 th iteration gives loss of 0.201667961305877\n",
      "The 18111 th iteration gives loss of 0.20166181498897282\n",
      "The 18112 th iteration gives loss of 0.20165566942519575\n",
      "The 18113 th iteration gives loss of 0.20164952461445843\n",
      "The 18114 th iteration gives loss of 0.2016433805566288\n",
      "The 18115 th iteration gives loss of 0.2016372372516027\n",
      "The 18116 th iteration gives loss of 0.20163109469926158\n",
      "The 18117 th iteration gives loss of 0.20162495289947494\n",
      "The 18118 th iteration gives loss of 0.20161881185214423\n",
      "The 18119 th iteration gives loss of 0.2016126715571411\n",
      "The 18120 th iteration gives loss of 0.20160653201435375\n",
      "The 18121 th iteration gives loss of 0.20160039322367604\n",
      "The 18122 th iteration gives loss of 0.20159425518497903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 18123 th iteration gives loss of 0.20158811789815773\n",
      "The 18124 th iteration gives loss of 0.2015819813630879\n",
      "The 18125 th iteration gives loss of 0.20157584557965844\n",
      "The 18126 th iteration gives loss of 0.20156971054774242\n",
      "The 18127 th iteration gives loss of 0.2015635762672426\n",
      "The 18128 th iteration gives loss of 0.20155744273803247\n",
      "The 18129 th iteration gives loss of 0.20155130995999082\n",
      "The 18130 th iteration gives loss of 0.20154517793301036\n",
      "The 18131 th iteration gives loss of 0.20153904665697897\n",
      "The 18132 th iteration gives loss of 0.20153291613176144\n",
      "The 18133 th iteration gives loss of 0.20152678635727508\n",
      "The 18134 th iteration gives loss of 0.20152065733338545\n",
      "The 18135 th iteration gives loss of 0.2015145290599654\n",
      "The 18136 th iteration gives loss of 0.20150840153691202\n",
      "The 18137 th iteration gives loss of 0.20150227476410953\n",
      "The 18138 th iteration gives loss of 0.20149614874144522\n",
      "The 18139 th iteration gives loss of 0.20149002346879538\n",
      "The 18140 th iteration gives loss of 0.20148389894605173\n",
      "The 18141 th iteration gives loss of 0.20147777517310228\n",
      "The 18142 th iteration gives loss of 0.2014716521498231\n",
      "The 18143 th iteration gives loss of 0.20146552987609895\n",
      "The 18144 th iteration gives loss of 0.20145940835181306\n",
      "The 18145 th iteration gives loss of 0.20145328757685557\n",
      "The 18146 th iteration gives loss of 0.20144716755110767\n",
      "The 18147 th iteration gives loss of 0.20144104827444614\n",
      "The 18148 th iteration gives loss of 0.20143492974677843\n",
      "The 18149 th iteration gives loss of 0.20142881196796447\n",
      "The 18150 th iteration gives loss of 0.20142269493790538\n",
      "The 18151 th iteration gives loss of 0.20141657865647858\n",
      "The 18152 th iteration gives loss of 0.20141046312356065\n",
      "The 18153 th iteration gives loss of 0.20140434833905832\n",
      "The 18154 th iteration gives loss of 0.20139823430284154\n",
      "The 18155 th iteration gives loss of 0.20139212101479764\n",
      "The 18156 th iteration gives loss of 0.2013860084748141\n",
      "The 18157 th iteration gives loss of 0.20137989668276773\n",
      "The 18158 th iteration gives loss of 0.20137378563854716\n",
      "The 18159 th iteration gives loss of 0.20136767534203537\n",
      "The 18160 th iteration gives loss of 0.20136156579312642\n",
      "The 18161 th iteration gives loss of 0.20135545699169743\n",
      "The 18162 th iteration gives loss of 0.2013493489376317\n",
      "The 18163 th iteration gives loss of 0.2013432416308158\n",
      "The 18164 th iteration gives loss of 0.20133713507113088\n",
      "The 18165 th iteration gives loss of 0.2013310292584756\n",
      "The 18166 th iteration gives loss of 0.20132492419272577\n",
      "The 18167 th iteration gives loss of 0.20131881987376155\n",
      "The 18168 th iteration gives loss of 0.2013127163014796\n",
      "The 18169 th iteration gives loss of 0.2013066134757561\n",
      "The 18170 th iteration gives loss of 0.20130051139647426\n",
      "The 18171 th iteration gives loss of 0.2012944100635211\n",
      "The 18172 th iteration gives loss of 0.20128830947678514\n",
      "The 18173 th iteration gives loss of 0.20128220963615656\n",
      "The 18174 th iteration gives loss of 0.20127611054149847\n",
      "The 18175 th iteration gives loss of 0.20127001219271784\n",
      "The 18176 th iteration gives loss of 0.20126391458969606\n",
      "The 18177 th iteration gives loss of 0.20125781773231227\n",
      "The 18178 th iteration gives loss of 0.20125172162045582\n",
      "The 18179 th iteration gives loss of 0.2012456262540108\n",
      "The 18180 th iteration gives loss of 0.20123953163286315\n",
      "The 18181 th iteration gives loss of 0.2012334377568974\n",
      "The 18182 th iteration gives loss of 0.20122734462600284\n",
      "The 18183 th iteration gives loss of 0.2012212522400522\n",
      "The 18184 th iteration gives loss of 0.20121516059893985\n",
      "The 18185 th iteration gives loss of 0.2012090697025503\n",
      "The 18186 th iteration gives loss of 0.20120297955076435\n",
      "The 18187 th iteration gives loss of 0.20119689014346748\n",
      "The 18188 th iteration gives loss of 0.20119080148055435\n",
      "The 18189 th iteration gives loss of 0.2011847135619076\n",
      "The 18190 th iteration gives loss of 0.20117862638740908\n",
      "The 18191 th iteration gives loss of 0.20117253995694823\n",
      "The 18192 th iteration gives loss of 0.2011664542704016\n",
      "The 18193 th iteration gives loss of 0.20116036932766973\n",
      "The 18194 th iteration gives loss of 0.20115428512861272\n",
      "The 18195 th iteration gives loss of 0.20114820167314293\n",
      "The 18196 th iteration gives loss of 0.20114211896112305\n",
      "The 18197 th iteration gives loss of 0.20113603699245164\n",
      "The 18198 th iteration gives loss of 0.2011299557670136\n",
      "The 18199 th iteration gives loss of 0.20112387528469788\n",
      "The 18200 th iteration gives loss of 0.20111779554538012\n",
      "The 18201 th iteration gives loss of 0.20111171654895266\n",
      "The 18202 th iteration gives loss of 0.2011056382953007\n",
      "The 18203 th iteration gives loss of 0.20109956078430982\n",
      "The 18204 th iteration gives loss of 0.20109348401586138\n",
      "The 18205 th iteration gives loss of 0.20108740798984245\n",
      "The 18206 th iteration gives loss of 0.20108133270614892\n",
      "The 18207 th iteration gives loss of 0.20107525816464195\n",
      "The 18208 th iteration gives loss of 0.2010691843652375\n",
      "The 18209 th iteration gives loss of 0.20106311130780247\n",
      "The 18210 th iteration gives loss of 0.20105703899222052\n",
      "The 18211 th iteration gives loss of 0.20105096741838033\n",
      "The 18212 th iteration gives loss of 0.20104489658618296\n",
      "The 18213 th iteration gives loss of 0.2010388264954958\n",
      "The 18214 th iteration gives loss of 0.2010327571462167\n",
      "The 18215 th iteration gives loss of 0.20102668853821934\n",
      "The 18216 th iteration gives loss of 0.20102062067139442\n",
      "The 18217 th iteration gives loss of 0.20101455354562642\n",
      "The 18218 th iteration gives loss of 0.2010084871608037\n",
      "The 18219 th iteration gives loss of 0.20100242151681866\n",
      "The 18220 th iteration gives loss of 0.20099635661354437\n",
      "The 18221 th iteration gives loss of 0.20099029245087874\n",
      "The 18222 th iteration gives loss of 0.20098422902869953\n",
      "The 18223 th iteration gives loss of 0.20097816634689264\n",
      "The 18224 th iteration gives loss of 0.2009721044053452\n",
      "The 18225 th iteration gives loss of 0.20096604320394856\n",
      "The 18226 th iteration gives loss of 0.20095998274257965\n",
      "The 18227 th iteration gives loss of 0.20095392302113654\n",
      "The 18228 th iteration gives loss of 0.2009478640394906\n",
      "The 18229 th iteration gives loss of 0.20094180579754012\n",
      "The 18230 th iteration gives loss of 0.2009357482951623\n",
      "The 18231 th iteration gives loss of 0.2009296915322476\n",
      "The 18232 th iteration gives loss of 0.2009236355086811\n",
      "The 18233 th iteration gives loss of 0.2009175802243399\n",
      "The 18234 th iteration gives loss of 0.20091152567913\n",
      "The 18235 th iteration gives loss of 0.20090547187293095\n",
      "The 18236 th iteration gives loss of 0.20089941880561996\n",
      "The 18237 th iteration gives loss of 0.20089336647708425\n",
      "The 18238 th iteration gives loss of 0.20088731488721412\n",
      "The 18239 th iteration gives loss of 0.2008812640359005\n",
      "The 18240 th iteration gives loss of 0.20087521392301924\n",
      "The 18241 th iteration gives loss of 0.20086916454846968\n",
      "The 18242 th iteration gives loss of 0.20086311591212383\n",
      "The 18243 th iteration gives loss of 0.20085706801386835\n",
      "The 18244 th iteration gives loss of 0.20085102085360543\n",
      "The 18245 th iteration gives loss of 0.20084497443121063\n",
      "The 18246 th iteration gives loss of 0.20083892874657092\n",
      "The 18247 th iteration gives loss of 0.2008328837995593\n",
      "The 18248 th iteration gives loss of 0.20082683959009087\n",
      "The 18249 th iteration gives loss of 0.20082079611803338\n",
      "The 18250 th iteration gives loss of 0.20081475338327343\n",
      "The 18251 th iteration gives loss of 0.20080871138570244\n",
      "The 18252 th iteration gives loss of 0.2008026701252079\n",
      "The 18253 th iteration gives loss of 0.2007966296016608\n",
      "The 18254 th iteration gives loss of 0.2007905898149638\n",
      "The 18255 th iteration gives loss of 0.20078455076500928\n",
      "The 18256 th iteration gives loss of 0.20077851245166692\n",
      "The 18257 th iteration gives loss of 0.20077247487482788\n",
      "The 18258 th iteration gives loss of 0.20076643803439106\n",
      "The 18259 th iteration gives loss of 0.20076040193022396\n",
      "The 18260 th iteration gives loss of 0.2007543665622247\n",
      "The 18261 th iteration gives loss of 0.20074833193026748\n",
      "The 18262 th iteration gives loss of 0.20074229803425486\n",
      "The 18263 th iteration gives loss of 0.2007362648740781\n",
      "The 18264 th iteration gives loss of 0.20073023244960148\n",
      "The 18265 th iteration gives loss of 0.20072420076073721\n",
      "The 18266 th iteration gives loss of 0.20071816980733798\n",
      "The 18267 th iteration gives loss of 0.20071213958931547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 18268 th iteration gives loss of 0.20070611010655798\n",
      "The 18269 th iteration gives loss of 0.2007000813589417\n",
      "The 18270 th iteration gives loss of 0.20069405334634824\n",
      "The 18271 th iteration gives loss of 0.2006880260686887\n",
      "The 18272 th iteration gives loss of 0.20068199952582572\n",
      "The 18273 th iteration gives loss of 0.20067597371765908\n",
      "The 18274 th iteration gives loss of 0.20066994864406834\n",
      "The 18275 th iteration gives loss of 0.20066392430493993\n",
      "The 18276 th iteration gives loss of 0.20065790070016368\n",
      "The 18277 th iteration gives loss of 0.2006518778296278\n",
      "The 18278 th iteration gives loss of 0.20064585569321974\n",
      "The 18279 th iteration gives loss of 0.20063983429082513\n",
      "The 18280 th iteration gives loss of 0.2006338136223292\n",
      "The 18281 th iteration gives loss of 0.20062779368761485\n",
      "The 18282 th iteration gives loss of 0.20062177448657773\n",
      "The 18283 th iteration gives loss of 0.20061575601910267\n",
      "The 18284 th iteration gives loss of 0.200609738285075\n",
      "The 18285 th iteration gives loss of 0.20060372128437853\n",
      "The 18286 th iteration gives loss of 0.20059770501690885\n",
      "The 18287 th iteration gives loss of 0.2005916894825428\n",
      "The 18288 th iteration gives loss of 0.20058567468116748\n",
      "The 18289 th iteration gives loss of 0.20057966061268367\n",
      "The 18290 th iteration gives loss of 0.20057364727696284\n",
      "The 18291 th iteration gives loss of 0.2005676346738968\n",
      "The 18292 th iteration gives loss of 0.20056162280338188\n",
      "The 18293 th iteration gives loss of 0.20055561166529634\n",
      "The 18294 th iteration gives loss of 0.2005496012595241\n",
      "The 18295 th iteration gives loss of 0.2005435915859601\n",
      "The 18296 th iteration gives loss of 0.20053758264448454\n",
      "The 18297 th iteration gives loss of 0.20053157443499678\n",
      "The 18298 th iteration gives loss of 0.20052556695736923\n",
      "The 18299 th iteration gives loss of 0.20051956021150205\n",
      "The 18300 th iteration gives loss of 0.2005135541972698\n",
      "The 18301 th iteration gives loss of 0.20050754891456737\n",
      "The 18302 th iteration gives loss of 0.20050154436326786\n",
      "The 18303 th iteration gives loss of 0.20049554054329033\n",
      "The 18304 th iteration gives loss of 0.20048953745449646\n",
      "The 18305 th iteration gives loss of 0.20048353509678224\n",
      "The 18306 th iteration gives loss of 0.2004775334700255\n",
      "The 18307 th iteration gives loss of 0.20047153257412698\n",
      "The 18308 th iteration gives loss of 0.2004655324089664\n",
      "The 18309 th iteration gives loss of 0.20045953297443145\n",
      "The 18310 th iteration gives loss of 0.20045353427041404\n",
      "The 18311 th iteration gives loss of 0.20044753629678286\n",
      "The 18312 th iteration gives loss of 0.20044153905345788\n",
      "The 18313 th iteration gives loss of 0.20043554254030096\n",
      "The 18314 th iteration gives loss of 0.20042954675721297\n",
      "The 18315 th iteration gives loss of 0.20042355170407902\n",
      "The 18316 th iteration gives loss of 0.20041755738078415\n",
      "The 18317 th iteration gives loss of 0.20041156378721184\n",
      "The 18318 th iteration gives loss of 0.20040557092325467\n",
      "The 18319 th iteration gives loss of 0.20039957878879527\n",
      "The 18320 th iteration gives loss of 0.20039358738373095\n",
      "The 18321 th iteration gives loss of 0.20038759670794123\n",
      "The 18322 th iteration gives loss of 0.2003816067613141\n",
      "The 18323 th iteration gives loss of 0.2003756175437385\n",
      "The 18324 th iteration gives loss of 0.20036962905511255\n",
      "The 18325 th iteration gives loss of 0.20036364129530135\n",
      "The 18326 th iteration gives loss of 0.2003576542642095\n",
      "The 18327 th iteration gives loss of 0.20035166796172063\n",
      "The 18328 th iteration gives loss of 0.2003456823877085\n",
      "The 18329 th iteration gives loss of 0.20033969754209743\n",
      "The 18330 th iteration gives loss of 0.2003337134247371\n",
      "The 18331 th iteration gives loss of 0.20032773003553378\n",
      "The 18332 th iteration gives loss of 0.2003217473743755\n",
      "The 18333 th iteration gives loss of 0.2003157654411457\n",
      "The 18334 th iteration gives loss of 0.20030978423573295\n",
      "The 18335 th iteration gives loss of 0.20030380375802467\n",
      "The 18336 th iteration gives loss of 0.20029782400791132\n",
      "The 18337 th iteration gives loss of 0.20029184498527225\n",
      "The 18338 th iteration gives loss of 0.20028586668999848\n",
      "The 18339 th iteration gives loss of 0.20027988912199138\n",
      "The 18340 th iteration gives loss of 0.20027391228112784\n",
      "The 18341 th iteration gives loss of 0.20026793616729235\n",
      "The 18342 th iteration gives loss of 0.2002619607803767\n",
      "The 18343 th iteration gives loss of 0.20025598612027184\n",
      "The 18344 th iteration gives loss of 0.2002500121868595\n",
      "The 18345 th iteration gives loss of 0.20024403898003182\n",
      "The 18346 th iteration gives loss of 0.20023806649967646\n",
      "The 18347 th iteration gives loss of 0.20023209474568307\n",
      "The 18348 th iteration gives loss of 0.20022612371793605\n",
      "The 18349 th iteration gives loss of 0.20022015341632884\n",
      "The 18350 th iteration gives loss of 0.2002141838407318\n",
      "The 18351 th iteration gives loss of 0.2002082149910574\n",
      "The 18352 th iteration gives loss of 0.20020224686718582\n",
      "The 18353 th iteration gives loss of 0.2001962794690002\n",
      "The 18354 th iteration gives loss of 0.20019031279639643\n",
      "The 18355 th iteration gives loss of 0.2001843468492553\n",
      "The 18356 th iteration gives loss of 0.20017838162745996\n",
      "The 18357 th iteration gives loss of 0.20017241713090742\n",
      "The 18358 th iteration gives loss of 0.20016645335948882\n",
      "The 18359 th iteration gives loss of 0.20016049031309155\n",
      "The 18360 th iteration gives loss of 0.20015452799159375\n",
      "The 18361 th iteration gives loss of 0.20014856639489784\n",
      "The 18362 th iteration gives loss of 0.2001426055228773\n",
      "The 18363 th iteration gives loss of 0.20013664537542675\n",
      "The 18364 th iteration gives loss of 0.20013068595243716\n",
      "The 18365 th iteration gives loss of 0.20012472725379973\n",
      "The 18366 th iteration gives loss of 0.20011876927939376\n",
      "The 18367 th iteration gives loss of 0.2001128120291179\n",
      "The 18368 th iteration gives loss of 0.20010685550284318\n",
      "The 18369 th iteration gives loss of 0.20010089970048164\n",
      "The 18370 th iteration gives loss of 0.20009494462190824\n",
      "The 18371 th iteration gives loss of 0.2000889902670022\n",
      "The 18372 th iteration gives loss of 0.20008303663567123\n",
      "The 18373 th iteration gives loss of 0.20007708372778935\n",
      "The 18374 th iteration gives loss of 0.20007113154325784\n",
      "The 18375 th iteration gives loss of 0.2000651800819571\n",
      "The 18376 th iteration gives loss of 0.20005922934376455\n",
      "The 18377 th iteration gives loss of 0.20005327932859032\n",
      "The 18378 th iteration gives loss of 0.20004733003631384\n",
      "The 18379 th iteration gives loss of 0.2000413814668205\n",
      "The 18380 th iteration gives loss of 0.20003543362001092\n",
      "The 18381 th iteration gives loss of 0.20002948649575825\n",
      "The 18382 th iteration gives loss of 0.20002354009395798\n",
      "The 18383 th iteration gives loss of 0.2000175944144992\n",
      "The 18384 th iteration gives loss of 0.20001164945726535\n",
      "The 18385 th iteration gives loss of 0.20000570522215502\n",
      "The 18386 th iteration gives loss of 0.19999976170905379\n",
      "The 18387 th iteration gives loss of 0.19999381891783993\n",
      "The 18388 th iteration gives loss of 0.19998787684841154\n",
      "The 18389 th iteration gives loss of 0.19998193550065918\n",
      "The 18390 th iteration gives loss of 0.19997599487446138\n",
      "The 18391 th iteration gives loss of 0.19997005496971842\n",
      "The 18392 th iteration gives loss of 0.19996411578631162\n",
      "The 18393 th iteration gives loss of 0.1999581773241404\n",
      "The 18394 th iteration gives loss of 0.19995223958308597\n",
      "The 18395 th iteration gives loss of 0.1999463025630304\n",
      "The 18396 th iteration gives loss of 0.19994036626386852\n",
      "The 18397 th iteration gives loss of 0.19993443068549888\n",
      "The 18398 th iteration gives loss of 0.19992849582779573\n",
      "The 18399 th iteration gives loss of 0.19992256169064276\n",
      "The 18400 th iteration gives loss of 0.1999166282739555\n",
      "The 18401 th iteration gives loss of 0.19991069557759714\n",
      "The 18402 th iteration gives loss of 0.19990476360147152\n",
      "The 18403 th iteration gives loss of 0.19989883234546077\n",
      "The 18404 th iteration gives loss of 0.1998929018094634\n",
      "The 18405 th iteration gives loss of 0.1998869719933579\n",
      "The 18406 th iteration gives loss of 0.19988104289703218\n",
      "The 18407 th iteration gives loss of 0.19987511452038115\n",
      "The 18408 th iteration gives loss of 0.19986918686329486\n",
      "The 18409 th iteration gives loss of 0.19986325992565102\n",
      "The 18410 th iteration gives loss of 0.1998573337073559\n",
      "The 18411 th iteration gives loss of 0.19985140820828293\n",
      "The 18412 th iteration gives loss of 0.19984548342833822\n",
      "The 18413 th iteration gives loss of 0.19983955936739536\n",
      "The 18414 th iteration gives loss of 0.19983363602535195\n",
      "The 18415 th iteration gives loss of 0.19982771340208866\n",
      "The 18416 th iteration gives loss of 0.1998217914975045\n",
      "The 18417 th iteration gives loss of 0.19981587031148484\n",
      "The 18418 th iteration gives loss of 0.1998099498439124\n",
      "The 18419 th iteration gives loss of 0.19980403009468894\n",
      "The 18420 th iteration gives loss of 0.19979811106369952\n",
      "The 18421 th iteration gives loss of 0.19979219275083585\n",
      "The 18422 th iteration gives loss of 0.19978627515596667\n",
      "The 18423 th iteration gives loss of 0.19978035827901397\n",
      "The 18424 th iteration gives loss of 0.19977444211983797\n",
      "The 18425 th iteration gives loss of 0.1997685266783467\n",
      "The 18426 th iteration gives loss of 0.1997626119544197\n",
      "The 18427 th iteration gives loss of 0.1997566979479604\n",
      "The 18428 th iteration gives loss of 0.19975078465884083\n",
      "The 18429 th iteration gives loss of 0.1997448720869636\n",
      "The 18430 th iteration gives loss of 0.1997389602322086\n",
      "The 18431 th iteration gives loss of 0.19973304909446357\n",
      "The 18432 th iteration gives loss of 0.19972713867362968\n",
      "The 18433 th iteration gives loss of 0.1997212289695826\n",
      "The 18434 th iteration gives loss of 0.19971531998222145\n",
      "The 18435 th iteration gives loss of 0.19970941171143536\n",
      "The 18436 th iteration gives loss of 0.19970350415711285\n",
      "The 18437 th iteration gives loss of 0.19969759731915174\n",
      "The 18438 th iteration gives loss of 0.1996916911974207\n",
      "The 18439 th iteration gives loss of 0.19968578579182658\n",
      "The 18440 th iteration gives loss of 0.19967988110224208\n",
      "The 18441 th iteration gives loss of 0.19967397712858512\n",
      "The 18442 th iteration gives loss of 0.19966807387072216\n",
      "The 18443 th iteration gives loss of 0.1996621713285372\n",
      "The 18444 th iteration gives loss of 0.1996562695019395\n",
      "The 18445 th iteration gives loss of 0.19965036839081432\n",
      "The 18446 th iteration gives loss of 0.1996444679950512\n",
      "The 18447 th iteration gives loss of 0.19963856831452775\n",
      "The 18448 th iteration gives loss of 0.19963266934914548\n",
      "The 18449 th iteration gives loss of 0.19962677109879762\n",
      "The 18450 th iteration gives loss of 0.19962087356336375\n",
      "The 18451 th iteration gives loss of 0.19961497674273374\n",
      "The 18452 th iteration gives loss of 0.19960908063680416\n",
      "The 18453 th iteration gives loss of 0.19960318524545947\n",
      "The 18454 th iteration gives loss of 0.199597290568593\n",
      "The 18455 th iteration gives loss of 0.1995913966060869\n",
      "The 18456 th iteration gives loss of 0.19958550335785105\n",
      "The 18457 th iteration gives loss of 0.19957961082376016\n",
      "The 18458 th iteration gives loss of 0.19957371900370433\n",
      "The 18459 th iteration gives loss of 0.1995678278975756\n",
      "The 18460 th iteration gives loss of 0.19956193750526155\n",
      "The 18461 th iteration gives loss of 0.19955604782665226\n",
      "The 18462 th iteration gives loss of 0.19955015886163777\n",
      "The 18463 th iteration gives loss of 0.19954427061010657\n",
      "The 18464 th iteration gives loss of 0.19953838307196714\n",
      "The 18465 th iteration gives loss of 0.19953249624708314\n",
      "The 18466 th iteration gives loss of 0.19952661013536088\n",
      "The 18467 th iteration gives loss of 0.19952072473668456\n",
      "The 18468 th iteration gives loss of 0.19951484005094114\n",
      "The 18469 th iteration gives loss of 0.19950895607802763\n",
      "The 18470 th iteration gives loss of 0.1995030728178227\n",
      "The 18471 th iteration gives loss of 0.19949719027023513\n",
      "The 18472 th iteration gives loss of 0.19949130843514384\n",
      "The 18473 th iteration gives loss of 0.19948542731243468\n",
      "The 18474 th iteration gives loss of 0.19947954690200242\n",
      "The 18475 th iteration gives loss of 0.19947366720373727\n",
      "The 18476 th iteration gives loss of 0.19946778821753705\n",
      "The 18477 th iteration gives loss of 0.19946190994328253\n",
      "The 18478 th iteration gives loss of 0.19945603238086926\n",
      "The 18479 th iteration gives loss of 0.1994501555301769\n",
      "The 18480 th iteration gives loss of 0.1994442793911067\n",
      "The 18481 th iteration gives loss of 0.19943840396354304\n",
      "The 18482 th iteration gives loss of 0.19943252924738675\n",
      "The 18483 th iteration gives loss of 0.19942665524251277\n",
      "The 18484 th iteration gives loss of 0.1994207819488184\n",
      "The 18485 th iteration gives loss of 0.19941490936620482\n",
      "The 18486 th iteration gives loss of 0.19940903749454475\n",
      "The 18487 th iteration gives loss of 0.19940316633372937\n",
      "The 18488 th iteration gives loss of 0.19939729588365948\n",
      "The 18489 th iteration gives loss of 0.19939142614422087\n",
      "The 18490 th iteration gives loss of 0.19938555711531003\n",
      "The 18491 th iteration gives loss of 0.19937968879681084\n",
      "The 18492 th iteration gives loss of 0.19937382118861613\n",
      "The 18493 th iteration gives loss of 0.19936795429060508\n",
      "The 18494 th iteration gives loss of 0.19936208810269737\n",
      "The 18495 th iteration gives loss of 0.19935622262475133\n",
      "The 18496 th iteration gives loss of 0.19935035785667143\n",
      "The 18497 th iteration gives loss of 0.19934449379834726\n",
      "The 18498 th iteration gives loss of 0.19933863044967648\n",
      "The 18499 th iteration gives loss of 0.19933276781053155\n",
      "The 18500 th iteration gives loss of 0.19932690588082894\n",
      "The 18501 th iteration gives loss of 0.19932104466043982\n",
      "The 18502 th iteration gives loss of 0.1993151841492501\n",
      "The 18503 th iteration gives loss of 0.1993093243471711\n",
      "The 18504 th iteration gives loss of 0.1993034652540734\n",
      "The 18505 th iteration gives loss of 0.19929760686986184\n",
      "The 18506 th iteration gives loss of 0.1992917491944207\n",
      "The 18507 th iteration gives loss of 0.1992858922276482\n",
      "The 18508 th iteration gives loss of 0.19928003596941715\n",
      "The 18509 th iteration gives loss of 0.19927418041963985\n",
      "The 18510 th iteration gives loss of 0.1992683255781966\n",
      "The 18511 th iteration gives loss of 0.1992624714449765\n",
      "The 18512 th iteration gives loss of 0.19925661801987035\n",
      "The 18513 th iteration gives loss of 0.19925076530277439\n",
      "The 18514 th iteration gives loss of 0.19924491329357086\n",
      "The 18515 th iteration gives loss of 0.1992390619921714\n",
      "The 18516 th iteration gives loss of 0.1992332113984352\n",
      "The 18517 th iteration gives loss of 0.19922736151227355\n",
      "The 18518 th iteration gives loss of 0.19922151233357624\n",
      "The 18519 th iteration gives loss of 0.19921566386222953\n",
      "The 18520 th iteration gives loss of 0.19920981609812238\n",
      "The 18521 th iteration gives loss of 0.1992039690411592\n",
      "The 18522 th iteration gives loss of 0.19919812269121784\n",
      "The 18523 th iteration gives loss of 0.19919227704819145\n",
      "The 18524 th iteration gives loss of 0.19918643211197032\n",
      "The 18525 th iteration gives loss of 0.19918058788245246\n",
      "The 18526 th iteration gives loss of 0.19917474435951185\n",
      "The 18527 th iteration gives loss of 0.19916890154306072\n",
      "The 18528 th iteration gives loss of 0.1991630594329804\n",
      "The 18529 th iteration gives loss of 0.1991572180291587\n",
      "The 18530 th iteration gives loss of 0.19915137733150143\n",
      "The 18531 th iteration gives loss of 0.199145537339882\n",
      "The 18532 th iteration gives loss of 0.1991396980542064\n",
      "The 18533 th iteration gives loss of 0.19913385947435144\n",
      "The 18534 th iteration gives loss of 0.19912802160021087\n",
      "The 18535 th iteration gives loss of 0.1991221844316819\n",
      "The 18536 th iteration gives loss of 0.19911634796865085\n",
      "The 18537 th iteration gives loss of 0.19911051221101633\n",
      "The 18538 th iteration gives loss of 0.1991046771586632\n",
      "The 18539 th iteration gives loss of 0.19909884281149176\n",
      "The 18540 th iteration gives loss of 0.19909300916937048\n",
      "The 18541 th iteration gives loss of 0.1990871762322182\n",
      "The 18542 th iteration gives loss of 0.1990813439999136\n",
      "The 18543 th iteration gives loss of 0.19907551247233932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 18544 th iteration gives loss of 0.1990696816493997\n",
      "The 18545 th iteration gives loss of 0.19906385153098366\n",
      "The 18546 th iteration gives loss of 0.19905802211698637\n",
      "The 18547 th iteration gives loss of 0.199052193407294\n",
      "The 18548 th iteration gives loss of 0.19904636540179063\n",
      "The 18549 th iteration gives loss of 0.19904053810037486\n",
      "The 18550 th iteration gives loss of 0.19903471150294025\n",
      "The 18551 th iteration gives loss of 0.19902888560937704\n",
      "The 18552 th iteration gives loss of 0.19902306041957765\n",
      "The 18553 th iteration gives loss of 0.19901723593342585\n",
      "The 18554 th iteration gives loss of 0.19901141215082302\n",
      "The 18555 th iteration gives loss of 0.19900558907166063\n",
      "The 18556 th iteration gives loss of 0.1989997666958252\n",
      "The 18557 th iteration gives loss of 0.19899394502320378\n",
      "The 18558 th iteration gives loss of 0.19898812405370406\n",
      "The 18559 th iteration gives loss of 0.19898230378719614\n",
      "The 18560 th iteration gives loss of 0.198976484223588\n",
      "The 18561 th iteration gives loss of 0.1989706653627598\n",
      "The 18562 th iteration gives loss of 0.1989648472046158\n",
      "The 18563 th iteration gives loss of 0.19895902974904534\n",
      "The 18564 th iteration gives loss of 0.19895321299592642\n",
      "The 18565 th iteration gives loss of 0.19894739694515856\n",
      "The 18566 th iteration gives loss of 0.19894158159664002\n",
      "The 18567 th iteration gives loss of 0.19893576695026075\n",
      "The 18568 th iteration gives loss of 0.19892995300590344\n",
      "The 18569 th iteration gives loss of 0.19892413976346676\n",
      "The 18570 th iteration gives loss of 0.19891832722284578\n",
      "The 18571 th iteration gives loss of 0.1989125153839265\n",
      "The 18572 th iteration gives loss of 0.19890670424660625\n",
      "The 18573 th iteration gives loss of 0.19890089381076995\n",
      "The 18574 th iteration gives loss of 0.19889508407630585\n",
      "The 18575 th iteration gives loss of 0.1988892750431142\n",
      "The 18576 th iteration gives loss of 0.19888346671108972\n",
      "The 18577 th iteration gives loss of 0.1988776590801056\n",
      "The 18578 th iteration gives loss of 0.19887185215009054\n",
      "The 18579 th iteration gives loss of 0.19886604592090246\n",
      "The 18580 th iteration gives loss of 0.19886024039244232\n",
      "The 18581 th iteration gives loss of 0.19885443556460955\n",
      "The 18582 th iteration gives loss of 0.19884863143727882\n",
      "The 18583 th iteration gives loss of 0.19884282801036665\n",
      "The 18584 th iteration gives loss of 0.1988370252837407\n",
      "The 18585 th iteration gives loss of 0.19883122325731192\n",
      "The 18586 th iteration gives loss of 0.19882542193096045\n",
      "The 18587 th iteration gives loss of 0.19881962130457467\n",
      "The 18588 th iteration gives loss of 0.19881382137806558\n",
      "The 18589 th iteration gives loss of 0.19880802215132437\n",
      "The 18590 th iteration gives loss of 0.19880222362422395\n",
      "The 18591 th iteration gives loss of 0.19879642579666318\n",
      "The 18592 th iteration gives loss of 0.1987906286685406\n",
      "The 18593 th iteration gives loss of 0.19878483223973825\n",
      "The 18594 th iteration gives loss of 0.19877903651015866\n",
      "The 18595 th iteration gives loss of 0.19877324147968745\n",
      "The 18596 th iteration gives loss of 0.19876744714822064\n",
      "The 18597 th iteration gives loss of 0.19876165351565211\n",
      "The 18598 th iteration gives loss of 0.19875586058187233\n",
      "The 18599 th iteration gives loss of 0.19875006834676834\n",
      "The 18600 th iteration gives loss of 0.19874427681022877\n",
      "The 18601 th iteration gives loss of 0.19873848597216145\n",
      "The 18602 th iteration gives loss of 0.19873269583245384\n",
      "The 18603 th iteration gives loss of 0.19872690639099264\n",
      "The 18604 th iteration gives loss of 0.1987211176476708\n",
      "The 18605 th iteration gives loss of 0.19871532960238847\n",
      "The 18606 th iteration gives loss of 0.19870954225501145\n",
      "The 18607 th iteration gives loss of 0.19870375560548054\n",
      "The 18608 th iteration gives loss of 0.19869796965363987\n",
      "The 18609 th iteration gives loss of 0.1986921843994163\n",
      "The 18610 th iteration gives loss of 0.19868639984268277\n",
      "The 18611 th iteration gives loss of 0.19868061598333825\n",
      "The 18612 th iteration gives loss of 0.1986748328212683\n",
      "The 18613 th iteration gives loss of 0.19866905035637616\n",
      "The 18614 th iteration gives loss of 0.19866326858854536\n",
      "The 18615 th iteration gives loss of 0.19865748751767157\n",
      "The 18616 th iteration gives loss of 0.19865170714365565\n",
      "The 18617 th iteration gives loss of 0.19864592746637336\n",
      "The 18618 th iteration gives loss of 0.19864014848572845\n",
      "The 18619 th iteration gives loss of 0.1986343702016087\n",
      "The 18620 th iteration gives loss of 0.19862859261391724\n",
      "The 18621 th iteration gives loss of 0.19862281572253676\n",
      "The 18622 th iteration gives loss of 0.19861703952736354\n",
      "The 18623 th iteration gives loss of 0.1986112640282844\n",
      "The 18624 th iteration gives loss of 0.19860548922519788\n",
      "The 18625 th iteration gives loss of 0.1985997151179998\n",
      "The 18626 th iteration gives loss of 0.19859394170657188\n",
      "The 18627 th iteration gives loss of 0.1985881689908136\n",
      "The 18628 th iteration gives loss of 0.1985823969706207\n",
      "The 18629 th iteration gives loss of 0.19857662564588044\n",
      "The 18630 th iteration gives loss of 0.1985708550164864\n",
      "The 18631 th iteration gives loss of 0.1985650850823349\n",
      "The 18632 th iteration gives loss of 0.19855931584331052\n",
      "The 18633 th iteration gives loss of 0.19855354729931352\n",
      "The 18634 th iteration gives loss of 0.19854777945022908\n",
      "The 18635 th iteration gives loss of 0.19854201229596416\n",
      "The 18636 th iteration gives loss of 0.19853624583640586\n",
      "The 18637 th iteration gives loss of 0.19853048007144136\n",
      "The 18638 th iteration gives loss of 0.19852471500096433\n",
      "The 18639 th iteration gives loss of 0.19851895062486818\n",
      "The 18640 th iteration gives loss of 0.19851318694305023\n",
      "The 18641 th iteration gives loss of 0.1985074239554023\n",
      "The 18642 th iteration gives loss of 0.19850166166181774\n",
      "The 18643 th iteration gives loss of 0.19849590006218995\n",
      "The 18644 th iteration gives loss of 0.1984901391564008\n",
      "The 18645 th iteration gives loss of 0.19848437894435436\n",
      "The 18646 th iteration gives loss of 0.19847861942595066\n",
      "The 18647 th iteration gives loss of 0.19847286060106606\n",
      "The 18648 th iteration gives loss of 0.19846710246960048\n",
      "The 18649 th iteration gives loss of 0.1984613450314445\n",
      "The 18650 th iteration gives loss of 0.1984555882865021\n",
      "The 18651 th iteration gives loss of 0.1984498322346531\n",
      "The 18652 th iteration gives loss of 0.1984440768757915\n",
      "The 18653 th iteration gives loss of 0.19843832220982358\n",
      "The 18654 th iteration gives loss of 0.19843256823662667\n",
      "The 18655 th iteration gives loss of 0.19842681495610254\n",
      "The 18656 th iteration gives loss of 0.19842106236814877\n",
      "The 18657 th iteration gives loss of 0.1984153104726476\n",
      "The 18658 th iteration gives loss of 0.19840955926950102\n",
      "The 18659 th iteration gives loss of 0.19840380875859187\n",
      "The 18660 th iteration gives loss of 0.19839805893982948\n",
      "The 18661 th iteration gives loss of 0.19839230981308786\n",
      "The 18662 th iteration gives loss of 0.19838656137827837\n",
      "The 18663 th iteration gives loss of 0.198380813635284\n",
      "The 18664 th iteration gives loss of 0.19837506658399276\n",
      "The 18665 th iteration gives loss of 0.1983693202243056\n",
      "The 18666 th iteration gives loss of 0.19836357455612325\n",
      "The 18667 th iteration gives loss of 0.198357829579324\n",
      "The 18668 th iteration gives loss of 0.19835208529381373\n",
      "The 18669 th iteration gives loss of 0.19834634169947982\n",
      "The 18670 th iteration gives loss of 0.19834059879621344\n",
      "The 18671 th iteration gives loss of 0.1983348565839149\n",
      "The 18672 th iteration gives loss of 0.19832911506247683\n",
      "The 18673 th iteration gives loss of 0.19832337423178842\n",
      "The 18674 th iteration gives loss of 0.1983176340917386\n",
      "The 18675 th iteration gives loss of 0.19831189464223192\n",
      "The 18676 th iteration gives loss of 0.19830615588315517\n",
      "The 18677 th iteration gives loss of 0.19830041781440005\n",
      "The 18678 th iteration gives loss of 0.198294680435866\n",
      "The 18679 th iteration gives loss of 0.19828894374744013\n",
      "The 18680 th iteration gives loss of 0.19828320774902564\n",
      "The 18681 th iteration gives loss of 0.198277472440509\n",
      "The 18682 th iteration gives loss of 0.19827173782177446\n",
      "The 18683 th iteration gives loss of 0.19826600389273993\n",
      "The 18684 th iteration gives loss of 0.19826027065327576\n",
      "The 18685 th iteration gives loss of 0.1982545381032875\n",
      "The 18686 th iteration gives loss of 0.1982488062426667\n",
      "The 18687 th iteration gives loss of 0.1982430750713104\n",
      "The 18688 th iteration gives loss of 0.1982373445891076\n",
      "The 18689 th iteration gives loss of 0.1982316147959471\n",
      "The 18690 th iteration gives loss of 0.19822588569173577\n",
      "The 18691 th iteration gives loss of 0.19822015727636627\n",
      "The 18692 th iteration gives loss of 0.1982144295497137\n",
      "The 18693 th iteration gives loss of 0.19820870251169495\n",
      "The 18694 th iteration gives loss of 0.19820297616217858\n",
      "The 18695 th iteration gives loss of 0.198197250501076\n",
      "The 18696 th iteration gives loss of 0.19819152552828287\n",
      "The 18697 th iteration gives loss of 0.19818580124369486\n",
      "The 18698 th iteration gives loss of 0.19818007764719062\n",
      "The 18699 th iteration gives loss of 0.1981743547386699\n",
      "The 18700 th iteration gives loss of 0.19816863251803093\n",
      "The 18701 th iteration gives loss of 0.19816291098517289\n",
      "The 18702 th iteration gives loss of 0.19815719013997915\n",
      "The 18703 th iteration gives loss of 0.1981514699823524\n",
      "The 18704 th iteration gives loss of 0.198145750512171\n",
      "The 18705 th iteration gives loss of 0.19814003172934772\n",
      "The 18706 th iteration gives loss of 0.19813431363376016\n",
      "The 18707 th iteration gives loss of 0.19812859622531276\n",
      "The 18708 th iteration gives loss of 0.19812287950389773\n",
      "The 18709 th iteration gives loss of 0.19811716346940678\n",
      "The 18710 th iteration gives loss of 0.19811144812173975\n",
      "The 18711 th iteration gives loss of 0.19810573346078872\n",
      "The 18712 th iteration gives loss of 0.1981000194864384\n",
      "The 18713 th iteration gives loss of 0.1980943061986022\n",
      "The 18714 th iteration gives loss of 0.19808859359715644\n",
      "The 18715 th iteration gives loss of 0.19808288168199445\n",
      "The 18716 th iteration gives loss of 0.1980771704530274\n",
      "The 18717 th iteration gives loss of 0.1980714599101289\n",
      "The 18718 th iteration gives loss of 0.19806575005320945\n",
      "The 18719 th iteration gives loss of 0.19806004088215637\n",
      "The 18720 th iteration gives loss of 0.1980543323968586\n",
      "The 18721 th iteration gives loss of 0.19804862459721562\n",
      "The 18722 th iteration gives loss of 0.19804291748313396\n",
      "The 18723 th iteration gives loss of 0.19803721105448727\n",
      "The 18724 th iteration gives loss of 0.19803150531118702\n",
      "The 18725 th iteration gives loss of 0.19802580025310773\n",
      "The 18726 th iteration gives loss of 0.19802009588015898\n",
      "The 18727 th iteration gives loss of 0.19801439219223765\n",
      "The 18728 th iteration gives loss of 0.19800868918922676\n",
      "The 18729 th iteration gives loss of 0.19800298687102685\n",
      "The 18730 th iteration gives loss of 0.19799728523753704\n",
      "The 18731 th iteration gives loss of 0.19799158428862754\n",
      "The 18732 th iteration gives loss of 0.19798588402421954\n",
      "The 18733 th iteration gives loss of 0.19798018444420482\n",
      "The 18734 th iteration gives loss of 0.1979744855484708\n",
      "The 18735 th iteration gives loss of 0.19796878733691142\n",
      "The 18736 th iteration gives loss of 0.19796308980942776\n",
      "The 18737 th iteration gives loss of 0.19795739296590617\n",
      "The 18738 th iteration gives loss of 0.19795169680624508\n",
      "The 18739 th iteration gives loss of 0.19794600133033016\n",
      "The 18740 th iteration gives loss of 0.19794030653806743\n",
      "The 18741 th iteration gives loss of 0.1979346124293482\n",
      "The 18742 th iteration gives loss of 0.19792891900407414\n",
      "The 18743 th iteration gives loss of 0.1979232262621187\n",
      "The 18744 th iteration gives loss of 0.1979175342034027\n",
      "The 18745 th iteration gives loss of 0.19791184282780536\n",
      "The 18746 th iteration gives loss of 0.19790615213521442\n",
      "The 18747 th iteration gives loss of 0.19790046212554724\n",
      "The 18748 th iteration gives loss of 0.19789477279868647\n",
      "The 18749 th iteration gives loss of 0.19788908415451828\n",
      "The 18750 th iteration gives loss of 0.19788339619294956\n",
      "The 18751 th iteration gives loss of 0.19787770891386883\n",
      "The 18752 th iteration gives loss of 0.1978720223171804\n",
      "The 18753 th iteration gives loss of 0.19786633640275603\n",
      "The 18754 th iteration gives loss of 0.1978606511705185\n",
      "The 18755 th iteration gives loss of 0.1978549666203385\n",
      "The 18756 th iteration gives loss of 0.1978492827521331\n",
      "The 18757 th iteration gives loss of 0.19784359956577707\n",
      "The 18758 th iteration gives loss of 0.1978379170611781\n",
      "The 18759 th iteration gives loss of 0.19783223523822457\n",
      "The 18760 th iteration gives loss of 0.19782655409681288\n",
      "The 18761 th iteration gives loss of 0.1978208736368475\n",
      "The 18762 th iteration gives loss of 0.19781519385820887\n",
      "The 18763 th iteration gives loss of 0.19780951476079464\n",
      "The 18764 th iteration gives loss of 0.19780383634450024\n",
      "The 18765 th iteration gives loss of 0.19779815860923422\n",
      "The 18766 th iteration gives loss of 0.19779248155487913\n",
      "The 18767 th iteration gives loss of 0.19778680518131966\n",
      "The 18768 th iteration gives loss of 0.19778112948848006\n",
      "The 18769 th iteration gives loss of 0.19777545447622719\n",
      "The 18770 th iteration gives loss of 0.1977697801444682\n",
      "The 18771 th iteration gives loss of 0.19776410649309795\n",
      "The 18772 th iteration gives loss of 0.19775843352199812\n",
      "The 18773 th iteration gives loss of 0.19775276123108315\n",
      "The 18774 th iteration gives loss of 0.19774708962024634\n",
      "The 18775 th iteration gives loss of 0.1977414186893742\n",
      "The 18776 th iteration gives loss of 0.1977357484383653\n",
      "The 18777 th iteration gives loss of 0.19773007886710922\n",
      "The 18778 th iteration gives loss of 0.19772440997550625\n",
      "The 18779 th iteration gives loss of 0.19771874176346152\n",
      "The 18780 th iteration gives loss of 0.19771307423084702\n",
      "The 18781 th iteration gives loss of 0.1977074073775802\n",
      "The 18782 th iteration gives loss of 0.19770174120353848\n",
      "The 18783 th iteration gives loss of 0.19769607570863362\n",
      "The 18784 th iteration gives loss of 0.1976904108927523\n",
      "The 18785 th iteration gives loss of 0.1976847467557805\n",
      "The 18786 th iteration gives loss of 0.19767908329762587\n",
      "The 18787 th iteration gives loss of 0.1976734205181915\n",
      "The 18788 th iteration gives loss of 0.19766775841735112\n",
      "The 18789 th iteration gives loss of 0.19766209699501694\n",
      "The 18790 th iteration gives loss of 0.19765643625108556\n",
      "The 18791 th iteration gives loss of 0.19765077618543736\n",
      "The 18792 th iteration gives loss of 0.197645116797967\n",
      "The 18793 th iteration gives loss of 0.19763945808858707\n",
      "The 18794 th iteration gives loss of 0.19763380005718137\n",
      "The 18795 th iteration gives loss of 0.19762814270365736\n",
      "The 18796 th iteration gives loss of 0.19762248602789917\n",
      "The 18797 th iteration gives loss of 0.19761683002980227\n",
      "The 18798 th iteration gives loss of 0.19761117470925976\n",
      "The 18799 th iteration gives loss of 0.19760552006617227\n",
      "The 18800 th iteration gives loss of 0.1975998661004398\n",
      "The 18801 th iteration gives loss of 0.19759421281195022\n",
      "The 18802 th iteration gives loss of 0.19758856020060486\n",
      "The 18803 th iteration gives loss of 0.19758290826628772\n",
      "The 18804 th iteration gives loss of 0.19757725700891848\n",
      "The 18805 th iteration gives loss of 0.19757160642837246\n",
      "The 18806 th iteration gives loss of 0.1975659565245446\n",
      "The 18807 th iteration gives loss of 0.19756030729733384\n",
      "The 18808 th iteration gives loss of 0.197554658746641\n",
      "The 18809 th iteration gives loss of 0.19754901087235066\n",
      "The 18810 th iteration gives loss of 0.19754336367437234\n",
      "The 18811 th iteration gives loss of 0.19753771715259952\n",
      "The 18812 th iteration gives loss of 0.19753207130691736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 18813 th iteration gives loss of 0.1975264261372312\n",
      "The 18814 th iteration gives loss of 0.19752078164343603\n",
      "The 18815 th iteration gives loss of 0.19751513782541827\n",
      "The 18816 th iteration gives loss of 0.19750949468307877\n",
      "The 18817 th iteration gives loss of 0.19750385221632355\n",
      "The 18818 th iteration gives loss of 0.19749821042504437\n",
      "The 18819 th iteration gives loss of 0.19749256930912235\n",
      "The 18820 th iteration gives loss of 0.19748692886845995\n",
      "The 18821 th iteration gives loss of 0.1974812891029674\n",
      "The 18822 th iteration gives loss of 0.19747565001253026\n",
      "The 18823 th iteration gives loss of 0.19747001159702887\n",
      "The 18824 th iteration gives loss of 0.19746437385639584\n",
      "The 18825 th iteration gives loss of 0.19745873679048753\n",
      "The 18826 th iteration gives loss of 0.19745310039922526\n",
      "The 18827 th iteration gives loss of 0.19744746468250143\n",
      "The 18828 th iteration gives loss of 0.19744182964019827\n",
      "The 18829 th iteration gives loss of 0.19743619527222783\n",
      "The 18830 th iteration gives loss of 0.19743056157847927\n",
      "The 18831 th iteration gives loss of 0.19742492855884114\n",
      "The 18832 th iteration gives loss of 0.19741929621322724\n",
      "The 18833 th iteration gives loss of 0.19741366454151923\n",
      "The 18834 th iteration gives loss of 0.1974080335436187\n",
      "The 18835 th iteration gives loss of 0.1974024032194199\n",
      "The 18836 th iteration gives loss of 0.19739677356881902\n",
      "The 18837 th iteration gives loss of 0.1973911445917045\n",
      "The 18838 th iteration gives loss of 0.19738551628799408\n",
      "The 18839 th iteration gives loss of 0.19737988865756537\n",
      "The 18840 th iteration gives loss of 0.19737426170031502\n",
      "The 18841 th iteration gives loss of 0.19736863541614932\n",
      "The 18842 th iteration gives loss of 0.19736300980496543\n",
      "The 18843 th iteration gives loss of 0.19735738486663823\n",
      "The 18844 th iteration gives loss of 0.1973517606010866\n",
      "The 18845 th iteration gives loss of 0.19734613700819617\n",
      "The 18846 th iteration gives loss of 0.19734051408786432\n",
      "The 18847 th iteration gives loss of 0.19733489183999006\n",
      "The 18848 th iteration gives loss of 0.197329270264468\n",
      "The 18849 th iteration gives loss of 0.19732364936119698\n",
      "The 18850 th iteration gives loss of 0.19731802913007085\n",
      "The 18851 th iteration gives loss of 0.19731240957098145\n",
      "The 18852 th iteration gives loss of 0.19730679068383272\n",
      "The 18853 th iteration gives loss of 0.19730117246851836\n",
      "The 18854 th iteration gives loss of 0.19729555492493592\n",
      "The 18855 th iteration gives loss of 0.1972899380529854\n",
      "The 18856 th iteration gives loss of 0.19728432185254527\n",
      "The 18857 th iteration gives loss of 0.19727870632353123\n",
      "The 18858 th iteration gives loss of 0.19727309146582608\n",
      "The 18859 th iteration gives loss of 0.1972674772793367\n",
      "The 18860 th iteration gives loss of 0.19726186376395557\n",
      "The 18861 th iteration gives loss of 0.1972562509195833\n",
      "The 18862 th iteration gives loss of 0.1972506387461168\n",
      "The 18863 th iteration gives loss of 0.1972450272434432\n",
      "The 18864 th iteration gives loss of 0.1972394164114582\n",
      "The 18865 th iteration gives loss of 0.19723380625006953\n",
      "The 18866 th iteration gives loss of 0.19722819675917572\n",
      "The 18867 th iteration gives loss of 0.19722258793865838\n",
      "The 18868 th iteration gives loss of 0.19721697978841682\n",
      "The 18869 th iteration gives loss of 0.19721137230835803\n",
      "The 18870 th iteration gives loss of 0.19720576549837712\n",
      "The 18871 th iteration gives loss of 0.19720015935836643\n",
      "The 18872 th iteration gives loss of 0.19719455388821985\n",
      "The 18873 th iteration gives loss of 0.19718894908783816\n",
      "The 18874 th iteration gives loss of 0.1971833449571133\n",
      "The 18875 th iteration gives loss of 0.19717774149594508\n",
      "The 18876 th iteration gives loss of 0.19717213870423536\n",
      "The 18877 th iteration gives loss of 0.19716653658187905\n",
      "The 18878 th iteration gives loss of 0.19716093512876343\n",
      "The 18879 th iteration gives loss of 0.19715533434480112\n",
      "The 18880 th iteration gives loss of 0.19714973422987658\n",
      "The 18881 th iteration gives loss of 0.19714413478388915\n",
      "The 18882 th iteration gives loss of 0.1971385360067336\n",
      "The 18883 th iteration gives loss of 0.19713293789830913\n",
      "The 18884 th iteration gives loss of 0.19712734045851638\n",
      "The 18885 th iteration gives loss of 0.19712174368724442\n",
      "The 18886 th iteration gives loss of 0.19711614758439028\n",
      "The 18887 th iteration gives loss of 0.1971105521498613\n",
      "The 18888 th iteration gives loss of 0.1971049573835422\n",
      "The 18889 th iteration gives loss of 0.1970993632853418\n",
      "The 18890 th iteration gives loss of 0.19709376985515148\n",
      "The 18891 th iteration gives loss of 0.19708817709285864\n",
      "The 18892 th iteration gives loss of 0.19708258499838266\n",
      "The 18893 th iteration gives loss of 0.19707699357160174\n",
      "The 18894 th iteration gives loss of 0.19707140281241503\n",
      "The 18895 th iteration gives loss of 0.19706581272072088\n",
      "The 18896 th iteration gives loss of 0.19706022329641765\n",
      "The 18897 th iteration gives loss of 0.19705463453940997\n",
      "The 18898 th iteration gives loss of 0.19704904644958862\n",
      "The 18899 th iteration gives loss of 0.19704345902683862\n",
      "The 18900 th iteration gives loss of 0.19703787227107997\n",
      "The 18901 th iteration gives loss of 0.19703228618219232\n",
      "The 18902 th iteration gives loss of 0.19702670076007273\n",
      "The 18903 th iteration gives loss of 0.1970211160046252\n",
      "The 18904 th iteration gives loss of 0.1970155319157556\n",
      "The 18905 th iteration gives loss of 0.1970099484933407\n",
      "The 18906 th iteration gives loss of 0.197004365737297\n",
      "The 18907 th iteration gives loss of 0.19699878364750756\n",
      "The 18908 th iteration gives loss of 0.1969932022238781\n",
      "The 18909 th iteration gives loss of 0.1969876214662972\n",
      "The 18910 th iteration gives loss of 0.19698204137466957\n",
      "The 18911 th iteration gives loss of 0.1969764619488913\n",
      "The 18912 th iteration gives loss of 0.19697088318885392\n",
      "The 18913 th iteration gives loss of 0.19696530509446655\n",
      "The 18914 th iteration gives loss of 0.19695972766562275\n",
      "The 18915 th iteration gives loss of 0.19695415090220358\n",
      "The 18916 th iteration gives loss of 0.1969485748041229\n",
      "The 18917 th iteration gives loss of 0.19694299937128035\n",
      "The 18918 th iteration gives loss of 0.19693742460356317\n",
      "The 18919 th iteration gives loss of 0.1969318505008707\n",
      "The 18920 th iteration gives loss of 0.1969262770631074\n",
      "The 18921 th iteration gives loss of 0.19692070429015837\n",
      "The 18922 th iteration gives loss of 0.19691513218194265\n",
      "The 18923 th iteration gives loss of 0.19690956073833632\n",
      "The 18924 th iteration gives loss of 0.19690398995923805\n",
      "The 18925 th iteration gives loss of 0.19689841984455436\n",
      "The 18926 th iteration gives loss of 0.19689285039419474\n",
      "The 18927 th iteration gives loss of 0.19688728160801733\n",
      "The 18928 th iteration gives loss of 0.1968817134859581\n",
      "The 18929 th iteration gives loss of 0.19687614602789918\n",
      "The 18930 th iteration gives loss of 0.19687057923373363\n",
      "The 18931 th iteration gives loss of 0.1968650131033721\n",
      "The 18932 th iteration gives loss of 0.1968594476367037\n",
      "The 18933 th iteration gives loss of 0.19685388283361563\n",
      "The 18934 th iteration gives loss of 0.19684831869403044\n",
      "The 18935 th iteration gives loss of 0.1968427552178244\n",
      "The 18936 th iteration gives loss of 0.19683719240490677\n",
      "The 18937 th iteration gives loss of 0.19683163025516748\n",
      "The 18938 th iteration gives loss of 0.1968260687685055\n",
      "The 18939 th iteration gives loss of 0.19682050794482703\n",
      "The 18940 th iteration gives loss of 0.19681494778402944\n",
      "The 18941 th iteration gives loss of 0.19680938828599567\n",
      "The 18942 th iteration gives loss of 0.19680382945063102\n",
      "The 18943 th iteration gives loss of 0.19679827127784685\n",
      "The 18944 th iteration gives loss of 0.19679271376752716\n",
      "The 18945 th iteration gives loss of 0.19678715691956572\n",
      "The 18946 th iteration gives loss of 0.19678160073386528\n",
      "The 18947 th iteration gives loss of 0.19677604521033062\n",
      "The 18948 th iteration gives loss of 0.1967704903488482\n",
      "The 18949 th iteration gives loss of 0.19676493614932078\n",
      "The 18950 th iteration gives loss of 0.19675938261165038\n",
      "The 18951 th iteration gives loss of 0.19675382973573247\n",
      "The 18952 th iteration gives loss of 0.19674827752146026\n",
      "The 18953 th iteration gives loss of 0.19674272596873504\n",
      "The 18954 th iteration gives loss of 0.19673717507745817\n",
      "The 18955 th iteration gives loss of 0.19673162484751713\n",
      "The 18956 th iteration gives loss of 0.196726075278819\n",
      "The 18957 th iteration gives loss of 0.19672052637126652\n",
      "The 18958 th iteration gives loss of 0.19671497812474376\n",
      "The 18959 th iteration gives loss of 0.1967094305391582\n",
      "The 18960 th iteration gives loss of 0.1967038836144027\n",
      "The 18961 th iteration gives loss of 0.19669833735037828\n",
      "The 18962 th iteration gives loss of 0.19669279174698795\n",
      "The 18963 th iteration gives loss of 0.1966872468041186\n",
      "The 18964 th iteration gives loss of 0.1966817025216765\n",
      "The 18965 th iteration gives loss of 0.19667615889956175\n",
      "The 18966 th iteration gives loss of 0.196670615937664\n",
      "The 18967 th iteration gives loss of 0.19666507363588892\n",
      "The 18968 th iteration gives loss of 0.19665953199412478\n",
      "The 18969 th iteration gives loss of 0.1966539910122811\n",
      "The 18970 th iteration gives loss of 0.19664845069024844\n",
      "The 18971 th iteration gives loss of 0.1966429110279248\n",
      "The 18972 th iteration gives loss of 0.19663737202521547\n",
      "The 18973 th iteration gives loss of 0.19663183368201545\n",
      "The 18974 th iteration gives loss of 0.19662629599821596\n",
      "The 18975 th iteration gives loss of 0.1966207589737367\n",
      "The 18976 th iteration gives loss of 0.196615222608442\n",
      "The 18977 th iteration gives loss of 0.19660968690225814\n",
      "The 18978 th iteration gives loss of 0.19660415185507413\n",
      "The 18979 th iteration gives loss of 0.19659861746678367\n",
      "The 18980 th iteration gives loss of 0.19659308373729473\n",
      "The 18981 th iteration gives loss of 0.19658755066649944\n",
      "The 18982 th iteration gives loss of 0.196582018254294\n",
      "The 18983 th iteration gives loss of 0.19657648650058035\n",
      "The 18984 th iteration gives loss of 0.1965709554052593\n",
      "The 18985 th iteration gives loss of 0.19656542496822677\n",
      "The 18986 th iteration gives loss of 0.19655989518937692\n",
      "The 18987 th iteration gives loss of 0.19655436606861493\n",
      "The 18988 th iteration gives loss of 0.19654883760583775\n",
      "The 18989 th iteration gives loss of 0.19654330980094376\n",
      "The 18990 th iteration gives loss of 0.19653778265383123\n",
      "The 18991 th iteration gives loss of 0.19653225616439027\n",
      "The 18992 th iteration gives loss of 0.1965267303325265\n",
      "The 18993 th iteration gives loss of 0.1965212051581457\n",
      "The 18994 th iteration gives loss of 0.19651568064113142\n",
      "The 18995 th iteration gives loss of 0.1965101567814016\n",
      "The 18996 th iteration gives loss of 0.1965046335788359\n",
      "The 18997 th iteration gives loss of 0.19649911103333745\n",
      "The 18998 th iteration gives loss of 0.19649358914481588\n",
      "The 18999 th iteration gives loss of 0.19648806791315335\n",
      "The 19000 th iteration gives loss of 0.1964825473382614\n",
      "The 19001 th iteration gives loss of 0.196477027420042\n",
      "The 19002 th iteration gives loss of 0.1964715081583751\n",
      "The 19003 th iteration gives loss of 0.19646598955317227\n",
      "The 19004 th iteration gives loss of 0.1964604716043327\n",
      "The 19005 th iteration gives loss of 0.19645495431174784\n",
      "The 19006 th iteration gives loss of 0.1964494376753279\n",
      "The 19007 th iteration gives loss of 0.19644392169495759\n",
      "The 19008 th iteration gives loss of 0.19643840637055177\n",
      "The 19009 th iteration gives loss of 0.19643289170199532\n",
      "The 19010 th iteration gives loss of 0.19642737768919705\n",
      "The 19011 th iteration gives loss of 0.19642186433205433\n",
      "The 19012 th iteration gives loss of 0.1964163516304514\n",
      "The 19013 th iteration gives loss of 0.19641083958429703\n",
      "The 19014 th iteration gives loss of 0.196405328193493\n",
      "The 19015 th iteration gives loss of 0.1963998174579508\n",
      "The 19016 th iteration gives loss of 0.19639430737754815\n",
      "The 19017 th iteration gives loss of 0.19638879795218178\n",
      "The 19018 th iteration gives loss of 0.19638328918177358\n",
      "The 19019 th iteration gives loss of 0.19637778106619505\n",
      "The 19020 th iteration gives loss of 0.1963722736053614\n",
      "The 19021 th iteration gives loss of 0.196366766799172\n",
      "The 19022 th iteration gives loss of 0.19636126064751538\n",
      "The 19023 th iteration gives loss of 0.1963557551503082\n",
      "The 19024 th iteration gives loss of 0.19635025030743086\n",
      "The 19025 th iteration gives loss of 0.1963447461188003\n",
      "The 19026 th iteration gives loss of 0.19633924258430538\n",
      "The 19027 th iteration gives loss of 0.1963337397038332\n",
      "The 19028 th iteration gives loss of 0.19632823747730768\n",
      "The 19029 th iteration gives loss of 0.1963227359046077\n",
      "The 19030 th iteration gives loss of 0.19631723498564968\n",
      "The 19031 th iteration gives loss of 0.19631173472031144\n",
      "The 19032 th iteration gives loss of 0.19630623510851886\n",
      "The 19033 th iteration gives loss of 0.19630073615013988\n",
      "The 19034 th iteration gives loss of 0.19629523784509878\n",
      "The 19035 th iteration gives loss of 0.19628974019328457\n",
      "The 19036 th iteration gives loss of 0.1962842431945997\n",
      "The 19037 th iteration gives loss of 0.19627874684893945\n",
      "The 19038 th iteration gives loss of 0.19627325115620195\n",
      "The 19039 th iteration gives loss of 0.19626775611629987\n",
      "The 19040 th iteration gives loss of 0.1962622617291116\n",
      "The 19041 th iteration gives loss of 0.19625676799455746\n",
      "The 19042 th iteration gives loss of 0.19625127491251784\n",
      "The 19043 th iteration gives loss of 0.19624578248289293\n",
      "The 19044 th iteration gives loss of 0.196240290705603\n",
      "The 19045 th iteration gives loss of 0.19623479958052426\n",
      "The 19046 th iteration gives loss of 0.196229309107574\n",
      "The 19047 th iteration gives loss of 0.19622381928664678\n",
      "The 19048 th iteration gives loss of 0.19621833011762665\n",
      "The 19049 th iteration gives loss of 0.196212841600426\n",
      "The 19050 th iteration gives loss of 0.19620735373495013\n",
      "The 19051 th iteration gives loss of 0.19620186652109112\n",
      "The 19052 th iteration gives loss of 0.19619637995874462\n",
      "The 19053 th iteration gives loss of 0.19619089404781628\n",
      "The 19054 th iteration gives loss of 0.19618540878819965\n",
      "The 19055 th iteration gives loss of 0.19617992417980631\n",
      "The 19056 th iteration gives loss of 0.1961744402225175\n",
      "The 19057 th iteration gives loss of 0.19616895691625102\n",
      "The 19058 th iteration gives loss of 0.19616347426089487\n",
      "The 19059 th iteration gives loss of 0.19615799225634917\n",
      "The 19060 th iteration gives loss of 0.19615251090252453\n",
      "The 19061 th iteration gives loss of 0.19614703019930493\n",
      "The 19062 th iteration gives loss of 0.19614155014659973\n",
      "The 19063 th iteration gives loss of 0.19613607074430595\n",
      "The 19064 th iteration gives loss of 0.1961305919923121\n",
      "The 19065 th iteration gives loss of 0.19612511389054252\n",
      "The 19066 th iteration gives loss of 0.19611963643888544\n",
      "The 19067 th iteration gives loss of 0.19611415963723156\n",
      "The 19068 th iteration gives loss of 0.19610868348548288\n",
      "The 19069 th iteration gives loss of 0.19610320798355635\n",
      "The 19070 th iteration gives loss of 0.19609773313133338\n",
      "The 19071 th iteration gives loss of 0.19609225892871368\n",
      "The 19072 th iteration gives loss of 0.19608678537561655\n",
      "The 19073 th iteration gives loss of 0.19608131247191368\n",
      "The 19074 th iteration gives loss of 0.19607584021751834\n",
      "The 19075 th iteration gives loss of 0.19607036861234098\n",
      "The 19076 th iteration gives loss of 0.19606489765626808\n",
      "The 19077 th iteration gives loss of 0.19605942734920404\n",
      "The 19078 th iteration gives loss of 0.19605395769104864\n",
      "The 19079 th iteration gives loss of 0.19604848868169988\n",
      "The 19080 th iteration gives loss of 0.1960430203210505\n",
      "The 19081 th iteration gives loss of 0.1960375526090146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 19082 th iteration gives loss of 0.19603208554548995\n",
      "The 19083 th iteration gives loss of 0.19602661913035968\n",
      "The 19084 th iteration gives loss of 0.19602115336354456\n",
      "The 19085 th iteration gives loss of 0.19601568824493384\n",
      "The 19086 th iteration gives loss of 0.19601022377443317\n",
      "The 19087 th iteration gives loss of 0.19600475995193578\n",
      "The 19088 th iteration gives loss of 0.19599929677735195\n",
      "The 19089 th iteration gives loss of 0.19599383425057226\n",
      "The 19090 th iteration gives loss of 0.1959883723714922\n",
      "The 19091 th iteration gives loss of 0.19598291114001992\n",
      "The 19092 th iteration gives loss of 0.1959774505560565\n",
      "The 19093 th iteration gives loss of 0.19597199061949733\n",
      "The 19094 th iteration gives loss of 0.1959665313302551\n",
      "The 19095 th iteration gives loss of 0.19596107268821777\n",
      "The 19096 th iteration gives loss of 0.19595561469328024\n",
      "The 19097 th iteration gives loss of 0.1959501573453542\n",
      "The 19098 th iteration gives loss of 0.19594470064433733\n",
      "The 19099 th iteration gives loss of 0.19593924459012635\n",
      "The 19100 th iteration gives loss of 0.1959337891826255\n",
      "The 19101 th iteration gives loss of 0.19592833442173008\n",
      "The 19102 th iteration gives loss of 0.19592288030733696\n",
      "The 19103 th iteration gives loss of 0.19591742683936947\n",
      "The 19104 th iteration gives loss of 0.19591197401770044\n",
      "The 19105 th iteration gives loss of 0.1959065218422352\n",
      "The 19106 th iteration gives loss of 0.19590107031288412\n",
      "The 19107 th iteration gives loss of 0.19589561942954267\n",
      "The 19108 th iteration gives loss of 0.19589016919210311\n",
      "The 19109 th iteration gives loss of 0.19588471960048234\n",
      "The 19110 th iteration gives loss of 0.19587927065457034\n",
      "The 19111 th iteration gives loss of 0.19587382235427342\n",
      "The 19112 th iteration gives loss of 0.1958683746994781\n",
      "The 19113 th iteration gives loss of 0.19586292769010377\n",
      "The 19114 th iteration gives loss of 0.19585748132603636\n",
      "The 19115 th iteration gives loss of 0.1958520356071868\n",
      "The 19116 th iteration gives loss of 0.1958465905334495\n",
      "The 19117 th iteration gives loss of 0.195841146104724\n",
      "The 19118 th iteration gives loss of 0.1958357023209089\n",
      "The 19119 th iteration gives loss of 0.19583025918190752\n",
      "The 19120 th iteration gives loss of 0.19582481668762733\n",
      "The 19121 th iteration gives loss of 0.19581937483794964\n",
      "The 19122 th iteration gives loss of 0.19581393363280122\n",
      "The 19123 th iteration gives loss of 0.19580849307205817\n",
      "The 19124 th iteration gives loss of 0.19580305315563803\n",
      "The 19125 th iteration gives loss of 0.19579761388343075\n",
      "The 19126 th iteration gives loss of 0.1957921752553503\n",
      "The 19127 th iteration gives loss of 0.1957867372712822\n",
      "The 19128 th iteration gives loss of 0.1957812999311338\n",
      "The 19129 th iteration gives loss of 0.19577586323480384\n",
      "The 19130 th iteration gives loss of 0.19577042718219703\n",
      "The 19131 th iteration gives loss of 0.19576499177320694\n",
      "The 19132 th iteration gives loss of 0.19575955700773723\n",
      "The 19133 th iteration gives loss of 0.195754122885693\n",
      "The 19134 th iteration gives loss of 0.1957486894069663\n",
      "The 19135 th iteration gives loss of 0.19574325657146766\n",
      "The 19136 th iteration gives loss of 0.19573782437909848\n",
      "The 19137 th iteration gives loss of 0.1957323928297475\n",
      "The 19138 th iteration gives loss of 0.1957269619233274\n",
      "The 19139 th iteration gives loss of 0.19572153165973025\n",
      "The 19140 th iteration gives loss of 0.1957161020388658\n",
      "The 19141 th iteration gives loss of 0.19571067306062281\n",
      "The 19142 th iteration gives loss of 0.19570524472491557\n",
      "The 19143 th iteration gives loss of 0.19569981703162834\n",
      "The 19144 th iteration gives loss of 0.19569438998067776\n",
      "The 19145 th iteration gives loss of 0.19568896357195825\n",
      "The 19146 th iteration gives loss of 0.19568353780538147\n",
      "The 19147 th iteration gives loss of 0.19567811268082494\n",
      "The 19148 th iteration gives loss of 0.19567268819820618\n",
      "The 19149 th iteration gives loss of 0.19566726435742468\n",
      "The 19150 th iteration gives loss of 0.1956618411583805\n",
      "The 19151 th iteration gives loss of 0.19565641860096505\n",
      "The 19152 th iteration gives loss of 0.19565099668509583\n",
      "The 19153 th iteration gives loss of 0.19564557541066666\n",
      "The 19154 th iteration gives loss of 0.19564015477757923\n",
      "The 19155 th iteration gives loss of 0.19563473478572227\n",
      "The 19156 th iteration gives loss of 0.1956293154350053\n",
      "The 19157 th iteration gives loss of 0.19562389672533936\n",
      "The 19158 th iteration gives loss of 0.1956184786566215\n",
      "The 19159 th iteration gives loss of 0.1956130612287444\n",
      "The 19160 th iteration gives loss of 0.1956076444416151\n",
      "The 19161 th iteration gives loss of 0.19560222829512894\n",
      "The 19162 th iteration gives loss of 0.19559681278919902\n",
      "The 19163 th iteration gives loss of 0.19559139792371344\n",
      "The 19164 th iteration gives loss of 0.19558598369857688\n",
      "The 19165 th iteration gives loss of 0.1955805701136921\n",
      "The 19166 th iteration gives loss of 0.19557515716897\n",
      "The 19167 th iteration gives loss of 0.19556974486428894\n",
      "The 19168 th iteration gives loss of 0.19556433319957578\n",
      "The 19169 th iteration gives loss of 0.19555892217471374\n",
      "The 19170 th iteration gives loss of 0.19555351178961208\n",
      "The 19171 th iteration gives loss of 0.1955481020441669\n",
      "The 19172 th iteration gives loss of 0.19554269293828716\n",
      "The 19173 th iteration gives loss of 0.19553728447186305\n",
      "The 19174 th iteration gives loss of 0.1955318766448097\n",
      "The 19175 th iteration gives loss of 0.19552646945701296\n",
      "The 19176 th iteration gives loss of 0.19552106290837976\n",
      "The 19177 th iteration gives loss of 0.19551565699882453\n",
      "The 19178 th iteration gives loss of 0.19551025172823297\n",
      "The 19179 th iteration gives loss of 0.1955048470965048\n",
      "The 19180 th iteration gives loss of 0.19549944310356268\n",
      "The 19181 th iteration gives loss of 0.19549403974928303\n",
      "The 19182 th iteration gives loss of 0.19548863703358135\n",
      "The 19183 th iteration gives loss of 0.19548323495635242\n",
      "The 19184 th iteration gives loss of 0.195477833517497\n",
      "The 19185 th iteration gives loss of 0.19547243271692658\n",
      "The 19186 th iteration gives loss of 0.19546703255453526\n",
      "The 19187 th iteration gives loss of 0.19546163303022288\n",
      "The 19188 th iteration gives loss of 0.19545623414390254\n",
      "The 19189 th iteration gives loss of 0.19545083589546086\n",
      "The 19190 th iteration gives loss of 0.1954454382848063\n",
      "The 19191 th iteration gives loss of 0.19544004131183543\n",
      "The 19192 th iteration gives loss of 0.19543464497645366\n",
      "The 19193 th iteration gives loss of 0.19542924927856262\n",
      "The 19194 th iteration gives loss of 0.19542385421806463\n",
      "The 19195 th iteration gives loss of 0.19541845979485928\n",
      "The 19196 th iteration gives loss of 0.19541306600885852\n",
      "The 19197 th iteration gives loss of 0.19540767285994243\n",
      "The 19198 th iteration gives loss of 0.19540228034802906\n",
      "The 19199 th iteration gives loss of 0.19539688847302072\n",
      "The 19200 th iteration gives loss of 0.19539149723481689\n",
      "The 19201 th iteration gives loss of 0.1953861066333152\n",
      "The 19202 th iteration gives loss of 0.19538071666842385\n",
      "The 19203 th iteration gives loss of 0.19537532734003119\n",
      "The 19204 th iteration gives loss of 0.19536993864805308\n",
      "The 19205 th iteration gives loss of 0.19536455059237043\n",
      "The 19206 th iteration gives loss of 0.19535916317292212\n",
      "The 19207 th iteration gives loss of 0.1953537763895809\n",
      "The 19208 th iteration gives loss of 0.19534839024226058\n",
      "The 19209 th iteration gives loss of 0.1953430047308531\n",
      "The 19210 th iteration gives loss of 0.19533761985526346\n",
      "The 19211 th iteration gives loss of 0.1953322356153969\n",
      "The 19212 th iteration gives loss of 0.195326852011157\n",
      "The 19213 th iteration gives loss of 0.19532146904244713\n",
      "The 19214 th iteration gives loss of 0.1953160867091578\n",
      "The 19215 th iteration gives loss of 0.19531070501119924\n",
      "The 19216 th iteration gives loss of 0.19530532394847028\n",
      "The 19217 th iteration gives loss of 0.19529994352087768\n",
      "The 19218 th iteration gives loss of 0.1952945637283199\n",
      "The 19219 th iteration gives loss of 0.1952891845707053\n",
      "The 19220 th iteration gives loss of 0.19528380604792508\n",
      "The 19221 th iteration gives loss of 0.19527842815988908\n",
      "The 19222 th iteration gives loss of 0.19527305090648575\n",
      "The 19223 th iteration gives loss of 0.19526767428763647\n",
      "The 19224 th iteration gives loss of 0.1952622983032427\n",
      "The 19225 th iteration gives loss of 0.19525692295318592\n",
      "The 19226 th iteration gives loss of 0.1952515482373839\n",
      "The 19227 th iteration gives loss of 0.19524617415573484\n",
      "The 19228 th iteration gives loss of 0.1952408007081475\n",
      "The 19229 th iteration gives loss of 0.19523542789451326\n",
      "The 19230 th iteration gives loss of 0.19523005571473828\n",
      "The 19231 th iteration gives loss of 0.1952246841687267\n",
      "The 19232 th iteration gives loss of 0.1952193132563882\n",
      "The 19233 th iteration gives loss of 0.19521394297760541\n",
      "The 19234 th iteration gives loss of 0.1952085733322944\n",
      "The 19235 th iteration gives loss of 0.195203204320359\n",
      "The 19236 th iteration gives loss of 0.19519783594169174\n",
      "The 19237 th iteration gives loss of 0.19519246819620012\n",
      "The 19238 th iteration gives loss of 0.19518710108379242\n",
      "The 19239 th iteration gives loss of 0.19518173460436003\n",
      "The 19240 th iteration gives loss of 0.1951763687578121\n",
      "The 19241 th iteration gives loss of 0.19517100354404734\n",
      "The 19242 th iteration gives loss of 0.19516563896297437\n",
      "The 19243 th iteration gives loss of 0.1951602750144851\n",
      "The 19244 th iteration gives loss of 0.19515491169848453\n",
      "The 19245 th iteration gives loss of 0.19514954901488202\n",
      "The 19246 th iteration gives loss of 0.19514418696357488\n",
      "The 19247 th iteration gives loss of 0.19513882554447115\n",
      "The 19248 th iteration gives loss of 0.19513346475746798\n",
      "The 19249 th iteration gives loss of 0.19512810460246782\n",
      "The 19250 th iteration gives loss of 0.19512274507938088\n",
      "The 19251 th iteration gives loss of 0.19511738618809707\n",
      "The 19252 th iteration gives loss of 0.19511202792852228\n",
      "The 19253 th iteration gives loss of 0.19510667030055626\n",
      "The 19254 th iteration gives loss of 0.1951013133041155\n",
      "The 19255 th iteration gives loss of 0.19509595693908857\n",
      "The 19256 th iteration gives loss of 0.1950906012053825\n",
      "The 19257 th iteration gives loss of 0.1950852461029062\n",
      "The 19258 th iteration gives loss of 0.19507989163155365\n",
      "The 19259 th iteration gives loss of 0.19507453779122652\n",
      "The 19260 th iteration gives loss of 0.1950691845818353\n",
      "The 19261 th iteration gives loss of 0.19506383200327682\n",
      "The 19262 th iteration gives loss of 0.19505848005545032\n",
      "The 19263 th iteration gives loss of 0.19505312873827285\n",
      "The 19264 th iteration gives loss of 0.19504777805162835\n",
      "The 19265 th iteration gives loss of 0.1950424279954299\n",
      "The 19266 th iteration gives loss of 0.19503707856958485\n",
      "The 19267 th iteration gives loss of 0.19503172977398953\n",
      "The 19268 th iteration gives loss of 0.1950263816085483\n",
      "The 19269 th iteration gives loss of 0.19502103407315372\n",
      "The 19270 th iteration gives loss of 0.1950156871677196\n",
      "The 19271 th iteration gives loss of 0.19501034089215144\n",
      "The 19272 th iteration gives loss of 0.19500499524633896\n",
      "The 19273 th iteration gives loss of 0.19499965023019877\n",
      "The 19274 th iteration gives loss of 0.1949943058436285\n",
      "The 19275 th iteration gives loss of 0.1949889620865244\n",
      "The 19276 th iteration gives loss of 0.19498361895881094\n",
      "The 19277 th iteration gives loss of 0.1949782764603642\n",
      "The 19278 th iteration gives loss of 0.19497293459110412\n",
      "The 19279 th iteration gives loss of 0.19496759335092112\n",
      "The 19280 th iteration gives loss of 0.19496225273972714\n",
      "The 19281 th iteration gives loss of 0.19495691275742177\n",
      "The 19282 th iteration gives loss of 0.19495157340391095\n",
      "The 19283 th iteration gives loss of 0.19494623467909844\n",
      "The 19284 th iteration gives loss of 0.1949408965828792\n",
      "The 19285 th iteration gives loss of 0.1949355591151596\n",
      "The 19286 th iteration gives loss of 0.1949302222758426\n",
      "The 19287 th iteration gives loss of 0.19492488606483757\n",
      "The 19288 th iteration gives loss of 0.19491955048204887\n",
      "The 19289 th iteration gives loss of 0.1949142155273631\n",
      "The 19290 th iteration gives loss of 0.1949088812007001\n",
      "The 19291 th iteration gives loss of 0.19490354750194644\n",
      "The 19292 th iteration gives loss of 0.19489821443103517\n",
      "The 19293 th iteration gives loss of 0.1948928819878316\n",
      "The 19294 th iteration gives loss of 0.19488755017226797\n",
      "The 19295 th iteration gives loss of 0.19488221898422795\n",
      "The 19296 th iteration gives loss of 0.1948768884236216\n",
      "The 19297 th iteration gives loss of 0.19487155849036492\n",
      "The 19298 th iteration gives loss of 0.19486622918434204\n",
      "The 19299 th iteration gives loss of 0.19486090050546212\n",
      "The 19300 th iteration gives loss of 0.19485557245363425\n",
      "The 19301 th iteration gives loss of 0.19485024502874804\n",
      "The 19302 th iteration gives loss of 0.19484491823072617\n",
      "The 19303 th iteration gives loss of 0.19483959205945414\n",
      "The 19304 th iteration gives loss of 0.19483426651484034\n",
      "The 19305 th iteration gives loss of 0.19482894159679667\n",
      "The 19306 th iteration gives loss of 0.1948236173052253\n",
      "The 19307 th iteration gives loss of 0.19481829364000947\n",
      "The 19308 th iteration gives loss of 0.1948129706010775\n",
      "The 19309 th iteration gives loss of 0.19480764818832177\n",
      "The 19310 th iteration gives loss of 0.19480232640164988\n",
      "The 19311 th iteration gives loss of 0.19479700524095694\n",
      "The 19312 th iteration gives loss of 0.1947916847061505\n",
      "The 19313 th iteration gives loss of 0.19478636479713232\n",
      "The 19314 th iteration gives loss of 0.19478104551380696\n",
      "The 19315 th iteration gives loss of 0.1947757268560877\n",
      "The 19316 th iteration gives loss of 0.1947704088238655\n",
      "The 19317 th iteration gives loss of 0.1947650914170394\n",
      "The 19318 th iteration gives loss of 0.19475977463552904\n",
      "The 19319 th iteration gives loss of 0.19475445847923056\n",
      "The 19320 th iteration gives loss of 0.19474914294805257\n",
      "The 19321 th iteration gives loss of 0.19474382804187876\n",
      "The 19322 th iteration gives loss of 0.19473851376063872\n",
      "The 19323 th iteration gives loss of 0.1947332001042168\n",
      "The 19324 th iteration gives loss of 0.19472788707252103\n",
      "The 19325 th iteration gives loss of 0.1947225746654667\n",
      "The 19326 th iteration gives loss of 0.19471726288293137\n",
      "The 19327 th iteration gives loss of 0.19471195172484754\n",
      "The 19328 th iteration gives loss of 0.1947066411911089\n",
      "The 19329 th iteration gives loss of 0.1947013312816109\n",
      "The 19330 th iteration gives loss of 0.19469602199626648\n",
      "The 19331 th iteration gives loss of 0.19469071333497867\n",
      "The 19332 th iteration gives loss of 0.1946854052976418\n",
      "The 19333 th iteration gives loss of 0.1946800978841665\n",
      "The 19334 th iteration gives loss of 0.19467479109446284\n",
      "The 19335 th iteration gives loss of 0.19466948492842182\n",
      "The 19336 th iteration gives loss of 0.1946641793859527\n",
      "The 19337 th iteration gives loss of 0.19465887446695782\n",
      "The 19338 th iteration gives loss of 0.19465357017134652\n",
      "The 19339 th iteration gives loss of 0.19464826649902406\n",
      "The 19340 th iteration gives loss of 0.1946429634498829\n",
      "The 19341 th iteration gives loss of 0.19463766102383181\n",
      "The 19342 th iteration gives loss of 0.19463235922078068\n",
      "The 19343 th iteration gives loss of 0.1946270580406236\n",
      "The 19344 th iteration gives loss of 0.1946217574832679\n",
      "The 19345 th iteration gives loss of 0.19461645754861828\n",
      "The 19346 th iteration gives loss of 0.1946111582365822\n",
      "The 19347 th iteration gives loss of 0.19460585954706722\n",
      "The 19348 th iteration gives loss of 0.19460056147996532\n",
      "The 19349 th iteration gives loss of 0.19459526403518146\n",
      "The 19350 th iteration gives loss of 0.1945899672126277\n",
      "The 19351 th iteration gives loss of 0.1945846710122005\n",
      "The 19352 th iteration gives loss of 0.19457937543380494\n",
      "The 19353 th iteration gives loss of 0.19457408047735722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 19354 th iteration gives loss of 0.1945687861427422\n",
      "The 19355 th iteration gives loss of 0.19456349242987866\n",
      "The 19356 th iteration gives loss of 0.19455819933866636\n",
      "The 19357 th iteration gives loss of 0.19455290686900775\n",
      "The 19358 th iteration gives loss of 0.19454761502079926\n",
      "The 19359 th iteration gives loss of 0.19454232379395678\n",
      "The 19360 th iteration gives loss of 0.1945370331883889\n",
      "The 19361 th iteration gives loss of 0.19453174320398461\n",
      "The 19362 th iteration gives loss of 0.19452645384064696\n",
      "The 19363 th iteration gives loss of 0.1945211650982946\n",
      "The 19364 th iteration gives loss of 0.19451587697682488\n",
      "The 19365 th iteration gives loss of 0.19451058947613917\n",
      "The 19366 th iteration gives loss of 0.19450530259615054\n",
      "The 19367 th iteration gives loss of 0.19450001633675448\n",
      "The 19368 th iteration gives loss of 0.19449473069785655\n",
      "The 19369 th iteration gives loss of 0.1944894456793567\n",
      "The 19370 th iteration gives loss of 0.19448416128117121\n",
      "The 19371 th iteration gives loss of 0.19447887750319465\n",
      "The 19372 th iteration gives loss of 0.1944735943453378\n",
      "The 19373 th iteration gives loss of 0.1944683118074914\n",
      "The 19374 th iteration gives loss of 0.19446302988958628\n",
      "The 19375 th iteration gives loss of 0.19445774859150136\n",
      "The 19376 th iteration gives loss of 0.1944524679131538\n",
      "The 19377 th iteration gives loss of 0.19444718785444182\n",
      "The 19378 th iteration gives loss of 0.19444190841526948\n",
      "The 19379 th iteration gives loss of 0.19443662959554364\n",
      "The 19380 th iteration gives loss of 0.19443135139517534\n",
      "The 19381 th iteration gives loss of 0.19442607381405758\n",
      "The 19382 th iteration gives loss of 0.1944207968520947\n",
      "The 19383 th iteration gives loss of 0.1944155205091931\n",
      "The 19384 th iteration gives loss of 0.19441024478527616\n",
      "The 19385 th iteration gives loss of 0.19440496968021675\n",
      "The 19386 th iteration gives loss of 0.19439969519394418\n",
      "The 19387 th iteration gives loss of 0.19439442132634965\n",
      "The 19388 th iteration gives loss of 0.19438914807734434\n",
      "The 19389 th iteration gives loss of 0.1943838754468294\n",
      "The 19390 th iteration gives loss of 0.19437860343470484\n",
      "The 19391 th iteration gives loss of 0.19437333204088147\n",
      "The 19392 th iteration gives loss of 0.19436806126526307\n",
      "The 19393 th iteration gives loss of 0.19436279110775886\n",
      "The 19394 th iteration gives loss of 0.1943575215682574\n",
      "The 19395 th iteration gives loss of 0.19435225264668587\n",
      "The 19396 th iteration gives loss of 0.19434698434293135\n",
      "The 19397 th iteration gives loss of 0.19434171665690525\n",
      "The 19398 th iteration gives loss of 0.19433644958849908\n",
      "The 19399 th iteration gives loss of 0.19433118313764647\n",
      "The 19400 th iteration gives loss of 0.19432591730422286\n",
      "The 19401 th iteration gives loss of 0.1943206520881597\n",
      "The 19402 th iteration gives loss of 0.19431538748933702\n",
      "The 19403 th iteration gives loss of 0.19431012350767227\n",
      "The 19404 th iteration gives loss of 0.19430486014307208\n",
      "The 19405 th iteration gives loss of 0.194299597395426\n",
      "The 19406 th iteration gives loss of 0.19429433526465706\n",
      "The 19407 th iteration gives loss of 0.1942890737506612\n",
      "The 19408 th iteration gives loss of 0.1942838128533466\n",
      "The 19409 th iteration gives loss of 0.19427855257261525\n",
      "The 19410 th iteration gives loss of 0.1942732929083741\n",
      "The 19411 th iteration gives loss of 0.19426803386052074\n",
      "The 19412 th iteration gives loss of 0.194262775428965\n",
      "The 19413 th iteration gives loss of 0.19425751761361945\n",
      "The 19414 th iteration gives loss of 0.19425226041438115\n",
      "The 19415 th iteration gives loss of 0.19424700383115137\n",
      "The 19416 th iteration gives loss of 0.19424174786383838\n",
      "The 19417 th iteration gives loss of 0.19423649251235345\n",
      "The 19418 th iteration gives loss of 0.1942312377765935\n",
      "The 19419 th iteration gives loss of 0.19422598365646798\n",
      "The 19420 th iteration gives loss of 0.1942207301518831\n",
      "The 19421 th iteration gives loss of 0.1942154772627412\n",
      "The 19422 th iteration gives loss of 0.19421022498894172\n",
      "The 19423 th iteration gives loss of 0.19420497333039033\n",
      "The 19424 th iteration gives loss of 0.1941997222870014\n",
      "The 19425 th iteration gives loss of 0.1941944718586739\n",
      "The 19426 th iteration gives loss of 0.19418922204532452\n",
      "The 19427 th iteration gives loss of 0.194183972846839\n",
      "The 19428 th iteration gives loss of 0.19417872426313282\n",
      "The 19429 th iteration gives loss of 0.19417347629410817\n",
      "The 19430 th iteration gives loss of 0.19416822893967287\n",
      "The 19431 th iteration gives loss of 0.1941629821997298\n",
      "The 19432 th iteration gives loss of 0.19415773607418535\n",
      "The 19433 th iteration gives loss of 0.19415249056294212\n",
      "The 19434 th iteration gives loss of 0.19414724566591116\n",
      "The 19435 th iteration gives loss of 0.19414200138299842\n",
      "The 19436 th iteration gives loss of 0.19413675771409458\n",
      "The 19437 th iteration gives loss of 0.19413151465911865\n",
      "The 19438 th iteration gives loss of 0.19412627221797812\n",
      "The 19439 th iteration gives loss of 0.19412103039056405\n",
      "The 19440 th iteration gives loss of 0.19411578917678832\n",
      "The 19441 th iteration gives loss of 0.1941105485765611\n",
      "The 19442 th iteration gives loss of 0.19410530858978411\n",
      "The 19443 th iteration gives loss of 0.19410006921637013\n",
      "The 19444 th iteration gives loss of 0.19409483045620984\n",
      "The 19445 th iteration gives loss of 0.1940895923092157\n",
      "The 19446 th iteration gives loss of 0.19408435477530006\n",
      "The 19447 th iteration gives loss of 0.19407911785435839\n",
      "The 19448 th iteration gives loss of 0.19407388154629532\n",
      "The 19449 th iteration gives loss of 0.19406864585101996\n",
      "The 19450 th iteration gives loss of 0.1940634107684408\n",
      "The 19451 th iteration gives loss of 0.1940581762984654\n",
      "The 19452 th iteration gives loss of 0.19405294244097662\n",
      "The 19453 th iteration gives loss of 0.19404770919591294\n",
      "The 19454 th iteration gives loss of 0.19404247656316168\n",
      "The 19455 th iteration gives loss of 0.19403724454263072\n",
      "The 19456 th iteration gives loss of 0.19403201313422358\n",
      "The 19457 th iteration gives loss of 0.19402678233784892\n",
      "The 19458 th iteration gives loss of 0.19402155215341\n",
      "The 19459 th iteration gives loss of 0.19401632258081578\n",
      "The 19460 th iteration gives loss of 0.1940110936199676\n",
      "The 19461 th iteration gives loss of 0.19400586527076535\n",
      "The 19462 th iteration gives loss of 0.19400063753313435\n",
      "The 19463 th iteration gives loss of 0.1939954104069643\n",
      "The 19464 th iteration gives loss of 0.19399018389216016\n",
      "The 19465 th iteration gives loss of 0.19398495798862997\n",
      "The 19466 th iteration gives loss of 0.1939797326962876\n",
      "The 19467 th iteration gives loss of 0.19397450801503316\n",
      "The 19468 th iteration gives loss of 0.1939692839447723\n",
      "The 19469 th iteration gives loss of 0.19396406048541104\n",
      "The 19470 th iteration gives loss of 0.19395883763684188\n",
      "The 19471 th iteration gives loss of 0.19395361539899705\n",
      "The 19472 th iteration gives loss of 0.1939483937717617\n",
      "The 19473 th iteration gives loss of 0.1939431727550447\n",
      "The 19474 th iteration gives loss of 0.19393795234875616\n",
      "The 19475 th iteration gives loss of 0.19393273255279814\n",
      "The 19476 th iteration gives loss of 0.19392751336708536\n",
      "The 19477 th iteration gives loss of 0.1939222947915136\n",
      "The 19478 th iteration gives loss of 0.19391707682600165\n",
      "The 19479 th iteration gives loss of 0.19391185947043063\n",
      "The 19480 th iteration gives loss of 0.19390664272472738\n",
      "The 19481 th iteration gives loss of 0.19390142658879314\n",
      "The 19482 th iteration gives loss of 0.19389621106253493\n",
      "The 19483 th iteration gives loss of 0.19389099614585717\n",
      "The 19484 th iteration gives loss of 0.19388578183865127\n",
      "The 19485 th iteration gives loss of 0.19388056814084673\n",
      "The 19486 th iteration gives loss of 0.19387535505232836\n",
      "The 19487 th iteration gives loss of 0.1938701425730267\n",
      "The 19488 th iteration gives loss of 0.19386493070282457\n",
      "The 19489 th iteration gives loss of 0.19385971944164299\n",
      "The 19490 th iteration gives loss of 0.19385450878938895\n",
      "The 19491 th iteration gives loss of 0.19384929874594556\n",
      "The 19492 th iteration gives loss of 0.19384408931124747\n",
      "The 19493 th iteration gives loss of 0.19383888048518497\n",
      "The 19494 th iteration gives loss of 0.19383367226766823\n",
      "The 19495 th iteration gives loss of 0.19382846465860057\n",
      "The 19496 th iteration gives loss of 0.19382325765788655\n",
      "The 19497 th iteration gives loss of 0.19381805126544024\n",
      "The 19498 th iteration gives loss of 0.19381284548116728\n",
      "The 19499 th iteration gives loss of 0.19380764030496436\n",
      "The 19500 th iteration gives loss of 0.19380243573674058\n",
      "The 19501 th iteration gives loss of 0.19379723177640334\n",
      "The 19502 th iteration gives loss of 0.1937920284238639\n",
      "The 19503 th iteration gives loss of 0.19378682567902078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 19504 th iteration gives loss of 0.19378162354179077\n",
      "The 19505 th iteration gives loss of 0.1937764220120708\n",
      "The 19506 th iteration gives loss of 0.1937712210897672\n",
      "The 19507 th iteration gives loss of 0.19376602077478805\n",
      "The 19508 th iteration gives loss of 0.19376082106704648\n",
      "The 19509 th iteration gives loss of 0.19375562196643525\n",
      "The 19510 th iteration gives loss of 0.1937504234728549\n",
      "The 19511 th iteration gives loss of 0.19374522558623988\n",
      "The 19512 th iteration gives loss of 0.19374002830647982\n",
      "The 19513 th iteration gives loss of 0.1937348316334788\n",
      "The 19514 th iteration gives loss of 0.1937296355671486\n",
      "The 19515 th iteration gives loss of 0.19372444010739906\n",
      "The 19516 th iteration gives loss of 0.1937192452541221\n",
      "The 19517 th iteration gives loss of 0.1937140510072328\n",
      "The 19518 th iteration gives loss of 0.19370885736663734\n",
      "The 19519 th iteration gives loss of 0.1937036643322509\n",
      "The 19520 th iteration gives loss of 0.19369847190396666\n",
      "The 19521 th iteration gives loss of 0.19369328008169087\n",
      "The 19522 th iteration gives loss of 0.19368808886534392\n",
      "The 19523 th iteration gives loss of 0.19368289825481255\n",
      "The 19524 th iteration gives loss of 0.19367770825001507\n",
      "The 19525 th iteration gives loss of 0.19367251885085635\n",
      "The 19526 th iteration gives loss of 0.193667330057246\n",
      "The 19527 th iteration gives loss of 0.1936621418690918\n",
      "The 19528 th iteration gives loss of 0.19365695428629628\n",
      "The 19529 th iteration gives loss of 0.19365176730875927\n",
      "The 19530 th iteration gives loss of 0.1936465809364015\n",
      "The 19531 th iteration gives loss of 0.1936413951691135\n",
      "The 19532 th iteration gives loss of 0.1936362100068163\n",
      "The 19533 th iteration gives loss of 0.19363102544941063\n",
      "The 19534 th iteration gives loss of 0.1936258414967988\n",
      "The 19535 th iteration gives loss of 0.19362065814889431\n",
      "The 19536 th iteration gives loss of 0.19361547540559834\n",
      "The 19537 th iteration gives loss of 0.1936102932668268\n",
      "The 19538 th iteration gives loss of 0.19360511173248052\n",
      "The 19539 th iteration gives loss of 0.1935999308024577\n",
      "The 19540 th iteration gives loss of 0.19359475047667696\n",
      "The 19541 th iteration gives loss of 0.19358957075503466\n",
      "The 19542 th iteration gives loss of 0.19358439163744806\n",
      "The 19543 th iteration gives loss of 0.19357921312381257\n",
      "The 19544 th iteration gives loss of 0.19357403521406027\n",
      "The 19545 th iteration gives loss of 0.1935688579080646\n",
      "The 19546 th iteration gives loss of 0.1935636812057582\n",
      "The 19547 th iteration gives loss of 0.19355850510702774\n",
      "The 19548 th iteration gives loss of 0.1935533296117968\n",
      "The 19549 th iteration gives loss of 0.19354815471995995\n",
      "The 19550 th iteration gives loss of 0.19354298043142987\n",
      "The 19551 th iteration gives loss of 0.19353780674610738\n",
      "The 19552 th iteration gives loss of 0.1935326336639066\n",
      "The 19553 th iteration gives loss of 0.19352746118474146\n",
      "The 19554 th iteration gives loss of 0.19352228930850335\n",
      "The 19555 th iteration gives loss of 0.19351711803509994\n",
      "The 19556 th iteration gives loss of 0.19351194736444516\n",
      "The 19557 th iteration gives loss of 0.193506777296449\n",
      "The 19558 th iteration gives loss of 0.19350160783101664\n",
      "The 19559 th iteration gives loss of 0.19349643896804497\n",
      "The 19560 th iteration gives loss of 0.193491270707449\n",
      "The 19561 th iteration gives loss of 0.19348610304913474\n",
      "The 19562 th iteration gives loss of 0.19348093599301386\n",
      "The 19563 th iteration gives loss of 0.19347576953898984\n",
      "The 19564 th iteration gives loss of 0.19347060368696317\n",
      "The 19565 th iteration gives loss of 0.19346543843684166\n",
      "The 19566 th iteration gives loss of 0.19346027378854935\n",
      "The 19567 th iteration gives loss of 0.1934551097419729\n",
      "The 19568 th iteration gives loss of 0.19344994629703818\n",
      "The 19569 th iteration gives loss of 0.19344478345363392\n",
      "The 19570 th iteration gives loss of 0.19343962121167285\n",
      "The 19571 th iteration gives loss of 0.1934344595710626\n",
      "The 19572 th iteration gives loss of 0.19342929853172308\n",
      "The 19573 th iteration gives loss of 0.1934241380935423\n",
      "The 19574 th iteration gives loss of 0.19341897825643725\n",
      "The 19575 th iteration gives loss of 0.19341381902031743\n",
      "The 19576 th iteration gives loss of 0.19340866038508464\n",
      "The 19577 th iteration gives loss of 0.19340350235064266\n",
      "The 19578 th iteration gives loss of 0.19339834491689936\n",
      "The 19579 th iteration gives loss of 0.19339318808377062\n",
      "The 19580 th iteration gives loss of 0.1933880318511687\n",
      "The 19581 th iteration gives loss of 0.1933828762189826\n",
      "The 19582 th iteration gives loss of 0.19337772118713303\n",
      "The 19583 th iteration gives loss of 0.19337256675551984\n",
      "The 19584 th iteration gives loss of 0.1933674129240494\n",
      "The 19585 th iteration gives loss of 0.19336225969263812\n",
      "The 19586 th iteration gives loss of 0.19335710706118583\n",
      "The 19587 th iteration gives loss of 0.1933519550296097\n",
      "The 19588 th iteration gives loss of 0.19334680359780448\n",
      "The 19589 th iteration gives loss of 0.19334165276568102\n",
      "The 19590 th iteration gives loss of 0.19333650253315268\n",
      "The 19591 th iteration gives loss of 0.19333135290011394\n",
      "The 19592 th iteration gives loss of 0.1933262038664905\n",
      "The 19593 th iteration gives loss of 0.1933210554321735\n",
      "The 19594 th iteration gives loss of 0.19331590759708508\n",
      "The 19595 th iteration gives loss of 0.19331076036111866\n",
      "The 19596 th iteration gives loss of 0.19330561372418753\n",
      "The 19597 th iteration gives loss of 0.19330046768619988\n",
      "The 19598 th iteration gives loss of 0.19329532224706347\n",
      "The 19599 th iteration gives loss of 0.19329017740668808\n",
      "The 19600 th iteration gives loss of 0.19328503316497597\n",
      "The 19601 th iteration gives loss of 0.1932798895218444\n",
      "The 19602 th iteration gives loss of 0.19327474647718818\n",
      "The 19603 th iteration gives loss of 0.19326960403092552\n",
      "The 19604 th iteration gives loss of 0.19326446218294965\n",
      "The 19605 th iteration gives loss of 0.19325932093318296\n",
      "The 19606 th iteration gives loss of 0.1932541802815266\n",
      "The 19607 th iteration gives loss of 0.19324904022788314\n",
      "The 19608 th iteration gives loss of 0.1932439007721712\n",
      "The 19609 th iteration gives loss of 0.19323876191429384\n",
      "The 19610 th iteration gives loss of 0.19323362365415822\n",
      "The 19611 th iteration gives loss of 0.19322848599166562\n",
      "The 19612 th iteration gives loss of 0.1932233489267438\n",
      "The 19613 th iteration gives loss of 0.19321821245929185\n",
      "The 19614 th iteration gives loss of 0.19321307658920153\n",
      "The 19615 th iteration gives loss of 0.19320794131639143\n",
      "The 19616 th iteration gives loss of 0.19320280664077086\n",
      "The 19617 th iteration gives loss of 0.19319767256224754\n",
      "The 19618 th iteration gives loss of 0.19319253908072784\n",
      "The 19619 th iteration gives loss of 0.19318740619612151\n",
      "The 19620 th iteration gives loss of 0.19318227390832973\n",
      "The 19621 th iteration gives loss of 0.19317714221727383\n",
      "The 19622 th iteration gives loss of 0.1931720111228486\n",
      "The 19623 th iteration gives loss of 0.19316688062495987\n",
      "The 19624 th iteration gives loss of 0.19316175072353262\n",
      "The 19625 th iteration gives loss of 0.1931566214184711\n",
      "The 19626 th iteration gives loss of 0.1931514927096699\n",
      "The 19627 th iteration gives loss of 0.19314636459704276\n",
      "The 19628 th iteration gives loss of 0.19314123708049907\n",
      "The 19629 th iteration gives loss of 0.19313611015994958\n",
      "The 19630 th iteration gives loss of 0.19313098383528876\n",
      "The 19631 th iteration gives loss of 0.19312585810643873\n",
      "The 19632 th iteration gives loss of 0.19312073297330498\n",
      "The 19633 th iteration gives loss of 0.19311560843579162\n",
      "The 19634 th iteration gives loss of 0.19311048449381968\n",
      "The 19635 th iteration gives loss of 0.19310536114727464\n",
      "The 19636 th iteration gives loss of 0.1931002383960837\n",
      "The 19637 th iteration gives loss of 0.1930951162401438\n",
      "The 19638 th iteration gives loss of 0.19308999467937518\n",
      "The 19639 th iteration gives loss of 0.19308487371367458\n",
      "The 19640 th iteration gives loss of 0.19307975334294944\n",
      "The 19641 th iteration gives loss of 0.19307463356711424\n",
      "The 19642 th iteration gives loss of 0.1930695143860747\n",
      "The 19643 th iteration gives loss of 0.193064395799736\n",
      "The 19644 th iteration gives loss of 0.1930592778080107\n",
      "The 19645 th iteration gives loss of 0.19305416041081133\n",
      "The 19646 th iteration gives loss of 0.19304904360803127\n",
      "The 19647 th iteration gives loss of 0.19304392739959894\n",
      "The 19648 th iteration gives loss of 0.1930388117854036\n",
      "The 19649 th iteration gives loss of 0.19303369676535956\n",
      "The 19650 th iteration gives loss of 0.19302858233938683\n",
      "The 19651 th iteration gives loss of 0.19302346850736987\n",
      "The 19652 th iteration gives loss of 0.19301835526923972\n",
      "The 19653 th iteration gives loss of 0.19301324262489486\n",
      "The 19654 th iteration gives loss of 0.1930081305742491\n",
      "The 19655 th iteration gives loss of 0.19300301911720108\n",
      "The 19656 th iteration gives loss of 0.19299790825367227\n",
      "The 19657 th iteration gives loss of 0.19299279798355584\n",
      "The 19658 th iteration gives loss of 0.19298768830676846\n",
      "The 19659 th iteration gives loss of 0.1929825792232231\n",
      "The 19660 th iteration gives loss of 0.1929774707328081\n",
      "The 19661 th iteration gives loss of 0.1929723628354546\n",
      "The 19662 th iteration gives loss of 0.19296725553106225\n",
      "The 19663 th iteration gives loss of 0.19296214881954354\n",
      "The 19664 th iteration gives loss of 0.19295704270079841\n",
      "The 19665 th iteration gives loss of 0.19295193717474898\n",
      "The 19666 th iteration gives loss of 0.19294683224128634\n",
      "The 19667 th iteration gives loss of 0.19294172790033248\n",
      "The 19668 th iteration gives loss of 0.19293662415178206\n",
      "The 19669 th iteration gives loss of 0.1929315209955613\n",
      "The 19670 th iteration gives loss of 0.19292641843156563\n",
      "The 19671 th iteration gives loss of 0.1929213164597117\n",
      "The 19672 th iteration gives loss of 0.19291621507990464\n",
      "The 19673 th iteration gives loss of 0.19291111429205052\n",
      "The 19674 th iteration gives loss of 0.1929060140960647\n",
      "The 19675 th iteration gives loss of 0.19290091449184973\n",
      "The 19676 th iteration gives loss of 0.19289581547931373\n",
      "The 19677 th iteration gives loss of 0.19289071705837083\n",
      "The 19678 th iteration gives loss of 0.1928856192289258\n",
      "The 19679 th iteration gives loss of 0.19288052199088968\n",
      "The 19680 th iteration gives loss of 0.19287542534415847\n",
      "The 19681 th iteration gives loss of 0.1928703292886638\n",
      "The 19682 th iteration gives loss of 0.19286523382429885\n",
      "The 19683 th iteration gives loss of 0.19286013895097073\n",
      "The 19684 th iteration gives loss of 0.19285504466859996\n",
      "The 19685 th iteration gives loss of 0.19284995097708924\n",
      "The 19686 th iteration gives loss of 0.19284485787634345\n",
      "The 19687 th iteration gives loss of 0.1928397653662766\n",
      "The 19688 th iteration gives loss of 0.1928346734467911\n",
      "The 19689 th iteration gives loss of 0.19282958211780077\n",
      "The 19690 th iteration gives loss of 0.19282449137921945\n",
      "The 19691 th iteration gives loss of 0.1928194012309462\n",
      "The 19692 th iteration gives loss of 0.19281431167289437\n",
      "The 19693 th iteration gives loss of 0.19280922270497647\n",
      "The 19694 th iteration gives loss of 0.19280413432709098\n",
      "The 19695 th iteration gives loss of 0.19279904653915644\n",
      "The 19696 th iteration gives loss of 0.19279395934108284\n",
      "The 19697 th iteration gives loss of 0.19278887273276582\n",
      "The 19698 th iteration gives loss of 0.19278378671413218\n",
      "The 19699 th iteration gives loss of 0.19277870128507998\n",
      "The 19700 th iteration gives loss of 0.19277361644551477\n",
      "The 19701 th iteration gives loss of 0.19276853219535908\n",
      "The 19702 th iteration gives loss of 0.19276344853450814\n",
      "The 19703 th iteration gives loss of 0.1927583654628864\n",
      "The 19704 th iteration gives loss of 0.19275328298037778\n",
      "The 19705 th iteration gives loss of 0.1927482010869028\n",
      "The 19706 th iteration gives loss of 0.19274311978239045\n",
      "The 19707 th iteration gives loss of 0.1927380390667332\n",
      "The 19708 th iteration gives loss of 0.19273295893983108\n",
      "The 19709 th iteration gives loss of 0.19272787940160857\n",
      "The 19710 th iteration gives loss of 0.1927228004519671\n",
      "The 19711 th iteration gives loss of 0.19271772209081342\n",
      "The 19712 th iteration gives loss of 0.19271264431806656\n",
      "The 19713 th iteration gives loss of 0.1927075671336287\n",
      "The 19714 th iteration gives loss of 0.19270249053741645\n",
      "The 19715 th iteration gives loss of 0.19269741452932376\n",
      "The 19716 th iteration gives loss of 0.19269233910927605\n",
      "The 19717 th iteration gives loss of 0.19268726427716903\n",
      "The 19718 th iteration gives loss of 0.19268219003291126\n",
      "The 19719 th iteration gives loss of 0.19267711637642237\n",
      "The 19720 th iteration gives loss of 0.1926720433076159\n",
      "The 19721 th iteration gives loss of 0.1926669708263963\n",
      "The 19722 th iteration gives loss of 0.1926618989326711\n",
      "The 19723 th iteration gives loss of 0.19265682762633127\n",
      "The 19724 th iteration gives loss of 0.192651756907316\n",
      "The 19725 th iteration gives loss of 0.19264668677552102\n",
      "The 19726 th iteration gives loss of 0.192641617230857\n",
      "The 19727 th iteration gives loss of 0.19263654827322546\n",
      "The 19728 th iteration gives loss of 0.1926314799025446\n",
      "The 19729 th iteration gives loss of 0.1926264121187231\n",
      "The 19730 th iteration gives loss of 0.19262134492167007\n",
      "The 19731 th iteration gives loss of 0.19261627831130015\n",
      "The 19732 th iteration gives loss of 0.19261121228752043\n",
      "The 19733 th iteration gives loss of 0.19260614685022653\n",
      "The 19734 th iteration gives loss of 0.19260108199933998\n",
      "The 19735 th iteration gives loss of 0.1925960177347652\n",
      "The 19736 th iteration gives loss of 0.19259095405641719\n",
      "The 19737 th iteration gives loss of 0.1925858909642097\n",
      "The 19738 th iteration gives loss of 0.19258082845804234\n",
      "The 19739 th iteration gives loss of 0.1925757665378263\n",
      "The 19740 th iteration gives loss of 0.19257070520347622\n",
      "The 19741 th iteration gives loss of 0.1925656444548908\n",
      "The 19742 th iteration gives loss of 0.19256058429198886\n",
      "The 19743 th iteration gives loss of 0.1925555247146844\n",
      "The 19744 th iteration gives loss of 0.19255046572286633\n",
      "The 19745 th iteration gives loss of 0.19254540731647637\n",
      "The 19746 th iteration gives loss of 0.1925403494953963\n",
      "The 19747 th iteration gives loss of 0.1925352922595417\n",
      "The 19748 th iteration gives loss of 0.19253023560883725\n",
      "The 19749 th iteration gives loss of 0.19252517954318318\n",
      "The 19750 th iteration gives loss of 0.19252012406247354\n",
      "The 19751 th iteration gives loss of 0.19251506916664193\n",
      "The 19752 th iteration gives loss of 0.19251001485558628\n",
      "The 19753 th iteration gives loss of 0.19250496112921422\n",
      "The 19754 th iteration gives loss of 0.19249990798744004\n",
      "The 19755 th iteration gives loss of 0.1924948554301754\n",
      "The 19756 th iteration gives loss of 0.19248980345732633\n",
      "The 19757 th iteration gives loss of 0.19248475206880739\n",
      "The 19758 th iteration gives loss of 0.19247970126452446\n",
      "The 19759 th iteration gives loss of 0.1924746510443788\n",
      "The 19760 th iteration gives loss of 0.1924696014082976\n",
      "The 19761 th iteration gives loss of 0.192464552356184\n",
      "The 19762 th iteration gives loss of 0.1924595038879322\n",
      "The 19763 th iteration gives loss of 0.19245445600348063\n",
      "The 19764 th iteration gives loss of 0.19244940870271757\n",
      "The 19765 th iteration gives loss of 0.19244436198556059\n",
      "The 19766 th iteration gives loss of 0.19243931585191493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 19767 th iteration gives loss of 0.1924342703016965\n",
      "The 19768 th iteration gives loss of 0.19242922533481766\n",
      "The 19769 th iteration gives loss of 0.1924241809511804\n",
      "The 19770 th iteration gives loss of 0.19241913715069497\n",
      "The 19771 th iteration gives loss of 0.19241409393327652\n",
      "The 19772 th iteration gives loss of 0.19240905129882957\n",
      "The 19773 th iteration gives loss of 0.19240400924727064\n",
      "The 19774 th iteration gives loss of 0.1923989677785117\n",
      "The 19775 th iteration gives loss of 0.1923939268924443\n",
      "The 19776 th iteration gives loss of 0.1923888865890067\n",
      "The 19777 th iteration gives loss of 0.1923838468680823\n",
      "The 19778 th iteration gives loss of 0.1923788077295921\n",
      "The 19779 th iteration gives loss of 0.1923737691734478\n",
      "The 19780 th iteration gives loss of 0.19236873119956857\n",
      "The 19781 th iteration gives loss of 0.19236369380784063\n",
      "The 19782 th iteration gives loss of 0.19235865699819166\n",
      "The 19783 th iteration gives loss of 0.19235362077052154\n",
      "The 19784 th iteration gives loss of 0.19234858512476188\n",
      "The 19785 th iteration gives loss of 0.19234355006079729\n",
      "The 19786 th iteration gives loss of 0.1923385155785483\n",
      "The 19787 th iteration gives loss of 0.19233348167792685\n",
      "The 19788 th iteration gives loss of 0.19232844835884608\n",
      "The 19789 th iteration gives loss of 0.1923234156212063\n",
      "The 19790 th iteration gives loss of 0.19231838346492433\n",
      "The 19791 th iteration gives loss of 0.19231335188990847\n",
      "The 19792 th iteration gives loss of 0.19230832089606506\n",
      "The 19793 th iteration gives loss of 0.19230329048330774\n",
      "The 19794 th iteration gives loss of 0.19229826065155547\n",
      "The 19795 th iteration gives loss of 0.19229323140070242\n",
      "The 19796 th iteration gives loss of 0.19228820273067268\n",
      "The 19797 th iteration gives loss of 0.1922831746413758\n",
      "The 19798 th iteration gives loss of 0.19227814713271466\n",
      "The 19799 th iteration gives loss of 0.19227312020459802\n",
      "The 19800 th iteration gives loss of 0.19226809385693538\n",
      "The 19801 th iteration gives loss of 0.19226306808965674\n",
      "The 19802 th iteration gives loss of 0.19225804290264767\n",
      "The 19803 th iteration gives loss of 0.1922530182958349\n",
      "The 19804 th iteration gives loss of 0.1922479942691236\n",
      "The 19805 th iteration gives loss of 0.19224297082241726\n",
      "The 19806 th iteration gives loss of 0.19223794795563431\n",
      "The 19807 th iteration gives loss of 0.1922329256686934\n",
      "The 19808 th iteration gives loss of 0.19222790396148695\n",
      "The 19809 th iteration gives loss of 0.1922228828339306\n",
      "The 19810 th iteration gives loss of 0.19221786228593543\n",
      "The 19811 th iteration gives loss of 0.1922128423174202\n",
      "The 19812 th iteration gives loss of 0.19220782292828498\n",
      "The 19813 th iteration gives loss of 0.19220280411845297\n",
      "The 19814 th iteration gives loss of 0.19219778588781783\n",
      "The 19815 th iteration gives loss of 0.1921927682363045\n",
      "The 19816 th iteration gives loss of 0.19218775116382047\n",
      "The 19817 th iteration gives loss of 0.19218273467026883\n",
      "The 19818 th iteration gives loss of 0.19217771875556308\n",
      "The 19819 th iteration gives loss of 0.1921727034196253\n",
      "The 19820 th iteration gives loss of 0.19216768866234799\n",
      "The 19821 th iteration gives loss of 0.19216267448364663\n",
      "The 19822 th iteration gives loss of 0.192157660883449\n",
      "The 19823 th iteration gives loss of 0.1921526478616479\n",
      "The 19824 th iteration gives loss of 0.19214763541814942\n",
      "The 19825 th iteration gives loss of 0.192142623552887\n",
      "The 19826 th iteration gives loss of 0.1921376122657492\n",
      "The 19827 th iteration gives loss of 0.1921326015566548\n",
      "The 19828 th iteration gives loss of 0.19212759142551938\n",
      "The 19829 th iteration gives loss of 0.19212258187224893\n",
      "The 19830 th iteration gives loss of 0.1921175728967518\n",
      "The 19831 th iteration gives loss of 0.19211256449894493\n",
      "The 19832 th iteration gives loss of 0.1921075566787447\n",
      "The 19833 th iteration gives loss of 0.19210254943603355\n",
      "The 19834 th iteration gives loss of 0.1920975427707592\n",
      "The 19835 th iteration gives loss of 0.19209253668280735\n",
      "The 19836 th iteration gives loss of 0.1920875311720882\n",
      "The 19837 th iteration gives loss of 0.19208252623853522\n",
      "The 19838 th iteration gives loss of 0.19207752188203922\n",
      "The 19839 th iteration gives loss of 0.1920725181025208\n",
      "The 19840 th iteration gives loss of 0.19206751489988066\n",
      "The 19841 th iteration gives loss of 0.19206251227404061\n",
      "The 19842 th iteration gives loss of 0.19205751022490264\n",
      "The 19843 th iteration gives loss of 0.1920525087523834\n",
      "The 19844 th iteration gives loss of 0.1920475078564013\n",
      "The 19845 th iteration gives loss of 0.19204250753685087\n",
      "The 19846 th iteration gives loss of 0.19203750779364956\n",
      "The 19847 th iteration gives loss of 0.19203250862671423\n",
      "The 19848 th iteration gives loss of 0.19202751003595137\n",
      "The 19849 th iteration gives loss of 0.1920225120212686\n",
      "The 19850 th iteration gives loss of 0.19201751458258545\n",
      "The 19851 th iteration gives loss of 0.19201251771980513\n",
      "The 19852 th iteration gives loss of 0.19200752143284\n",
      "The 19853 th iteration gives loss of 0.19200252572160278\n",
      "The 19854 th iteration gives loss of 0.19199753058600647\n",
      "The 19855 th iteration gives loss of 0.19199253602595595\n",
      "The 19856 th iteration gives loss of 0.19198754204136764\n",
      "The 19857 th iteration gives loss of 0.19198254863215183\n",
      "The 19858 th iteration gives loss of 0.19197755579822903\n",
      "The 19859 th iteration gives loss of 0.19197256353948625\n",
      "The 19860 th iteration gives loss of 0.1919675718558608\n",
      "The 19861 th iteration gives loss of 0.1919625807472463\n",
      "The 19862 th iteration gives loss of 0.1919575902135639\n",
      "The 19863 th iteration gives loss of 0.19195260025470914\n",
      "The 19864 th iteration gives loss of 0.1919476108706105\n",
      "The 19865 th iteration gives loss of 0.19194262206118218\n",
      "The 19866 th iteration gives loss of 0.1919376338263214\n",
      "The 19867 th iteration gives loss of 0.19193264616595096\n",
      "The 19868 th iteration gives loss of 0.19192765907996398\n",
      "The 19869 th iteration gives loss of 0.19192267256829573\n",
      "The 19870 th iteration gives loss of 0.19191768663083858\n",
      "The 19871 th iteration gives loss of 0.19191270126750976\n",
      "The 19872 th iteration gives loss of 0.1919077164782292\n",
      "The 19873 th iteration gives loss of 0.1919027322628987\n",
      "The 19874 th iteration gives loss of 0.19189774862143316\n",
      "The 19875 th iteration gives loss of 0.19189276555374052\n",
      "The 19876 th iteration gives loss of 0.1918877830597306\n",
      "The 19877 th iteration gives loss of 0.19188280113931935\n",
      "The 19878 th iteration gives loss of 0.1918778197924122\n",
      "The 19879 th iteration gives loss of 0.19187283901893373\n",
      "The 19880 th iteration gives loss of 0.1918678588187885\n",
      "The 19881 th iteration gives loss of 0.1918628791918882\n",
      "The 19882 th iteration gives loss of 0.19185790013813375\n",
      "The 19883 th iteration gives loss of 0.19185292165744652\n",
      "The 19884 th iteration gives loss of 0.19184794374974434\n",
      "The 19885 th iteration gives loss of 0.1918429664149341\n",
      "The 19886 th iteration gives loss of 0.1918379896529206\n",
      "The 19887 th iteration gives loss of 0.19183301346362344\n",
      "The 19888 th iteration gives loss of 0.19182803784695435\n",
      "The 19889 th iteration gives loss of 0.19182306280280823\n",
      "The 19890 th iteration gives loss of 0.1918180883311178\n",
      "The 19891 th iteration gives loss of 0.1918131144317903\n",
      "The 19892 th iteration gives loss of 0.19180814110471828\n",
      "The 19893 th iteration gives loss of 0.1918031683498373\n",
      "The 19894 th iteration gives loss of 0.19179819616706237\n",
      "The 19895 th iteration gives loss of 0.19179322455627945\n",
      "The 19896 th iteration gives loss of 0.1917882535174186\n",
      "The 19897 th iteration gives loss of 0.1917832830503756\n",
      "The 19898 th iteration gives loss of 0.19177831315508745\n",
      "The 19899 th iteration gives loss of 0.1917733438314445\n",
      "The 19900 th iteration gives loss of 0.19176837507937938\n",
      "The 19901 th iteration gives loss of 0.19176340689877022\n",
      "The 19902 th iteration gives loss of 0.19175843928955877\n",
      "The 19903 th iteration gives loss of 0.19175347225164624\n",
      "The 19904 th iteration gives loss of 0.19174850578494446\n",
      "The 19905 th iteration gives loss of 0.191743539889363\n",
      "The 19906 th iteration gives loss of 0.1917385745648229\n",
      "The 19907 th iteration gives loss of 0.19173360981121923\n",
      "The 19908 th iteration gives loss of 0.19172864562848324\n",
      "The 19909 th iteration gives loss of 0.19172368201651133\n",
      "The 19910 th iteration gives loss of 0.19171871897522827\n",
      "The 19911 th iteration gives loss of 0.19171375650454006\n",
      "The 19912 th iteration gives loss of 0.19170879460435036\n",
      "The 19913 th iteration gives loss of 0.19170383327457952\n",
      "The 19914 th iteration gives loss of 0.19169887251513903\n",
      "The 19915 th iteration gives loss of 0.19169391232593677\n",
      "The 19916 th iteration gives loss of 0.19168895270689917\n",
      "The 19917 th iteration gives loss of 0.19168399365792915\n",
      "The 19918 th iteration gives loss of 0.19167903517892293\n",
      "The 19919 th iteration gives loss of 0.19167407726981134\n",
      "The 19920 th iteration gives loss of 0.19166911993050595\n",
      "The 19921 th iteration gives loss of 0.1916641631609054\n",
      "The 19922 th iteration gives loss of 0.19165920696093358\n",
      "The 19923 th iteration gives loss of 0.19165425133050262\n",
      "The 19924 th iteration gives loss of 0.19164929626951957\n",
      "The 19925 th iteration gives loss of 0.19164434177789202\n",
      "The 19926 th iteration gives loss of 0.19163938785554738\n",
      "The 19927 th iteration gives loss of 0.19163443450238493\n",
      "The 19928 th iteration gives loss of 0.1916294817183195\n",
      "The 19929 th iteration gives loss of 0.1916245295032681\n",
      "The 19930 th iteration gives loss of 0.1916195778571387\n",
      "The 19931 th iteration gives loss of 0.19161462677983512\n",
      "The 19932 th iteration gives loss of 0.19160967627129194\n",
      "The 19933 th iteration gives loss of 0.19160472633139933\n",
      "The 19934 th iteration gives loss of 0.19159977696007596\n",
      "The 19935 th iteration gives loss of 0.1915948281572404\n",
      "The 19936 th iteration gives loss of 0.19158987992279672\n",
      "The 19937 th iteration gives loss of 0.19158493225666437\n",
      "The 19938 th iteration gives loss of 0.19157998515874047\n",
      "The 19939 th iteration gives loss of 0.19157503862896605\n",
      "The 19940 th iteration gives loss of 0.19157009266722902\n",
      "The 19941 th iteration gives loss of 0.19156514727344554\n",
      "The 19942 th iteration gives loss of 0.1915602024475335\n",
      "The 19943 th iteration gives loss of 0.19155525818940466\n",
      "The 19944 th iteration gives loss of 0.19155031449896517\n",
      "The 19945 th iteration gives loss of 0.19154537137613917\n",
      "The 19946 th iteration gives loss of 0.1915404288208244\n",
      "The 19947 th iteration gives loss of 0.1915354868329393\n",
      "The 19948 th iteration gives loss of 0.19153054541239772\n",
      "The 19949 th iteration gives loss of 0.19152560455911943\n",
      "The 19950 th iteration gives loss of 0.19152066427299685\n",
      "The 19951 th iteration gives loss of 0.1915157245539578\n",
      "The 19952 th iteration gives loss of 0.19151078540191785\n",
      "The 19953 th iteration gives loss of 0.1915058468167742\n",
      "The 19954 th iteration gives loss of 0.19150090879846302\n",
      "The 19955 th iteration gives loss of 0.1914959713468625\n",
      "The 19956 th iteration gives loss of 0.1914910344619245\n",
      "The 19957 th iteration gives loss of 0.1914860981435279\n",
      "The 19958 th iteration gives loss of 0.19148116239159715\n",
      "The 19959 th iteration gives loss of 0.1914762272060518\n",
      "The 19960 th iteration gives loss of 0.19147129258680057\n",
      "The 19961 th iteration gives loss of 0.19146635853374896\n",
      "The 19962 th iteration gives loss of 0.19146142504682132\n",
      "The 19963 th iteration gives loss of 0.19145649212592503\n",
      "The 19964 th iteration gives loss of 0.19145155977096218\n",
      "The 19965 th iteration gives loss of 0.19144662798185777\n",
      "The 19966 th iteration gives loss of 0.1914416967585263\n",
      "The 19967 th iteration gives loss of 0.19143676610087035\n",
      "The 19968 th iteration gives loss of 0.19143183600881494\n",
      "The 19969 th iteration gives loss of 0.19142690648225855\n",
      "The 19970 th iteration gives loss of 0.19142197752112985\n",
      "The 19971 th iteration gives loss of 0.19141704912532206\n",
      "The 19972 th iteration gives loss of 0.19141212129476057\n",
      "The 19973 th iteration gives loss of 0.19140719402936088\n",
      "The 19974 th iteration gives loss of 0.19140226732903018\n",
      "The 19975 th iteration gives loss of 0.1913973411936773\n",
      "The 19976 th iteration gives loss of 0.19139241562322246\n",
      "The 19977 th iteration gives loss of 0.19138749061757834\n",
      "The 19978 th iteration gives loss of 0.19138256617665605\n",
      "The 19979 th iteration gives loss of 0.19137764230035767\n",
      "The 19980 th iteration gives loss of 0.1913727189886157\n",
      "The 19981 th iteration gives loss of 0.19136779624132572\n",
      "The 19982 th iteration gives loss of 0.19136287405840627\n",
      "The 19983 th iteration gives loss of 0.19135795243977768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 19984 th iteration gives loss of 0.19135303138534138\n",
      "The 19985 th iteration gives loss of 0.19134811089501388\n",
      "The 19986 th iteration gives loss of 0.19134319096871355\n",
      "The 19987 th iteration gives loss of 0.19133827160634465\n",
      "The 19988 th iteration gives loss of 0.19133335280783131\n",
      "The 19989 th iteration gives loss of 0.19132843457308155\n",
      "The 19990 th iteration gives loss of 0.19132351690200786\n",
      "The 19991 th iteration gives loss of 0.19131859979451324\n",
      "The 19992 th iteration gives loss of 0.19131368325053016\n",
      "The 19993 th iteration gives loss of 0.19130876726994947\n",
      "The 19994 th iteration gives loss of 0.19130385185269905\n",
      "The 19995 th iteration gives loss of 0.1912989369986977\n",
      "The 19996 th iteration gives loss of 0.19129402270784635\n",
      "The 19997 th iteration gives loss of 0.19128910898006002\n",
      "The 19998 th iteration gives loss of 0.19128419581524667\n",
      "The 19999 th iteration gives loss of 0.19127928321333043\n",
      "The 20000 th iteration gives loss of 0.19127437117421953\n",
      "The 20001 th iteration gives loss of 0.1912694596978223\n",
      "The 20002 th iteration gives loss of 0.19126454878405624\n",
      "The 20003 th iteration gives loss of 0.19125963843283672\n",
      "The 20004 th iteration gives loss of 0.19125472864407708\n",
      "The 20005 th iteration gives loss of 0.1912498194176917\n",
      "The 20006 th iteration gives loss of 0.19124491075358774\n",
      "The 20007 th iteration gives loss of 0.19124000265168223\n",
      "The 20008 th iteration gives loss of 0.1912350951118801\n",
      "The 20009 th iteration gives loss of 0.19123018813410947\n",
      "The 20010 th iteration gives loss of 0.1912252817182763\n",
      "The 20011 th iteration gives loss of 0.19122037586429047\n",
      "The 20012 th iteration gives loss of 0.19121547057206237\n",
      "The 20013 th iteration gives loss of 0.19121056584151305\n",
      "The 20014 th iteration gives loss of 0.1912056616725544\n",
      "The 20015 th iteration gives loss of 0.19120075806510156\n",
      "The 20016 th iteration gives loss of 0.19119585501906153\n",
      "The 20017 th iteration gives loss of 0.19119095253435517\n",
      "The 20018 th iteration gives loss of 0.19118605061089142\n",
      "The 20019 th iteration gives loss of 0.19118114924857776\n",
      "The 20020 th iteration gives loss of 0.1911762484473396\n",
      "The 20021 th iteration gives loss of 0.1911713482070924\n",
      "The 20022 th iteration gives loss of 0.19116644852772155\n",
      "The 20023 th iteration gives loss of 0.1911615494091726\n",
      "The 20024 th iteration gives loss of 0.19115665085134384\n",
      "The 20025 th iteration gives loss of 0.1911517528541541\n",
      "The 20026 th iteration gives loss of 0.19114685541751017\n",
      "The 20027 th iteration gives loss of 0.1911419585413326\n",
      "The 20028 th iteration gives loss of 0.1911370622255319\n",
      "The 20029 th iteration gives loss of 0.19113216647002385\n",
      "The 20030 th iteration gives loss of 0.19112727127471488\n",
      "The 20031 th iteration gives loss of 0.19112237663952913\n",
      "The 20032 th iteration gives loss of 0.19111748256436917\n",
      "The 20033 th iteration gives loss of 0.1911125890491579\n",
      "The 20034 th iteration gives loss of 0.19110769609379313\n",
      "The 20035 th iteration gives loss of 0.19110280369821014\n",
      "The 20036 th iteration gives loss of 0.191097911862319\n",
      "The 20037 th iteration gives loss of 0.1910930205860135\n",
      "The 20038 th iteration gives loss of 0.1910881298692259\n",
      "The 20039 th iteration gives loss of 0.19108323971185617\n",
      "The 20040 th iteration gives loss of 0.19107835011383664\n",
      "The 20041 th iteration gives loss of 0.19107346107506978\n",
      "The 20042 th iteration gives loss of 0.19106857259546287\n",
      "The 20043 th iteration gives loss of 0.19106368467493648\n",
      "The 20044 th iteration gives loss of 0.19105879731341102\n",
      "The 20045 th iteration gives loss of 0.19105391051078466\n",
      "The 20046 th iteration gives loss of 0.19104902426698614\n",
      "The 20047 th iteration gives loss of 0.1910441385819135\n",
      "The 20048 th iteration gives loss of 0.19103925345549808\n",
      "The 20049 th iteration gives loss of 0.19103436888764258\n",
      "The 20050 th iteration gives loss of 0.19102948487825944\n",
      "The 20051 th iteration gives loss of 0.19102460142727068\n",
      "The 20052 th iteration gives loss of 0.19101971853458358\n",
      "The 20053 th iteration gives loss of 0.19101483620011991\n",
      "The 20054 th iteration gives loss of 0.19100995442378796\n",
      "The 20055 th iteration gives loss of 0.19100507320549612\n",
      "The 20056 th iteration gives loss of 0.1910001925451641\n",
      "The 20057 th iteration gives loss of 0.19099531244270512\n",
      "The 20058 th iteration gives loss of 0.19099043289803272\n",
      "The 20059 th iteration gives loss of 0.19098555391105476\n",
      "The 20060 th iteration gives loss of 0.1909806754816942\n",
      "The 20061 th iteration gives loss of 0.19097579760986885\n",
      "The 20062 th iteration gives loss of 0.19097092029547763\n",
      "The 20063 th iteration gives loss of 0.19096604353845892\n",
      "The 20064 th iteration gives loss of 0.19096116733869606\n",
      "The 20065 th iteration gives loss of 0.19095629169611716\n",
      "The 20066 th iteration gives loss of 0.1909514166106353\n",
      "The 20067 th iteration gives loss of 0.19094654208216597\n",
      "The 20068 th iteration gives loss of 0.19094166811062976\n",
      "The 20069 th iteration gives loss of 0.19093679469592498\n",
      "The 20070 th iteration gives loss of 0.19093192183797977\n",
      "The 20071 th iteration gives loss of 0.1909270495367066\n",
      "The 20072 th iteration gives loss of 0.1909221777920023\n",
      "The 20073 th iteration gives loss of 0.19091730660380457\n",
      "The 20074 th iteration gives loss of 0.19091243597201743\n",
      "The 20075 th iteration gives loss of 0.190907565896546\n",
      "The 20076 th iteration gives loss of 0.190902696377317\n",
      "The 20077 th iteration gives loss of 0.19089782741423894\n",
      "The 20078 th iteration gives loss of 0.19089295900722814\n",
      "The 20079 th iteration gives loss of 0.19088809115619915\n",
      "The 20080 th iteration gives loss of 0.1908832238610616\n",
      "The 20081 th iteration gives loss of 0.19087835712174017\n",
      "The 20082 th iteration gives loss of 0.19087349093814013\n",
      "The 20083 th iteration gives loss of 0.19086862531017407\n",
      "The 20084 th iteration gives loss of 0.19086376023776036\n",
      "The 20085 th iteration gives loss of 0.1908588957208178\n",
      "The 20086 th iteration gives loss of 0.19085403175925114\n",
      "The 20087 th iteration gives loss of 0.19084916835297977\n",
      "The 20088 th iteration gives loss of 0.19084430550191409\n",
      "The 20089 th iteration gives loss of 0.1908394432059719\n",
      "The 20090 th iteration gives loss of 0.19083458146506116\n",
      "The 20091 th iteration gives loss of 0.19082972027911738\n",
      "The 20092 th iteration gives loss of 0.1908248596480239\n",
      "The 20093 th iteration gives loss of 0.19081999957171517\n",
      "The 20094 th iteration gives loss of 0.1908151400500952\n",
      "The 20095 th iteration gives loss of 0.19081028108308842\n",
      "The 20096 th iteration gives loss of 0.19080542267061423\n",
      "The 20097 th iteration gives loss of 0.19080056481256746\n",
      "The 20098 th iteration gives loss of 0.19079570750887126\n",
      "The 20099 th iteration gives loss of 0.19079085075943997\n",
      "The 20100 th iteration gives loss of 0.19078599456419806\n",
      "The 20101 th iteration gives loss of 0.19078113892304957\n",
      "The 20102 th iteration gives loss of 0.19077628383590683\n",
      "The 20103 th iteration gives loss of 0.190771429302693\n",
      "The 20104 th iteration gives loss of 0.19076657532331048\n",
      "The 20105 th iteration gives loss of 0.19076172189767632\n",
      "The 20106 th iteration gives loss of 0.19075686902571543\n",
      "The 20107 th iteration gives loss of 0.19075201670733785\n",
      "The 20108 th iteration gives loss of 0.19074716494245436\n",
      "The 20109 th iteration gives loss of 0.1907423137309841\n",
      "The 20110 th iteration gives loss of 0.19073746307283648\n",
      "The 20111 th iteration gives loss of 0.19073261296793262\n",
      "The 20112 th iteration gives loss of 0.1907277634161828\n",
      "The 20113 th iteration gives loss of 0.19072291441749462\n",
      "The 20114 th iteration gives loss of 0.19071806597179614\n",
      "The 20115 th iteration gives loss of 0.19071321807899255\n",
      "The 20116 th iteration gives loss of 0.19070837073900673\n",
      "The 20117 th iteration gives loss of 0.19070352395173776\n",
      "The 20118 th iteration gives loss of 0.19069867771712687\n",
      "The 20119 th iteration gives loss of 0.1906938320350647\n",
      "The 20120 th iteration gives loss of 0.19068898690547736\n",
      "The 20121 th iteration gives loss of 0.19068414232827618\n",
      "The 20122 th iteration gives loss of 0.19067929830337335\n",
      "The 20123 th iteration gives loss of 0.1906744548306851\n",
      "The 20124 th iteration gives loss of 0.19066961191012835\n",
      "The 20125 th iteration gives loss of 0.1906647695416164\n",
      "The 20126 th iteration gives loss of 0.19065992772505438\n",
      "The 20127 th iteration gives loss of 0.1906550864603809\n",
      "The 20128 th iteration gives loss of 0.1906502457474946\n",
      "The 20129 th iteration gives loss of 0.1906454055863049\n",
      "The 20130 th iteration gives loss of 0.19064056597673737\n",
      "The 20131 th iteration gives loss of 0.1906357269187073\n",
      "The 20132 th iteration gives loss of 0.19063088841212736\n",
      "The 20133 th iteration gives loss of 0.190626050456911\n",
      "The 20134 th iteration gives loss of 0.19062121305296312\n",
      "The 20135 th iteration gives loss of 0.19061637620021674\n",
      "The 20136 th iteration gives loss of 0.19061153989857807\n",
      "The 20137 th iteration gives loss of 0.19060670414795813\n",
      "The 20138 th iteration gives loss of 0.19060186894828354\n",
      "The 20139 th iteration gives loss of 0.19059703429945327\n",
      "The 20140 th iteration gives loss of 0.1905922002013928\n",
      "The 20141 th iteration gives loss of 0.1905873666540125\n",
      "The 20142 th iteration gives loss of 0.19058253365723835\n",
      "The 20143 th iteration gives loss of 0.1905777012109704\n",
      "The 20144 th iteration gives loss of 0.19057286931513395\n",
      "The 20145 th iteration gives loss of 0.19056803796964106\n",
      "The 20146 th iteration gives loss of 0.19056320717440692\n",
      "The 20147 th iteration gives loss of 0.19055837692934222\n",
      "The 20148 th iteration gives loss of 0.1905535472343618\n",
      "The 20149 th iteration gives loss of 0.19054871808938367\n",
      "The 20150 th iteration gives loss of 0.19054388949432427\n",
      "The 20151 th iteration gives loss of 0.1905390614491044\n",
      "The 20152 th iteration gives loss of 0.1905342339536248\n",
      "The 20153 th iteration gives loss of 0.19052940700781398\n",
      "The 20154 th iteration gives loss of 0.19052458061157812\n",
      "The 20155 th iteration gives loss of 0.19051975476483818\n",
      "The 20156 th iteration gives loss of 0.1905149294675044\n",
      "The 20157 th iteration gives loss of 0.19051010471949173\n",
      "The 20158 th iteration gives loss of 0.19050528052071822\n",
      "The 20159 th iteration gives loss of 0.1905004568711102\n",
      "The 20160 th iteration gives loss of 0.19049563377056042\n",
      "The 20161 th iteration gives loss of 0.19049081121900005\n",
      "The 20162 th iteration gives loss of 0.19048598921633225\n",
      "The 20163 th iteration gives loss of 0.1904811677624876\n",
      "The 20164 th iteration gives loss of 0.1904763468573747\n",
      "The 20165 th iteration gives loss of 0.19047152650090016\n",
      "The 20166 th iteration gives loss of 0.19046670669298468\n",
      "The 20167 th iteration gives loss of 0.19046188743354905\n",
      "The 20168 th iteration gives loss of 0.1904570687225082\n",
      "The 20169 th iteration gives loss of 0.19045225055977366\n",
      "The 20170 th iteration gives loss of 0.1904474329452562\n",
      "The 20171 th iteration gives loss of 0.19044261587888126\n",
      "The 20172 th iteration gives loss of 0.19043779936055247\n",
      "The 20173 th iteration gives loss of 0.19043298339019538\n",
      "The 20174 th iteration gives loss of 0.19042816796772685\n",
      "The 20175 th iteration gives loss of 0.19042335309304712\n",
      "The 20176 th iteration gives loss of 0.19041853876607956\n",
      "The 20177 th iteration gives loss of 0.1904137249867466\n",
      "The 20178 th iteration gives loss of 0.19040891175496658\n",
      "The 20179 th iteration gives loss of 0.19040409907063804\n",
      "The 20180 th iteration gives loss of 0.19039928693369051\n",
      "The 20181 th iteration gives loss of 0.19039447534403414\n",
      "The 20182 th iteration gives loss of 0.19038966430158227\n",
      "The 20183 th iteration gives loss of 0.19038485380624856\n",
      "The 20184 th iteration gives loss of 0.19038004385795856\n",
      "The 20185 th iteration gives loss of 0.19037523445662605\n",
      "The 20186 th iteration gives loss of 0.19037042560215434\n",
      "The 20187 th iteration gives loss of 0.19036561729446674\n",
      "The 20188 th iteration gives loss of 0.19036080953348133\n",
      "The 20189 th iteration gives loss of 0.19035600231911642\n",
      "The 20190 th iteration gives loss of 0.19035119565128178\n",
      "The 20191 th iteration gives loss of 0.19034638952989275\n",
      "The 20192 th iteration gives loss of 0.19034158395486772\n",
      "The 20193 th iteration gives loss of 0.19033677892612383\n",
      "The 20194 th iteration gives loss of 0.19033197444356362\n",
      "The 20195 th iteration gives loss of 0.19032717050711376\n",
      "The 20196 th iteration gives loss of 0.1903223671166971\n",
      "The 20197 th iteration gives loss of 0.1903175642722206\n",
      "The 20198 th iteration gives loss of 0.19031276197359318\n",
      "The 20199 th iteration gives loss of 0.19030796022074756\n",
      "The 20200 th iteration gives loss of 0.19030315901357883\n",
      "The 20201 th iteration gives loss of 0.1902983583520252\n",
      "The 20202 th iteration gives loss of 0.1902935582359869\n",
      "The 20203 th iteration gives loss of 0.19028875866537934\n",
      "The 20204 th iteration gives loss of 0.19028395964012068\n",
      "The 20205 th iteration gives loss of 0.1902791611601366\n",
      "The 20206 th iteration gives loss of 0.1902743632253289\n",
      "The 20207 th iteration gives loss of 0.19026956583562893\n",
      "The 20208 th iteration gives loss of 0.19026476899093553\n",
      "The 20209 th iteration gives loss of 0.19025997269117337\n",
      "The 20210 th iteration gives loss of 0.19025517693625943\n",
      "The 20211 th iteration gives loss of 0.19025038172610828\n",
      "The 20212 th iteration gives loss of 0.19024558706062936\n",
      "The 20213 th iteration gives loss of 0.1902407929397478\n",
      "The 20214 th iteration gives loss of 0.19023599936337895\n",
      "The 20215 th iteration gives loss of 0.19023120633143106\n",
      "The 20216 th iteration gives loss of 0.19022641384382633\n",
      "The 20217 th iteration gives loss of 0.19022162190047362\n",
      "The 20218 th iteration gives loss of 0.1902168305012982\n",
      "The 20219 th iteration gives loss of 0.19021203964621175\n",
      "The 20220 th iteration gives loss of 0.1902072493351257\n",
      "The 20221 th iteration gives loss of 0.19020245956797063\n",
      "The 20222 th iteration gives loss of 0.19019767034465143\n",
      "The 20223 th iteration gives loss of 0.19019288166508405\n",
      "The 20224 th iteration gives loss of 0.19018809352917385\n",
      "The 20225 th iteration gives loss of 0.19018330593685775\n",
      "The 20226 th iteration gives loss of 0.19017851888804735\n",
      "The 20227 th iteration gives loss of 0.19017373238264593\n",
      "The 20228 th iteration gives loss of 0.19016894642058782\n",
      "The 20229 th iteration gives loss of 0.19016416100177425\n",
      "The 20230 th iteration gives loss of 0.19015937612612333\n",
      "The 20231 th iteration gives loss of 0.19015459179356317\n",
      "The 20232 th iteration gives loss of 0.19014980800399095\n",
      "The 20233 th iteration gives loss of 0.19014502475733155\n",
      "The 20234 th iteration gives loss of 0.19014024205351548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 20235 th iteration gives loss of 0.19013545989243577\n",
      "The 20236 th iteration gives loss of 0.19013067827402438\n",
      "The 20237 th iteration gives loss of 0.19012589719819048\n",
      "The 20238 th iteration gives loss of 0.19012111666485043\n",
      "The 20239 th iteration gives loss of 0.19011633667391606\n",
      "The 20240 th iteration gives loss of 0.19011155722531928\n",
      "The 20241 th iteration gives loss of 0.19010677831896716\n",
      "The 20242 th iteration gives loss of 0.1901019999547694\n",
      "The 20243 th iteration gives loss of 0.19009722213265096\n",
      "The 20244 th iteration gives loss of 0.19009244485252122\n",
      "The 20245 th iteration gives loss of 0.19008766811429492\n",
      "The 20246 th iteration gives loss of 0.19008289191790284\n",
      "The 20247 th iteration gives loss of 0.19007811626325374\n",
      "The 20248 th iteration gives loss of 0.1900733411502572\n",
      "The 20249 th iteration gives loss of 0.19006856657883456\n",
      "The 20250 th iteration gives loss of 0.19006379254890735\n",
      "The 20251 th iteration gives loss of 0.19005901906038525\n",
      "The 20252 th iteration gives loss of 0.19005424611317803\n",
      "The 20253 th iteration gives loss of 0.19004947370721967\n",
      "The 20254 th iteration gives loss of 0.1900447018424216\n",
      "The 20255 th iteration gives loss of 0.1900399305186873\n",
      "The 20256 th iteration gives loss of 0.19003515973595286\n",
      "The 20257 th iteration gives loss of 0.1900303894941143\n",
      "The 20258 th iteration gives loss of 0.1900256197931012\n",
      "The 20259 th iteration gives loss of 0.19002085063283183\n",
      "The 20260 th iteration gives loss of 0.19001608201321615\n",
      "The 20261 th iteration gives loss of 0.19001131393416903\n",
      "The 20262 th iteration gives loss of 0.19000654639559833\n",
      "The 20263 th iteration gives loss of 0.19000177939744414\n",
      "The 20264 th iteration gives loss of 0.18999701293961033\n",
      "The 20265 th iteration gives loss of 0.18999224702201195\n",
      "The 20266 th iteration gives loss of 0.18998748164457685\n",
      "The 20267 th iteration gives loss of 0.18998271680720438\n",
      "The 20268 th iteration gives loss of 0.1899779525098276\n",
      "The 20269 th iteration gives loss of 0.1899731887523451\n",
      "The 20270 th iteration gives loss of 0.18996842553468526\n",
      "The 20271 th iteration gives loss of 0.18996366285676528\n",
      "The 20272 th iteration gives loss of 0.18995890071850044\n",
      "The 20273 th iteration gives loss of 0.18995413911981046\n",
      "The 20274 th iteration gives loss of 0.18994937806059525\n",
      "The 20275 th iteration gives loss of 0.1899446175407933\n",
      "The 20276 th iteration gives loss of 0.18993985756031675\n",
      "The 20277 th iteration gives loss of 0.18993509811906373\n",
      "The 20278 th iteration gives loss of 0.18993033921697436\n",
      "The 20279 th iteration gives loss of 0.1899255808539604\n",
      "The 20280 th iteration gives loss of 0.18992082302992352\n",
      "The 20281 th iteration gives loss of 0.18991606574480335\n",
      "The 20282 th iteration gives loss of 0.1899113089984954\n",
      "The 20283 th iteration gives loss of 0.18990655279093197\n",
      "The 20284 th iteration gives loss of 0.1899017971220124\n",
      "The 20285 th iteration gives loss of 0.18989704199167537\n",
      "The 20286 th iteration gives loss of 0.18989228739982042\n",
      "The 20287 th iteration gives loss of 0.18988753334637615\n",
      "The 20288 th iteration gives loss of 0.18988277983124735\n",
      "The 20289 th iteration gives loss of 0.18987802685436822\n",
      "The 20290 th iteration gives loss of 0.18987327441563756\n",
      "The 20291 th iteration gives loss of 0.18986852251498088\n",
      "The 20292 th iteration gives loss of 0.18986377115231898\n",
      "The 20293 th iteration gives loss of 0.18985902032755858\n",
      "The 20294 th iteration gives loss of 0.18985427004062558\n",
      "The 20295 th iteration gives loss of 0.18984952029143312\n",
      "The 20296 th iteration gives loss of 0.18984477107990225\n",
      "The 20297 th iteration gives loss of 0.18984002240594164\n",
      "The 20298 th iteration gives loss of 0.1898352742694728\n",
      "The 20299 th iteration gives loss of 0.18983052667040415\n",
      "The 20300 th iteration gives loss of 0.18982577960867159\n",
      "The 20301 th iteration gives loss of 0.18982103308417592\n",
      "The 20302 th iteration gives loss of 0.18981628709684412\n",
      "The 20303 th iteration gives loss of 0.1898115416465846\n",
      "The 20304 th iteration gives loss of 0.18980679673332862\n",
      "The 20305 th iteration gives loss of 0.18980205235698308\n",
      "The 20306 th iteration gives loss of 0.18979730851746196\n",
      "The 20307 th iteration gives loss of 0.18979256521468074\n",
      "The 20308 th iteration gives loss of 0.18978782244856796\n",
      "The 20309 th iteration gives loss of 0.18978308021903625\n",
      "The 20310 th iteration gives loss of 0.1897783385259969\n",
      "The 20311 th iteration gives loss of 0.1897735973693652\n",
      "The 20312 th iteration gives loss of 0.18976885674907398\n",
      "The 20313 th iteration gives loss of 0.18976411666502316\n",
      "The 20314 th iteration gives loss of 0.18975937711714416\n",
      "The 20315 th iteration gives loss of 0.1897546381053437\n",
      "The 20316 th iteration gives loss of 0.18974989962955122\n",
      "The 20317 th iteration gives loss of 0.1897451616896737\n",
      "The 20318 th iteration gives loss of 0.1897404242856249\n",
      "The 20319 th iteration gives loss of 0.18973568741732946\n",
      "The 20320 th iteration gives loss of 0.18973095108470167\n",
      "The 20321 th iteration gives loss of 0.18972621528765743\n",
      "The 20322 th iteration gives loss of 0.18972148002612388\n",
      "The 20323 th iteration gives loss of 0.18971674530000607\n",
      "The 20324 th iteration gives loss of 0.1897120111092248\n",
      "The 20325 th iteration gives loss of 0.18970727745370974\n",
      "The 20326 th iteration gives loss of 0.18970254433335518\n",
      "The 20327 th iteration gives loss of 0.18969781174809075\n",
      "The 20328 th iteration gives loss of 0.18969307969783547\n",
      "The 20329 th iteration gives loss of 0.18968834818251315\n",
      "The 20330 th iteration gives loss of 0.18968361720202917\n",
      "The 20331 th iteration gives loss of 0.18967888675630373\n",
      "The 20332 th iteration gives loss of 0.18967415684525177\n",
      "The 20333 th iteration gives loss of 0.18966942746880014\n",
      "The 20334 th iteration gives loss of 0.18966469862685623\n",
      "The 20335 th iteration gives loss of 0.18965997031934542\n",
      "The 20336 th iteration gives loss of 0.18965524254617652\n",
      "The 20337 th iteration gives loss of 0.1896505153072695\n",
      "The 20338 th iteration gives loss of 0.18964578860255416\n",
      "The 20339 th iteration gives loss of 0.18964106243192824\n",
      "The 20340 th iteration gives loss of 0.18963633679532765\n",
      "The 20341 th iteration gives loss of 0.18963161169266107\n",
      "The 20342 th iteration gives loss of 0.189626887123843\n",
      "The 20343 th iteration gives loss of 0.1896221630887964\n",
      "The 20344 th iteration gives loss of 0.18961743958742944\n",
      "The 20345 th iteration gives loss of 0.18961271661967458\n",
      "The 20346 th iteration gives loss of 0.18960799418544033\n",
      "The 20347 th iteration gives loss of 0.18960327228464524\n",
      "The 20348 th iteration gives loss of 0.1895985509172058\n",
      "The 20349 th iteration gives loss of 0.18959383008304465\n",
      "The 20350 th iteration gives loss of 0.18958910978207996\n",
      "The 20351 th iteration gives loss of 0.18958439001421945\n",
      "The 20352 th iteration gives loss of 0.18957967077938842\n",
      "The 20353 th iteration gives loss of 0.18957495207750907\n",
      "The 20354 th iteration gives loss of 0.18957023390849403\n",
      "The 20355 th iteration gives loss of 0.18956551627225207\n",
      "The 20356 th iteration gives loss of 0.18956079916871985\n",
      "The 20357 th iteration gives loss of 0.18955608259780207\n",
      "The 20358 th iteration gives loss of 0.18955136655940863\n",
      "The 20359 th iteration gives loss of 0.1895466510534739\n",
      "The 20360 th iteration gives loss of 0.18954193607990272\n",
      "The 20361 th iteration gives loss of 0.18953722163862444\n",
      "The 20362 th iteration gives loss of 0.18953250772955374\n",
      "The 20363 th iteration gives loss of 0.1895277943526048\n",
      "The 20364 th iteration gives loss of 0.18952308150769764\n",
      "The 20365 th iteration gives loss of 0.18951836919475426\n",
      "The 20366 th iteration gives loss of 0.189513657413679\n",
      "The 20367 th iteration gives loss of 0.1895089461644038\n",
      "The 20368 th iteration gives loss of 0.18950423544684128\n",
      "The 20369 th iteration gives loss of 0.18949952526091185\n",
      "The 20370 th iteration gives loss of 0.18949481560653464\n",
      "The 20371 th iteration gives loss of 0.1894901064836216\n",
      "The 20372 th iteration gives loss of 0.18948539789208022\n",
      "The 20373 th iteration gives loss of 0.18948068983185448\n",
      "The 20374 th iteration gives loss of 0.1894759823028503\n",
      "The 20375 th iteration gives loss of 0.18947127530498162\n",
      "The 20376 th iteration gives loss of 0.1894665688381632\n",
      "The 20377 th iteration gives loss of 0.18946186290232347\n",
      "The 20378 th iteration gives loss of 0.18945715749737568\n",
      "The 20379 th iteration gives loss of 0.18945245262323823\n",
      "The 20380 th iteration gives loss of 0.1894477482798327\n",
      "The 20381 th iteration gives loss of 0.1894430444670757\n",
      "The 20382 th iteration gives loss of 0.18943834118487307\n",
      "The 20383 th iteration gives loss of 0.1894336384331628\n",
      "The 20384 th iteration gives loss of 0.18942893621184942\n",
      "The 20385 th iteration gives loss of 0.1894242345208486\n",
      "The 20386 th iteration gives loss of 0.18941953336009473\n",
      "The 20387 th iteration gives loss of 0.18941483272949688\n",
      "The 20388 th iteration gives loss of 0.189410132628968\n",
      "The 20389 th iteration gives loss of 0.1894054330584276\n",
      "The 20390 th iteration gives loss of 0.18940073401780272\n",
      "The 20391 th iteration gives loss of 0.18939603550700332\n",
      "The 20392 th iteration gives loss of 0.18939133752594456\n",
      "The 20393 th iteration gives loss of 0.18938664007455386\n",
      "The 20394 th iteration gives loss of 0.1893819431527468\n",
      "The 20395 th iteration gives loss of 0.1893772467604412\n",
      "The 20396 th iteration gives loss of 0.18937255089755436\n",
      "The 20397 th iteration gives loss of 0.18936785556400254\n",
      "The 20398 th iteration gives loss of 0.1893631607597068\n",
      "The 20399 th iteration gives loss of 0.18935846648458224\n",
      "The 20400 th iteration gives loss of 0.18935377273855505\n",
      "The 20401 th iteration gives loss of 0.18934907952153376\n",
      "The 20402 th iteration gives loss of 0.18934438683344487\n",
      "The 20403 th iteration gives loss of 0.1893396946741958\n",
      "The 20404 th iteration gives loss of 0.1893350030437174\n",
      "The 20405 th iteration gives loss of 0.18933031194192454\n",
      "The 20406 th iteration gives loss of 0.18932562136872216\n",
      "The 20407 th iteration gives loss of 0.18932093132405373\n",
      "The 20408 th iteration gives loss of 0.18931624180781226\n",
      "The 20409 th iteration gives loss of 0.1893115528199357\n",
      "The 20410 th iteration gives loss of 0.18930686436033872\n",
      "The 20411 th iteration gives loss of 0.18930217642892871\n",
      "The 20412 th iteration gives loss of 0.18929748902564036\n",
      "The 20413 th iteration gives loss of 0.18929280215036837\n",
      "The 20414 th iteration gives loss of 0.18928811580305657\n",
      "The 20415 th iteration gives loss of 0.18928342998360212\n",
      "The 20416 th iteration gives loss of 0.1892787446919483\n",
      "The 20417 th iteration gives loss of 0.1892740599279969\n",
      "The 20418 th iteration gives loss of 0.18926937569165506\n",
      "The 20419 th iteration gives loss of 0.189264691982864\n",
      "The 20420 th iteration gives loss of 0.18926000880153548\n",
      "The 20421 th iteration gives loss of 0.18925532614757995\n",
      "The 20422 th iteration gives loss of 0.18925064402092506\n",
      "The 20423 th iteration gives loss of 0.18924596242148248\n",
      "The 20424 th iteration gives loss of 0.18924128134918203\n",
      "The 20425 th iteration gives loss of 0.18923660080392904\n",
      "The 20426 th iteration gives loss of 0.1892319207856542\n",
      "The 20427 th iteration gives loss of 0.18922724129425972\n",
      "The 20428 th iteration gives loss of 0.18922256232968385\n",
      "The 20429 th iteration gives loss of 0.18921788389182886\n",
      "The 20430 th iteration gives loss of 0.1892132059806251\n",
      "The 20431 th iteration gives loss of 0.18920852859598788\n",
      "The 20432 th iteration gives loss of 0.1892038517378345\n",
      "The 20433 th iteration gives loss of 0.18919917540608466\n",
      "The 20434 th iteration gives loss of 0.18919449960065562\n",
      "The 20435 th iteration gives loss of 0.18918982432145665\n",
      "The 20436 th iteration gives loss of 0.18918514956842059\n",
      "The 20437 th iteration gives loss of 0.189180475341468\n",
      "The 20438 th iteration gives loss of 0.18917580164051645\n",
      "The 20439 th iteration gives loss of 0.18917112846547532\n",
      "The 20440 th iteration gives loss of 0.18916645581626887\n",
      "The 20441 th iteration gives loss of 0.18916178369280578\n",
      "The 20442 th iteration gives loss of 0.18915711209501546\n",
      "The 20443 th iteration gives loss of 0.18915244102282072\n",
      "The 20444 th iteration gives loss of 0.18914777047613762\n",
      "The 20445 th iteration gives loss of 0.18914310045487587\n",
      "The 20446 th iteration gives loss of 0.18913843095896396\n",
      "The 20447 th iteration gives loss of 0.18913376198832238\n",
      "The 20448 th iteration gives loss of 0.18912909354286342\n",
      "The 20449 th iteration gives loss of 0.1891244256225038\n",
      "The 20450 th iteration gives loss of 0.1891197582271732\n",
      "The 20451 th iteration gives loss of 0.1891150913567804\n",
      "The 20452 th iteration gives loss of 0.18911042501124503\n",
      "The 20453 th iteration gives loss of 0.1891057591904901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 20454 th iteration gives loss of 0.1891010938944407\n",
      "The 20455 th iteration gives loss of 0.18909642912299585\n",
      "The 20456 th iteration gives loss of 0.18909176487610155\n",
      "The 20457 th iteration gives loss of 0.18908710115365096\n",
      "The 20458 th iteration gives loss of 0.1890824379555854\n",
      "The 20459 th iteration gives loss of 0.1890777752818066\n",
      "The 20460 th iteration gives loss of 0.18907311313223538\n",
      "The 20461 th iteration gives loss of 0.18906845150681129\n",
      "The 20462 th iteration gives loss of 0.18906379040542814\n",
      "The 20463 th iteration gives loss of 0.18905912982801576\n",
      "The 20464 th iteration gives loss of 0.1890544697744838\n",
      "The 20465 th iteration gives loss of 0.1890498102447721\n",
      "The 20466 th iteration gives loss of 0.18904515123877988\n",
      "The 20467 th iteration gives loss of 0.18904049275643114\n",
      "The 20468 th iteration gives loss of 0.18903583479765296\n",
      "The 20469 th iteration gives loss of 0.18903117736236252\n",
      "The 20470 th iteration gives loss of 0.18902652045048016\n",
      "The 20471 th iteration gives loss of 0.18902186406190974\n",
      "The 20472 th iteration gives loss of 0.18901720819658255\n",
      "The 20473 th iteration gives loss of 0.18901255285441307\n",
      "The 20474 th iteration gives loss of 0.18900789803532997\n",
      "The 20475 th iteration gives loss of 0.18900324373924826\n",
      "The 20476 th iteration gives loss of 0.18899858996607982\n",
      "The 20477 th iteration gives loss of 0.18899393671575587\n",
      "The 20478 th iteration gives loss of 0.18898928398818754\n",
      "The 20479 th iteration gives loss of 0.18898463178329647\n",
      "The 20480 th iteration gives loss of 0.1889799801009979\n",
      "The 20481 th iteration gives loss of 0.1889753289412215\n",
      "The 20482 th iteration gives loss of 0.18897067830387018\n",
      "The 20483 th iteration gives loss of 0.18896602818887184\n",
      "The 20484 th iteration gives loss of 0.18896137859615092\n",
      "The 20485 th iteration gives loss of 0.188956729525629\n",
      "The 20486 th iteration gives loss of 0.18895208097722271\n",
      "The 20487 th iteration gives loss of 0.18894743295083466\n",
      "The 20488 th iteration gives loss of 0.18894278544639773\n",
      "The 20489 th iteration gives loss of 0.18893813846384053\n",
      "The 20490 th iteration gives loss of 0.18893349200306217\n",
      "The 20491 th iteration gives loss of 0.18892884606400603\n",
      "The 20492 th iteration gives loss of 0.18892420064657198\n",
      "The 20493 th iteration gives loss of 0.18891955575067837\n",
      "The 20494 th iteration gives loss of 0.18891491137626146\n",
      "The 20495 th iteration gives loss of 0.18891026752322848\n",
      "The 20496 th iteration gives loss of 0.18890562419150342\n",
      "The 20497 th iteration gives loss of 0.18890098138099815\n",
      "The 20498 th iteration gives loss of 0.1888963390916457\n",
      "The 20499 th iteration gives loss of 0.18889169732336727\n",
      "The 20500 th iteration gives loss of 0.18888705607606038\n",
      "The 20501 th iteration gives loss of 0.18888241534966174\n",
      "The 20502 th iteration gives loss of 0.18887777514408755\n",
      "The 20503 th iteration gives loss of 0.18887313545924947\n",
      "The 20504 th iteration gives loss of 0.18886849629507668\n",
      "The 20505 th iteration gives loss of 0.18886385765148428\n",
      "The 20506 th iteration gives loss of 0.18885921952840162\n",
      "The 20507 th iteration gives loss of 0.18885458192573304\n",
      "The 20508 th iteration gives loss of 0.188849944843415\n",
      "The 20509 th iteration gives loss of 0.18884530828135418\n",
      "The 20510 th iteration gives loss of 0.1888406722394767\n",
      "The 20511 th iteration gives loss of 0.18883603671770016\n",
      "The 20512 th iteration gives loss of 0.1888314017159373\n",
      "The 20513 th iteration gives loss of 0.18882676723411876\n",
      "The 20514 th iteration gives loss of 0.18882213327215527\n",
      "The 20515 th iteration gives loss of 0.18881749982997273\n",
      "The 20516 th iteration gives loss of 0.18881286690749438\n",
      "The 20517 th iteration gives loss of 0.18880823450463274\n",
      "The 20518 th iteration gives loss of 0.18880360262130286\n",
      "The 20519 th iteration gives loss of 0.1887989712574355\n",
      "The 20520 th iteration gives loss of 0.18879434041294782\n",
      "The 20521 th iteration gives loss of 0.18878971008775366\n",
      "The 20522 th iteration gives loss of 0.18878508028178098\n",
      "The 20523 th iteration gives loss of 0.18878045099494772\n",
      "The 20524 th iteration gives loss of 0.18877582222717806\n",
      "The 20525 th iteration gives loss of 0.18877119397838166\n",
      "The 20526 th iteration gives loss of 0.18876656624846846\n",
      "The 20527 th iteration gives loss of 0.18876193903737684\n",
      "The 20528 th iteration gives loss of 0.18875731234502957\n",
      "The 20529 th iteration gives loss of 0.18875268617133723\n",
      "The 20530 th iteration gives loss of 0.18874806051622472\n",
      "The 20531 th iteration gives loss of 0.1887434353796051\n",
      "The 20532 th iteration gives loss of 0.18873881076140284\n",
      "The 20533 th iteration gives loss of 0.1887341866615373\n",
      "The 20534 th iteration gives loss of 0.18872956307993297\n",
      "The 20535 th iteration gives loss of 0.18872494001649504\n",
      "The 20536 th iteration gives loss of 0.18872031747116566\n",
      "The 20537 th iteration gives loss of 0.18871569544383748\n",
      "The 20538 th iteration gives loss of 0.18871107393445802\n",
      "The 20539 th iteration gives loss of 0.18870645294293362\n",
      "The 20540 th iteration gives loss of 0.18870183246918876\n",
      "The 20541 th iteration gives loss of 0.18869721251313007\n",
      "The 20542 th iteration gives loss of 0.18869259307470182\n",
      "The 20543 th iteration gives loss of 0.1886879741538058\n",
      "The 20544 th iteration gives loss of 0.188683355750358\n",
      "The 20545 th iteration gives loss of 0.18867873786429373\n",
      "The 20546 th iteration gives loss of 0.18867412049552143\n",
      "The 20547 th iteration gives loss of 0.18866950364397295\n",
      "The 20548 th iteration gives loss of 0.18866488730955816\n",
      "The 20549 th iteration gives loss of 0.18866027149220346\n",
      "The 20550 th iteration gives loss of 0.18865565619182753\n",
      "The 20551 th iteration gives loss of 0.18865104140834882\n",
      "The 20552 th iteration gives loss of 0.1886464271416866\n",
      "The 20553 th iteration gives loss of 0.1886418133917604\n",
      "The 20554 th iteration gives loss of 0.18863720015850471\n",
      "The 20555 th iteration gives loss of 0.18863258744181502\n",
      "The 20556 th iteration gives loss of 0.1886279752416295\n",
      "The 20557 th iteration gives loss of 0.18862336355785828\n",
      "The 20558 th iteration gives loss of 0.1886187523904294\n",
      "The 20559 th iteration gives loss of 0.188614141739258\n",
      "The 20560 th iteration gives loss of 0.18860953160427912\n",
      "The 20561 th iteration gives loss of 0.1886049219853936\n",
      "The 20562 th iteration gives loss of 0.18860031288252532\n",
      "The 20563 th iteration gives loss of 0.18859570429560452\n",
      "The 20564 th iteration gives loss of 0.18859109622453815\n",
      "The 20565 th iteration gives loss of 0.18858648866925665\n",
      "The 20566 th iteration gives loss of 0.18858188162967593\n",
      "The 20567 th iteration gives loss of 0.18857727510571376\n",
      "The 20568 th iteration gives loss of 0.18857266909730006\n",
      "The 20569 th iteration gives loss of 0.18856806360435371\n",
      "The 20570 th iteration gives loss of 0.18856345862678248\n",
      "The 20571 th iteration gives loss of 0.1885588541645185\n",
      "The 20572 th iteration gives loss of 0.18855425021748215\n",
      "The 20573 th iteration gives loss of 0.18854964678558495\n",
      "The 20574 th iteration gives loss of 0.18854504386876736\n",
      "The 20575 th iteration gives loss of 0.18854044146692153\n",
      "The 20576 th iteration gives loss of 0.18853583957998896\n",
      "The 20577 th iteration gives loss of 0.18853123820787931\n",
      "The 20578 th iteration gives loss of 0.1885266373505215\n",
      "The 20579 th iteration gives loss of 0.1885220370078238\n",
      "The 20580 th iteration gives loss of 0.18851743717971725\n",
      "The 20581 th iteration gives loss of 0.18851283786612144\n",
      "The 20582 th iteration gives loss of 0.18850823906695563\n",
      "The 20583 th iteration gives loss of 0.18850364078214138\n",
      "The 20584 th iteration gives loss of 0.18849904301160111\n",
      "The 20585 th iteration gives loss of 0.18849444575524973\n",
      "The 20586 th iteration gives loss of 0.18848984901300497\n",
      "The 20587 th iteration gives loss of 0.18848525278479616\n",
      "The 20588 th iteration gives loss of 0.1884806570705476\n",
      "The 20589 th iteration gives loss of 0.18847606187016142\n",
      "The 20590 th iteration gives loss of 0.18847146718357768\n",
      "The 20591 th iteration gives loss of 0.18846687301070744\n",
      "The 20592 th iteration gives loss of 0.1884622793514738\n",
      "The 20593 th iteration gives loss of 0.18845768620580397\n",
      "The 20594 th iteration gives loss of 0.18845309357359558\n",
      "The 20595 th iteration gives loss of 0.18844850145479994\n",
      "The 20596 th iteration gives loss of 0.18844390984930656\n",
      "The 20597 th iteration gives loss of 0.18843931875706238\n",
      "The 20598 th iteration gives loss of 0.18843472817798051\n",
      "The 20599 th iteration gives loss of 0.18843013811197903\n",
      "The 20600 th iteration gives loss of 0.18842554855897853\n",
      "The 20601 th iteration gives loss of 0.18842095951889543\n",
      "The 20602 th iteration gives loss of 0.18841637099166683\n",
      "The 20603 th iteration gives loss of 0.1884117829772005\n",
      "The 20604 th iteration gives loss of 0.18840719547541027\n",
      "The 20605 th iteration gives loss of 0.18840260848623638\n",
      "The 20606 th iteration gives loss of 0.18839802200958009\n",
      "The 20607 th iteration gives loss of 0.18839343604537095\n",
      "The 20608 th iteration gives loss of 0.18838885059354096\n",
      "The 20609 th iteration gives loss of 0.1883842656539917\n",
      "The 20610 th iteration gives loss of 0.1883796812266592\n",
      "The 20611 th iteration gives loss of 0.18837509731145352\n",
      "The 20612 th iteration gives loss of 0.18837051390830845\n",
      "The 20613 th iteration gives loss of 0.18836593101712473\n",
      "The 20614 th iteration gives loss of 0.188361348637834\n",
      "The 20615 th iteration gives loss of 0.18835676677036972\n",
      "The 20616 th iteration gives loss of 0.18835218541464127\n",
      "The 20617 th iteration gives loss of 0.1883476045705665\n",
      "The 20618 th iteration gives loss of 0.18834302423806668\n",
      "The 20619 th iteration gives loss of 0.18833844441706923\n",
      "The 20620 th iteration gives loss of 0.18833386510749275\n",
      "The 20621 th iteration gives loss of 0.18832928630925663\n",
      "The 20622 th iteration gives loss of 0.1883247080222785\n",
      "The 20623 th iteration gives loss of 0.18832013024648606\n",
      "The 20624 th iteration gives loss of 0.1883155529818034\n",
      "The 20625 th iteration gives loss of 0.18831097622814708\n",
      "The 20626 th iteration gives loss of 0.18830639998542922\n",
      "The 20627 th iteration gives loss of 0.1883018242535735\n",
      "The 20628 th iteration gives loss of 0.18829724903251807\n",
      "The 20629 th iteration gives loss of 0.18829267432217006\n",
      "The 20630 th iteration gives loss of 0.18828810012245248\n",
      "The 20631 th iteration gives loss of 0.18828352643329002\n",
      "The 20632 th iteration gives loss of 0.18827895325459856\n",
      "The 20633 th iteration gives loss of 0.18827438058630688\n",
      "The 20634 th iteration gives loss of 0.18826980842832505\n",
      "The 20635 th iteration gives loss of 0.1882652367805747\n",
      "The 20636 th iteration gives loss of 0.1882606656429908\n",
      "The 20637 th iteration gives loss of 0.18825609501548896\n",
      "The 20638 th iteration gives loss of 0.18825152489798377\n",
      "The 20639 th iteration gives loss of 0.18824695529040275\n",
      "The 20640 th iteration gives loss of 0.188242386192666\n",
      "The 20641 th iteration gives loss of 0.18823781760468347\n",
      "The 20642 th iteration gives loss of 0.18823324952639522\n",
      "The 20643 th iteration gives loss of 0.1882286819577135\n",
      "The 20644 th iteration gives loss of 0.18822411489856145\n",
      "The 20645 th iteration gives loss of 0.18821954834886243\n",
      "The 20646 th iteration gives loss of 0.18821498230852607\n",
      "The 20647 th iteration gives loss of 0.18821041677748354\n",
      "The 20648 th iteration gives loss of 0.1882058517556597\n",
      "The 20649 th iteration gives loss of 0.1882012872429676\n",
      "The 20650 th iteration gives loss of 0.18819672323934075\n",
      "The 20651 th iteration gives loss of 0.18819215974468784\n",
      "The 20652 th iteration gives loss of 0.18818759675893293\n",
      "The 20653 th iteration gives loss of 0.18818303428199298\n",
      "The 20654 th iteration gives loss of 0.18817847231380422\n",
      "The 20655 th iteration gives loss of 0.18817391085427732\n",
      "The 20656 th iteration gives loss of 0.1881693499033285\n",
      "The 20657 th iteration gives loss of 0.18816478946089202\n",
      "The 20658 th iteration gives loss of 0.18816022952689135\n",
      "The 20659 th iteration gives loss of 0.18815567010123618\n",
      "The 20660 th iteration gives loss of 0.18815111118385383\n",
      "The 20661 th iteration gives loss of 0.188146552774658\n",
      "The 20662 th iteration gives loss of 0.18814199487358105\n",
      "The 20663 th iteration gives loss of 0.18813743748054104\n",
      "The 20664 th iteration gives loss of 0.18813288059545108\n",
      "The 20665 th iteration gives loss of 0.18812832421825404\n",
      "The 20666 th iteration gives loss of 0.1881237683488413\n",
      "The 20667 th iteration gives loss of 0.18811921298715792\n",
      "The 20668 th iteration gives loss of 0.18811465813312642\n",
      "The 20669 th iteration gives loss of 0.1881101037866479\n",
      "The 20670 th iteration gives loss of 0.18810554994766132\n",
      "The 20671 th iteration gives loss of 0.18810099661608232\n",
      "The 20672 th iteration gives loss of 0.18809644379183801\n",
      "The 20673 th iteration gives loss of 0.1880918914748433\n",
      "The 20674 th iteration gives loss of 0.18808733966502536\n",
      "The 20675 th iteration gives loss of 0.18808278836230383\n",
      "The 20676 th iteration gives loss of 0.18807823756659414\n",
      "The 20677 th iteration gives loss of 0.18807368727782936\n",
      "The 20678 th iteration gives loss of 0.18806913749591966\n",
      "The 20679 th iteration gives loss of 0.18806458822080252\n",
      "The 20680 th iteration gives loss of 0.1880600394523786\n",
      "The 20681 th iteration gives loss of 0.1880554911905849\n",
      "The 20682 th iteration gives loss of 0.18805094343534232\n",
      "The 20683 th iteration gives loss of 0.18804639618655683\n",
      "The 20684 th iteration gives loss of 0.18804184944416955\n",
      "The 20685 th iteration gives loss of 0.188037303208101\n",
      "The 20686 th iteration gives loss of 0.18803275747826487\n",
      "The 20687 th iteration gives loss of 0.18802821225458424\n",
      "The 20688 th iteration gives loss of 0.18802366753698446\n",
      "The 20689 th iteration gives loss of 0.1880191233253813\n",
      "The 20690 th iteration gives loss of 0.18801457961970872\n",
      "The 20691 th iteration gives loss of 0.18801003641987482\n",
      "The 20692 th iteration gives loss of 0.18800549372580302\n",
      "The 20693 th iteration gives loss of 0.18800095153742247\n",
      "The 20694 th iteration gives loss of 0.1879964098546604\n",
      "The 20695 th iteration gives loss of 0.1879918686774227\n",
      "The 20696 th iteration gives loss of 0.18798732800564208\n",
      "The 20697 th iteration gives loss of 0.18798278783923564\n",
      "The 20698 th iteration gives loss of 0.18797824817813283\n",
      "The 20699 th iteration gives loss of 0.18797370902224853\n",
      "The 20700 th iteration gives loss of 0.18796917037150185\n",
      "The 20701 th iteration gives loss of 0.18796463222582002\n",
      "The 20702 th iteration gives loss of 0.1879600945851194\n",
      "The 20703 th iteration gives loss of 0.18795555744933856\n",
      "The 20704 th iteration gives loss of 0.18795102081837628\n",
      "The 20705 th iteration gives loss of 0.18794648469217004\n",
      "The 20706 th iteration gives loss of 0.18794194907064599\n",
      "The 20707 th iteration gives loss of 0.18793741395370694\n",
      "The 20708 th iteration gives loss of 0.1879328793412906\n",
      "The 20709 th iteration gives loss of 0.18792834523331725\n",
      "The 20710 th iteration gives loss of 0.18792381162970542\n",
      "The 20711 th iteration gives loss of 0.18791927853037996\n",
      "The 20712 th iteration gives loss of 0.18791474593526056\n",
      "The 20713 th iteration gives loss of 0.18791021384426934\n",
      "The 20714 th iteration gives loss of 0.1879056822573298\n",
      "The 20715 th iteration gives loss of 0.18790115117436684\n",
      "The 20716 th iteration gives loss of 0.18789662059529186\n",
      "The 20717 th iteration gives loss of 0.18789209052004394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 20718 th iteration gives loss of 0.18788756094853679\n",
      "The 20719 th iteration gives loss of 0.18788303188068742\n",
      "The 20720 th iteration gives loss of 0.18787850331642303\n",
      "The 20721 th iteration gives loss of 0.18787397525565605\n",
      "The 20722 th iteration gives loss of 0.18786944769833674\n",
      "The 20723 th iteration gives loss of 0.18786492064435806\n",
      "The 20724 th iteration gives loss of 0.18786039409365288\n",
      "The 20725 th iteration gives loss of 0.18785586804615118\n",
      "The 20726 th iteration gives loss of 0.18785134250175828\n",
      "The 20727 th iteration gives loss of 0.1878468174604169\n",
      "The 20728 th iteration gives loss of 0.1878422929220309\n",
      "The 20729 th iteration gives loss of 0.1878377688865337\n",
      "The 20730 th iteration gives loss of 0.18783324535383963\n",
      "The 20731 th iteration gives loss of 0.18782872232387635\n",
      "The 20732 th iteration gives loss of 0.18782419979656895\n",
      "The 20733 th iteration gives loss of 0.1878196777718349\n",
      "The 20734 th iteration gives loss of 0.1878151562496004\n",
      "The 20735 th iteration gives loss of 0.187810635229793\n",
      "The 20736 th iteration gives loss of 0.18780611471231154\n",
      "The 20737 th iteration gives loss of 0.18780159469710653\n",
      "The 20738 th iteration gives loss of 0.18779707518408967\n",
      "The 20739 th iteration gives loss of 0.18779255617317142\n",
      "The 20740 th iteration gives loss of 0.1877880376642906\n",
      "The 20741 th iteration gives loss of 0.18778351965736448\n",
      "The 20742 th iteration gives loss of 0.18777900215231194\n",
      "The 20743 th iteration gives loss of 0.18777448514906675\n",
      "The 20744 th iteration gives loss of 0.1877699686475417\n",
      "The 20745 th iteration gives loss of 0.18776545264765673\n",
      "The 20746 th iteration gives loss of 0.18776093714934222\n",
      "The 20747 th iteration gives loss of 0.1877564221525268\n",
      "The 20748 th iteration gives loss of 0.18775190765711153\n",
      "The 20749 th iteration gives loss of 0.1877473936630312\n",
      "The 20750 th iteration gives loss of 0.18774288017021362\n",
      "The 20751 th iteration gives loss of 0.18773836717857686\n",
      "The 20752 th iteration gives loss of 0.1877338546880388\n",
      "The 20753 th iteration gives loss of 0.18772934269852506\n",
      "The 20754 th iteration gives loss of 0.18772483120995823\n",
      "The 20755 th iteration gives loss of 0.18772032022227697\n",
      "The 20756 th iteration gives loss of 0.18771580973537175\n",
      "The 20757 th iteration gives loss of 0.18771129974919276\n",
      "The 20758 th iteration gives loss of 0.1877067902636521\n",
      "The 20759 th iteration gives loss of 0.18770228127866959\n",
      "The 20760 th iteration gives loss of 0.18769777279417935\n",
      "The 20761 th iteration gives loss of 0.18769326481008447\n",
      "The 20762 th iteration gives loss of 0.1876887573263263\n",
      "The 20763 th iteration gives loss of 0.18768425034282177\n",
      "The 20764 th iteration gives loss of 0.18767974385948707\n",
      "The 20765 th iteration gives loss of 0.18767523787625456\n",
      "The 20766 th iteration gives loss of 0.1876707323930423\n",
      "The 20767 th iteration gives loss of 0.18766622740977024\n",
      "The 20768 th iteration gives loss of 0.18766172292636993\n",
      "The 20769 th iteration gives loss of 0.18765721894275897\n",
      "The 20770 th iteration gives loss of 0.18765271545885875\n",
      "The 20771 th iteration gives loss of 0.18764821247460062\n",
      "The 20772 th iteration gives loss of 0.18764370998988558\n",
      "The 20773 th iteration gives loss of 0.18763920800466652\n",
      "The 20774 th iteration gives loss of 0.18763470651884032\n",
      "The 20775 th iteration gives loss of 0.18763020553234103\n",
      "The 20776 th iteration gives loss of 0.18762570504509393\n",
      "The 20777 th iteration gives loss of 0.1876212050570198\n",
      "The 20778 th iteration gives loss of 0.18761670556804227\n",
      "The 20779 th iteration gives loss of 0.18761220657808989\n",
      "The 20780 th iteration gives loss of 0.1876077080870671\n",
      "The 20781 th iteration gives loss of 0.18760321009491568\n",
      "The 20782 th iteration gives loss of 0.18759871260155292\n",
      "The 20783 th iteration gives loss of 0.18759421560689363\n",
      "The 20784 th iteration gives loss of 0.1875897191108745\n",
      "The 20785 th iteration gives loss of 0.1875852231134079\n",
      "The 20786 th iteration gives loss of 0.1875807276144222\n",
      "The 20787 th iteration gives loss of 0.18757623261383627\n",
      "The 20788 th iteration gives loss of 0.18757173811157896\n",
      "The 20789 th iteration gives loss of 0.1875672441075615\n",
      "The 20790 th iteration gives loss of 0.18756275060173042\n",
      "The 20791 th iteration gives loss of 0.18755825759398875\n",
      "The 20792 th iteration gives loss of 0.18755376508426294\n",
      "The 20793 th iteration gives loss of 0.1875492730724812\n",
      "The 20794 th iteration gives loss of 0.18754478155855617\n",
      "The 20795 th iteration gives loss of 0.1875402905424291\n",
      "The 20796 th iteration gives loss of 0.1875358000240046\n",
      "The 20797 th iteration gives loss of 0.18753131000322076\n",
      "The 20798 th iteration gives loss of 0.18752682047999478\n",
      "The 20799 th iteration gives loss of 0.18752233145423616\n",
      "The 20800 th iteration gives loss of 0.18751784292589085\n",
      "The 20801 th iteration gives loss of 0.18751335489487148\n",
      "The 20802 th iteration gives loss of 0.18750886736109895\n",
      "The 20803 th iteration gives loss of 0.187504380324502\n",
      "The 20804 th iteration gives loss of 0.18749989378499532\n",
      "The 20805 th iteration gives loss of 0.1874954077425145\n",
      "The 20806 th iteration gives loss of 0.18749092219696767\n",
      "The 20807 th iteration gives loss of 0.18748643714829494\n",
      "The 20808 th iteration gives loss of 0.1874819525964201\n",
      "The 20809 th iteration gives loss of 0.18747746854123684\n",
      "The 20810 th iteration gives loss of 0.18747298498270418\n",
      "The 20811 th iteration gives loss of 0.18746850192073436\n",
      "The 20812 th iteration gives loss of 0.1874640193552396\n",
      "The 20813 th iteration gives loss of 0.18745953728614634\n",
      "The 20814 th iteration gives loss of 0.18745505571339388\n",
      "The 20815 th iteration gives loss of 0.18745057463687548\n",
      "The 20816 th iteration gives loss of 0.1874460940565508\n",
      "The 20817 th iteration gives loss of 0.18744161397232184\n",
      "The 20818 th iteration gives loss of 0.18743713438411716\n",
      "The 20819 th iteration gives loss of 0.1874326552918557\n",
      "The 20820 th iteration gives loss of 0.1874281766954672\n",
      "The 20821 th iteration gives loss of 0.1874236985948666\n",
      "The 20822 th iteration gives loss of 0.1874192209899851\n",
      "The 20823 th iteration gives loss of 0.18741474388074028\n",
      "The 20824 th iteration gives loss of 0.18741026726706816\n",
      "The 20825 th iteration gives loss of 0.1874057911488784\n",
      "The 20826 th iteration gives loss of 0.18740131552609893\n",
      "The 20827 th iteration gives loss of 0.18739684039865953\n",
      "The 20828 th iteration gives loss of 0.1873923657664728\n",
      "The 20829 th iteration gives loss of 0.18738789162946184\n",
      "The 20830 th iteration gives loss of 0.18738341798755812\n",
      "The 20831 th iteration gives loss of 0.18737894484068926\n",
      "The 20832 th iteration gives loss of 0.1873744721887677\n",
      "The 20833 th iteration gives loss of 0.18737000003172932\n",
      "The 20834 th iteration gives loss of 0.1873655283694821\n",
      "The 20835 th iteration gives loss of 0.18736105720195514\n",
      "The 20836 th iteration gives loss of 0.18735658652908685\n",
      "The 20837 th iteration gives loss of 0.187352116350777\n",
      "The 20838 th iteration gives loss of 0.1873476466669665\n",
      "The 20839 th iteration gives loss of 0.1873431774775765\n",
      "The 20840 th iteration gives loss of 0.18733870878251677\n",
      "The 20841 th iteration gives loss of 0.18733424058172968\n",
      "The 20842 th iteration gives loss of 0.1873297728751366\n",
      "The 20843 th iteration gives loss of 0.1873253056626468\n",
      "The 20844 th iteration gives loss of 0.18732083894420176\n",
      "The 20845 th iteration gives loss of 0.18731637271970614\n",
      "The 20846 th iteration gives loss of 0.18731190698910838\n",
      "The 20847 th iteration gives loss of 0.18730744175230196\n",
      "The 20848 th iteration gives loss of 0.187302977009228\n",
      "The 20849 th iteration gives loss of 0.18729851275981885\n",
      "The 20850 th iteration gives loss of 0.18729404900398539\n",
      "The 20851 th iteration gives loss of 0.18728958574164634\n",
      "The 20852 th iteration gives loss of 0.18728512297273914\n",
      "The 20853 th iteration gives loss of 0.18728066069718888\n",
      "The 20854 th iteration gives loss of 0.18727619891489666\n",
      "The 20855 th iteration gives loss of 0.18727173762581822\n",
      "The 20856 th iteration gives loss of 0.18726727682985234\n",
      "The 20857 th iteration gives loss of 0.18726281652693869\n",
      "The 20858 th iteration gives loss of 0.18725835671699126\n",
      "The 20859 th iteration gives loss of 0.18725389739993586\n",
      "The 20860 th iteration gives loss of 0.18724943857570206\n",
      "The 20861 th iteration gives loss of 0.18724498024420652\n",
      "The 20862 th iteration gives loss of 0.18724052240538006\n",
      "The 20863 th iteration gives loss of 0.18723606505913296\n",
      "The 20864 th iteration gives loss of 0.1872316082054024\n",
      "The 20865 th iteration gives loss of 0.18722715184411695\n",
      "The 20866 th iteration gives loss of 0.18722269597518348\n",
      "The 20867 th iteration gives loss of 0.18721824059853528\n",
      "The 20868 th iteration gives loss of 0.18721378571409678\n",
      "The 20869 th iteration gives loss of 0.18720933132179854\n",
      "The 20870 th iteration gives loss of 0.187204877421549\n",
      "The 20871 th iteration gives loss of 0.18720042401329268\n",
      "The 20872 th iteration gives loss of 0.18719597109693112\n",
      "The 20873 th iteration gives loss of 0.18719151867239642\n",
      "The 20874 th iteration gives loss of 0.18718706673962163\n",
      "The 20875 th iteration gives loss of 0.1871826152985162\n",
      "The 20876 th iteration gives loss of 0.18717816434902787\n",
      "The 20877 th iteration gives loss of 0.18717371389105195\n",
      "The 20878 th iteration gives loss of 0.18716926392453\n",
      "The 20879 th iteration gives loss of 0.18716481444938354\n",
      "The 20880 th iteration gives loss of 0.1871603654655383\n",
      "The 20881 th iteration gives loss of 0.1871559169729116\n",
      "The 20882 th iteration gives loss of 0.18715146897142734\n",
      "The 20883 th iteration gives loss of 0.1871470214610201\n",
      "The 20884 th iteration gives loss of 0.18714257444160326\n",
      "The 20885 th iteration gives loss of 0.18713812791310921\n",
      "The 20886 th iteration gives loss of 0.18713368187545085\n",
      "The 20887 th iteration gives loss of 0.18712923632856834\n",
      "The 20888 th iteration gives loss of 0.1871247912723719\n",
      "The 20889 th iteration gives loss of 0.18712034670679886\n",
      "The 20890 th iteration gives loss of 0.18711590263176062\n",
      "The 20891 th iteration gives loss of 0.18711145904718895\n",
      "The 20892 th iteration gives loss of 0.18710701595300813\n",
      "The 20893 th iteration gives loss of 0.18710257334914246\n",
      "The 20894 th iteration gives loss of 0.18709813123550315\n",
      "The 20895 th iteration gives loss of 0.18709368961203682\n",
      "The 20896 th iteration gives loss of 0.18708924847864958\n",
      "The 20897 th iteration gives loss of 0.1870848078352739\n",
      "The 20898 th iteration gives loss of 0.1870803676818321\n",
      "The 20899 th iteration gives loss of 0.18707592801824988\n",
      "The 20900 th iteration gives loss of 0.18707148884445474\n",
      "The 20901 th iteration gives loss of 0.1870670501603635\n",
      "The 20902 th iteration gives loss of 0.18706261196590718\n",
      "The 20903 th iteration gives loss of 0.18705817426100643\n",
      "The 20904 th iteration gives loss of 0.18705373704558076\n",
      "The 20905 th iteration gives loss of 0.1870493003195676\n",
      "The 20906 th iteration gives loss of 0.18704486408289575\n",
      "The 20907 th iteration gives loss of 0.18704042833546228\n",
      "The 20908 th iteration gives loss of 0.18703599307721788\n",
      "The 20909 th iteration gives loss of 0.18703155830807633\n",
      "The 20910 th iteration gives loss of 0.18702712402795718\n",
      "The 20911 th iteration gives loss of 0.18702269023679827\n",
      "The 20912 th iteration gives loss of 0.18701825693451013\n",
      "The 20913 th iteration gives loss of 0.1870138241210253\n",
      "The 20914 th iteration gives loss of 0.1870093917962623\n",
      "The 20915 th iteration gives loss of 0.1870049599601626\n",
      "The 20916 th iteration gives loss of 0.18700052861262817\n",
      "The 20917 th iteration gives loss of 0.1869960977536048\n",
      "The 20918 th iteration gives loss of 0.1869916673829909\n",
      "The 20919 th iteration gives loss of 0.1869872375007293\n",
      "The 20920 th iteration gives loss of 0.18698280810674894\n",
      "The 20921 th iteration gives loss of 0.18697837920096647\n",
      "The 20922 th iteration gives loss of 0.18697395078330548\n",
      "The 20923 th iteration gives loss of 0.1869695228536919\n",
      "The 20924 th iteration gives loss of 0.18696509541204082\n",
      "The 20925 th iteration gives loss of 0.18696066845830303\n",
      "The 20926 th iteration gives loss of 0.18695624199237917\n",
      "The 20927 th iteration gives loss of 0.18695181601420732\n",
      "The 20928 th iteration gives loss of 0.1869473905237071\n",
      "The 20929 th iteration gives loss of 0.1869429655207969\n",
      "The 20930 th iteration gives loss of 0.18693854100540563\n",
      "The 20931 th iteration gives loss of 0.18693411697746112\n",
      "The 20932 th iteration gives loss of 0.18692969343689017\n",
      "The 20933 th iteration gives loss of 0.18692527038360957\n",
      "The 20934 th iteration gives loss of 0.1869208478175534\n",
      "The 20935 th iteration gives loss of 0.18691642573864597\n",
      "The 20936 th iteration gives loss of 0.18691200414680442\n",
      "The 20937 th iteration gives loss of 0.18690758304194782\n",
      "The 20938 th iteration gives loss of 0.18690316242402127\n",
      "The 20939 th iteration gives loss of 0.18689874229293352\n",
      "The 20940 th iteration gives loss of 0.186894322648625\n",
      "The 20941 th iteration gives loss of 0.1868899034910002\n",
      "The 20942 th iteration gives loss of 0.18688548481999234\n",
      "The 20943 th iteration gives loss of 0.1868810666355317\n",
      "The 20944 th iteration gives loss of 0.1868766489375386\n",
      "The 20945 th iteration gives loss of 0.18687223172594297\n",
      "The 20946 th iteration gives loss of 0.18686781500065805\n",
      "The 20947 th iteration gives loss of 0.18686339876161734\n",
      "The 20948 th iteration gives loss of 0.18685898300875162\n",
      "The 20949 th iteration gives loss of 0.18685456774197626\n",
      "The 20950 th iteration gives loss of 0.18685015296121704\n",
      "The 20951 th iteration gives loss of 0.18684573866640086\n",
      "The 20952 th iteration gives loss of 0.18684132485745641\n",
      "The 20953 th iteration gives loss of 0.18683691153430337\n",
      "The 20954 th iteration gives loss of 0.18683249869686186\n",
      "The 20955 th iteration gives loss of 0.1868280863450641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 20956 th iteration gives loss of 0.18682367447884335\n",
      "The 20957 th iteration gives loss of 0.18681926309811261\n",
      "The 20958 th iteration gives loss of 0.1868148522028012\n",
      "The 20959 th iteration gives loss of 0.18681044179282216\n",
      "The 20960 th iteration gives loss of 0.18680603186812503\n",
      "The 20961 th iteration gives loss of 0.1868016224286126\n",
      "The 20962 th iteration gives loss of 0.18679721347422334\n",
      "The 20963 th iteration gives loss of 0.18679280500487527\n",
      "The 20964 th iteration gives loss of 0.18678839702050604\n",
      "The 20965 th iteration gives loss of 0.18678398952101805\n",
      "The 20966 th iteration gives loss of 0.18677958250634552\n",
      "The 20967 th iteration gives loss of 0.18677517597643148\n",
      "The 20968 th iteration gives loss of 0.18677076993118885\n",
      "The 20969 th iteration gives loss of 0.18676636437052965\n",
      "The 20970 th iteration gives loss of 0.18676195929439413\n",
      "The 20971 th iteration gives loss of 0.18675755470270675\n",
      "The 20972 th iteration gives loss of 0.18675315059538294\n",
      "The 20973 th iteration gives loss of 0.18674874697235883\n",
      "The 20974 th iteration gives loss of 0.18674434383355437\n",
      "The 20975 th iteration gives loss of 0.18673994117889592\n",
      "The 20976 th iteration gives loss of 0.18673553900830658\n",
      "The 20977 th iteration gives loss of 0.18673113732172145\n",
      "The 20978 th iteration gives loss of 0.186726736119053\n",
      "The 20979 th iteration gives loss of 0.1867223354002338\n",
      "The 20980 th iteration gives loss of 0.18671793516519045\n",
      "The 20981 th iteration gives loss of 0.18671353541383895\n",
      "The 20982 th iteration gives loss of 0.186709136146115\n",
      "The 20983 th iteration gives loss of 0.18670473736194346\n",
      "The 20984 th iteration gives loss of 0.1867003390612333\n",
      "The 20985 th iteration gives loss of 0.1866959412439289\n",
      "The 20986 th iteration gives loss of 0.1866915439099497\n",
      "The 20987 th iteration gives loss of 0.18668714705922185\n",
      "The 20988 th iteration gives loss of 0.18668275069167178\n",
      "The 20989 th iteration gives loss of 0.1866783548072242\n",
      "The 20990 th iteration gives loss of 0.18667395940579354\n",
      "The 20991 th iteration gives loss of 0.18666956448732305\n",
      "The 20992 th iteration gives loss of 0.18666517005173172\n",
      "The 20993 th iteration gives loss of 0.18666077609893886\n",
      "The 20994 th iteration gives loss of 0.1866563826288716\n",
      "The 20995 th iteration gives loss of 0.18665198964145643\n",
      "The 20996 th iteration gives loss of 0.1866475971366221\n",
      "The 20997 th iteration gives loss of 0.18664320511429894\n",
      "The 20998 th iteration gives loss of 0.186638813574407\n",
      "The 20999 th iteration gives loss of 0.1866344225168648\n",
      "The 21000 th iteration gives loss of 0.1866300319416102\n",
      "The 21001 th iteration gives loss of 0.18662564184856295\n",
      "The 21002 th iteration gives loss of 0.18662125223764459\n",
      "The 21003 th iteration gives loss of 0.18661686310878361\n",
      "The 21004 th iteration gives loss of 0.18661247446190965\n",
      "The 21005 th iteration gives loss of 0.18660808629692904\n",
      "The 21006 th iteration gives loss of 0.1866036986137973\n",
      "The 21007 th iteration gives loss of 0.18659931141242841\n",
      "The 21008 th iteration gives loss of 0.18659492469274824\n",
      "The 21009 th iteration gives loss of 0.18659053845467294\n",
      "The 21010 th iteration gives loss of 0.18658615269813664\n",
      "The 21011 th iteration gives loss of 0.1865817674230741\n",
      "The 21012 th iteration gives loss of 0.18657738262938947\n",
      "The 21013 th iteration gives loss of 0.18657299831702095\n",
      "The 21014 th iteration gives loss of 0.1865686144858942\n",
      "The 21015 th iteration gives loss of 0.1865642311359347\n",
      "The 21016 th iteration gives loss of 0.18655984826706878\n",
      "The 21017 th iteration gives loss of 0.1865554658792184\n",
      "The 21018 th iteration gives loss of 0.18655108397230427\n",
      "The 21019 th iteration gives loss of 0.18654670254626832\n",
      "The 21020 th iteration gives loss of 0.18654232160102163\n",
      "The 21021 th iteration gives loss of 0.1865379411365088\n",
      "The 21022 th iteration gives loss of 0.18653356115263156\n",
      "The 21023 th iteration gives loss of 0.18652918164932433\n",
      "The 21024 th iteration gives loss of 0.186524802626515\n",
      "The 21025 th iteration gives loss of 0.18652042408413744\n",
      "The 21026 th iteration gives loss of 0.18651604602210936\n",
      "The 21027 th iteration gives loss of 0.18651166844035627\n",
      "The 21028 th iteration gives loss of 0.18650729133880337\n",
      "The 21029 th iteration gives loss of 0.18650291471737265\n",
      "The 21030 th iteration gives loss of 0.18649853857601023\n",
      "The 21031 th iteration gives loss of 0.18649416291462054\n",
      "The 21032 th iteration gives loss of 0.18648978773312994\n",
      "The 21033 th iteration gives loss of 0.18648541303147595\n",
      "The 21034 th iteration gives loss of 0.18648103880957304\n",
      "The 21035 th iteration gives loss of 0.18647666506735913\n",
      "The 21036 th iteration gives loss of 0.18647229180475908\n",
      "The 21037 th iteration gives loss of 0.18646791902169058\n",
      "The 21038 th iteration gives loss of 0.18646354671807705\n",
      "The 21039 th iteration gives loss of 0.18645917489385563\n",
      "The 21040 th iteration gives loss of 0.1864548035489455\n",
      "The 21041 th iteration gives loss of 0.18645043268327138\n",
      "The 21042 th iteration gives loss of 0.18644606229676622\n",
      "The 21043 th iteration gives loss of 0.1864416923893544\n",
      "The 21044 th iteration gives loss of 0.1864373229609587\n",
      "The 21045 th iteration gives loss of 0.18643295401150783\n",
      "The 21046 th iteration gives loss of 0.18642858554092856\n",
      "The 21047 th iteration gives loss of 0.18642421754913627\n",
      "The 21048 th iteration gives loss of 0.18641985003607564\n",
      "The 21049 th iteration gives loss of 0.18641548300165658\n",
      "The 21050 th iteration gives loss of 0.18641111644581368\n",
      "The 21051 th iteration gives loss of 0.18640675036846865\n",
      "The 21052 th iteration gives loss of 0.18640238476955776\n",
      "The 21053 th iteration gives loss of 0.18639801964899566\n",
      "The 21054 th iteration gives loss of 0.18639365500670754\n",
      "The 21055 th iteration gives loss of 0.18638929084262337\n",
      "The 21056 th iteration gives loss of 0.18638492715667213\n",
      "The 21057 th iteration gives loss of 0.18638056394878513\n",
      "The 21058 th iteration gives loss of 0.18637620121886395\n",
      "The 21059 th iteration gives loss of 0.18637183896686668\n",
      "The 21060 th iteration gives loss of 0.18636747719270533\n",
      "The 21061 th iteration gives loss of 0.1863631158963077\n",
      "The 21062 th iteration gives loss of 0.18635875507759955\n",
      "The 21063 th iteration gives loss of 0.18635439473650092\n",
      "The 21064 th iteration gives loss of 0.18635003487294144\n",
      "The 21065 th iteration gives loss of 0.1863456754868516\n",
      "The 21066 th iteration gives loss of 0.18634131657814956\n",
      "The 21067 th iteration gives loss of 0.18633695814677909\n",
      "The 21068 th iteration gives loss of 0.18633260019264208\n",
      "The 21069 th iteration gives loss of 0.1863282427156868\n",
      "The 21070 th iteration gives loss of 0.18632388571583255\n",
      "The 21071 th iteration gives loss of 0.18631952919299713\n",
      "The 21072 th iteration gives loss of 0.18631517314711707\n",
      "The 21073 th iteration gives loss of 0.18631081757811266\n",
      "The 21074 th iteration gives loss of 0.18630646248590974\n",
      "The 21075 th iteration gives loss of 0.18630210787044926\n",
      "The 21076 th iteration gives loss of 0.18629775373163326\n",
      "The 21077 th iteration gives loss of 0.18629340006941422\n",
      "The 21078 th iteration gives loss of 0.18628904688369977\n",
      "The 21079 th iteration gives loss of 0.18628469417441484\n",
      "The 21080 th iteration gives loss of 0.18628034194149878\n",
      "The 21081 th iteration gives loss of 0.18627599018486724\n",
      "The 21082 th iteration gives loss of 0.1862716389044607\n",
      "The 21083 th iteration gives loss of 0.18626728810019677\n",
      "The 21084 th iteration gives loss of 0.18626293777199876\n",
      "The 21085 th iteration gives loss of 0.1862585879197962\n",
      "The 21086 th iteration gives loss of 0.1862542385435182\n",
      "The 21087 th iteration gives loss of 0.18624988964308695\n",
      "The 21088 th iteration gives loss of 0.1862455412184347\n",
      "The 21089 th iteration gives loss of 0.18624119326947705\n",
      "The 21090 th iteration gives loss of 0.18623684579615396\n",
      "The 21091 th iteration gives loss of 0.1862324987983909\n",
      "The 21092 th iteration gives loss of 0.18622815227609474\n",
      "The 21093 th iteration gives loss of 0.18622380622922027\n",
      "The 21094 th iteration gives loss of 0.1862194606576845\n",
      "The 21095 th iteration gives loss of 0.186215115561397\n",
      "The 21096 th iteration gives loss of 0.18621077094030075\n",
      "The 21097 th iteration gives loss of 0.18620642679432284\n",
      "The 21098 th iteration gives loss of 0.18620208312338535\n",
      "The 21099 th iteration gives loss of 0.18619773992741523\n",
      "The 21100 th iteration gives loss of 0.18619339720634578\n",
      "The 21101 th iteration gives loss of 0.1861890549600935\n",
      "The 21102 th iteration gives loss of 0.18618471318858826\n",
      "The 21103 th iteration gives loss of 0.18618037189176453\n",
      "The 21104 th iteration gives loss of 0.1861760310695378\n",
      "The 21105 th iteration gives loss of 0.186171690721841\n",
      "The 21106 th iteration gives loss of 0.1861673508485997\n",
      "The 21107 th iteration gives loss of 0.1861630114497423\n",
      "The 21108 th iteration gives loss of 0.18615867252519377\n",
      "The 21109 th iteration gives loss of 0.18615433407488022\n",
      "The 21110 th iteration gives loss of 0.18614999609873192\n",
      "The 21111 th iteration gives loss of 0.18614565859666907\n",
      "The 21112 th iteration gives loss of 0.18614132156862162\n",
      "The 21113 th iteration gives loss of 0.18613698501451956\n",
      "The 21114 th iteration gives loss of 0.18613264893429146\n",
      "The 21115 th iteration gives loss of 0.18612831332785684\n",
      "The 21116 th iteration gives loss of 0.186123978195148\n",
      "The 21117 th iteration gives loss of 0.1861196435360895\n",
      "The 21118 th iteration gives loss of 0.18611530935061457\n",
      "The 21119 th iteration gives loss of 0.18611097563863796\n",
      "The 21120 th iteration gives loss of 0.18610664240009064\n",
      "The 21121 th iteration gives loss of 0.1861023096349067\n",
      "The 21122 th iteration gives loss of 0.18609797734300731\n",
      "The 21123 th iteration gives loss of 0.1860936455243149\n",
      "The 21124 th iteration gives loss of 0.18608931417877483\n",
      "The 21125 th iteration gives loss of 0.1860849833062873\n",
      "The 21126 th iteration gives loss of 0.18608065290680517\n",
      "The 21127 th iteration gives loss of 0.1860763229802267\n",
      "The 21128 th iteration gives loss of 0.18607199352650683\n",
      "The 21129 th iteration gives loss of 0.18606766454556142\n",
      "The 21130 th iteration gives loss of 0.18606333603731373\n",
      "The 21131 th iteration gives loss of 0.18605900800170402\n",
      "The 21132 th iteration gives loss of 0.18605468043863746\n",
      "The 21133 th iteration gives loss of 0.18605035334806092\n",
      "The 21134 th iteration gives loss of 0.18604602672989803\n",
      "The 21135 th iteration gives loss of 0.18604170058407074\n",
      "The 21136 th iteration gives loss of 0.1860373749104945\n",
      "The 21137 th iteration gives loss of 0.18603304970911405\n",
      "The 21138 th iteration gives loss of 0.1860287249798578\n",
      "The 21139 th iteration gives loss of 0.18602440072264742\n",
      "The 21140 th iteration gives loss of 0.1860200769374062\n",
      "The 21141 th iteration gives loss of 0.18601575362407516\n",
      "The 21142 th iteration gives loss of 0.18601143078256074\n",
      "The 21143 th iteration gives loss of 0.1860071084127985\n",
      "The 21144 th iteration gives loss of 0.18600278651471702\n",
      "The 21145 th iteration gives loss of 0.18599846508824627\n",
      "The 21146 th iteration gives loss of 0.18599414413331367\n",
      "The 21147 th iteration gives loss of 0.18598982364984631\n",
      "The 21148 th iteration gives loss of 0.1859855036377689\n",
      "The 21149 th iteration gives loss of 0.18598118409700884\n",
      "The 21150 th iteration gives loss of 0.18597686502749228\n",
      "The 21151 th iteration gives loss of 0.18597254642915095\n",
      "The 21152 th iteration gives loss of 0.18596822830190202\n",
      "The 21153 th iteration gives loss of 0.1859639106456805\n",
      "The 21154 th iteration gives loss of 0.18595959346041593\n",
      "The 21155 th iteration gives loss of 0.1859552767460403\n",
      "The 21156 th iteration gives loss of 0.18595096050246732\n",
      "The 21157 th iteration gives loss of 0.18594664472962405\n",
      "The 21158 th iteration gives loss of 0.1859423294274418\n",
      "The 21159 th iteration gives loss of 0.1859380145958629\n",
      "The 21160 th iteration gives loss of 0.18593370023479508\n",
      "The 21161 th iteration gives loss of 0.18592938634417575\n",
      "The 21162 th iteration gives loss of 0.18592507292392663\n",
      "The 21163 th iteration gives loss of 0.18592075997397395\n",
      "The 21164 th iteration gives loss of 0.1859164474942515\n",
      "The 21165 th iteration gives loss of 0.18591213548469324\n",
      "The 21166 th iteration gives loss of 0.18590782394520636\n",
      "The 21167 th iteration gives loss of 0.18590351287573986\n",
      "The 21168 th iteration gives loss of 0.1858992022762064\n",
      "The 21169 th iteration gives loss of 0.1858948921465402\n",
      "The 21170 th iteration gives loss of 0.18589058248665963\n",
      "The 21171 th iteration gives loss of 0.18588627329650442\n",
      "The 21172 th iteration gives loss of 0.1858819645759986\n",
      "The 21173 th iteration gives loss of 0.18587765632506603\n",
      "The 21174 th iteration gives loss of 0.18587334854363327\n",
      "The 21175 th iteration gives loss of 0.18586904123163148\n",
      "The 21176 th iteration gives loss of 0.18586473438898585\n",
      "The 21177 th iteration gives loss of 0.18586042801563105\n",
      "The 21178 th iteration gives loss of 0.18585612211148583\n",
      "The 21179 th iteration gives loss of 0.18585181667648307\n",
      "The 21180 th iteration gives loss of 0.18584751171055247\n",
      "The 21181 th iteration gives loss of 0.18584320721360842\n",
      "The 21182 th iteration gives loss of 0.18583890318559496\n",
      "The 21183 th iteration gives loss of 0.18583459962643034\n",
      "The 21184 th iteration gives loss of 0.1858302965360491\n",
      "The 21185 th iteration gives loss of 0.1858259939143657\n",
      "The 21186 th iteration gives loss of 0.1858216917613259\n",
      "The 21187 th iteration gives loss of 0.18581739007684264\n",
      "The 21188 th iteration gives loss of 0.18581308886084905\n",
      "The 21189 th iteration gives loss of 0.18580878811326962\n",
      "The 21190 th iteration gives loss of 0.18580448783403225\n",
      "The 21191 th iteration gives loss of 0.18580018802307444\n",
      "The 21192 th iteration gives loss of 0.18579588868031746\n",
      "The 21193 th iteration gives loss of 0.18579158980568858\n",
      "The 21194 th iteration gives loss of 0.18578729139910616\n",
      "The 21195 th iteration gives loss of 0.18578299346051314\n",
      "The 21196 th iteration gives loss of 0.18577869598983074\n",
      "The 21197 th iteration gives loss of 0.1857743989869974\n",
      "The 21198 th iteration gives loss of 0.1857701024519269\n",
      "The 21199 th iteration gives loss of 0.185765806384552\n",
      "The 21200 th iteration gives loss of 0.1857615107847954\n",
      "The 21201 th iteration gives loss of 0.1857572156525894\n",
      "The 21202 th iteration gives loss of 0.18575292098786156\n",
      "The 21203 th iteration gives loss of 0.1857486267905403\n",
      "The 21204 th iteration gives loss of 0.1857443330605545\n",
      "The 21205 th iteration gives loss of 0.18574003979783182\n",
      "The 21206 th iteration gives loss of 0.18573574700229684\n",
      "The 21207 th iteration gives loss of 0.18573145467388236\n",
      "The 21208 th iteration gives loss of 0.18572716281251309\n",
      "The 21209 th iteration gives loss of 0.18572287141811825\n",
      "The 21210 th iteration gives loss of 0.18571858049062315\n",
      "The 21211 th iteration gives loss of 0.1857142900299534\n",
      "The 21212 th iteration gives loss of 0.18571000003605426\n",
      "The 21213 th iteration gives loss of 0.1857057105088352\n",
      "The 21214 th iteration gives loss of 0.18570142144822202\n",
      "The 21215 th iteration gives loss of 0.18569713285416192\n",
      "The 21216 th iteration gives loss of 0.18569284472656655\n",
      "The 21217 th iteration gives loss of 0.18568855706536788\n",
      "The 21218 th iteration gives loss of 0.18568426987049533\n",
      "The 21219 th iteration gives loss of 0.18567998314186995\n",
      "The 21220 th iteration gives loss of 0.18567569687943616\n",
      "The 21221 th iteration gives loss of 0.1856714110831057\n",
      "The 21222 th iteration gives loss of 0.18566712575281627\n",
      "The 21223 th iteration gives loss of 0.18566284088850452\n",
      "The 21224 th iteration gives loss of 0.18565855649006704\n",
      "The 21225 th iteration gives loss of 0.18565427255745448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 21226 th iteration gives loss of 0.18564998909060232\n",
      "The 21227 th iteration gives loss of 0.18564570608942266\n",
      "The 21228 th iteration gives loss of 0.18564142355385946\n",
      "The 21229 th iteration gives loss of 0.185637141483822\n",
      "The 21230 th iteration gives loss of 0.18563285987924713\n",
      "The 21231 th iteration gives loss of 0.18562857874006308\n",
      "The 21232 th iteration gives loss of 0.18562429806619862\n",
      "The 21233 th iteration gives loss of 0.1856200178575818\n",
      "The 21234 th iteration gives loss of 0.18561573811414536\n",
      "The 21235 th iteration gives loss of 0.18561145883581376\n",
      "The 21236 th iteration gives loss of 0.18560718002251375\n",
      "The 21237 th iteration gives loss of 0.18560290167416924\n",
      "The 21238 th iteration gives loss of 0.18559862379071557\n",
      "The 21239 th iteration gives loss of 0.1855943463720739\n",
      "The 21240 th iteration gives loss of 0.18559006941818904\n",
      "The 21241 th iteration gives loss of 0.18558579292896593\n",
      "The 21242 th iteration gives loss of 0.18558151690435673\n",
      "The 21243 th iteration gives loss of 0.1855772413442717\n",
      "The 21244 th iteration gives loss of 0.18557296624864444\n",
      "The 21245 th iteration gives loss of 0.18556869161739822\n",
      "The 21246 th iteration gives loss of 0.18556441745047883\n",
      "The 21247 th iteration gives loss of 0.1855601437477982\n",
      "The 21248 th iteration gives loss of 0.1855558705092912\n",
      "The 21249 th iteration gives loss of 0.1855515977348843\n",
      "The 21250 th iteration gives loss of 0.1855473254245036\n",
      "The 21251 th iteration gives loss of 0.1855430535780849\n",
      "The 21252 th iteration gives loss of 0.18553878219554987\n",
      "The 21253 th iteration gives loss of 0.18553451127683127\n",
      "The 21254 th iteration gives loss of 0.18553024082184594\n",
      "The 21255 th iteration gives loss of 0.18552597083053401\n",
      "The 21256 th iteration gives loss of 0.18552170130282436\n",
      "The 21257 th iteration gives loss of 0.1855174322386441\n",
      "The 21258 th iteration gives loss of 0.1855131636379148\n",
      "The 21259 th iteration gives loss of 0.1855088955005787\n",
      "The 21260 th iteration gives loss of 0.18550462782655533\n",
      "The 21261 th iteration gives loss of 0.18550036061576708\n",
      "The 21262 th iteration gives loss of 0.18549609386815533\n",
      "The 21263 th iteration gives loss of 0.1854918275836387\n",
      "The 21264 th iteration gives loss of 0.18548756176215422\n",
      "The 21265 th iteration gives loss of 0.18548329640361516\n",
      "The 21266 th iteration gives loss of 0.18547903150797504\n",
      "The 21267 th iteration gives loss of 0.18547476707513808\n",
      "The 21268 th iteration gives loss of 0.18547050310504604\n",
      "The 21269 th iteration gives loss of 0.185466239597624\n",
      "The 21270 th iteration gives loss of 0.18546197655279623\n",
      "The 21271 th iteration gives loss of 0.18545771397050376\n",
      "The 21272 th iteration gives loss of 0.18545345185066345\n",
      "The 21273 th iteration gives loss of 0.18544919019320913\n",
      "The 21274 th iteration gives loss of 0.1854449289980726\n",
      "The 21275 th iteration gives loss of 0.18544066826517763\n",
      "The 21276 th iteration gives loss of 0.18543640799445488\n",
      "The 21277 th iteration gives loss of 0.18543214818582182\n",
      "The 21278 th iteration gives loss of 0.18542788883922412\n",
      "The 21279 th iteration gives loss of 0.18542362995458012\n",
      "The 21280 th iteration gives loss of 0.18541937153181856\n",
      "The 21281 th iteration gives loss of 0.18541511357087911\n",
      "The 21282 th iteration gives loss of 0.1854108560716852\n",
      "The 21283 th iteration gives loss of 0.18540659903415752\n",
      "The 21284 th iteration gives loss of 0.18540234245823783\n",
      "The 21285 th iteration gives loss of 0.18539808634383345\n",
      "The 21286 th iteration gives loss of 0.18539383069090049\n",
      "The 21287 th iteration gives loss of 0.1853895754993505\n",
      "The 21288 th iteration gives loss of 0.18538532076911496\n",
      "The 21289 th iteration gives loss of 0.1853810665001333\n",
      "The 21290 th iteration gives loss of 0.18537681269231454\n",
      "The 21291 th iteration gives loss of 0.18537255934560845\n",
      "The 21292 th iteration gives loss of 0.18536830645993174\n",
      "The 21293 th iteration gives loss of 0.18536405403520156\n",
      "The 21294 th iteration gives loss of 0.18535980207136588\n",
      "The 21295 th iteration gives loss of 0.1853555505683557\n",
      "The 21296 th iteration gives loss of 0.18535129952608917\n",
      "The 21297 th iteration gives loss of 0.18534704894449705\n",
      "The 21298 th iteration gives loss of 0.18534279882351654\n",
      "The 21299 th iteration gives loss of 0.18533854916306122\n",
      "The 21300 th iteration gives loss of 0.18533429996307185\n",
      "The 21301 th iteration gives loss of 0.18533005122348004\n",
      "The 21302 th iteration gives loss of 0.18532580294420253\n",
      "The 21303 th iteration gives loss of 0.1853215551251711\n",
      "The 21304 th iteration gives loss of 0.18531730776631833\n",
      "The 21305 th iteration gives loss of 0.18531306086757493\n",
      "The 21306 th iteration gives loss of 0.18530881442887215\n",
      "The 21307 th iteration gives loss of 0.1853045684501376\n",
      "The 21308 th iteration gives loss of 0.1853003229312942\n",
      "The 21309 th iteration gives loss of 0.1852960778722698\n",
      "The 21310 th iteration gives loss of 0.185291833273006\n",
      "The 21311 th iteration gives loss of 0.185287589133422\n",
      "The 21312 th iteration gives loss of 0.18528334545344388\n",
      "The 21313 th iteration gives loss of 0.1852791022330084\n",
      "The 21314 th iteration gives loss of 0.185274859472045\n",
      "The 21315 th iteration gives loss of 0.18527061717047266\n",
      "The 21316 th iteration gives loss of 0.1852663753282369\n",
      "The 21317 th iteration gives loss of 0.18526213394525537\n",
      "The 21318 th iteration gives loss of 0.1852578930214524\n",
      "The 21319 th iteration gives loss of 0.18525365255676926\n",
      "The 21320 th iteration gives loss of 0.18524941255113392\n",
      "The 21321 th iteration gives loss of 0.1852451730044702\n",
      "The 21322 th iteration gives loss of 0.18524093391671406\n",
      "The 21323 th iteration gives loss of 0.1852366952877817\n",
      "The 21324 th iteration gives loss of 0.18523245711761094\n",
      "The 21325 th iteration gives loss of 0.18522821940612505\n",
      "The 21326 th iteration gives loss of 0.18522398215326885\n",
      "The 21327 th iteration gives loss of 0.1852197453589521\n",
      "The 21328 th iteration gives loss of 0.18521550902311484\n",
      "The 21329 th iteration gives loss of 0.18521127314569572\n",
      "The 21330 th iteration gives loss of 0.18520703772660219\n",
      "The 21331 th iteration gives loss of 0.1852028027657803\n",
      "The 21332 th iteration gives loss of 0.1851985682631511\n",
      "The 21333 th iteration gives loss of 0.1851943342186389\n",
      "The 21334 th iteration gives loss of 0.18519010063218885\n",
      "The 21335 th iteration gives loss of 0.18518586750371827\n",
      "The 21336 th iteration gives loss of 0.1851816348331547\n",
      "The 21337 th iteration gives loss of 0.1851774026204332\n",
      "The 21338 th iteration gives loss of 0.18517317086548662\n",
      "The 21339 th iteration gives loss of 0.18516893956824718\n",
      "The 21340 th iteration gives loss of 0.1851647087286387\n",
      "The 21341 th iteration gives loss of 0.18516047834657967\n",
      "The 21342 th iteration gives loss of 0.18515624842201525\n",
      "The 21343 th iteration gives loss of 0.18515201895486771\n",
      "The 21344 th iteration gives loss of 0.18514778994506023\n",
      "The 21345 th iteration gives loss of 0.18514356139253918\n",
      "The 21346 th iteration gives loss of 0.18513933329722795\n",
      "The 21347 th iteration gives loss of 0.18513510565904748\n",
      "The 21348 th iteration gives loss of 0.18513087847792986\n",
      "The 21349 th iteration gives loss of 0.1851266517537984\n",
      "The 21350 th iteration gives loss of 0.18512242548660254\n",
      "The 21351 th iteration gives loss of 0.1851181996762565\n",
      "The 21352 th iteration gives loss of 0.1851139743226961\n",
      "The 21353 th iteration gives loss of 0.18510974942584787\n",
      "The 21354 th iteration gives loss of 0.18510552498565025\n",
      "The 21355 th iteration gives loss of 0.1851013010020109\n",
      "The 21356 th iteration gives loss of 0.18509707747488818\n",
      "The 21357 th iteration gives loss of 0.18509285440417667\n",
      "The 21358 th iteration gives loss of 0.18508863178983384\n",
      "The 21359 th iteration gives loss of 0.18508440963178546\n",
      "The 21360 th iteration gives loss of 0.1850801879299492\n",
      "The 21361 th iteration gives loss of 0.1850759666842793\n",
      "The 21362 th iteration gives loss of 0.18507174589467165\n",
      "The 21363 th iteration gives loss of 0.1850675255610842\n",
      "The 21364 th iteration gives loss of 0.18506330568343268\n",
      "The 21365 th iteration gives loss of 0.18505908626164308\n",
      "The 21366 th iteration gives loss of 0.18505486729565085\n",
      "The 21367 th iteration gives loss of 0.18505064878539149\n",
      "The 21368 th iteration gives loss of 0.18504643073078672\n",
      "The 21369 th iteration gives loss of 0.18504221313176963\n",
      "The 21370 th iteration gives loss of 0.1850379959882733\n",
      "The 21371 th iteration gives loss of 0.18503377930021422\n",
      "The 21372 th iteration gives loss of 0.1850295630675405\n",
      "The 21373 th iteration gives loss of 0.18502534729016532\n",
      "The 21374 th iteration gives loss of 0.1850211319680336\n",
      "The 21375 th iteration gives loss of 0.18501691710106527\n",
      "The 21376 th iteration gives loss of 0.18501270268918743\n",
      "The 21377 th iteration gives loss of 0.18500848873233477\n",
      "The 21378 th iteration gives loss of 0.18500427523043989\n",
      "The 21379 th iteration gives loss of 0.18500006218342158\n",
      "The 21380 th iteration gives loss of 0.18499584959122833\n",
      "The 21381 th iteration gives loss of 0.18499163745377203\n",
      "The 21382 th iteration gives loss of 0.1849874257710025\n",
      "The 21383 th iteration gives loss of 0.18498321454282926\n",
      "The 21384 th iteration gives loss of 0.18497900376919038\n",
      "The 21385 th iteration gives loss of 0.18497479345001042\n",
      "The 21386 th iteration gives loss of 0.1849705835852269\n",
      "The 21387 th iteration gives loss of 0.18496637417476552\n",
      "The 21388 th iteration gives loss of 0.18496216521855727\n",
      "The 21389 th iteration gives loss of 0.18495795671653273\n",
      "The 21390 th iteration gives loss of 0.18495374866862718\n",
      "The 21391 th iteration gives loss of 0.18494954107476147\n",
      "The 21392 th iteration gives loss of 0.18494533393486662\n",
      "The 21393 th iteration gives loss of 0.18494112724887996\n",
      "The 21394 th iteration gives loss of 0.1849369210167203\n",
      "The 21395 th iteration gives loss of 0.18493271523832955\n",
      "The 21396 th iteration gives loss of 0.1849285099136217\n",
      "The 21397 th iteration gives loss of 0.18492430504255095\n",
      "The 21398 th iteration gives loss of 0.18492010062502176\n",
      "The 21399 th iteration gives loss of 0.18491589666098004\n",
      "The 21400 th iteration gives loss of 0.1849116931503565\n",
      "The 21401 th iteration gives loss of 0.18490749009307456\n",
      "The 21402 th iteration gives loss of 0.184903287489065\n",
      "The 21403 th iteration gives loss of 0.18489908533825258\n",
      "The 21404 th iteration gives loss of 0.18489488364057882\n",
      "The 21405 th iteration gives loss of 0.18489068239596962\n",
      "The 21406 th iteration gives loss of 0.18488648160435558\n",
      "The 21407 th iteration gives loss of 0.18488228126566428\n",
      "The 21408 th iteration gives loss of 0.18487808137983058\n",
      "The 21409 th iteration gives loss of 0.18487388194677332\n",
      "The 21410 th iteration gives loss of 0.18486968296643866\n",
      "The 21411 th iteration gives loss of 0.18486548443874568\n",
      "The 21412 th iteration gives loss of 0.18486128636361818\n",
      "The 21413 th iteration gives loss of 0.18485708874099982\n",
      "The 21414 th iteration gives loss of 0.1848528915708235\n",
      "The 21415 th iteration gives loss of 0.18484869485300215\n",
      "The 21416 th iteration gives loss of 0.18484449858749422\n",
      "The 21417 th iteration gives loss of 0.1848403027741949\n",
      "The 21418 th iteration gives loss of 0.18483610741305673\n",
      "The 21419 th iteration gives loss of 0.18483191250400874\n",
      "The 21420 th iteration gives loss of 0.18482771804697404\n",
      "The 21421 th iteration gives loss of 0.184823524041889\n",
      "The 21422 th iteration gives loss of 0.18481933048867727\n",
      "The 21423 th iteration gives loss of 0.18481513738726948\n",
      "The 21424 th iteration gives loss of 0.18481094473760332\n",
      "The 21425 th iteration gives loss of 0.18480675253960832\n",
      "The 21426 th iteration gives loss of 0.1848025607932142\n",
      "The 21427 th iteration gives loss of 0.1847983694983392\n",
      "The 21428 th iteration gives loss of 0.1847941786549226\n",
      "The 21429 th iteration gives loss of 0.18478998826290585\n",
      "The 21430 th iteration gives loss of 0.1847857983222076\n",
      "The 21431 th iteration gives loss of 0.1847816088327553\n",
      "The 21432 th iteration gives loss of 0.18477741979448759\n",
      "The 21433 th iteration gives loss of 0.18477323120732703\n",
      "The 21434 th iteration gives loss of 0.18476904307120914\n",
      "The 21435 th iteration gives loss of 0.18476485538606296\n",
      "The 21436 th iteration gives loss of 0.18476066815182277\n",
      "The 21437 th iteration gives loss of 0.18475648136840805\n",
      "The 21438 th iteration gives loss of 0.1847522950357563\n",
      "The 21439 th iteration gives loss of 0.1847481091538027\n",
      "The 21440 th iteration gives loss of 0.18474392372247236\n",
      "The 21441 th iteration gives loss of 0.18473973874169683\n",
      "The 21442 th iteration gives loss of 0.1847355542114019\n",
      "The 21443 th iteration gives loss of 0.1847313701315322\n",
      "The 21444 th iteration gives loss of 0.1847271865020008\n",
      "The 21445 th iteration gives loss of 0.18472300332274866\n",
      "The 21446 th iteration gives loss of 0.18471882059370362\n",
      "The 21447 th iteration gives loss of 0.1847146383147978\n",
      "The 21448 th iteration gives loss of 0.18471045648595755\n",
      "The 21449 th iteration gives loss of 0.18470627510712131\n",
      "The 21450 th iteration gives loss of 0.18470209417821248\n",
      "The 21451 th iteration gives loss of 0.18469791369916347\n",
      "The 21452 th iteration gives loss of 0.18469373366990904\n",
      "The 21453 th iteration gives loss of 0.1846895540903725\n",
      "The 21454 th iteration gives loss of 0.18468537496047863\n",
      "The 21455 th iteration gives loss of 0.18468119628017277\n",
      "The 21456 th iteration gives loss of 0.18467701804939252\n",
      "The 21457 th iteration gives loss of 0.18467284026804182\n",
      "The 21458 th iteration gives loss of 0.1846686629360714\n",
      "The 21459 th iteration gives loss of 0.18466448605339952\n",
      "The 21460 th iteration gives loss of 0.18466030961997296\n",
      "The 21461 th iteration gives loss of 0.1846561336357098\n",
      "The 21462 th iteration gives loss of 0.18465195810054655\n",
      "The 21463 th iteration gives loss of 0.18464778301441723\n",
      "The 21464 th iteration gives loss of 0.1846436083772357\n",
      "The 21465 th iteration gives loss of 0.18463943418895035\n",
      "The 21466 th iteration gives loss of 0.1846352604494778\n",
      "The 21467 th iteration gives loss of 0.18463108715877027\n",
      "The 21468 th iteration gives loss of 0.18462691431673442\n",
      "The 21469 th iteration gives loss of 0.1846227419233092\n",
      "The 21470 th iteration gives loss of 0.18461856997843396\n",
      "The 21471 th iteration gives loss of 0.18461439848203176\n",
      "The 21472 th iteration gives loss of 0.18461022743402927\n",
      "The 21473 th iteration gives loss of 0.184606056834371\n",
      "The 21474 th iteration gives loss of 0.18460188668298105\n",
      "The 21475 th iteration gives loss of 0.18459771697978541\n",
      "The 21476 th iteration gives loss of 0.18459354772472006\n",
      "The 21477 th iteration gives loss of 0.18458937891770819\n",
      "The 21478 th iteration gives loss of 0.18458521055869057\n",
      "The 21479 th iteration gives loss of 0.18458104264759015\n",
      "The 21480 th iteration gives loss of 0.1845768751843443\n",
      "The 21481 th iteration gives loss of 0.18457270816888785\n",
      "The 21482 th iteration gives loss of 0.18456854160113523\n",
      "The 21483 th iteration gives loss of 0.1845643754810381\n",
      "The 21484 th iteration gives loss of 0.18456020980851612\n",
      "The 21485 th iteration gives loss of 0.1845560445835025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 21486 th iteration gives loss of 0.18455187980591697\n",
      "The 21487 th iteration gives loss of 0.18454771547570126\n",
      "The 21488 th iteration gives loss of 0.18454355159279165\n",
      "The 21489 th iteration gives loss of 0.18453938815711843\n",
      "The 21490 th iteration gives loss of 0.18453522516859844\n",
      "The 21491 th iteration gives loss of 0.18453106262717148\n",
      "The 21492 th iteration gives loss of 0.1845269005327759\n",
      "The 21493 th iteration gives loss of 0.18452273888533602\n",
      "The 21494 th iteration gives loss of 0.1845185776847773\n",
      "The 21495 th iteration gives loss of 0.18451441693103932\n",
      "The 21496 th iteration gives loss of 0.1845102566240377\n",
      "The 21497 th iteration gives loss of 0.18450609676372245\n",
      "The 21498 th iteration gives loss of 0.18450193735001894\n",
      "The 21499 th iteration gives loss of 0.18449777838286374\n",
      "The 21500 th iteration gives loss of 0.18449361986217278\n",
      "The 21501 th iteration gives loss of 0.18448946178788678\n",
      "The 21502 th iteration gives loss of 0.18448530415993586\n",
      "The 21503 th iteration gives loss of 0.18448114697825724\n",
      "The 21504 th iteration gives loss of 0.1844769902427647\n",
      "The 21505 th iteration gives loss of 0.18447283395341868\n",
      "The 21506 th iteration gives loss of 0.18446867811012435\n",
      "The 21507 th iteration gives loss of 0.18446452271281713\n",
      "The 21508 th iteration gives loss of 0.18446036776143296\n",
      "The 21509 th iteration gives loss of 0.18445621325590156\n",
      "The 21510 th iteration gives loss of 0.18445205919615587\n",
      "The 21511 th iteration gives loss of 0.18444790558212731\n",
      "The 21512 th iteration gives loss of 0.184443752413747\n",
      "The 21513 th iteration gives loss of 0.18443959969094523\n",
      "The 21514 th iteration gives loss of 0.18443544741365522\n",
      "The 21515 th iteration gives loss of 0.18443129558180116\n",
      "The 21516 th iteration gives loss of 0.18442714419532275\n",
      "The 21517 th iteration gives loss of 0.1844229932541495\n",
      "The 21518 th iteration gives loss of 0.18441884275820566\n",
      "The 21519 th iteration gives loss of 0.18441469270743266\n",
      "The 21520 th iteration gives loss of 0.18441054310175298\n",
      "The 21521 th iteration gives loss of 0.1844063939411087\n",
      "The 21522 th iteration gives loss of 0.18440224522542426\n",
      "The 21523 th iteration gives loss of 0.1843980969546241\n",
      "The 21524 th iteration gives loss of 0.18439394912865456\n",
      "The 21525 th iteration gives loss of 0.1843898017474364\n",
      "The 21526 th iteration gives loss of 0.18438565481090774\n",
      "The 21527 th iteration gives loss of 0.184381508318997\n",
      "The 21528 th iteration gives loss of 0.1843773622716307\n",
      "The 21529 th iteration gives loss of 0.1843732166687455\n",
      "The 21530 th iteration gives loss of 0.18436907151027235\n",
      "The 21531 th iteration gives loss of 0.18436492679614358\n",
      "The 21532 th iteration gives loss of 0.1843607825262851\n",
      "The 21533 th iteration gives loss of 0.18435663870064395\n",
      "The 21534 th iteration gives loss of 0.1843524953191328\n",
      "The 21535 th iteration gives loss of 0.18434835238168717\n",
      "The 21536 th iteration gives loss of 0.18434420988824968\n",
      "The 21537 th iteration gives loss of 0.1843400678387414\n",
      "The 21538 th iteration gives loss of 0.18433592623310033\n",
      "The 21539 th iteration gives loss of 0.1843317850712463\n",
      "The 21540 th iteration gives loss of 0.18432764435312493\n",
      "The 21541 th iteration gives loss of 0.18432350407866382\n",
      "The 21542 th iteration gives loss of 0.1843193642477872\n",
      "The 21543 th iteration gives loss of 0.1843152248604371\n",
      "The 21544 th iteration gives loss of 0.1843110859165423\n",
      "The 21545 th iteration gives loss of 0.18430694741602602\n",
      "The 21546 th iteration gives loss of 0.18430280935882934\n",
      "The 21547 th iteration gives loss of 0.18429867174488032\n",
      "The 21548 th iteration gives loss of 0.18429453457410702\n",
      "The 21549 th iteration gives loss of 0.18429039784645274\n",
      "The 21550 th iteration gives loss of 0.18428626156183794\n",
      "The 21551 th iteration gives loss of 0.1842821257201924\n",
      "The 21552 th iteration gives loss of 0.18427799032145875\n",
      "The 21553 th iteration gives loss of 0.18427385536556048\n",
      "The 21554 th iteration gives loss of 0.18426972085243526\n",
      "The 21555 th iteration gives loss of 0.1842655867820143\n",
      "The 21556 th iteration gives loss of 0.18426145315422376\n",
      "The 21557 th iteration gives loss of 0.18425731996900113\n",
      "The 21558 th iteration gives loss of 0.18425318722626813\n",
      "The 21559 th iteration gives loss of 0.1842490549259668\n",
      "The 21560 th iteration gives loss of 0.1842449230680355\n",
      "The 21561 th iteration gives loss of 0.18424079165238336\n",
      "The 21562 th iteration gives loss of 0.18423666067895197\n",
      "The 21563 th iteration gives loss of 0.18423253014768506\n",
      "The 21564 th iteration gives loss of 0.18422840005850213\n",
      "The 21565 th iteration gives loss of 0.1842242704113345\n",
      "The 21566 th iteration gives loss of 0.1842201412061268\n",
      "The 21567 th iteration gives loss of 0.18421601244279592\n",
      "The 21568 th iteration gives loss of 0.18421188412128767\n",
      "The 21569 th iteration gives loss of 0.18420775624152078\n",
      "The 21570 th iteration gives loss of 0.18420362880343918\n",
      "The 21571 th iteration gives loss of 0.1841995018069631\n",
      "The 21572 th iteration gives loss of 0.18419537525202542\n",
      "The 21573 th iteration gives loss of 0.1841912491385674\n",
      "The 21574 th iteration gives loss of 0.18418712346650631\n",
      "The 21575 th iteration gives loss of 0.1841829982357917\n",
      "The 21576 th iteration gives loss of 0.18417887344634967\n",
      "The 21577 th iteration gives loss of 0.18417474909810808\n",
      "The 21578 th iteration gives loss of 0.18417062519099014\n",
      "The 21579 th iteration gives loss of 0.18416650172494772\n",
      "The 21580 th iteration gives loss of 0.18416237869989888\n",
      "The 21581 th iteration gives loss of 0.1841582561157868\n",
      "The 21582 th iteration gives loss of 0.18415413397253044\n",
      "The 21583 th iteration gives loss of 0.1841500122700733\n",
      "The 21584 th iteration gives loss of 0.18414589100833959\n",
      "The 21585 th iteration gives loss of 0.18414177018726227\n",
      "The 21586 th iteration gives loss of 0.1841376498067703\n",
      "The 21587 th iteration gives loss of 0.18413352986680873\n",
      "The 21588 th iteration gives loss of 0.18412941036729547\n",
      "The 21589 th iteration gives loss of 0.1841252913081755\n",
      "The 21590 th iteration gives loss of 0.18412117268936742\n",
      "The 21591 th iteration gives loss of 0.1841170545108172\n",
      "The 21592 th iteration gives loss of 0.18411293677244525\n",
      "The 21593 th iteration gives loss of 0.18410881947418856\n",
      "The 21594 th iteration gives loss of 0.18410470261597917\n",
      "The 21595 th iteration gives loss of 0.1841005861977477\n",
      "The 21596 th iteration gives loss of 0.1840964702194268\n",
      "The 21597 th iteration gives loss of 0.18409235468095467\n",
      "The 21598 th iteration gives loss of 0.18408823958224624\n",
      "The 21599 th iteration gives loss of 0.1840841249232522\n",
      "The 21600 th iteration gives loss of 0.18408001070390168\n",
      "The 21601 th iteration gives loss of 0.18407589692411447\n",
      "The 21602 th iteration gives loss of 0.1840717835838454\n",
      "The 21603 th iteration gives loss of 0.18406767068300453\n",
      "The 21604 th iteration gives loss of 0.18406355822153175\n",
      "The 21605 th iteration gives loss of 0.18405944619936032\n",
      "The 21606 th iteration gives loss of 0.1840553346164157\n",
      "The 21607 th iteration gives loss of 0.18405122347264702\n",
      "The 21608 th iteration gives loss of 0.18404711276797397\n",
      "The 21609 th iteration gives loss of 0.18404300250232616\n",
      "The 21610 th iteration gives loss of 0.18403889267564053\n",
      "The 21611 th iteration gives loss of 0.18403478328785602\n",
      "The 21612 th iteration gives loss of 0.1840306743388949\n",
      "The 21613 th iteration gives loss of 0.18402656582869575\n",
      "The 21614 th iteration gives loss of 0.1840224577571883\n",
      "The 21615 th iteration gives loss of 0.18401835012430237\n",
      "The 21616 th iteration gives loss of 0.1840142429299747\n",
      "The 21617 th iteration gives loss of 0.18401013617413392\n",
      "The 21618 th iteration gives loss of 0.18400602985671527\n",
      "The 21619 th iteration gives loss of 0.18400192397764828\n",
      "The 21620 th iteration gives loss of 0.1839978185368736\n",
      "The 21621 th iteration gives loss of 0.1839937135343086\n",
      "The 21622 th iteration gives loss of 0.18398960896990002\n",
      "The 21623 th iteration gives loss of 0.18398550484357773\n",
      "The 21624 th iteration gives loss of 0.18398140115527084\n",
      "The 21625 th iteration gives loss of 0.1839772979049037\n",
      "The 21626 th iteration gives loss of 0.18397319509242213\n",
      "The 21627 th iteration gives loss of 0.18396909271775524\n",
      "The 21628 th iteration gives loss of 0.1839649907808327\n",
      "The 21629 th iteration gives loss of 0.18396088928158086\n",
      "The 21630 th iteration gives loss of 0.18395678821995218\n",
      "The 21631 th iteration gives loss of 0.1839526875958618\n",
      "The 21632 th iteration gives loss of 0.18394858740924097\n",
      "The 21633 th iteration gives loss of 0.18394448766003452\n",
      "The 21634 th iteration gives loss of 0.18394038834816834\n",
      "The 21635 th iteration gives loss of 0.1839362894735743\n",
      "The 21636 th iteration gives loss of 0.18393219103619118\n",
      "The 21637 th iteration gives loss of 0.18392809303593888\n",
      "The 21638 th iteration gives loss of 0.18392399547276217\n",
      "The 21639 th iteration gives loss of 0.18391989834658867\n",
      "The 21640 th iteration gives loss of 0.1839158016573587\n",
      "The 21641 th iteration gives loss of 0.18391170540499194\n",
      "The 21642 th iteration gives loss of 0.18390760958942504\n",
      "The 21643 th iteration gives loss of 0.1839035142105857\n",
      "The 21644 th iteration gives loss of 0.183899419268422\n",
      "The 21645 th iteration gives loss of 0.18389532476285786\n",
      "The 21646 th iteration gives loss of 0.18389123069382673\n",
      "The 21647 th iteration gives loss of 0.18388713706125562\n",
      "The 21648 th iteration gives loss of 0.1838830438650828\n",
      "The 21649 th iteration gives loss of 0.18387895110524818\n",
      "The 21650 th iteration gives loss of 0.18387485878166251\n",
      "The 21651 th iteration gives loss of 0.18387076689428594\n",
      "The 21652 th iteration gives loss of 0.18386667544303173\n",
      "The 21653 th iteration gives loss of 0.18386258442783782\n",
      "The 21654 th iteration gives loss of 0.18385849384864353\n",
      "The 21655 th iteration gives loss of 0.18385440370537867\n",
      "The 21656 th iteration gives loss of 0.1838503139979625\n",
      "The 21657 th iteration gives loss of 0.18384622472634368\n",
      "The 21658 th iteration gives loss of 0.18384213589045567\n",
      "The 21659 th iteration gives loss of 0.18383804749022026\n",
      "The 21660 th iteration gives loss of 0.18383395952557388\n",
      "The 21661 th iteration gives loss of 0.18382987199645187\n",
      "The 21662 th iteration gives loss of 0.18382578490278667\n",
      "The 21663 th iteration gives loss of 0.18382169824451414\n",
      "The 21664 th iteration gives loss of 0.18381761202156585\n",
      "The 21665 th iteration gives loss of 0.18381352623386837\n",
      "The 21666 th iteration gives loss of 0.18380944088135992\n",
      "The 21667 th iteration gives loss of 0.18380535596397224\n",
      "The 21668 th iteration gives loss of 0.1838012714816393\n",
      "The 21669 th iteration gives loss of 0.18379718743429363\n",
      "The 21670 th iteration gives loss of 0.18379310382186248\n",
      "The 21671 th iteration gives loss of 0.18378902064429403\n",
      "The 21672 th iteration gives loss of 0.1837849379015046\n",
      "The 21673 th iteration gives loss of 0.1837808555934371\n",
      "The 21674 th iteration gives loss of 0.18377677372001894\n",
      "The 21675 th iteration gives loss of 0.1837726922811803\n",
      "The 21676 th iteration gives loss of 0.18376861127686478\n",
      "The 21677 th iteration gives loss of 0.1837645307069957\n",
      "The 21678 th iteration gives loss of 0.18376045057151383\n",
      "The 21679 th iteration gives loss of 0.18375637087035301\n",
      "The 21680 th iteration gives loss of 0.183752291603436\n",
      "The 21681 th iteration gives loss of 0.1837482127707088\n",
      "The 21682 th iteration gives loss of 0.18374413437209092\n",
      "The 21683 th iteration gives loss of 0.1837400564075243\n",
      "The 21684 th iteration gives loss of 0.18373597887693854\n",
      "The 21685 th iteration gives loss of 0.18373190178026894\n",
      "The 21686 th iteration gives loss of 0.1837278251174401\n",
      "The 21687 th iteration gives loss of 0.18372374888839746\n",
      "The 21688 th iteration gives loss of 0.18371967309306805\n",
      "The 21689 th iteration gives loss of 0.1837155977313872\n",
      "The 21690 th iteration gives loss of 0.18371152280329103\n",
      "The 21691 th iteration gives loss of 0.18370744830870106\n",
      "The 21692 th iteration gives loss of 0.1837033742475611\n",
      "The 21693 th iteration gives loss of 0.18369930061981216\n",
      "The 21694 th iteration gives loss of 0.1836952274253642\n",
      "The 21695 th iteration gives loss of 0.18369115466416544\n",
      "The 21696 th iteration gives loss of 0.18368708233614536\n",
      "The 21697 th iteration gives loss of 0.18368301044123522\n",
      "The 21698 th iteration gives loss of 0.18367893897938184\n",
      "The 21699 th iteration gives loss of 0.18367486795049554\n",
      "The 21700 th iteration gives loss of 0.18367079735452532\n",
      "The 21701 th iteration gives loss of 0.18366672719140473\n",
      "The 21702 th iteration gives loss of 0.18366265746105861\n",
      "The 21703 th iteration gives loss of 0.18365858816343006\n",
      "The 21704 th iteration gives loss of 0.1836545192984459\n",
      "The 21705 th iteration gives loss of 0.18365045086603818\n",
      "The 21706 th iteration gives loss of 0.18364638286614673\n",
      "The 21707 th iteration gives loss of 0.18364231529869934\n",
      "The 21708 th iteration gives loss of 0.1836382481636276\n",
      "The 21709 th iteration gives loss of 0.18363418146087565\n",
      "The 21710 th iteration gives loss of 0.18363011519036132\n",
      "The 21711 th iteration gives loss of 0.18362604935203158\n",
      "The 21712 th iteration gives loss of 0.1836219839458138\n",
      "The 21713 th iteration gives loss of 0.18361791897163585\n",
      "The 21714 th iteration gives loss of 0.18361385442944095\n",
      "The 21715 th iteration gives loss of 0.183609790319156\n",
      "The 21716 th iteration gives loss of 0.1836057266407172\n",
      "The 21717 th iteration gives loss of 0.1836016633940615\n",
      "The 21718 th iteration gives loss of 0.1835976005791207\n",
      "The 21719 th iteration gives loss of 0.18359353819582502\n",
      "The 21720 th iteration gives loss of 0.18358947624410804\n",
      "The 21721 th iteration gives loss of 0.1835854147238936\n",
      "The 21722 th iteration gives loss of 0.18358135363513828\n",
      "The 21723 th iteration gives loss of 0.18357729297775696\n",
      "The 21724 th iteration gives loss of 0.18357323275169904\n",
      "The 21725 th iteration gives loss of 0.18356917295688016\n",
      "The 21726 th iteration gives loss of 0.1835651135932445\n",
      "The 21727 th iteration gives loss of 0.1835610546607231\n",
      "The 21728 th iteration gives loss of 0.18355699615924895\n",
      "The 21729 th iteration gives loss of 0.1835529380887525\n",
      "The 21730 th iteration gives loss of 0.18354888044917042\n",
      "The 21731 th iteration gives loss of 0.18354482324044527\n",
      "The 21732 th iteration gives loss of 0.1835407664624941\n",
      "The 21733 th iteration gives loss of 0.18353671011526065\n",
      "The 21734 th iteration gives loss of 0.1835326541986797\n",
      "The 21735 th iteration gives loss of 0.1835285987126725\n",
      "The 21736 th iteration gives loss of 0.18352454365718088\n",
      "The 21737 th iteration gives loss of 0.1835204890321519\n",
      "The 21738 th iteration gives loss of 0.1835164348375005\n",
      "The 21739 th iteration gives loss of 0.1835123810731661\n",
      "The 21740 th iteration gives loss of 0.18350832773908082\n",
      "The 21741 th iteration gives loss of 0.18350427483518456\n",
      "The 21742 th iteration gives loss of 0.18350022236140545\n",
      "The 21743 th iteration gives loss of 0.18349617031768145\n",
      "The 21744 th iteration gives loss of 0.18349211870393997\n",
      "The 21745 th iteration gives loss of 0.183488067520115\n",
      "The 21746 th iteration gives loss of 0.18348401676614617\n",
      "The 21747 th iteration gives loss of 0.1834799664419619\n",
      "The 21748 th iteration gives loss of 0.18347591654750134\n",
      "The 21749 th iteration gives loss of 0.1834718670826936\n",
      "The 21750 th iteration gives loss of 0.18346781804746903\n",
      "The 21751 th iteration gives loss of 0.18346376944177822\n",
      "The 21752 th iteration gives loss of 0.18345972126553767\n",
      "The 21753 th iteration gives loss of 0.183455673518683\n",
      "The 21754 th iteration gives loss of 0.1834516262011524\n",
      "The 21755 th iteration gives loss of 0.18344757931287542\n",
      "The 21756 th iteration gives loss of 0.1834435328537937\n",
      "The 21757 th iteration gives loss of 0.18343948682383796\n",
      "The 21758 th iteration gives loss of 0.1834354412229465\n",
      "The 21759 th iteration gives loss of 0.18343139605103095\n",
      "The 21760 th iteration gives loss of 0.1834273513080533\n",
      "The 21761 th iteration gives loss of 0.18342330699394185\n",
      "The 21762 th iteration gives loss of 0.18341926310861023\n",
      "The 21763 th iteration gives loss of 0.18341521965201468\n",
      "The 21764 th iteration gives loss of 0.18341117662407994\n",
      "The 21765 th iteration gives loss of 0.18340713402473352\n",
      "The 21766 th iteration gives loss of 0.1834030918539321\n",
      "The 21767 th iteration gives loss of 0.18339905011158347\n",
      "The 21768 th iteration gives loss of 0.18339500879763396\n",
      "The 21769 th iteration gives loss of 0.18339096791202555\n",
      "The 21770 th iteration gives loss of 0.18338692745467483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 21771 th iteration gives loss of 0.18338288742552245\n",
      "The 21772 th iteration gives loss of 0.18337884782450267\n",
      "The 21773 th iteration gives loss of 0.18337480865155478\n",
      "The 21774 th iteration gives loss of 0.18337076990660484\n",
      "The 21775 th iteration gives loss of 0.18336673158959133\n",
      "The 21776 th iteration gives loss of 0.1833626937004485\n",
      "The 21777 th iteration gives loss of 0.1833586562391142\n",
      "The 21778 th iteration gives loss of 0.1833546192055135\n",
      "The 21779 th iteration gives loss of 0.1833505825995802\n",
      "The 21780 th iteration gives loss of 0.18334654642126058\n",
      "The 21781 th iteration gives loss of 0.18334251067047727\n",
      "The 21782 th iteration gives loss of 0.1833384753471679\n",
      "The 21783 th iteration gives loss of 0.18333444045126307\n",
      "The 21784 th iteration gives loss of 0.18333040598270553\n",
      "The 21785 th iteration gives loss of 0.18332637194141774\n",
      "The 21786 th iteration gives loss of 0.18332233832734426\n",
      "The 21787 th iteration gives loss of 0.1833183051404263\n",
      "The 21788 th iteration gives loss of 0.1833142723805722\n",
      "The 21789 th iteration gives loss of 0.18331024004773522\n",
      "The 21790 th iteration gives loss of 0.1833062081418428\n",
      "The 21791 th iteration gives loss of 0.18330217666283938\n",
      "The 21792 th iteration gives loss of 0.18329814561064817\n",
      "The 21793 th iteration gives loss of 0.18329411498521267\n",
      "The 21794 th iteration gives loss of 0.1832900847864536\n",
      "The 21795 th iteration gives loss of 0.1832860550143138\n",
      "The 21796 th iteration gives loss of 0.18328202566872587\n",
      "The 21797 th iteration gives loss of 0.1832779967496188\n",
      "The 21798 th iteration gives loss of 0.1832739682569379\n",
      "The 21799 th iteration gives loss of 0.18326994019061243\n",
      "The 21800 th iteration gives loss of 0.18326591255058364\n",
      "The 21801 th iteration gives loss of 0.18326188533677595\n",
      "The 21802 th iteration gives loss of 0.18325785854912213\n",
      "The 21803 th iteration gives loss of 0.18325383218756325\n",
      "The 21804 th iteration gives loss of 0.18324980625202691\n",
      "The 21805 th iteration gives loss of 0.1832457807424556\n",
      "The 21806 th iteration gives loss of 0.18324175565877568\n",
      "The 21807 th iteration gives loss of 0.18323773100093135\n",
      "The 21808 th iteration gives loss of 0.1832337067688468\n",
      "The 21809 th iteration gives loss of 0.18322968296246483\n",
      "The 21810 th iteration gives loss of 0.18322565958171325\n",
      "The 21811 th iteration gives loss of 0.18322163662652502\n",
      "The 21812 th iteration gives loss of 0.18321761409684342\n",
      "The 21813 th iteration gives loss of 0.18321359199259682\n",
      "The 21814 th iteration gives loss of 0.18320957031372395\n",
      "The 21815 th iteration gives loss of 0.18320554906014358\n",
      "The 21816 th iteration gives loss of 0.18320152823181218\n",
      "The 21817 th iteration gives loss of 0.1831975078286581\n",
      "The 21818 th iteration gives loss of 0.18319348785061332\n",
      "The 21819 th iteration gives loss of 0.18318946829760316\n",
      "The 21820 th iteration gives loss of 0.18318544916957444\n",
      "The 21821 th iteration gives loss of 0.1831814304664623\n",
      "The 21822 th iteration gives loss of 0.18317741218818248\n",
      "The 21823 th iteration gives loss of 0.18317339433469002\n",
      "The 21824 th iteration gives loss of 0.18316937690592003\n",
      "The 21825 th iteration gives loss of 0.18316535990179822\n",
      "The 21826 th iteration gives loss of 0.18316134332225378\n",
      "The 21827 th iteration gives loss of 0.18315732716722843\n",
      "The 21828 th iteration gives loss of 0.18315331143665992\n",
      "The 21829 th iteration gives loss of 0.18314929613048098\n",
      "The 21830 th iteration gives loss of 0.18314528124862314\n",
      "The 21831 th iteration gives loss of 0.18314126679102144\n",
      "The 21832 th iteration gives loss of 0.18313725275761766\n",
      "The 21833 th iteration gives loss of 0.1831332391483324\n",
      "The 21834 th iteration gives loss of 0.18312922596311612\n",
      "The 21835 th iteration gives loss of 0.18312521320188965\n",
      "The 21836 th iteration gives loss of 0.1831212008645934\n",
      "The 21837 th iteration gives loss of 0.18311718895116652\n",
      "The 21838 th iteration gives loss of 0.18311317746153377\n",
      "The 21839 th iteration gives loss of 0.18310916639563532\n",
      "The 21840 th iteration gives loss of 0.1831051557534037\n",
      "The 21841 th iteration gives loss of 0.18310114553478787\n",
      "The 21842 th iteration gives loss of 0.1830971357397031\n",
      "The 21843 th iteration gives loss of 0.1830931263680907\n",
      "The 21844 th iteration gives loss of 0.18308911741989492\n",
      "The 21845 th iteration gives loss of 0.18308510889503019\n",
      "The 21846 th iteration gives loss of 0.18308110079344248\n",
      "The 21847 th iteration gives loss of 0.1830770931150773\n",
      "The 21848 th iteration gives loss of 0.18307308585984614\n",
      "The 21849 th iteration gives loss of 0.18306907902771008\n",
      "The 21850 th iteration gives loss of 0.1830650726185889\n",
      "The 21851 th iteration gives loss of 0.1830610666324127\n",
      "The 21852 th iteration gives loss of 0.18305706106912553\n",
      "The 21853 th iteration gives loss of 0.18305305592865548\n",
      "The 21854 th iteration gives loss of 0.18304905121094128\n",
      "The 21855 th iteration gives loss of 0.1830450469159282\n",
      "The 21856 th iteration gives loss of 0.18304104304353283\n",
      "The 21857 th iteration gives loss of 0.18303703959369533\n",
      "The 21858 th iteration gives loss of 0.1830330365663606\n",
      "The 21859 th iteration gives loss of 0.18302903396145367\n",
      "The 21860 th iteration gives loss of 0.1830250317789103\n",
      "The 21861 th iteration gives loss of 0.1830210300186653\n",
      "The 21862 th iteration gives loss of 0.1830170286806498\n",
      "The 21863 th iteration gives loss of 0.18301302776481282\n",
      "The 21864 th iteration gives loss of 0.1830090272710798\n",
      "The 21865 th iteration gives loss of 0.18300502719938774\n",
      "The 21866 th iteration gives loss of 0.18300102754966477\n",
      "The 21867 th iteration gives loss of 0.18299702832185805\n",
      "The 21868 th iteration gives loss of 0.18299302951589067\n",
      "The 21869 th iteration gives loss of 0.18298903113170276\n",
      "The 21870 th iteration gives loss of 0.18298503316923717\n",
      "The 21871 th iteration gives loss of 0.18298103562841275\n",
      "The 21872 th iteration gives loss of 0.18297703850917582\n",
      "The 21873 th iteration gives loss of 0.18297304181145435\n",
      "The 21874 th iteration gives loss of 0.18296904553518784\n",
      "The 21875 th iteration gives loss of 0.18296504968031618\n",
      "The 21876 th iteration gives loss of 0.18296105424677406\n",
      "The 21877 th iteration gives loss of 0.18295705923447567\n",
      "The 21878 th iteration gives loss of 0.1829530646433834\n",
      "The 21879 th iteration gives loss of 0.1829490704734121\n",
      "The 21880 th iteration gives loss of 0.18294507672451443\n",
      "The 21881 th iteration gives loss of 0.1829410833966209\n",
      "The 21882 th iteration gives loss of 0.18293709048965878\n",
      "The 21883 th iteration gives loss of 0.18293309800356441\n",
      "The 21884 th iteration gives loss of 0.18292910593827424\n",
      "The 21885 th iteration gives loss of 0.1829251142937292\n",
      "The 21886 th iteration gives loss of 0.18292112306985694\n",
      "The 21887 th iteration gives loss of 0.18291713226659886\n",
      "The 21888 th iteration gives loss of 0.18291314188387378\n",
      "The 21889 th iteration gives loss of 0.18290915192164475\n",
      "The 21890 th iteration gives loss of 0.18290516237982693\n",
      "The 21891 th iteration gives loss of 0.18290117325835523\n",
      "The 21892 th iteration gives loss of 0.18289718455717752\n",
      "The 21893 th iteration gives loss of 0.18289319627622463\n",
      "The 21894 th iteration gives loss of 0.18288920841542716\n",
      "The 21895 th iteration gives loss of 0.1828852209747187\n",
      "The 21896 th iteration gives loss of 0.18288123395404018\n",
      "The 21897 th iteration gives loss of 0.1828772473533292\n",
      "The 21898 th iteration gives loss of 0.1828732611725091\n",
      "The 21899 th iteration gives loss of 0.1828692754115297\n",
      "The 21900 th iteration gives loss of 0.18286529007031188\n",
      "The 21901 th iteration gives loss of 0.18286130514880602\n",
      "The 21902 th iteration gives loss of 0.18285732064693425\n",
      "The 21903 th iteration gives loss of 0.18285333656463887\n",
      "The 21904 th iteration gives loss of 0.1828493529018597\n",
      "The 21905 th iteration gives loss of 0.18284536965851947\n",
      "The 21906 th iteration gives loss of 0.18284138683456322\n",
      "The 21907 th iteration gives loss of 0.1828374044299224\n",
      "The 21908 th iteration gives loss of 0.18283342244453393\n",
      "The 21909 th iteration gives loss of 0.18282944087832928\n",
      "The 21910 th iteration gives loss of 0.1828254597312546\n",
      "The 21911 th iteration gives loss of 0.1828214790032412\n",
      "The 21912 th iteration gives loss of 0.18281749869419883\n",
      "The 21913 th iteration gives loss of 0.1828135188040985\n",
      "The 21914 th iteration gives loss of 0.18280953933286775\n",
      "The 21915 th iteration gives loss of 0.18280556028043637\n",
      "The 21916 th iteration gives loss of 0.18280158164673554\n",
      "The 21917 th iteration gives loss of 0.1827976034317084\n",
      "The 21918 th iteration gives loss of 0.18279362563528037\n",
      "The 21919 th iteration gives loss of 0.18278964825740251\n",
      "The 21920 th iteration gives loss of 0.18278567129799672\n",
      "The 21921 th iteration gives loss of 0.18278169475700784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 21922 th iteration gives loss of 0.18277771863437214\n",
      "The 21923 th iteration gives loss of 0.1827737429300046\n",
      "The 21924 th iteration gives loss of 0.18276976764386732\n",
      "The 21925 th iteration gives loss of 0.18276579277587757\n",
      "The 21926 th iteration gives loss of 0.18276181832598618\n",
      "The 21927 th iteration gives loss of 0.18275784429411676\n",
      "The 21928 th iteration gives loss of 0.18275387068021232\n",
      "The 21929 th iteration gives loss of 0.18274989748419893\n",
      "The 21930 th iteration gives loss of 0.18274592470602183\n",
      "The 21931 th iteration gives loss of 0.18274195234561283\n",
      "The 21932 th iteration gives loss of 0.18273798040290665\n",
      "The 21933 th iteration gives loss of 0.18273400887784558\n",
      "The 21934 th iteration gives loss of 0.182730037770353\n",
      "The 21935 th iteration gives loss of 0.18272606708037623\n",
      "The 21936 th iteration gives loss of 0.1827220968078377\n",
      "The 21937 th iteration gives loss of 0.18271812695269143\n",
      "The 21938 th iteration gives loss of 0.18271415751485848\n",
      "The 21939 th iteration gives loss of 0.1827101884942773\n",
      "The 21940 th iteration gives loss of 0.18270621989088726\n",
      "The 21941 th iteration gives loss of 0.18270225170462615\n",
      "The 21942 th iteration gives loss of 0.1826982839354202\n",
      "The 21943 th iteration gives loss of 0.18269431658320898\n",
      "The 21944 th iteration gives loss of 0.18269034964793385\n",
      "The 21945 th iteration gives loss of 0.1826863831295304\n",
      "The 21946 th iteration gives loss of 0.18268241702792337\n",
      "The 21947 th iteration gives loss of 0.18267845134305666\n",
      "The 21948 th iteration gives loss of 0.18267448607486778\n",
      "The 21949 th iteration gives loss of 0.18267052122329053\n",
      "The 21950 th iteration gives loss of 0.18266655678826418\n",
      "The 21951 th iteration gives loss of 0.1826625927697105\n",
      "The 21952 th iteration gives loss of 0.18265862916758213\n",
      "The 21953 th iteration gives loss of 0.18265466598180516\n",
      "The 21954 th iteration gives loss of 0.1826507032123156\n",
      "The 21955 th iteration gives loss of 0.1826467408590574\n",
      "The 21956 th iteration gives loss of 0.18264277892195815\n",
      "The 21957 th iteration gives loss of 0.18263881740095636\n",
      "The 21958 th iteration gives loss of 0.18263485629599147\n",
      "The 21959 th iteration gives loss of 0.1826308956069934\n",
      "The 21960 th iteration gives loss of 0.18262693533390661\n",
      "The 21961 th iteration gives loss of 0.18262297547665518\n",
      "The 21962 th iteration gives loss of 0.1826190160351824\n",
      "The 21963 th iteration gives loss of 0.1826150570094178\n",
      "The 21964 th iteration gives loss of 0.18261109839931092\n",
      "The 21965 th iteration gives loss of 0.18260714020478191\n",
      "The 21966 th iteration gives loss of 0.18260318242577864\n",
      "The 21967 th iteration gives loss of 0.1825992250622317\n",
      "The 21968 th iteration gives loss of 0.18259526811407542\n",
      "The 21969 th iteration gives loss of 0.1825913115812526\n",
      "The 21970 th iteration gives loss of 0.18258735546368787\n",
      "The 21971 th iteration gives loss of 0.18258339976133128\n",
      "The 21972 th iteration gives loss of 0.18257944447410981\n",
      "The 21973 th iteration gives loss of 0.182575489601958\n",
      "The 21974 th iteration gives loss of 0.18257153514482272\n",
      "The 21975 th iteration gives loss of 0.18256758110262347\n",
      "The 21976 th iteration gives loss of 0.1825636274753052\n",
      "The 21977 th iteration gives loss of 0.18255967426281092\n",
      "The 21978 th iteration gives loss of 0.1825557214650708\n",
      "The 21979 th iteration gives loss of 0.18255176908201876\n",
      "The 21980 th iteration gives loss of 0.18254781711358636\n",
      "The 21981 th iteration gives loss of 0.18254386555972055\n",
      "The 21982 th iteration gives loss of 0.182539914420349\n",
      "The 21983 th iteration gives loss of 0.18253596369541567\n",
      "The 21984 th iteration gives loss of 0.18253201338485084\n",
      "The 21985 th iteration gives loss of 0.1825280634885932\n",
      "The 21986 th iteration gives loss of 0.18252411400657179\n",
      "The 21987 th iteration gives loss of 0.18252016493874243\n",
      "The 21988 th iteration gives loss of 0.18251621628501852\n",
      "The 21989 th iteration gives loss of 0.1825122680453426\n",
      "The 21990 th iteration gives loss of 0.18250832021966257\n",
      "The 21991 th iteration gives loss of 0.18250437280789716\n",
      "The 21992 th iteration gives loss of 0.18250042580999767\n",
      "The 21993 th iteration gives loss of 0.18249647922589074\n",
      "The 21994 th iteration gives loss of 0.18249253305551116\n",
      "The 21995 th iteration gives loss of 0.18248858729880074\n",
      "The 21996 th iteration gives loss of 0.18248464195569913\n",
      "The 21997 th iteration gives loss of 0.1824806970261425\n",
      "The 21998 th iteration gives loss of 0.18247675251006135\n",
      "The 21999 th iteration gives loss of 0.18247280840738808\n",
      "The 22000 th iteration gives loss of 0.18246886471806648\n",
      "The 22001 th iteration gives loss of 0.18246492144203497\n",
      "The 22002 th iteration gives loss of 0.18246097857922322\n",
      "The 22003 th iteration gives loss of 0.18245703612957223\n",
      "The 22004 th iteration gives loss of 0.18245309409300947\n",
      "The 22005 th iteration gives loss of 0.1824491524694869\n",
      "The 22006 th iteration gives loss of 0.1824452112589285\n",
      "The 22007 th iteration gives loss of 0.1824412704612704\n",
      "The 22008 th iteration gives loss of 0.182437330076457\n",
      "The 22009 th iteration gives loss of 0.18243339010441822\n",
      "The 22010 th iteration gives loss of 0.18242945054510434\n",
      "The 22011 th iteration gives loss of 0.1824255113984215\n",
      "The 22012 th iteration gives loss of 0.18242157266433343\n",
      "The 22013 th iteration gives loss of 0.1824176343427671\n",
      "The 22014 th iteration gives loss of 0.18241369643365413\n",
      "The 22015 th iteration gives loss of 0.182409758936943\n",
      "The 22016 th iteration gives loss of 0.18240582185256238\n",
      "The 22017 th iteration gives loss of 0.18240188518045294\n",
      "The 22018 th iteration gives loss of 0.18239794892054734\n",
      "The 22019 th iteration gives loss of 0.18239401307277933\n",
      "The 22020 th iteration gives loss of 0.18239007763709264\n",
      "The 22021 th iteration gives loss of 0.18238614261342262\n",
      "The 22022 th iteration gives loss of 0.18238220800170138\n",
      "The 22023 th iteration gives loss of 0.18237827380186372\n",
      "The 22024 th iteration gives loss of 0.1823743400138472\n",
      "The 22025 th iteration gives loss of 0.1823704066375928\n",
      "The 22026 th iteration gives loss of 0.1823664736730426\n",
      "The 22027 th iteration gives loss of 0.18236254112012656\n",
      "The 22028 th iteration gives loss of 0.1823586089787783\n",
      "The 22029 th iteration gives loss of 0.18235467724893317\n",
      "The 22030 th iteration gives loss of 0.18235074593053588\n",
      "The 22031 th iteration gives loss of 0.18234681502351452\n",
      "The 22032 th iteration gives loss of 0.18234288452780784\n",
      "The 22033 th iteration gives loss of 0.18233895444336112\n",
      "The 22034 th iteration gives loss of 0.18233502477010372\n",
      "The 22035 th iteration gives loss of 0.18233109550797205\n",
      "The 22036 th iteration gives loss of 0.18232716665689572\n",
      "The 22037 th iteration gives loss of 0.18232323821683202\n",
      "The 22038 th iteration gives loss of 0.18231931018769504\n",
      "The 22039 th iteration gives loss of 0.18231538256943647\n",
      "The 22040 th iteration gives loss of 0.18231145536198715\n",
      "The 22041 th iteration gives loss of 0.1823075285652803\n",
      "The 22042 th iteration gives loss of 0.1823036021792577\n",
      "The 22043 th iteration gives loss of 0.18229967620385235\n",
      "The 22044 th iteration gives loss of 0.1822957506390112\n",
      "The 22045 th iteration gives loss of 0.18229182548466236\n",
      "The 22046 th iteration gives loss of 0.18228790074074464\n",
      "The 22047 th iteration gives loss of 0.1822839764071949\n",
      "The 22048 th iteration gives loss of 0.18228005248394055\n",
      "The 22049 th iteration gives loss of 0.1822761289709276\n",
      "The 22050 th iteration gives loss of 0.18227220586809298\n",
      "The 22051 th iteration gives loss of 0.18226828317537735\n",
      "The 22052 th iteration gives loss of 0.18226436089271622\n",
      "The 22053 th iteration gives loss of 0.1822604390200393\n",
      "The 22054 th iteration gives loss of 0.18225651755728248\n",
      "The 22055 th iteration gives loss of 0.18225259650439643\n",
      "The 22056 th iteration gives loss of 0.18224867586129836\n",
      "The 22057 th iteration gives loss of 0.1822447556279394\n",
      "The 22058 th iteration gives loss of 0.18224083580425351\n",
      "The 22059 th iteration gives loss of 0.18223691639017936\n",
      "The 22060 th iteration gives loss of 0.18223299738564896\n",
      "The 22061 th iteration gives loss of 0.18222907879060113\n",
      "The 22062 th iteration gives loss of 0.18222516060497596\n",
      "The 22063 th iteration gives loss of 0.18222124282870836\n",
      "The 22064 th iteration gives loss of 0.18221732546173308\n",
      "The 22065 th iteration gives loss of 0.18221340850398557\n",
      "The 22066 th iteration gives loss of 0.18220949195540195\n",
      "The 22067 th iteration gives loss of 0.1822055758159245\n",
      "The 22068 th iteration gives loss of 0.1822016600854926\n",
      "The 22069 th iteration gives loss of 0.18219774476403391\n",
      "The 22070 th iteration gives loss of 0.18219382985150082\n",
      "The 22071 th iteration gives loss of 0.18218991534781398\n",
      "The 22072 th iteration gives loss of 0.1821860012529121\n",
      "The 22073 th iteration gives loss of 0.18218208756674273\n",
      "The 22074 th iteration gives loss of 0.182178174289235\n",
      "The 22075 th iteration gives loss of 0.18217426142032764\n",
      "The 22076 th iteration gives loss of 0.18217034895995404\n",
      "The 22077 th iteration gives loss of 0.1821664369080698\n",
      "The 22078 th iteration gives loss of 0.182162525264582\n",
      "The 22079 th iteration gives loss of 0.18215861402944655\n",
      "The 22080 th iteration gives loss of 0.1821547032026009\n",
      "The 22081 th iteration gives loss of 0.18215079278397728\n",
      "The 22082 th iteration gives loss of 0.1821468827735163\n",
      "The 22083 th iteration gives loss of 0.1821429731711469\n",
      "The 22084 th iteration gives loss of 0.18213906397681012\n",
      "The 22085 th iteration gives loss of 0.18213515519045154\n",
      "The 22086 th iteration gives loss of 0.18213124681200216\n",
      "The 22087 th iteration gives loss of 0.18212733884139284\n",
      "The 22088 th iteration gives loss of 0.1821234312785722\n",
      "The 22089 th iteration gives loss of 0.18211952412346488\n",
      "The 22090 th iteration gives loss of 0.18211561737601728\n",
      "The 22091 th iteration gives loss of 0.18211171103616614\n",
      "The 22092 th iteration gives loss of 0.18210780510384716\n",
      "The 22093 th iteration gives loss of 0.182103899578998\n",
      "The 22094 th iteration gives loss of 0.18209999446155148\n",
      "The 22095 th iteration gives loss of 0.1820960897514499\n",
      "The 22096 th iteration gives loss of 0.18209218544862835\n",
      "The 22097 th iteration gives loss of 0.18208828155303353\n",
      "The 22098 th iteration gives loss of 0.1820843780645842\n",
      "The 22099 th iteration gives loss of 0.1820804749832355\n",
      "The 22100 th iteration gives loss of 0.18207657230890806\n",
      "The 22101 th iteration gives loss of 0.18207267004155148\n",
      "The 22102 th iteration gives loss of 0.18206876818109435\n",
      "The 22103 th iteration gives loss of 0.18206486672748634\n",
      "The 22104 th iteration gives loss of 0.18206096568065583\n",
      "The 22105 th iteration gives loss of 0.1820570650405435\n",
      "The 22106 th iteration gives loss of 0.18205316480708636\n",
      "The 22107 th iteration gives loss of 0.18204926498022134\n",
      "The 22108 th iteration gives loss of 0.18204536555987877\n",
      "The 22109 th iteration gives loss of 0.18204146654600042\n",
      "The 22110 th iteration gives loss of 0.18203756793852696\n",
      "The 22111 th iteration gives loss of 0.18203366973739304\n",
      "The 22112 th iteration gives loss of 0.18202977194254147\n",
      "The 22113 th iteration gives loss of 0.18202587455390842\n",
      "The 22114 th iteration gives loss of 0.18202197757142025\n",
      "The 22115 th iteration gives loss of 0.1820180809950225\n",
      "The 22116 th iteration gives loss of 0.18201418482466009\n",
      "The 22117 th iteration gives loss of 0.18201028906026098\n",
      "The 22118 th iteration gives loss of 0.18200639370176455\n",
      "The 22119 th iteration gives loss of 0.18200249874910565\n",
      "The 22120 th iteration gives loss of 0.18199860420222455\n",
      "The 22121 th iteration gives loss of 0.1819947100610565\n",
      "The 22122 th iteration gives loss of 0.1819908163255432\n",
      "The 22123 th iteration gives loss of 0.18198692299562705\n",
      "The 22124 th iteration gives loss of 0.18198303007123096\n",
      "The 22125 th iteration gives loss of 0.18197913755230344\n",
      "The 22126 th iteration gives loss of 0.18197524543877497\n",
      "The 22127 th iteration gives loss of 0.18197135373058507\n",
      "The 22128 th iteration gives loss of 0.18196746242768796\n",
      "The 22129 th iteration gives loss of 0.1819635715299931\n",
      "The 22130 th iteration gives loss of 0.18195968103745458\n",
      "The 22131 th iteration gives loss of 0.18195579095000977\n",
      "The 22132 th iteration gives loss of 0.18195190126758676\n",
      "The 22133 th iteration gives loss of 0.18194801199013017\n",
      "The 22134 th iteration gives loss of 0.1819441231175813\n",
      "The 22135 th iteration gives loss of 0.18194023464987766\n",
      "The 22136 th iteration gives loss of 0.18193634658694852\n",
      "The 22137 th iteration gives loss of 0.18193245892872323\n",
      "The 22138 th iteration gives loss of 0.18192857167516355\n",
      "The 22139 th iteration gives loss of 0.18192468482620436\n",
      "The 22140 th iteration gives loss of 0.1819207983817642\n",
      "The 22141 th iteration gives loss of 0.18191691234179447\n",
      "The 22142 th iteration gives loss of 0.18191302670623186\n",
      "The 22143 th iteration gives loss of 0.1819091414750041\n",
      "The 22144 th iteration gives loss of 0.18190525664806986\n",
      "The 22145 th iteration gives loss of 0.18190137222534086\n",
      "The 22146 th iteration gives loss of 0.181897488206771\n",
      "The 22147 th iteration gives loss of 0.18189360459229256\n",
      "The 22148 th iteration gives loss of 0.18188972138184545\n",
      "The 22149 th iteration gives loss of 0.18188583857537613\n",
      "The 22150 th iteration gives loss of 0.18188195617280933\n",
      "The 22151 th iteration gives loss of 0.18187807417408328\n",
      "The 22152 th iteration gives loss of 0.18187419257914125\n",
      "The 22153 th iteration gives loss of 0.18187031138792772\n",
      "The 22154 th iteration gives loss of 0.1818664306003624\n",
      "The 22155 th iteration gives loss of 0.18186255021639455\n",
      "The 22156 th iteration gives loss of 0.18185867023596355\n",
      "The 22157 th iteration gives loss of 0.18185479065899735\n",
      "The 22158 th iteration gives loss of 0.18185091148544616\n",
      "The 22159 th iteration gives loss of 0.181847032715239\n",
      "The 22160 th iteration gives loss of 0.18184315434831969\n",
      "The 22161 th iteration gives loss of 0.18183927638461725\n",
      "The 22162 th iteration gives loss of 0.18183539882409167\n",
      "The 22163 th iteration gives loss of 0.1818315216666479\n",
      "The 22164 th iteration gives loss of 0.1818276449122491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 22165 th iteration gives loss of 0.18182376856081844\n",
      "The 22166 th iteration gives loss of 0.181819892612312\n",
      "The 22167 th iteration gives loss of 0.18181601706665107\n",
      "The 22168 th iteration gives loss of 0.18181214192377104\n",
      "The 22169 th iteration gives loss of 0.1818082671836314\n",
      "The 22170 th iteration gives loss of 0.18180439284614877\n",
      "The 22171 th iteration gives loss of 0.18180051891127091\n",
      "The 22172 th iteration gives loss of 0.18179664537892812\n",
      "The 22173 th iteration gives loss of 0.18179277224906584\n",
      "The 22174 th iteration gives loss of 0.18178889952161853\n",
      "The 22175 th iteration gives loss of 0.1817850271965254\n",
      "The 22176 th iteration gives loss of 0.18178115527373273\n",
      "The 22177 th iteration gives loss of 0.18177728375316576\n",
      "The 22178 th iteration gives loss of 0.18177341263476596\n",
      "The 22179 th iteration gives loss of 0.1817695419184733\n",
      "The 22180 th iteration gives loss of 0.1817656716042262\n",
      "The 22181 th iteration gives loss of 0.181761801691965\n",
      "The 22182 th iteration gives loss of 0.18175793218161462\n",
      "The 22183 th iteration gives loss of 0.18175406307313302\n",
      "The 22184 th iteration gives loss of 0.18175019436644607\n",
      "The 22185 th iteration gives loss of 0.1817463260614874\n",
      "The 22186 th iteration gives loss of 0.18174245815821302\n",
      "The 22187 th iteration gives loss of 0.1817385906565463\n",
      "The 22188 th iteration gives loss of 0.18173472355642536\n",
      "The 22189 th iteration gives loss of 0.18173085685780227\n",
      "The 22190 th iteration gives loss of 0.18172699056059693\n",
      "The 22191 th iteration gives loss of 0.1817231246647541\n",
      "The 22192 th iteration gives loss of 0.181719259170223\n",
      "The 22193 th iteration gives loss of 0.18171539407692108\n",
      "The 22194 th iteration gives loss of 0.18171152938480606\n",
      "The 22195 th iteration gives loss of 0.1817076650938029\n",
      "The 22196 th iteration gives loss of 0.18170380120385618\n",
      "The 22197 th iteration gives loss of 0.1816999377148976\n",
      "The 22198 th iteration gives loss of 0.1816960746268829\n",
      "The 22199 th iteration gives loss of 0.18169221193972818\n",
      "The 22200 th iteration gives loss of 0.18168834965339328\n",
      "The 22201 th iteration gives loss of 0.1816844877677883\n",
      "The 22202 th iteration gives loss of 0.1816806262828754\n",
      "The 22203 th iteration gives loss of 0.18167676519858922\n",
      "The 22204 th iteration gives loss of 0.1816729045148566\n",
      "The 22205 th iteration gives loss of 0.18166904423162933\n",
      "The 22206 th iteration gives loss of 0.18166518434884107\n",
      "The 22207 th iteration gives loss of 0.18166132486642542\n",
      "The 22208 th iteration gives loss of 0.18165746578432695\n",
      "The 22209 th iteration gives loss of 0.18165360710247958\n",
      "The 22210 th iteration gives loss of 0.18164974882082402\n",
      "The 22211 th iteration gives loss of 0.18164589093930278\n",
      "The 22212 th iteration gives loss of 0.18164203345784188\n",
      "The 22213 th iteration gives loss of 0.18163817637639273\n",
      "The 22214 th iteration gives loss of 0.1816343196948877\n",
      "The 22215 th iteration gives loss of 0.18163046341326705\n",
      "The 22216 th iteration gives loss of 0.18162660753146626\n",
      "The 22217 th iteration gives loss of 0.18162275204942727\n",
      "The 22218 th iteration gives loss of 0.18161889696707875\n",
      "The 22219 th iteration gives loss of 0.1816150422843711\n",
      "The 22220 th iteration gives loss of 0.18161118800124035\n",
      "The 22221 th iteration gives loss of 0.18160733411762184\n",
      "The 22222 th iteration gives loss of 0.18160348063346257\n",
      "The 22223 th iteration gives loss of 0.1815996275486897\n",
      "The 22224 th iteration gives loss of 0.1815957748632481\n",
      "The 22225 th iteration gives loss of 0.18159192257707332\n",
      "The 22226 th iteration gives loss of 0.1815880706901067\n",
      "The 22227 th iteration gives loss of 0.18158421920227924\n",
      "The 22228 th iteration gives loss of 0.18158036811353853\n",
      "The 22229 th iteration gives loss of 0.18157651742381903\n",
      "The 22230 th iteration gives loss of 0.1815726671330577\n",
      "The 22231 th iteration gives loss of 0.18156881724120202\n",
      "The 22232 th iteration gives loss of 0.18156496774817363\n",
      "The 22233 th iteration gives loss of 0.18156111865393393\n",
      "The 22234 th iteration gives loss of 0.18155726995840463\n",
      "The 22235 th iteration gives loss of 0.181553421661536\n",
      "The 22236 th iteration gives loss of 0.18154957376324135\n",
      "The 22237 th iteration gives loss of 0.18154572626348964\n",
      "The 22238 th iteration gives loss of 0.18154187916220954\n",
      "The 22239 th iteration gives loss of 0.18153803245932523\n",
      "The 22240 th iteration gives loss of 0.18153418615478678\n",
      "The 22241 th iteration gives loss of 0.18153034024855128\n",
      "The 22242 th iteration gives loss of 0.1815264947405229\n",
      "The 22243 th iteration gives loss of 0.18152264963067\n",
      "The 22244 th iteration gives loss of 0.18151880491891006\n",
      "The 22245 th iteration gives loss of 0.1815149606051925\n",
      "The 22246 th iteration gives loss of 0.18151111668944953\n",
      "The 22247 th iteration gives loss of 0.1815072731716309\n",
      "The 22248 th iteration gives loss of 0.18150343005167005\n",
      "The 22249 th iteration gives loss of 0.18149958732950025\n",
      "The 22250 th iteration gives loss of 0.1814957450050613\n",
      "The 22251 th iteration gives loss of 0.1814919030782942\n",
      "The 22252 th iteration gives loss of 0.18148806154914207\n",
      "The 22253 th iteration gives loss of 0.18148422041754328\n",
      "The 22254 th iteration gives loss of 0.18148037968342548\n",
      "The 22255 th iteration gives loss of 0.18147653934673444\n",
      "The 22256 th iteration gives loss of 0.18147269940741476\n",
      "The 22257 th iteration gives loss of 0.18146885986539887\n",
      "The 22258 th iteration gives loss of 0.18146502072062498\n",
      "The 22259 th iteration gives loss of 0.1814611819730418\n",
      "The 22260 th iteration gives loss of 0.1814573436225754\n",
      "The 22261 th iteration gives loss of 0.18145350566916352\n",
      "The 22262 th iteration gives loss of 0.18144966811276067\n",
      "The 22263 th iteration gives loss of 0.18144583095328623\n",
      "The 22264 th iteration gives loss of 0.18144199419069743\n",
      "The 22265 th iteration gives loss of 0.1814381578249211\n",
      "The 22266 th iteration gives loss of 0.18143432185589475\n",
      "The 22267 th iteration gives loss of 0.1814304862835636\n",
      "The 22268 th iteration gives loss of 0.18142665110786443\n",
      "The 22269 th iteration gives loss of 0.18142281632874652\n",
      "The 22270 th iteration gives loss of 0.18141898194612888\n",
      "The 22271 th iteration gives loss of 0.18141514795996408\n",
      "The 22272 th iteration gives loss of 0.18141131437018523\n",
      "The 22273 th iteration gives loss of 0.18140748117673786\n",
      "The 22274 th iteration gives loss of 0.18140364837955647\n",
      "The 22275 th iteration gives loss of 0.18139981597858443\n",
      "The 22276 th iteration gives loss of 0.18139598397375406\n",
      "The 22277 th iteration gives loss of 0.18139215236499973\n",
      "The 22278 th iteration gives loss of 0.18138832115227646\n",
      "The 22279 th iteration gives loss of 0.18138449033551104\n",
      "The 22280 th iteration gives loss of 0.18138065991464142\n",
      "The 22281 th iteration gives loss of 0.18137682988961285\n",
      "The 22282 th iteration gives loss of 0.18137300026036832\n",
      "The 22283 th iteration gives loss of 0.18136917102684036\n",
      "The 22284 th iteration gives loss of 0.1813653421889692\n",
      "The 22285 th iteration gives loss of 0.1813615137466871\n",
      "The 22286 th iteration gives loss of 0.18135768569993896\n",
      "The 22287 th iteration gives loss of 0.18135385804867885\n",
      "The 22288 th iteration gives loss of 0.18135003079281614\n",
      "The 22289 th iteration gives loss of 0.18134620393231174\n",
      "The 22290 th iteration gives loss of 0.18134237746709844\n",
      "The 22291 th iteration gives loss of 0.18133855139712218\n",
      "The 22292 th iteration gives loss of 0.18133472572230186\n",
      "The 22293 th iteration gives loss of 0.1813309004425961\n",
      "The 22294 th iteration gives loss of 0.18132707555794472\n",
      "The 22295 th iteration gives loss of 0.18132325106826924\n",
      "The 22296 th iteration gives loss of 0.18131942697352388\n",
      "The 22297 th iteration gives loss of 0.18131560327364973\n",
      "The 22298 th iteration gives loss of 0.1813117799685771\n",
      "The 22299 th iteration gives loss of 0.18130795705824207\n",
      "The 22300 th iteration gives loss of 0.18130413454259794\n",
      "The 22301 th iteration gives loss of 0.18130031242157463\n",
      "The 22302 th iteration gives loss of 0.18129649069511042\n",
      "The 22303 th iteration gives loss of 0.1812926693631511\n",
      "The 22304 th iteration gives loss of 0.18128884842562623\n",
      "The 22305 th iteration gives loss of 0.1812850278824888\n",
      "The 22306 th iteration gives loss of 0.18128120773365952\n",
      "The 22307 th iteration gives loss of 0.18127738797909532\n",
      "The 22308 th iteration gives loss of 0.18127356861872038\n",
      "The 22309 th iteration gives loss of 0.18126974965248874\n",
      "The 22310 th iteration gives loss of 0.18126593108032932\n",
      "The 22311 th iteration gives loss of 0.18126211290218366\n",
      "The 22312 th iteration gives loss of 0.1812582951180004\n",
      "The 22313 th iteration gives loss of 0.18125447772770695\n",
      "The 22314 th iteration gives loss of 0.18125066073124896\n",
      "The 22315 th iteration gives loss of 0.18124684412856046\n",
      "The 22316 th iteration gives loss of 0.1812430279195834\n",
      "The 22317 th iteration gives loss of 0.18123921210425445\n",
      "The 22318 th iteration gives loss of 0.18123539668251884\n",
      "The 22319 th iteration gives loss of 0.18123158165431413\n",
      "The 22320 th iteration gives loss of 0.1812277670195792\n",
      "The 22321 th iteration gives loss of 0.18122395277825698\n",
      "The 22322 th iteration gives loss of 0.18122013893027614\n",
      "The 22323 th iteration gives loss of 0.18121632547558175\n",
      "The 22324 th iteration gives loss of 0.18121251241411768\n",
      "The 22325 th iteration gives loss of 0.18120869974582657\n",
      "The 22326 th iteration gives loss of 0.18120488747062843\n",
      "The 22327 th iteration gives loss of 0.18120107558848608\n",
      "The 22328 th iteration gives loss of 0.1811972640993236\n",
      "The 22329 th iteration gives loss of 0.18119345300308862\n",
      "The 22330 th iteration gives loss of 0.18118964229971649\n",
      "The 22331 th iteration gives loss of 0.18118583198915167\n",
      "The 22332 th iteration gives loss of 0.18118202207133302\n",
      "The 22333 th iteration gives loss of 0.18117821254619226\n",
      "The 22334 th iteration gives loss of 0.1811744034136646\n",
      "The 22335 th iteration gives loss of 0.18117059467370503\n",
      "The 22336 th iteration gives loss of 0.18116678632625016\n",
      "The 22337 th iteration gives loss of 0.18116297837123876\n",
      "The 22338 th iteration gives loss of 0.18115917080860516\n",
      "The 22339 th iteration gives loss of 0.18115536363828239\n",
      "The 22340 th iteration gives loss of 0.18115155686022874\n",
      "The 22341 th iteration gives loss of 0.18114775047437728\n",
      "The 22342 th iteration gives loss of 0.18114394448065863\n",
      "The 22343 th iteration gives loss of 0.18114013887901972\n",
      "The 22344 th iteration gives loss of 0.1811363336693971\n",
      "The 22345 th iteration gives loss of 0.18113252885173392\n",
      "The 22346 th iteration gives loss of 0.18112872442596498\n",
      "The 22347 th iteration gives loss of 0.18112492039203235\n",
      "The 22348 th iteration gives loss of 0.1811211167498848\n",
      "The 22349 th iteration gives loss of 0.1811173134994525\n",
      "The 22350 th iteration gives loss of 0.18111351064067427\n",
      "The 22351 th iteration gives loss of 0.1811097081734897\n",
      "The 22352 th iteration gives loss of 0.18110590609783672\n",
      "The 22353 th iteration gives loss of 0.18110210441366711\n",
      "The 22354 th iteration gives loss of 0.18109830312090602\n",
      "The 22355 th iteration gives loss of 0.18109450221950965\n",
      "The 22356 th iteration gives loss of 0.1810907017094029\n",
      "The 22357 th iteration gives loss of 0.18108690159052665\n",
      "The 22358 th iteration gives loss of 0.18108310186282683\n",
      "The 22359 th iteration gives loss of 0.1810793025262406\n",
      "The 22360 th iteration gives loss of 0.18107550358071098\n",
      "The 22361 th iteration gives loss of 0.18107170502616504\n",
      "The 22362 th iteration gives loss of 0.18106790686256002\n",
      "The 22363 th iteration gives loss of 0.18106410908982154\n",
      "The 22364 th iteration gives loss of 0.1810603117079075\n",
      "The 22365 th iteration gives loss of 0.18105651471674708\n",
      "The 22366 th iteration gives loss of 0.181052718116273\n",
      "The 22367 th iteration gives loss of 0.18104892190643368\n",
      "The 22368 th iteration gives loss of 0.18104512608716744\n",
      "The 22369 th iteration gives loss of 0.18104133065840178\n",
      "The 22370 th iteration gives loss of 0.18103753562009697\n",
      "The 22371 th iteration gives loss of 0.18103374097218772\n",
      "The 22372 th iteration gives loss of 0.18102994671460906\n",
      "The 22373 th iteration gives loss of 0.18102615284729745\n",
      "The 22374 th iteration gives loss of 0.18102235937019925\n",
      "The 22375 th iteration gives loss of 0.18101856628325638\n",
      "The 22376 th iteration gives loss of 0.18101477358640444\n",
      "The 22377 th iteration gives loss of 0.1810109812795828\n",
      "The 22378 th iteration gives loss of 0.18100718936272978\n",
      "The 22379 th iteration gives loss of 0.1810033978357891\n",
      "The 22380 th iteration gives loss of 0.1809996066987027\n",
      "The 22381 th iteration gives loss of 0.18099581595140035\n",
      "The 22382 th iteration gives loss of 0.1809920255938324\n",
      "The 22383 th iteration gives loss of 0.18098823562594515\n",
      "The 22384 th iteration gives loss of 0.18098444604766567\n",
      "The 22385 th iteration gives loss of 0.1809806568589388\n",
      "The 22386 th iteration gives loss of 0.1809768680596967\n",
      "The 22387 th iteration gives loss of 0.1809730796498915\n",
      "The 22388 th iteration gives loss of 0.18096929162944714\n",
      "The 22389 th iteration gives loss of 0.18096550399832056\n",
      "The 22390 th iteration gives loss of 0.18096171675644712\n",
      "The 22391 th iteration gives loss of 0.1809579299037673\n",
      "The 22392 th iteration gives loss of 0.18095414344021926\n",
      "The 22393 th iteration gives loss of 0.1809503573657451\n",
      "The 22394 th iteration gives loss of 0.1809465716802784\n",
      "The 22395 th iteration gives loss of 0.18094278638376254\n",
      "The 22396 th iteration gives loss of 0.18093900147614164\n",
      "The 22397 th iteration gives loss of 0.18093521695735526\n",
      "The 22398 th iteration gives loss of 0.18093143282734056\n",
      "The 22399 th iteration gives loss of 0.18092764908604245\n",
      "The 22400 th iteration gives loss of 0.18092386573339317\n",
      "The 22401 th iteration gives loss of 0.18092008276933505\n",
      "The 22402 th iteration gives loss of 0.18091630019381466\n",
      "The 22403 th iteration gives loss of 0.1809125180067601\n",
      "The 22404 th iteration gives loss of 0.1809087362081254\n",
      "The 22405 th iteration gives loss of 0.18090495479784474\n",
      "The 22406 th iteration gives loss of 0.18090117377585663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 22407 th iteration gives loss of 0.18089739314210035\n",
      "The 22408 th iteration gives loss of 0.18089361289652794\n",
      "The 22409 th iteration gives loss of 0.180889833039064\n",
      "The 22410 th iteration gives loss of 0.180886053569656\n",
      "The 22411 th iteration gives loss of 0.180882274488237\n",
      "The 22412 th iteration gives loss of 0.18087849579476054\n",
      "The 22413 th iteration gives loss of 0.18087471748915981\n",
      "The 22414 th iteration gives loss of 0.18087093957137104\n",
      "The 22415 th iteration gives loss of 0.1808671620413455\n",
      "The 22416 th iteration gives loss of 0.18086338489900963\n",
      "The 22417 th iteration gives loss of 0.18085960814432245\n",
      "The 22418 th iteration gives loss of 0.18085583177721107\n",
      "The 22419 th iteration gives loss of 0.18085205579760608\n",
      "The 22420 th iteration gives loss of 0.180848280205466\n",
      "The 22421 th iteration gives loss of 0.18084450500072297\n",
      "The 22422 th iteration gives loss of 0.1808407301833197\n",
      "The 22423 th iteration gives loss of 0.18083695575319858\n",
      "The 22424 th iteration gives loss of 0.18083318171029011\n",
      "The 22425 th iteration gives loss of 0.18082940805454703\n",
      "The 22426 th iteration gives loss of 0.18082563478590064\n",
      "The 22427 th iteration gives loss of 0.1808218619042975\n",
      "The 22428 th iteration gives loss of 0.18081808940968103\n",
      "The 22429 th iteration gives loss of 0.18081431730198494\n",
      "The 22430 th iteration gives loss of 0.18081054558114912\n",
      "The 22431 th iteration gives loss of 0.1808067742471112\n",
      "The 22432 th iteration gives loss of 0.18080300329982343\n",
      "The 22433 th iteration gives loss of 0.1807992327392184\n",
      "The 22434 th iteration gives loss of 0.1807954625652277\n",
      "The 22435 th iteration gives loss of 0.1807916927778143\n",
      "The 22436 th iteration gives loss of 0.1807879233769002\n",
      "The 22437 th iteration gives loss of 0.1807841543624399\n",
      "The 22438 th iteration gives loss of 0.18078038573435565\n",
      "The 22439 th iteration gives loss of 0.18077661749260157\n",
      "The 22440 th iteration gives loss of 0.18077284963711124\n",
      "The 22441 th iteration gives loss of 0.18076908216783827\n",
      "The 22442 th iteration gives loss of 0.1807653150847025\n",
      "The 22443 th iteration gives loss of 0.18076154838766048\n",
      "The 22444 th iteration gives loss of 0.18075778207665263\n",
      "The 22445 th iteration gives loss of 0.18075401615161424\n",
      "The 22446 th iteration gives loss of 0.1807502506124797\n",
      "The 22447 th iteration gives loss of 0.1807464854592007\n",
      "The 22448 th iteration gives loss of 0.1807427206917088\n",
      "The 22449 th iteration gives loss of 0.1807389563099511\n",
      "The 22450 th iteration gives loss of 0.1807351923138749\n",
      "The 22451 th iteration gives loss of 0.18073142870341263\n",
      "The 22452 th iteration gives loss of 0.18072766547849156\n",
      "The 22453 th iteration gives loss of 0.18072390263907345\n",
      "The 22454 th iteration gives loss of 0.18072014018509436\n",
      "The 22455 th iteration gives loss of 0.1807163781164865\n",
      "The 22456 th iteration gives loss of 0.18071261643319822\n",
      "The 22457 th iteration gives loss of 0.18070885513515814\n",
      "The 22458 th iteration gives loss of 0.18070509422232875\n",
      "The 22459 th iteration gives loss of 0.18070133369463545\n",
      "The 22460 th iteration gives loss of 0.18069757355201843\n",
      "The 22461 th iteration gives loss of 0.18069381379443555\n",
      "The 22462 th iteration gives loss of 0.1806900544218002\n",
      "The 22463 th iteration gives loss of 0.18068629543407244\n",
      "The 22464 th iteration gives loss of 0.1806825368311841\n",
      "The 22465 th iteration gives loss of 0.1806787786130752\n",
      "The 22466 th iteration gives loss of 0.18067502077969935\n",
      "The 22467 th iteration gives loss of 0.18067126333099323\n",
      "The 22468 th iteration gives loss of 0.1806675062668852\n",
      "The 22469 th iteration gives loss of 0.18066374958732695\n",
      "The 22470 th iteration gives loss of 0.1806599932922592\n",
      "The 22471 th iteration gives loss of 0.1806562373816135\n",
      "The 22472 th iteration gives loss of 0.1806524818553441\n",
      "The 22473 th iteration gives loss of 0.18064872671338286\n",
      "The 22474 th iteration gives loss of 0.1806449719556675\n",
      "The 22475 th iteration gives loss of 0.18064121758214519\n",
      "The 22476 th iteration gives loss of 0.1806374635927569\n",
      "The 22477 th iteration gives loss of 0.1806337099874494\n",
      "The 22478 th iteration gives loss of 0.18062995676615787\n",
      "The 22479 th iteration gives loss of 0.18062620392881107\n",
      "The 22480 th iteration gives loss of 0.18062245147536501\n",
      "The 22481 th iteration gives loss of 0.1806186994057551\n",
      "The 22482 th iteration gives loss of 0.18061494771991943\n",
      "The 22483 th iteration gives loss of 0.18061119641780954\n",
      "The 22484 th iteration gives loss of 0.18060744549935517\n",
      "The 22485 th iteration gives loss of 0.18060369496451043\n",
      "The 22486 th iteration gives loss of 0.18059994481320052\n",
      "The 22487 th iteration gives loss of 0.18059619504537808\n",
      "The 22488 th iteration gives loss of 0.18059244566097893\n",
      "The 22489 th iteration gives loss of 0.18058869665993904\n",
      "The 22490 th iteration gives loss of 0.18058494804220993\n",
      "The 22491 th iteration gives loss of 0.18058119980773518\n",
      "The 22492 th iteration gives loss of 0.1805774519564292\n",
      "The 22493 th iteration gives loss of 0.18057370448827048\n",
      "The 22494 th iteration gives loss of 0.180569957403177\n",
      "The 22495 th iteration gives loss of 0.1805662107010987\n",
      "The 22496 th iteration gives loss of 0.18056246438196144\n",
      "The 22497 th iteration gives loss of 0.18055871844572788\n",
      "The 22498 th iteration gives loss of 0.1805549728923288\n",
      "The 22499 th iteration gives loss of 0.18055122772169752\n",
      "The 22500 th iteration gives loss of 0.18054748293378506\n",
      "The 22501 th iteration gives loss of 0.18054373852853453\n",
      "The 22502 th iteration gives loss of 0.1805399945058745\n",
      "The 22503 th iteration gives loss of 0.1805362508657627\n",
      "The 22504 th iteration gives loss of 0.1805325076081324\n",
      "The 22505 th iteration gives loss of 0.18052876473292565\n",
      "The 22506 th iteration gives loss of 0.18052502224006783\n",
      "The 22507 th iteration gives loss of 0.18052128012952728\n",
      "The 22508 th iteration gives loss of 0.18051753840123339\n",
      "The 22509 th iteration gives loss of 0.18051379705512033\n",
      "The 22510 th iteration gives loss of 0.18051005609113666\n",
      "The 22511 th iteration gives loss of 0.1805063155092255\n",
      "The 22512 th iteration gives loss of 0.1805025753093271\n",
      "The 22513 th iteration gives loss of 0.18049883549137535\n",
      "The 22514 th iteration gives loss of 0.18049509605531577\n",
      "The 22515 th iteration gives loss of 0.1804913570010933\n",
      "The 22516 th iteration gives loss of 0.18048761832864352\n",
      "The 22517 th iteration gives loss of 0.1804838800379094\n",
      "The 22518 th iteration gives loss of 0.18048014212883587\n",
      "The 22519 th iteration gives loss of 0.1804764046013633\n",
      "The 22520 th iteration gives loss of 0.1804726674554354\n",
      "The 22521 th iteration gives loss of 0.18046893069098646\n",
      "The 22522 th iteration gives loss of 0.1804651943079569\n",
      "The 22523 th iteration gives loss of 0.1804614583062934\n",
      "The 22524 th iteration gives loss of 0.18045772268593424\n",
      "The 22525 th iteration gives loss of 0.1804539874468186\n",
      "The 22526 th iteration gives loss of 0.18045025258889755\n",
      "The 22527 th iteration gives loss of 0.18044651811210266\n",
      "The 22528 th iteration gives loss of 0.18044278401637276\n",
      "The 22529 th iteration gives loss of 0.18043905030167615\n",
      "The 22530 th iteration gives loss of 0.1804353169679179\n",
      "The 22531 th iteration gives loss of 0.1804315840150511\n",
      "The 22532 th iteration gives loss of 0.18042785144302748\n",
      "The 22533 th iteration gives loss of 0.18042411925178392\n",
      "The 22534 th iteration gives loss of 0.1804203874412611\n",
      "The 22535 th iteration gives loss of 0.18041665601139745\n",
      "The 22536 th iteration gives loss of 0.18041292496213024\n",
      "The 22537 th iteration gives loss of 0.18040919429340993\n",
      "The 22538 th iteration gives loss of 0.18040546400517035\n",
      "The 22539 th iteration gives loss of 0.18040173409736185\n",
      "The 22540 th iteration gives loss of 0.18039800456992144\n",
      "The 22541 th iteration gives loss of 0.18039427542279124\n",
      "The 22542 th iteration gives loss of 0.18039054665590218\n",
      "The 22543 th iteration gives loss of 0.18038681826922084\n",
      "The 22544 th iteration gives loss of 0.18038309026266322\n",
      "The 22545 th iteration gives loss of 0.18037936263618812\n",
      "The 22546 th iteration gives loss of 0.18037563538973236\n",
      "The 22547 th iteration gives loss of 0.18037190852323085\n",
      "The 22548 th iteration gives loss of 0.18036818203662983\n",
      "The 22549 th iteration gives loss of 0.1803644559298686\n",
      "The 22550 th iteration gives loss of 0.18036073020288673\n",
      "The 22551 th iteration gives loss of 0.1803570048556404\n",
      "The 22552 th iteration gives loss of 0.18035327988805117\n",
      "The 22553 th iteration gives loss of 0.18034955530007116\n",
      "The 22554 th iteration gives loss of 0.1803458310916382\n",
      "The 22555 th iteration gives loss of 0.18034210726269562\n",
      "The 22556 th iteration gives loss of 0.18033838381319522\n",
      "The 22557 th iteration gives loss of 0.18033466074306725\n",
      "The 22558 th iteration gives loss of 0.18033093805225067\n",
      "The 22559 th iteration gives loss of 0.18032721574069577\n",
      "The 22560 th iteration gives loss of 0.1803234938083361\n",
      "The 22561 th iteration gives loss of 0.18031977225511867\n",
      "The 22562 th iteration gives loss of 0.18031605108098434\n",
      "The 22563 th iteration gives loss of 0.1803123302858712\n",
      "The 22564 th iteration gives loss of 0.1803086098697317\n",
      "The 22565 th iteration gives loss of 0.1803048898324953\n",
      "The 22566 th iteration gives loss of 0.18030117017410743\n",
      "The 22567 th iteration gives loss of 0.18029745089451304\n",
      "The 22568 th iteration gives loss of 0.18029373199364748\n",
      "The 22569 th iteration gives loss of 0.18029001347145848\n",
      "The 22570 th iteration gives loss of 0.18028629532788298\n",
      "The 22571 th iteration gives loss of 0.18028257756287203\n",
      "The 22572 th iteration gives loss of 0.18027886017635245\n",
      "The 22573 th iteration gives loss of 0.18027514316828164\n",
      "The 22574 th iteration gives loss of 0.18027142653859055\n",
      "The 22575 th iteration gives loss of 0.18026771028722585\n",
      "The 22576 th iteration gives loss of 0.18026399441412744\n",
      "The 22577 th iteration gives loss of 0.1802602789192338\n",
      "The 22578 th iteration gives loss of 0.18025656380249683\n",
      "The 22579 th iteration gives loss of 0.1802528490638522\n",
      "The 22580 th iteration gives loss of 0.18024913470323706\n",
      "The 22581 th iteration gives loss of 0.18024542072060087\n",
      "The 22582 th iteration gives loss of 0.18024170711587684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 22583 th iteration gives loss of 0.18023799388901585\n",
      "The 22584 th iteration gives loss of 0.18023428103995096\n",
      "The 22585 th iteration gives loss of 0.180230568568638\n",
      "The 22586 th iteration gives loss of 0.18022685647500844\n",
      "The 22587 th iteration gives loss of 0.18022314475900858\n",
      "The 22588 th iteration gives loss of 0.1802194334205732\n",
      "The 22589 th iteration gives loss of 0.18021572245965534\n",
      "The 22590 th iteration gives loss of 0.18021201187618702\n",
      "The 22591 th iteration gives loss of 0.18020830167011193\n",
      "The 22592 th iteration gives loss of 0.18020459184137622\n",
      "The 22593 th iteration gives loss of 0.18020088238991283\n",
      "The 22594 th iteration gives loss of 0.18019717331567367\n",
      "The 22595 th iteration gives loss of 0.18019346461860014\n",
      "The 22596 th iteration gives loss of 0.18018975629862358\n",
      "The 22597 th iteration gives loss of 0.18018604835569532\n",
      "The 22598 th iteration gives loss of 0.18018234078975748\n",
      "The 22599 th iteration gives loss of 0.1801786336007551\n",
      "The 22600 th iteration gives loss of 0.1801749267886259\n",
      "The 22601 th iteration gives loss of 0.1801712203533046\n",
      "The 22602 th iteration gives loss of 0.18016751429474048\n",
      "The 22603 th iteration gives loss of 0.18016380861288073\n",
      "The 22604 th iteration gives loss of 0.18016010330765606\n",
      "The 22605 th iteration gives loss of 0.18015639837902098\n",
      "The 22606 th iteration gives loss of 0.18015269382690458\n",
      "The 22607 th iteration gives loss of 0.18014898965125467\n",
      "The 22608 th iteration gives loss of 0.18014528585201903\n",
      "The 22609 th iteration gives loss of 0.18014158242913214\n",
      "The 22610 th iteration gives loss of 0.18013787938253006\n",
      "The 22611 th iteration gives loss of 0.18013417671217513\n",
      "The 22612 th iteration gives loss of 0.1801304744179959\n",
      "The 22613 th iteration gives loss of 0.18012677249993653\n",
      "The 22614 th iteration gives loss of 0.18012307095793229\n",
      "The 22615 th iteration gives loss of 0.18011936979193685\n",
      "The 22616 th iteration gives loss of 0.18011566900188788\n",
      "The 22617 th iteration gives loss of 0.18011196858772088\n",
      "The 22618 th iteration gives loss of 0.180108268549387\n",
      "The 22619 th iteration gives loss of 0.18010456888682855\n",
      "The 22620 th iteration gives loss of 0.18010086959999014\n",
      "The 22621 th iteration gives loss of 0.18009717068880055\n",
      "The 22622 th iteration gives loss of 0.18009347215320792\n",
      "The 22623 th iteration gives loss of 0.18008977399316095\n",
      "The 22624 th iteration gives loss of 0.18008607620859865\n",
      "The 22625 th iteration gives loss of 0.1800823787994576\n",
      "The 22626 th iteration gives loss of 0.1800786817656851\n",
      "The 22627 th iteration gives loss of 0.18007498510723324\n",
      "The 22628 th iteration gives loss of 0.18007128882402876\n",
      "The 22629 th iteration gives loss of 0.1800675929160135\n",
      "The 22630 th iteration gives loss of 0.18006389738313594\n",
      "The 22631 th iteration gives loss of 0.1800602022253445\n",
      "The 22632 th iteration gives loss of 0.18005650744256788\n",
      "The 22633 th iteration gives loss of 0.18005281303475573\n",
      "The 22634 th iteration gives loss of 0.18004911900185705\n",
      "The 22635 th iteration gives loss of 0.1800454253438012\n",
      "The 22636 th iteration gives loss of 0.18004173206053478\n",
      "The 22637 th iteration gives loss of 0.18003803915201136\n",
      "The 22638 th iteration gives loss of 0.1800343466181543\n",
      "The 22639 th iteration gives loss of 0.180030654458915\n",
      "The 22640 th iteration gives loss of 0.18002696267423782\n",
      "The 22641 th iteration gives loss of 0.18002327126406747\n",
      "The 22642 th iteration gives loss of 0.180019580228345\n",
      "The 22643 th iteration gives loss of 0.18001588956700612\n",
      "The 22644 th iteration gives loss of 0.1800121992800004\n",
      "The 22645 th iteration gives loss of 0.18000850936726714\n",
      "The 22646 th iteration gives loss of 0.180004819828739\n",
      "The 22647 th iteration gives loss of 0.18000113066437046\n",
      "The 22648 th iteration gives loss of 0.17999744187410582\n",
      "The 22649 th iteration gives loss of 0.17999375345788035\n",
      "The 22650 th iteration gives loss of 0.1799900654156406\n",
      "The 22651 th iteration gives loss of 0.17998637774733095\n",
      "The 22652 th iteration gives loss of 0.17998269045289553\n",
      "The 22653 th iteration gives loss of 0.17997900353226395\n",
      "The 22654 th iteration gives loss of 0.17997531698538802\n",
      "The 22655 th iteration gives loss of 0.17997163081220885\n",
      "The 22656 th iteration gives loss of 0.17996794501267632\n",
      "The 22657 th iteration gives loss of 0.1799642595867254\n",
      "The 22658 th iteration gives loss of 0.17996057453428926\n",
      "The 22659 th iteration gives loss of 0.17995688985532554\n",
      "The 22660 th iteration gives loss of 0.17995320554977418\n",
      "The 22661 th iteration gives loss of 0.17994952161757258\n",
      "The 22662 th iteration gives loss of 0.17994583805866854\n",
      "The 22663 th iteration gives loss of 0.17994215487300086\n",
      "The 22664 th iteration gives loss of 0.179938472060512\n",
      "The 22665 th iteration gives loss of 0.179934789621148\n",
      "The 22666 th iteration gives loss of 0.17993110755485417\n",
      "The 22667 th iteration gives loss of 0.1799274258615573\n",
      "The 22668 th iteration gives loss of 0.1799237445412192\n",
      "The 22669 th iteration gives loss of 0.17992006359377077\n",
      "The 22670 th iteration gives loss of 0.17991638301915983\n",
      "The 22671 th iteration gives loss of 0.17991270281732338\n",
      "The 22672 th iteration gives loss of 0.17990902298821645\n",
      "The 22673 th iteration gives loss of 0.17990534353176127\n",
      "The 22674 th iteration gives loss of 0.17990166444792677\n",
      "The 22675 th iteration gives loss of 0.179897985736639\n",
      "The 22676 th iteration gives loss of 0.1798943073978306\n",
      "The 22677 th iteration gives loss of 0.17989062943146755\n",
      "The 22678 th iteration gives loss of 0.17988695183747905\n",
      "The 22679 th iteration gives loss of 0.17988327461581322\n",
      "The 22680 th iteration gives loss of 0.17987959776640733\n",
      "The 22681 th iteration gives loss of 0.17987592128920088\n",
      "The 22682 th iteration gives loss of 0.17987224518415168\n",
      "The 22683 th iteration gives loss of 0.1798685694511958\n",
      "The 22684 th iteration gives loss of 0.17986489409027182\n",
      "The 22685 th iteration gives loss of 0.17986121910131647\n",
      "The 22686 th iteration gives loss of 0.1798575444842873\n",
      "The 22687 th iteration gives loss of 0.17985387023912697\n",
      "The 22688 th iteration gives loss of 0.17985019636576155\n",
      "The 22689 th iteration gives loss of 0.17984652286414132\n",
      "The 22690 th iteration gives loss of 0.17984284973422332\n",
      "The 22691 th iteration gives loss of 0.17983917697594212\n",
      "The 22692 th iteration gives loss of 0.17983550458922173\n",
      "The 22693 th iteration gives loss of 0.17983183257402333\n",
      "The 22694 th iteration gives loss of 0.17982816093029005\n",
      "The 22695 th iteration gives loss of 0.17982448965796488\n",
      "The 22696 th iteration gives loss of 0.17982081875698172\n",
      "The 22697 th iteration gives loss of 0.17981714822729564\n",
      "The 22698 th iteration gives loss of 0.17981347806883807\n",
      "The 22699 th iteration gives loss of 0.17980980828156104\n",
      "The 22700 th iteration gives loss of 0.17980613886540145\n",
      "The 22701 th iteration gives loss of 0.17980246982029516\n",
      "The 22702 th iteration gives loss of 0.17979880114620947\n",
      "The 22703 th iteration gives loss of 0.17979513284306403\n",
      "The 22704 th iteration gives loss of 0.17979146491081646\n",
      "The 22705 th iteration gives loss of 0.17978779734939382\n",
      "The 22706 th iteration gives loss of 0.17978413015875436\n",
      "The 22707 th iteration gives loss of 0.17978046333883455\n",
      "The 22708 th iteration gives loss of 0.17977679688957948\n",
      "The 22709 th iteration gives loss of 0.17977313081092963\n",
      "The 22710 th iteration gives loss of 0.17976946510281827\n",
      "The 22711 th iteration gives loss of 0.17976579976520535\n",
      "The 22712 th iteration gives loss of 0.17976213479803238\n",
      "The 22713 th iteration gives loss of 0.1797584702012302\n",
      "The 22714 th iteration gives loss of 0.17975480597475632\n",
      "The 22715 th iteration gives loss of 0.17975114211854373\n",
      "The 22716 th iteration gives loss of 0.17974747863254326\n",
      "The 22717 th iteration gives loss of 0.1797438155166865\n",
      "The 22718 th iteration gives loss of 0.17974015277092376\n",
      "The 22719 th iteration gives loss of 0.1797364903951927\n",
      "The 22720 th iteration gives loss of 0.1797328283894564\n",
      "The 22721 th iteration gives loss of 0.1797291667536351\n",
      "The 22722 th iteration gives loss of 0.179725505487673\n",
      "The 22723 th iteration gives loss of 0.17972184459152346\n",
      "The 22724 th iteration gives loss of 0.17971818406512752\n",
      "The 22725 th iteration gives loss of 0.17971452390842785\n",
      "The 22726 th iteration gives loss of 0.17971086412136297\n",
      "The 22727 th iteration gives loss of 0.17970720470388302\n",
      "The 22728 th iteration gives loss of 0.17970354565592836\n",
      "The 22729 th iteration gives loss of 0.17969988697743647\n",
      "The 22730 th iteration gives loss of 0.1796962286683544\n",
      "The 22731 th iteration gives loss of 0.17969257072863387\n",
      "The 22732 th iteration gives loss of 0.17968891315821225\n",
      "The 22733 th iteration gives loss of 0.17968525595703075\n",
      "The 22734 th iteration gives loss of 0.17968159912502984\n",
      "The 22735 th iteration gives loss of 0.17967794266214798\n",
      "The 22736 th iteration gives loss of 0.17967428656834478\n",
      "The 22737 th iteration gives loss of 0.17967063084355636\n",
      "The 22738 th iteration gives loss of 0.1796669754877157\n",
      "The 22739 th iteration gives loss of 0.17966332050078831\n",
      "The 22740 th iteration gives loss of 0.1796596658826918\n",
      "The 22741 th iteration gives loss of 0.17965601163338987\n",
      "The 22742 th iteration gives loss of 0.17965235775281532\n",
      "The 22743 th iteration gives loss of 0.1796487042409129\n",
      "The 22744 th iteration gives loss of 0.17964505109763768\n",
      "The 22745 th iteration gives loss of 0.17964139832290263\n",
      "The 22746 th iteration gives loss of 0.17963774591668596\n",
      "The 22747 th iteration gives loss of 0.17963409387891502\n",
      "The 22748 th iteration gives loss of 0.17963044220952967\n",
      "The 22749 th iteration gives loss of 0.17962679090847625\n",
      "The 22750 th iteration gives loss of 0.1796231399756949\n",
      "The 22751 th iteration gives loss of 0.17961948941114333\n",
      "The 22752 th iteration gives loss of 0.17961583921474525\n",
      "The 22753 th iteration gives loss of 0.1796121893864568\n",
      "The 22754 th iteration gives loss of 0.17960853992622539\n",
      "The 22755 th iteration gives loss of 0.17960489083398326\n",
      "The 22756 th iteration gives loss of 0.17960124210967823\n",
      "The 22757 th iteration gives loss of 0.17959759375324696\n",
      "The 22758 th iteration gives loss of 0.17959394576465246\n",
      "The 22759 th iteration gives loss of 0.1795902981438175\n",
      "The 22760 th iteration gives loss of 0.17958665089069048\n",
      "The 22761 th iteration gives loss of 0.17958300400522342\n",
      "The 22762 th iteration gives loss of 0.17957935748734669\n",
      "The 22763 th iteration gives loss of 0.17957571133701794\n",
      "The 22764 th iteration gives loss of 0.1795720655541735\n",
      "The 22765 th iteration gives loss of 0.1795684201387497\n",
      "The 22766 th iteration gives loss of 0.17956477509069957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 22767 th iteration gives loss of 0.17956113040996438\n",
      "The 22768 th iteration gives loss of 0.17955748609648683\n",
      "The 22769 th iteration gives loss of 0.17955384215021902\n",
      "The 22770 th iteration gives loss of 0.17955019857108917\n",
      "The 22771 th iteration gives loss of 0.1795465553590503\n",
      "The 22772 th iteration gives loss of 0.17954291251404886\n",
      "The 22773 th iteration gives loss of 0.17953927003602008\n",
      "The 22774 th iteration gives loss of 0.17953562792491437\n",
      "The 22775 th iteration gives loss of 0.1795319861806678\n",
      "The 22776 th iteration gives loss of 0.17952834480322682\n",
      "The 22777 th iteration gives loss of 0.17952470379254076\n",
      "The 22778 th iteration gives loss of 0.1795210631485455\n",
      "The 22779 th iteration gives loss of 0.17951742287118844\n",
      "The 22780 th iteration gives loss of 0.17951378296041365\n",
      "The 22781 th iteration gives loss of 0.17951014341616445\n",
      "The 22782 th iteration gives loss of 0.1795065042383793\n",
      "The 22783 th iteration gives loss of 0.17950286542700988\n",
      "The 22784 th iteration gives loss of 0.17949922698200027\n",
      "The 22785 th iteration gives loss of 0.1794955889032894\n",
      "The 22786 th iteration gives loss of 0.1794919511908224\n",
      "The 22787 th iteration gives loss of 0.17948831384453437\n",
      "The 22788 th iteration gives loss of 0.17948467686438235\n",
      "The 22789 th iteration gives loss of 0.17948104025030923\n",
      "The 22790 th iteration gives loss of 0.17947740400224718\n",
      "The 22791 th iteration gives loss of 0.17947376812015026\n",
      "The 22792 th iteration gives loss of 0.17947013260395345\n",
      "The 22793 th iteration gives loss of 0.17946649745361806\n",
      "The 22794 th iteration gives loss of 0.17946286266906297\n",
      "The 22795 th iteration gives loss of 0.17945922825025054\n",
      "The 22796 th iteration gives loss of 0.17945559419712512\n",
      "The 22797 th iteration gives loss of 0.17945196050960713\n",
      "The 22798 th iteration gives loss of 0.17944832718767037\n",
      "The 22799 th iteration gives loss of 0.17944469423124754\n",
      "The 22800 th iteration gives loss of 0.1794410616402736\n",
      "The 22801 th iteration gives loss of 0.17943742941470042\n",
      "The 22802 th iteration gives loss of 0.17943379755448505\n",
      "The 22803 th iteration gives loss of 0.1794301660595378\n",
      "The 22804 th iteration gives loss of 0.1794265349298184\n",
      "The 22805 th iteration gives loss of 0.17942290416528456\n",
      "The 22806 th iteration gives loss of 0.17941927376586325\n",
      "The 22807 th iteration gives loss of 0.17941564373151378\n",
      "The 22808 th iteration gives loss of 0.17941201406216276\n",
      "The 22809 th iteration gives loss of 0.17940838475776494\n",
      "The 22810 th iteration gives loss of 0.17940475581826465\n",
      "The 22811 th iteration gives loss of 0.17940112724360033\n",
      "The 22812 th iteration gives loss of 0.1793974990337148\n",
      "The 22813 th iteration gives loss of 0.17939387118856492\n",
      "The 22814 th iteration gives loss of 0.1793902437080758\n",
      "The 22815 th iteration gives loss of 0.17938661659220023\n",
      "The 22816 th iteration gives loss of 0.17938298984088646\n",
      "The 22817 th iteration gives loss of 0.17937936345407846\n",
      "The 22818 th iteration gives loss of 0.17937573743170965\n",
      "The 22819 th iteration gives loss of 0.17937211177372814\n",
      "The 22820 th iteration gives loss of 0.17936848648008574\n",
      "The 22821 th iteration gives loss of 0.17936486155071218\n",
      "The 22822 th iteration gives loss of 0.17936123698556644\n",
      "The 22823 th iteration gives loss of 0.17935761278458737\n",
      "The 22824 th iteration gives loss of 0.17935398894771393\n",
      "The 22825 th iteration gives loss of 0.1793503654749047\n",
      "The 22826 th iteration gives loss of 0.17934674236608186\n",
      "The 22827 th iteration gives loss of 0.17934311962120675\n",
      "The 22828 th iteration gives loss of 0.179339497240213\n",
      "The 22829 th iteration gives loss of 0.1793358752230547\n",
      "The 22830 th iteration gives loss of 0.17933225356966592\n",
      "The 22831 th iteration gives loss of 0.17932863227999685\n",
      "The 22832 th iteration gives loss of 0.17932501135398587\n",
      "The 22833 th iteration gives loss of 0.17932139079158346\n",
      "The 22834 th iteration gives loss of 0.1793177705927369\n",
      "The 22835 th iteration gives loss of 0.17931415075738666\n",
      "The 22836 th iteration gives loss of 0.17931053128546687\n",
      "The 22837 th iteration gives loss of 0.1793069121769263\n",
      "The 22838 th iteration gives loss of 0.17930329343171814\n",
      "The 22839 th iteration gives loss of 0.17929967504978148\n",
      "The 22840 th iteration gives loss of 0.17929605703105936\n",
      "The 22841 th iteration gives loss of 0.17929243937549702\n",
      "The 22842 th iteration gives loss of 0.17928882208303723\n",
      "The 22843 th iteration gives loss of 0.17928520515362673\n",
      "The 22844 th iteration gives loss of 0.1792815885871998\n",
      "The 22845 th iteration gives loss of 0.17927797238371374\n",
      "The 22846 th iteration gives loss of 0.17927435654311194\n",
      "The 22847 th iteration gives loss of 0.17927074106533003\n",
      "The 22848 th iteration gives loss of 0.17926712595032407\n",
      "The 22849 th iteration gives loss of 0.17926351119801787\n",
      "The 22850 th iteration gives loss of 0.17925989680838045\n",
      "The 22851 th iteration gives loss of 0.17925628278133623\n",
      "The 22852 th iteration gives loss of 0.1792526691168449\n",
      "The 22853 th iteration gives loss of 0.1792490558148381\n",
      "The 22854 th iteration gives loss of 0.17924544287526595\n",
      "The 22855 th iteration gives loss of 0.17924183029807264\n",
      "The 22856 th iteration gives loss of 0.1792382180832109\n",
      "The 22857 th iteration gives loss of 0.17923460623060106\n",
      "The 22858 th iteration gives loss of 0.17923099474020934\n",
      "The 22859 th iteration gives loss of 0.17922738361197688\n",
      "The 22860 th iteration gives loss of 0.17922377284583818\n",
      "The 22861 th iteration gives loss of 0.17922016244175068\n",
      "The 22862 th iteration gives loss of 0.17921655239964482\n",
      "The 22863 th iteration gives loss of 0.17921294271947572\n",
      "The 22864 th iteration gives loss of 0.1792093334011841\n",
      "The 22865 th iteration gives loss of 0.17920572444471203\n",
      "The 22866 th iteration gives loss of 0.17920211584999973\n",
      "The 22867 th iteration gives loss of 0.17919850761700423\n",
      "The 22868 th iteration gives loss of 0.1791948997456688\n",
      "The 22869 th iteration gives loss of 0.17919129223593042\n",
      "The 22870 th iteration gives loss of 0.1791876850877325\n",
      "The 22871 th iteration gives loss of 0.17918407830102712\n",
      "The 22872 th iteration gives loss of 0.17918047187575198\n",
      "The 22873 th iteration gives loss of 0.17917686581185674\n",
      "The 22874 th iteration gives loss of 0.17917326010927442\n",
      "The 22875 th iteration gives loss of 0.1791696547679637\n",
      "The 22876 th iteration gives loss of 0.17916604978786876\n",
      "The 22877 th iteration gives loss of 0.17916244516891472\n",
      "The 22878 th iteration gives loss of 0.17915884091107268\n",
      "The 22879 th iteration gives loss of 0.17915523701427064\n",
      "The 22880 th iteration gives loss of 0.17915163347845703\n",
      "The 22881 th iteration gives loss of 0.17914803030357063\n",
      "The 22882 th iteration gives loss of 0.17914442748956863\n",
      "The 22883 th iteration gives loss of 0.1791408250363839\n",
      "The 22884 th iteration gives loss of 0.17913722294396883\n",
      "The 22885 th iteration gives loss of 0.17913362121226464\n",
      "The 22886 th iteration gives loss of 0.1791300198412121\n",
      "The 22887 th iteration gives loss of 0.179126418830766\n",
      "The 22888 th iteration gives loss of 0.1791228181808548\n",
      "The 22889 th iteration gives loss of 0.17911921789144505\n",
      "The 22890 th iteration gives loss of 0.1791156179624653\n",
      "The 22891 th iteration gives loss of 0.17911201839385987\n",
      "The 22892 th iteration gives loss of 0.17910841918558112\n",
      "The 22893 th iteration gives loss of 0.1791048203375686\n",
      "The 22894 th iteration gives loss of 0.1791012218497737\n",
      "The 22895 th iteration gives loss of 0.17909762372212681\n",
      "The 22896 th iteration gives loss of 0.17909402595458568\n",
      "The 22897 th iteration gives loss of 0.17909042854709034\n",
      "The 22898 th iteration gives loss of 0.1790868314995868\n",
      "The 22899 th iteration gives loss of 0.17908323481201982\n",
      "The 22900 th iteration gives loss of 0.17907963848432917\n",
      "The 22901 th iteration gives loss of 0.17907604251646916\n",
      "The 22902 th iteration gives loss of 0.179072446908376\n",
      "The 22903 th iteration gives loss of 0.17906885165999456\n",
      "The 22904 th iteration gives loss of 0.1790652567712713\n",
      "The 22905 th iteration gives loss of 0.1790616622421506\n",
      "The 22906 th iteration gives loss of 0.1790580680725846\n",
      "The 22907 th iteration gives loss of 0.1790544742625117\n",
      "The 22908 th iteration gives loss of 0.1790508808118808\n",
      "The 22909 th iteration gives loss of 0.1790472877206227\n",
      "The 22910 th iteration gives loss of 0.17904369498870004\n",
      "The 22911 th iteration gives loss of 0.1790401026160363\n",
      "The 22912 th iteration gives loss of 0.17903651060259626\n",
      "The 22913 th iteration gives loss of 0.17903291894832604\n",
      "The 22914 th iteration gives loss of 0.1790293276531598\n",
      "The 22915 th iteration gives loss of 0.17902573671704136\n",
      "The 22916 th iteration gives loss of 0.1790221461399225\n",
      "The 22917 th iteration gives loss of 0.17901855592173593\n",
      "The 22918 th iteration gives loss of 0.1790149660624433\n",
      "The 22919 th iteration gives loss of 0.17901137656198032\n",
      "The 22920 th iteration gives loss of 0.17900778742029602\n",
      "The 22921 th iteration gives loss of 0.17900419863733105\n",
      "The 22922 th iteration gives loss of 0.1790006102130338\n",
      "The 22923 th iteration gives loss of 0.17899702214733337\n",
      "The 22924 th iteration gives loss of 0.1789934344402057\n",
      "The 22925 th iteration gives loss of 0.17898984709156035\n",
      "The 22926 th iteration gives loss of 0.17898626010136048\n",
      "The 22927 th iteration gives loss of 0.17898267346956176\n",
      "The 22928 th iteration gives loss of 0.1789790871960976\n",
      "The 22929 th iteration gives loss of 0.1789755012809057\n",
      "The 22930 th iteration gives loss of 0.17897191572393636\n",
      "The 22931 th iteration gives loss of 0.17896833052514358\n",
      "The 22932 th iteration gives loss of 0.1789647456844725\n",
      "The 22933 th iteration gives loss of 0.1789611612018528\n",
      "The 22934 th iteration gives loss of 0.1789575770772342\n",
      "The 22935 th iteration gives loss of 0.1789539933105718\n",
      "The 22936 th iteration gives loss of 0.17895040990179417\n",
      "The 22937 th iteration gives loss of 0.17894682685086086\n",
      "The 22938 th iteration gives loss of 0.17894324415771592\n",
      "The 22939 th iteration gives loss of 0.1789396618222962\n",
      "The 22940 th iteration gives loss of 0.17893607984455093\n",
      "The 22941 th iteration gives loss of 0.1789324982244271\n",
      "The 22942 th iteration gives loss of 0.17892891696186816\n",
      "The 22943 th iteration gives loss of 0.1789253360568181\n",
      "The 22944 th iteration gives loss of 0.17892175550922096\n",
      "The 22945 th iteration gives loss of 0.1789181753190243\n",
      "The 22946 th iteration gives loss of 0.17891459548616956\n",
      "The 22947 th iteration gives loss of 0.17891101601061113\n",
      "The 22948 th iteration gives loss of 0.17890743689227973\n",
      "The 22949 th iteration gives loss of 0.17890385813113557\n",
      "The 22950 th iteration gives loss of 0.17890027972711783\n",
      "The 22951 th iteration gives loss of 0.1788967016801653\n",
      "The 22952 th iteration gives loss of 0.17889312399022508\n",
      "The 22953 th iteration gives loss of 0.1788895466572444\n",
      "The 22954 th iteration gives loss of 0.17888596968118414\n",
      "The 22955 th iteration gives loss of 0.17888239306196538\n",
      "The 22956 th iteration gives loss of 0.17887881679953627\n",
      "The 22957 th iteration gives loss of 0.17887524089385598\n",
      "The 22958 th iteration gives loss of 0.17887166534485904\n",
      "The 22959 th iteration gives loss of 0.17886809015249258\n",
      "The 22960 th iteration gives loss of 0.17886451531670317\n",
      "The 22961 th iteration gives loss of 0.178860940837435\n",
      "The 22962 th iteration gives loss of 0.17885736671464086\n",
      "The 22963 th iteration gives loss of 0.17885379294825016\n",
      "The 22964 th iteration gives loss of 0.17885021953822575\n",
      "The 22965 th iteration gives loss of 0.1788466464845041\n",
      "The 22966 th iteration gives loss of 0.17884307378702588\n",
      "The 22967 th iteration gives loss of 0.17883950144573343\n",
      "The 22968 th iteration gives loss of 0.1788359294605833\n",
      "The 22969 th iteration gives loss of 0.17883235783152374\n",
      "The 22970 th iteration gives loss of 0.17882878655849166\n",
      "The 22971 th iteration gives loss of 0.17882521564143086\n",
      "The 22972 th iteration gives loss of 0.17882164508028986\n",
      "The 22973 th iteration gives loss of 0.1788180748750214\n",
      "The 22974 th iteration gives loss of 0.1788145050255532\n",
      "The 22975 th iteration gives loss of 0.1788109355318467\n",
      "The 22976 th iteration gives loss of 0.17880736639383984\n",
      "The 22977 th iteration gives loss of 0.17880379761147533\n",
      "The 22978 th iteration gives loss of 0.1788002291847044\n",
      "The 22979 th iteration gives loss of 0.1787966611134722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 22980 th iteration gives loss of 0.1787930933977253\n",
      "The 22981 th iteration gives loss of 0.17878952603740358\n",
      "The 22982 th iteration gives loss of 0.17878595903245173\n",
      "The 22983 th iteration gives loss of 0.17878239238281896\n",
      "The 22984 th iteration gives loss of 0.1787788260884588\n",
      "The 22985 th iteration gives loss of 0.17877526014929593\n",
      "The 22986 th iteration gives loss of 0.17877169456529363\n",
      "The 22987 th iteration gives loss of 0.1787681293363953\n",
      "The 22988 th iteration gives loss of 0.17876456446254457\n",
      "The 22989 th iteration gives loss of 0.17876099994367534\n",
      "The 22990 th iteration gives loss of 0.17875743577974051\n",
      "The 22991 th iteration gives loss of 0.17875387197068998\n",
      "The 22992 th iteration gives loss of 0.17875030851647666\n",
      "The 22993 th iteration gives loss of 0.17874674541702237\n",
      "The 22994 th iteration gives loss of 0.1787431826722899\n",
      "The 22995 th iteration gives loss of 0.17873962028222773\n",
      "The 22996 th iteration gives loss of 0.17873605824677655\n",
      "The 22997 th iteration gives loss of 0.1787324965658806\n",
      "The 22998 th iteration gives loss of 0.17872893523947558\n",
      "The 22999 th iteration gives loss of 0.1787253742675313\n",
      "The 23000 th iteration gives loss of 0.17872181364996467\n",
      "The 23001 th iteration gives loss of 0.1787182533867403\n",
      "The 23002 th iteration gives loss of 0.1787146934777874\n",
      "The 23003 th iteration gives loss of 0.17871113392306978\n",
      "The 23004 th iteration gives loss of 0.1787075747225259\n",
      "The 23005 th iteration gives loss of 0.17870401587610976\n",
      "The 23006 th iteration gives loss of 0.17870045738375698\n",
      "The 23007 th iteration gives loss of 0.17869689924541377\n",
      "The 23008 th iteration gives loss of 0.17869334146101679\n",
      "The 23009 th iteration gives loss of 0.17868978403053715\n",
      "The 23010 th iteration gives loss of 0.17868622695389083\n",
      "The 23011 th iteration gives loss of 0.17868267023104423\n",
      "The 23012 th iteration gives loss of 0.17867911386194188\n",
      "The 23013 th iteration gives loss of 0.17867555784651293\n",
      "The 23014 th iteration gives loss of 0.1786720021847219\n",
      "The 23015 th iteration gives loss of 0.17866844687649727\n",
      "The 23016 th iteration gives loss of 0.17866489192180182\n",
      "The 23017 th iteration gives loss of 0.1786613373205704\n",
      "The 23018 th iteration gives loss of 0.1786577830727531\n",
      "The 23019 th iteration gives loss of 0.17865422917829313\n",
      "The 23020 th iteration gives loss of 0.17865067563713993\n",
      "The 23021 th iteration gives loss of 0.1786471224492399\n",
      "The 23022 th iteration gives loss of 0.1786435696145285\n",
      "The 23023 th iteration gives loss of 0.1786400171329543\n",
      "The 23024 th iteration gives loss of 0.17863646500447117\n",
      "The 23025 th iteration gives loss of 0.17863291322902158\n",
      "The 23026 th iteration gives loss of 0.17862936180655237\n",
      "The 23027 th iteration gives loss of 0.17862581073700623\n",
      "The 23028 th iteration gives loss of 0.1786222600203286\n",
      "The 23029 th iteration gives loss of 0.17861870965646215\n",
      "The 23030 th iteration gives loss of 0.1786151596453642\n",
      "The 23031 th iteration gives loss of 0.17861160998697634\n",
      "The 23032 th iteration gives loss of 0.17860806068123805\n",
      "The 23033 th iteration gives loss of 0.17860451172810077\n",
      "The 23034 th iteration gives loss of 0.1786009631275049\n",
      "The 23035 th iteration gives loss of 0.17859741487940045\n",
      "The 23036 th iteration gives loss of 0.17859386698373242\n",
      "The 23037 th iteration gives loss of 0.17859031944044823\n",
      "The 23038 th iteration gives loss of 0.1785867722494985\n",
      "The 23039 th iteration gives loss of 0.17858322541081914\n",
      "The 23040 th iteration gives loss of 0.1785796789243526\n",
      "The 23041 th iteration gives loss of 0.17857613279005613\n",
      "The 23042 th iteration gives loss of 0.17857258700787268\n",
      "The 23043 th iteration gives loss of 0.17856904157774986\n",
      "The 23044 th iteration gives loss of 0.178565496499621\n",
      "The 23045 th iteration gives loss of 0.17856195177344586\n",
      "The 23046 th iteration gives loss of 0.17855840739917117\n",
      "The 23047 th iteration gives loss of 0.17855486337673337\n",
      "The 23048 th iteration gives loss of 0.1785513197060871\n",
      "The 23049 th iteration gives loss of 0.1785477763871718\n",
      "The 23050 th iteration gives loss of 0.17854423341993209\n",
      "The 23051 th iteration gives loss of 0.17854069080431925\n",
      "The 23052 th iteration gives loss of 0.1785371485402763\n",
      "The 23053 th iteration gives loss of 0.17853360662775467\n",
      "The 23054 th iteration gives loss of 0.1785300650667003\n",
      "The 23055 th iteration gives loss of 0.17852652385704917\n",
      "The 23056 th iteration gives loss of 0.17852298299875155\n",
      "The 23057 th iteration gives loss of 0.17851944249176313\n",
      "The 23058 th iteration gives loss of 0.17851590233601305\n",
      "The 23059 th iteration gives loss of 0.17851236253146538\n",
      "The 23060 th iteration gives loss of 0.17850882307805535\n",
      "The 23061 th iteration gives loss of 0.17850528397572354\n",
      "The 23062 th iteration gives loss of 0.17850174522442064\n",
      "The 23063 th iteration gives loss of 0.17849820682411047\n",
      "The 23064 th iteration gives loss of 0.17849466877470732\n",
      "The 23065 th iteration gives loss of 0.17849113107618061\n",
      "The 23066 th iteration gives loss of 0.17848759372847747\n",
      "The 23067 th iteration gives loss of 0.17848405673152748\n",
      "The 23068 th iteration gives loss of 0.1784805200852882\n",
      "The 23069 th iteration gives loss of 0.17847698378970173\n",
      "The 23070 th iteration gives loss of 0.17847344784471658\n",
      "The 23071 th iteration gives loss of 0.17846991225028178\n",
      "The 23072 th iteration gives loss of 0.1784663770063344\n",
      "The 23073 th iteration gives loss of 0.1784628421128302\n",
      "The 23074 th iteration gives loss of 0.17845930756970366\n",
      "The 23075 th iteration gives loss of 0.1784557733769131\n",
      "The 23076 th iteration gives loss of 0.178452239534399\n",
      "The 23077 th iteration gives loss of 0.17844870604211738\n",
      "The 23078 th iteration gives loss of 0.17844517289999223\n",
      "The 23079 th iteration gives loss of 0.178441640107992\n",
      "The 23080 th iteration gives loss of 0.17843810766604692\n",
      "The 23081 th iteration gives loss of 0.1784345755741143\n",
      "The 23082 th iteration gives loss of 0.17843104383213465\n",
      "The 23083 th iteration gives loss of 0.17842751244005883\n",
      "The 23084 th iteration gives loss of 0.17842398139782303\n",
      "The 23085 th iteration gives loss of 0.178420450705384\n",
      "The 23086 th iteration gives loss of 0.17841692036269002\n",
      "The 23087 th iteration gives loss of 0.17841339036967852\n",
      "The 23088 th iteration gives loss of 0.17840986072629061\n",
      "The 23089 th iteration gives loss of 0.1784063314324939\n",
      "The 23090 th iteration gives loss of 0.17840280248821144\n",
      "The 23091 th iteration gives loss of 0.1783992738934143\n",
      "The 23092 th iteration gives loss of 0.17839574564802593\n",
      "The 23093 th iteration gives loss of 0.1783922177520016\n",
      "The 23094 th iteration gives loss of 0.17838869020528475\n",
      "The 23095 th iteration gives loss of 0.1783851630078277\n",
      "The 23096 th iteration gives loss of 0.17838163615957578\n",
      "The 23097 th iteration gives loss of 0.17837810966047205\n",
      "The 23098 th iteration gives loss of 0.1783745835104599\n",
      "The 23099 th iteration gives loss of 0.17837105770948825\n",
      "The 23100 th iteration gives loss of 0.1783675322575123\n",
      "The 23101 th iteration gives loss of 0.1783640071544635\n",
      "The 23102 th iteration gives loss of 0.17836048240030614\n",
      "The 23103 th iteration gives loss of 0.17835695799496254\n",
      "The 23104 th iteration gives loss of 0.17835343393840467\n",
      "The 23105 th iteration gives loss of 0.17834991023055785\n",
      "The 23106 th iteration gives loss of 0.17834638687138152\n",
      "The 23107 th iteration gives loss of 0.1783428638608202\n",
      "The 23108 th iteration gives loss of 0.17833934119881764\n",
      "The 23109 th iteration gives loss of 0.17833581888531855\n",
      "The 23110 th iteration gives loss of 0.17833229692027425\n",
      "The 23111 th iteration gives loss of 0.17832877530363106\n",
      "The 23112 th iteration gives loss of 0.17832525403534025\n",
      "The 23113 th iteration gives loss of 0.17832173311533336\n",
      "The 23114 th iteration gives loss of 0.17831821254356015\n",
      "The 23115 th iteration gives loss of 0.17831469231997318\n",
      "The 23116 th iteration gives loss of 0.17831117244452602\n",
      "The 23117 th iteration gives loss of 0.17830765291714923\n",
      "The 23118 th iteration gives loss of 0.17830413373779627\n",
      "The 23119 th iteration gives loss of 0.17830061490641727\n",
      "The 23120 th iteration gives loss of 0.17829709642296174\n",
      "The 23121 th iteration gives loss of 0.17829357828736914\n",
      "The 23122 th iteration gives loss of 0.17829006049958324\n",
      "The 23123 th iteration gives loss of 0.17828654305956462\n",
      "The 23124 th iteration gives loss of 0.17828302596723763\n",
      "The 23125 th iteration gives loss of 0.17827950922256047\n",
      "The 23126 th iteration gives loss of 0.1782759928254851\n",
      "The 23127 th iteration gives loss of 0.17827247677596006\n",
      "The 23128 th iteration gives loss of 0.1782689610739181\n",
      "The 23129 th iteration gives loss of 0.1782654457193098\n",
      "The 23130 th iteration gives loss of 0.17826193071208654\n",
      "The 23131 th iteration gives loss of 0.1782584160522008\n",
      "The 23132 th iteration gives loss of 0.17825490173958858\n",
      "The 23133 th iteration gives loss of 0.17825138777420274\n",
      "The 23134 th iteration gives loss of 0.17824787415598328\n",
      "The 23135 th iteration gives loss of 0.17824436088487516\n",
      "The 23136 th iteration gives loss of 0.17824084796084386\n",
      "The 23137 th iteration gives loss of 0.17823733538381806\n",
      "The 23138 th iteration gives loss of 0.17823382315375008\n",
      "The 23139 th iteration gives loss of 0.178230311270585\n",
      "The 23140 th iteration gives loss of 0.1782267997342694\n",
      "The 23141 th iteration gives loss of 0.17822328854475086\n",
      "The 23142 th iteration gives loss of 0.17821977770197675\n",
      "The 23143 th iteration gives loss of 0.1782162672058972\n",
      "The 23144 th iteration gives loss of 0.1782127570564559\n",
      "The 23145 th iteration gives loss of 0.1782092472535973\n",
      "The 23146 th iteration gives loss of 0.17820573779726306\n",
      "The 23147 th iteration gives loss of 0.17820222868741462\n",
      "The 23148 th iteration gives loss of 0.17819871992398403\n",
      "The 23149 th iteration gives loss of 0.17819521150693124\n",
      "The 23150 th iteration gives loss of 0.17819170343619337\n",
      "The 23151 th iteration gives loss of 0.17818819571172873\n",
      "The 23152 th iteration gives loss of 0.17818468833345824\n",
      "The 23153 th iteration gives loss of 0.17818118130136051\n",
      "The 23154 th iteration gives loss of 0.17817767461537107\n",
      "The 23155 th iteration gives loss of 0.17817416827542468\n",
      "The 23156 th iteration gives loss of 0.17817066228148326\n",
      "The 23157 th iteration gives loss of 0.17816715663348676\n",
      "The 23158 th iteration gives loss of 0.17816365133138343\n",
      "The 23159 th iteration gives loss of 0.17816014637511524\n",
      "The 23160 th iteration gives loss of 0.17815664176464055\n",
      "The 23161 th iteration gives loss of 0.17815313749989625\n",
      "The 23162 th iteration gives loss of 0.1781496335808301\n",
      "The 23163 th iteration gives loss of 0.1781461300073928\n",
      "The 23164 th iteration gives loss of 0.17814262677953463\n",
      "The 23165 th iteration gives loss of 0.17813912389719233\n",
      "The 23166 th iteration gives loss of 0.17813562136032018\n",
      "The 23167 th iteration gives loss of 0.17813211916886112\n",
      "The 23168 th iteration gives loss of 0.17812861732277296\n",
      "The 23169 th iteration gives loss of 0.1781251158219909\n",
      "The 23170 th iteration gives loss of 0.17812161466645243\n",
      "The 23171 th iteration gives loss of 0.17811811385613988\n",
      "The 23172 th iteration gives loss of 0.17811461339096116\n",
      "The 23173 th iteration gives loss of 0.17811111327087858\n",
      "The 23174 th iteration gives loss of 0.17810761349585144\n",
      "The 23175 th iteration gives loss of 0.1781041140658095\n",
      "The 23176 th iteration gives loss of 0.17810061498070473\n",
      "The 23177 th iteration gives loss of 0.17809711624048485\n",
      "The 23178 th iteration gives loss of 0.1780936178451\n",
      "The 23179 th iteration gives loss of 0.17809011979448622\n",
      "The 23180 th iteration gives loss of 0.17808662208861437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 23181 th iteration gives loss of 0.17808312472741078\n",
      "The 23182 th iteration gives loss of 0.17807962771081537\n",
      "The 23183 th iteration gives loss of 0.1780761310388002\n",
      "The 23184 th iteration gives loss of 0.17807263471129894\n",
      "The 23185 th iteration gives loss of 0.17806913872825875\n",
      "The 23186 th iteration gives loss of 0.17806564308962017\n",
      "The 23187 th iteration gives loss of 0.17806214779534327\n",
      "The 23188 th iteration gives loss of 0.17805865284536773\n",
      "The 23189 th iteration gives loss of 0.1780551582396432\n",
      "The 23190 th iteration gives loss of 0.17805166397811695\n",
      "The 23191 th iteration gives loss of 0.17804817006073093\n",
      "The 23192 th iteration gives loss of 0.17804467648744135\n",
      "The 23193 th iteration gives loss of 0.17804118325818868\n",
      "The 23194 th iteration gives loss of 0.17803769037292844\n",
      "The 23195 th iteration gives loss of 0.17803419783159202\n",
      "The 23196 th iteration gives loss of 0.1780307056341424\n",
      "The 23197 th iteration gives loss of 0.17802721378051956\n",
      "The 23198 th iteration gives loss of 0.17802372227066496\n",
      "The 23199 th iteration gives loss of 0.17802023110453885\n",
      "The 23200 th iteration gives loss of 0.17801674028207545\n",
      "The 23201 th iteration gives loss of 0.17801324980323682\n",
      "The 23202 th iteration gives loss of 0.1780097596679577\n",
      "The 23203 th iteration gives loss of 0.1780062698761921\n",
      "The 23204 th iteration gives loss of 0.17800278042788256\n",
      "The 23205 th iteration gives loss of 0.17799929132297634\n",
      "The 23206 th iteration gives loss of 0.17799580256142614\n",
      "The 23207 th iteration gives loss of 0.17799231414316913\n",
      "The 23208 th iteration gives loss of 0.17798882606817307\n",
      "The 23209 th iteration gives loss of 0.1779853383363643\n",
      "The 23210 th iteration gives loss of 0.17798185094769206\n",
      "The 23211 th iteration gives loss of 0.17797836390211957\n",
      "The 23212 th iteration gives loss of 0.17797487719957936\n",
      "The 23213 th iteration gives loss of 0.17797139084002392\n",
      "The 23214 th iteration gives loss of 0.1779679048233981\n",
      "The 23215 th iteration gives loss of 0.17796441914965064\n",
      "The 23216 th iteration gives loss of 0.17796093381873163\n",
      "The 23217 th iteration gives loss of 0.17795744883058334\n",
      "The 23218 th iteration gives loss of 0.17795396418515447\n",
      "The 23219 th iteration gives loss of 0.17795047988239301\n",
      "The 23220 th iteration gives loss of 0.1779469959222502\n",
      "The 23221 th iteration gives loss of 0.17794351230467567\n",
      "The 23222 th iteration gives loss of 0.17794002902959818\n",
      "The 23223 th iteration gives loss of 0.17793654609698636\n",
      "The 23224 th iteration gives loss of 0.17793306350678195\n",
      "The 23225 th iteration gives loss of 0.1779295812589275\n",
      "The 23226 th iteration gives loss of 0.1779260993533606\n",
      "The 23227 th iteration gives loss of 0.17792261779005453\n",
      "The 23228 th iteration gives loss of 0.1779191365689422\n",
      "The 23229 th iteration gives loss of 0.17791565568996562\n",
      "The 23230 th iteration gives loss of 0.17791217515307844\n",
      "The 23231 th iteration gives loss of 0.17790869495824363\n",
      "The 23232 th iteration gives loss of 0.17790521510538576\n",
      "The 23233 th iteration gives loss of 0.17790173559445877\n",
      "The 23234 th iteration gives loss of 0.1778982564254138\n",
      "The 23235 th iteration gives loss of 0.17789477759818517\n",
      "The 23236 th iteration gives loss of 0.17789129911273896\n",
      "The 23237 th iteration gives loss of 0.1778878209690182\n",
      "The 23238 th iteration gives loss of 0.17788434316696733\n",
      "The 23239 th iteration gives loss of 0.17788086570652917\n",
      "The 23240 th iteration gives loss of 0.17787738858765628\n",
      "The 23241 th iteration gives loss of 0.1778739118103031\n",
      "The 23242 th iteration gives loss of 0.1778704353743992\n",
      "The 23243 th iteration gives loss of 0.17786695927990034\n",
      "The 23244 th iteration gives loss of 0.17786348352677675\n",
      "The 23245 th iteration gives loss of 0.17786000811493688\n",
      "The 23246 th iteration gives loss of 0.17785653304435417\n",
      "The 23247 th iteration gives loss of 0.17785305831497294\n",
      "The 23248 th iteration gives loss of 0.17784958392673067\n",
      "The 23249 th iteration gives loss of 0.17784610987957974\n",
      "The 23250 th iteration gives loss of 0.17784263617348092\n",
      "The 23251 th iteration gives loss of 0.1778391628083616\n",
      "The 23252 th iteration gives loss of 0.1778356897841839\n",
      "The 23253 th iteration gives loss of 0.1778322171008798\n",
      "The 23254 th iteration gives loss of 0.1778287447584144\n",
      "The 23255 th iteration gives loss of 0.1778252727567277\n",
      "The 23256 th iteration gives loss of 0.1778218010957694\n",
      "The 23257 th iteration gives loss of 0.17781832977549034\n",
      "The 23258 th iteration gives loss of 0.17781485879582673\n",
      "The 23259 th iteration gives loss of 0.17781138815673855\n",
      "The 23260 th iteration gives loss of 0.17780791785816255\n",
      "The 23261 th iteration gives loss of 0.17780444790005198\n",
      "The 23262 th iteration gives loss of 0.17780097828235814\n",
      "The 23263 th iteration gives loss of 0.17779750900502672\n",
      "The 23264 th iteration gives loss of 0.1777940400679939\n",
      "The 23265 th iteration gives loss of 0.1777905714712274\n",
      "The 23266 th iteration gives loss of 0.1777871032146571\n",
      "The 23267 th iteration gives loss of 0.17778363529823984\n",
      "The 23268 th iteration gives loss of 0.1777801677219297\n",
      "The 23269 th iteration gives loss of 0.1777767004856621\n",
      "The 23270 th iteration gives loss of 0.17777323358939143\n",
      "The 23271 th iteration gives loss of 0.17776976703305694\n",
      "The 23272 th iteration gives loss of 0.17776630081662217\n",
      "The 23273 th iteration gives loss of 0.1777628349400291\n",
      "The 23274 th iteration gives loss of 0.17775936940321835\n",
      "The 23275 th iteration gives loss of 0.17775590420613652\n",
      "The 23276 th iteration gives loss of 0.17775243934874743\n",
      "The 23277 th iteration gives loss of 0.17774897483096808\n",
      "The 23278 th iteration gives loss of 0.17774551065278543\n",
      "The 23279 th iteration gives loss of 0.17774204681413022\n",
      "The 23280 th iteration gives loss of 0.1777385833149346\n",
      "The 23281 th iteration gives loss of 0.17773512015517084\n",
      "The 23282 th iteration gives loss of 0.17773165733477475\n",
      "The 23283 th iteration gives loss of 0.17772819485369315\n",
      "The 23284 th iteration gives loss of 0.1777247327118767\n",
      "The 23285 th iteration gives loss of 0.17772127090928536\n",
      "The 23286 th iteration gives loss of 0.17771780944584642\n",
      "The 23287 th iteration gives loss of 0.17771434832151825\n",
      "The 23288 th iteration gives loss of 0.177710887536247\n",
      "The 23289 th iteration gives loss of 0.17770742708997664\n",
      "The 23290 th iteration gives loss of 0.1777039669826596\n",
      "The 23291 th iteration gives loss of 0.17770050721424516\n",
      "The 23292 th iteration gives loss of 0.17769704778468112\n",
      "The 23293 th iteration gives loss of 0.1776935886939165\n",
      "The 23294 th iteration gives loss of 0.17769012994189398\n",
      "The 23295 th iteration gives loss of 0.1776866715285697\n",
      "The 23296 th iteration gives loss of 0.1776832134538771\n",
      "The 23297 th iteration gives loss of 0.17767975571778274\n",
      "The 23298 th iteration gives loss of 0.17767629832021792\n",
      "The 23299 th iteration gives loss of 0.17767284126113767\n",
      "The 23300 th iteration gives loss of 0.1776693845404926\n",
      "The 23301 th iteration gives loss of 0.17766592815822987\n",
      "The 23302 th iteration gives loss of 0.17766247211429584\n",
      "The 23303 th iteration gives loss of 0.17765901640863624\n",
      "The 23304 th iteration gives loss of 0.17765556104119995\n",
      "The 23305 th iteration gives loss of 0.17765210601194253\n",
      "The 23306 th iteration gives loss of 0.17764865132080995\n",
      "The 23307 th iteration gives loss of 0.17764519696773723\n",
      "The 23308 th iteration gives loss of 0.17764174295268836\n",
      "The 23309 th iteration gives loss of 0.1776382892756067\n",
      "The 23310 th iteration gives loss of 0.17763483593643561\n",
      "The 23311 th iteration gives loss of 0.17763138293512662\n",
      "The 23312 th iteration gives loss of 0.17762793027163215\n",
      "The 23313 th iteration gives loss of 0.1776244779458961\n",
      "The 23314 th iteration gives loss of 0.17762102595785814\n",
      "The 23315 th iteration gives loss of 0.17761757430748074\n",
      "The 23316 th iteration gives loss of 0.17761412299470603\n",
      "The 23317 th iteration gives loss of 0.17761067201948155\n",
      "The 23318 th iteration gives loss of 0.17760722138176274\n",
      "The 23319 th iteration gives loss of 0.1776037710814906\n",
      "The 23320 th iteration gives loss of 0.17760032111861176\n",
      "The 23321 th iteration gives loss of 0.17759687149307388\n",
      "The 23322 th iteration gives loss of 0.17759342220483182\n",
      "The 23323 th iteration gives loss of 0.17758997325383122\n",
      "The 23324 th iteration gives loss of 0.1775865246400101\n",
      "The 23325 th iteration gives loss of 0.17758307636333034\n",
      "The 23326 th iteration gives loss of 0.17757962842374303\n",
      "The 23327 th iteration gives loss of 0.1775761808211785\n",
      "The 23328 th iteration gives loss of 0.17757273355559922\n",
      "The 23329 th iteration gives loss of 0.17756928662694468\n",
      "The 23330 th iteration gives loss of 0.17756584003516954\n",
      "The 23331 th iteration gives loss of 0.17756239378022773\n",
      "The 23332 th iteration gives loss of 0.17755894786206258\n",
      "The 23333 th iteration gives loss of 0.17755550228061473\n",
      "The 23334 th iteration gives loss of 0.17755205703584817\n",
      "The 23335 th iteration gives loss of 0.17754861212769132\n",
      "The 23336 th iteration gives loss of 0.17754516755611074\n",
      "The 23337 th iteration gives loss of 0.17754172332102994\n",
      "The 23338 th iteration gives loss of 0.17753827942243164\n",
      "The 23339 th iteration gives loss of 0.17753483586024005\n",
      "The 23340 th iteration gives loss of 0.1775313926344148\n",
      "The 23341 th iteration gives loss of 0.17752794974489022\n",
      "The 23342 th iteration gives loss of 0.17752450719163138\n",
      "The 23343 th iteration gives loss of 0.17752106497457837\n",
      "The 23344 th iteration gives loss of 0.17751762309367716\n",
      "The 23345 th iteration gives loss of 0.17751418154887627\n",
      "The 23346 th iteration gives loss of 0.17751074034012818\n",
      "The 23347 th iteration gives loss of 0.1775072994673893\n",
      "The 23348 th iteration gives loss of 0.1775038589305953\n",
      "The 23349 th iteration gives loss of 0.1775004187296978\n",
      "The 23350 th iteration gives loss of 0.17749697886464955\n",
      "The 23351 th iteration gives loss of 0.1774935393353861\n",
      "The 23352 th iteration gives loss of 0.17749010014187194\n",
      "The 23353 th iteration gives loss of 0.17748666128404328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 23354 th iteration gives loss of 0.17748322276185804\n",
      "The 23355 th iteration gives loss of 0.17747978457526256\n",
      "The 23356 th iteration gives loss of 0.17747634672419582\n",
      "The 23357 th iteration gives loss of 0.1774729092086283\n",
      "The 23358 th iteration gives loss of 0.17746947202848723\n",
      "The 23359 th iteration gives loss of 0.17746603518372964\n",
      "The 23360 th iteration gives loss of 0.1774625986743001\n",
      "The 23361 th iteration gives loss of 0.1774591625001495\n",
      "The 23362 th iteration gives loss of 0.17745572666122691\n",
      "The 23363 th iteration gives loss of 0.17745229115748204\n",
      "The 23364 th iteration gives loss of 0.17744885598886267\n",
      "The 23365 th iteration gives loss of 0.17744542115531858\n",
      "The 23366 th iteration gives loss of 0.1774419866567963\n",
      "The 23367 th iteration gives loss of 0.17743855249324464\n",
      "The 23368 th iteration gives loss of 0.1774351186645996\n",
      "The 23369 th iteration gives loss of 0.17743168517083224\n",
      "The 23370 th iteration gives loss of 0.17742825201188756\n",
      "The 23371 th iteration gives loss of 0.17742481918769565\n",
      "The 23372 th iteration gives loss of 0.17742138669823063\n",
      "The 23373 th iteration gives loss of 0.17741795454342507\n",
      "The 23374 th iteration gives loss of 0.17741452272322294\n",
      "The 23375 th iteration gives loss of 0.1774110912375877\n",
      "The 23376 th iteration gives loss of 0.1774076600864544\n",
      "The 23377 th iteration gives loss of 0.17740422926978505\n",
      "The 23378 th iteration gives loss of 0.1774007987875157\n",
      "The 23379 th iteration gives loss of 0.1773973686395989\n",
      "The 23380 th iteration gives loss of 0.17739393882599205\n",
      "The 23381 th iteration gives loss of 0.17739050934663564\n",
      "The 23382 th iteration gives loss of 0.17738708020147217\n",
      "The 23383 th iteration gives loss of 0.17738365139046733\n",
      "The 23384 th iteration gives loss of 0.17738022291355465\n",
      "The 23385 th iteration gives loss of 0.17737679477068646\n",
      "The 23386 th iteration gives loss of 0.1773733669618177\n",
      "The 23387 th iteration gives loss of 0.1773699394868949\n",
      "The 23388 th iteration gives loss of 0.177366512345862\n",
      "The 23389 th iteration gives loss of 0.17736308553867075\n",
      "The 23390 th iteration gives loss of 0.17735965906527357\n",
      "The 23391 th iteration gives loss of 0.17735623292561326\n",
      "The 23392 th iteration gives loss of 0.17735280711963428\n",
      "The 23393 th iteration gives loss of 0.17734938164730446\n",
      "The 23394 th iteration gives loss of 0.17734595650855742\n",
      "The 23395 th iteration gives loss of 0.17734253170333403\n",
      "The 23396 th iteration gives loss of 0.17733910723160284\n",
      "The 23397 th iteration gives loss of 0.17733568309329834\n",
      "The 23398 th iteration gives loss of 0.17733225928837676\n",
      "The 23399 th iteration gives loss of 0.1773288358167869\n",
      "The 23400 th iteration gives loss of 0.1773254126784799\n",
      "The 23401 th iteration gives loss of 0.17732198987339678\n",
      "The 23402 th iteration gives loss of 0.17731856740148658\n",
      "The 23403 th iteration gives loss of 0.1773151452627074\n",
      "The 23404 th iteration gives loss of 0.17731172345699672\n",
      "The 23405 th iteration gives loss of 0.17730830198430608\n",
      "The 23406 th iteration gives loss of 0.1773048808445979\n",
      "The 23407 th iteration gives loss of 0.17730146003780223\n",
      "The 23408 th iteration gives loss of 0.17729803956388118\n",
      "The 23409 th iteration gives loss of 0.1772946194227821\n",
      "The 23410 th iteration gives loss of 0.17729119961444129\n",
      "The 23411 th iteration gives loss of 0.17728778013881943\n",
      "The 23412 th iteration gives loss of 0.17728436099586828\n",
      "The 23413 th iteration gives loss of 0.177280942185529\n",
      "The 23414 th iteration gives loss of 0.17727752370775188\n",
      "The 23415 th iteration gives loss of 0.17727410556248635\n",
      "The 23416 th iteration gives loss of 0.17727068774967944\n",
      "The 23417 th iteration gives loss of 0.1772672702692871\n",
      "The 23418 th iteration gives loss of 0.17726385312125256\n",
      "The 23419 th iteration gives loss of 0.17726043630553\n",
      "The 23420 th iteration gives loss of 0.17725701982206526\n",
      "The 23421 th iteration gives loss of 0.1772536036707965\n",
      "The 23422 th iteration gives loss of 0.17725018785169203\n",
      "The 23423 th iteration gives loss of 0.1772467723646924\n",
      "The 23424 th iteration gives loss of 0.17724335720974663\n",
      "The 23425 th iteration gives loss of 0.17723994238679683\n",
      "The 23426 th iteration gives loss of 0.17723652789580385\n",
      "The 23427 th iteration gives loss of 0.17723311373671216\n",
      "The 23428 th iteration gives loss of 0.17722969990947215\n",
      "The 23429 th iteration gives loss of 0.17722628641402116\n",
      "The 23430 th iteration gives loss of 0.17722287325032515\n",
      "The 23431 th iteration gives loss of 0.17721946041833112\n",
      "The 23432 th iteration gives loss of 0.1772160479179821\n",
      "The 23433 th iteration gives loss of 0.17721263574922655\n",
      "The 23434 th iteration gives loss of 0.17720922391200855\n",
      "The 23435 th iteration gives loss of 0.17720581240628888\n",
      "The 23436 th iteration gives loss of 0.17720240123200925\n",
      "The 23437 th iteration gives loss of 0.17719899038912187\n",
      "The 23438 th iteration gives loss of 0.17719557987758028\n",
      "The 23439 th iteration gives loss of 0.1771921696973275\n",
      "The 23440 th iteration gives loss of 0.17718875984831595\n",
      "The 23441 th iteration gives loss of 0.17718535033049054\n",
      "The 23442 th iteration gives loss of 0.17718194114379937\n",
      "The 23443 th iteration gives loss of 0.17717853228819574\n",
      "The 23444 th iteration gives loss of 0.17717512376362993\n",
      "The 23445 th iteration gives loss of 0.17717171557005057\n",
      "The 23446 th iteration gives loss of 0.17716830770740502\n",
      "The 23447 th iteration gives loss of 0.17716490017564662\n",
      "The 23448 th iteration gives loss of 0.1771614929747221\n",
      "The 23449 th iteration gives loss of 0.17715808610457348\n",
      "The 23450 th iteration gives loss of 0.17715467956516043\n",
      "The 23451 th iteration gives loss of 0.17715127335642436\n",
      "The 23452 th iteration gives loss of 0.17714786747831684\n",
      "The 23453 th iteration gives loss of 0.17714446193079547\n",
      "The 23454 th iteration gives loss of 0.1771410567138009\n",
      "The 23455 th iteration gives loss of 0.17713765182728705\n",
      "The 23456 th iteration gives loss of 0.17713424727119445\n",
      "The 23457 th iteration gives loss of 0.1771308430454801\n",
      "The 23458 th iteration gives loss of 0.17712743915009052\n",
      "The 23459 th iteration gives loss of 0.17712403558497247\n",
      "The 23460 th iteration gives loss of 0.17712063235008718\n",
      "The 23461 th iteration gives loss of 0.1771172294453764\n",
      "The 23462 th iteration gives loss of 0.1771138268707877\n",
      "The 23463 th iteration gives loss of 0.17711042462627027\n",
      "The 23464 th iteration gives loss of 0.17710702271177498\n",
      "The 23465 th iteration gives loss of 0.17710362112724826\n",
      "The 23466 th iteration gives loss of 0.17710021987263994\n",
      "The 23467 th iteration gives loss of 0.17709681894790652\n",
      "The 23468 th iteration gives loss of 0.17709341835299508\n",
      "The 23469 th iteration gives loss of 0.17709001808785163\n",
      "The 23470 th iteration gives loss of 0.17708661815242321\n",
      "The 23471 th iteration gives loss of 0.17708321854666662\n",
      "The 23472 th iteration gives loss of 0.17707981927052224\n",
      "The 23473 th iteration gives loss of 0.1770764203239529\n",
      "The 23474 th iteration gives loss of 0.17707302170688277\n",
      "The 23475 th iteration gives loss of 0.17706962341928886\n",
      "The 23476 th iteration gives loss of 0.17706622546111633\n",
      "The 23477 th iteration gives loss of 0.17706282783230737\n",
      "The 23478 th iteration gives loss of 0.17705943053280232\n",
      "The 23479 th iteration gives loss of 0.1770560335625657\n",
      "The 23480 th iteration gives loss of 0.17705263692154605\n",
      "The 23481 th iteration gives loss of 0.1770492406096817\n",
      "The 23482 th iteration gives loss of 0.17704584462692974\n",
      "The 23483 th iteration gives loss of 0.17704244897324567\n",
      "The 23484 th iteration gives loss of 0.1770390536485647\n",
      "The 23485 th iteration gives loss of 0.17703565865284715\n",
      "The 23486 th iteration gives loss of 0.1770322639860425\n",
      "The 23487 th iteration gives loss of 0.17702886964809814\n",
      "The 23488 th iteration gives loss of 0.17702547563895926\n",
      "The 23489 th iteration gives loss of 0.17702208195858085\n",
      "The 23490 th iteration gives loss of 0.177018688606913\n",
      "The 23491 th iteration gives loss of 0.1770152955838948\n",
      "The 23492 th iteration gives loss of 0.17701190288948754\n",
      "The 23493 th iteration gives loss of 0.17700851052363692\n",
      "The 23494 th iteration gives loss of 0.17700511848630865\n",
      "The 23495 th iteration gives loss of 0.17700172677742515\n",
      "The 23496 th iteration gives loss of 0.176998335396945\n",
      "The 23497 th iteration gives loss of 0.17699494434481491\n",
      "The 23498 th iteration gives loss of 0.17699155362099425\n",
      "The 23499 th iteration gives loss of 0.1769881632254342\n",
      "The 23500 th iteration gives loss of 0.17698477315807717\n",
      "The 23501 th iteration gives loss of 0.1769813834188713\n",
      "The 23502 th iteration gives loss of 0.17697799400776512\n",
      "The 23503 th iteration gives loss of 0.17697460492471856\n",
      "The 23504 th iteration gives loss of 0.17697121616967865\n",
      "The 23505 th iteration gives loss of 0.17696782774258324\n",
      "The 23506 th iteration gives loss of 0.17696443964339229\n",
      "The 23507 th iteration gives loss of 0.17696105187205632\n",
      "The 23508 th iteration gives loss of 0.17695766442852115\n",
      "The 23509 th iteration gives loss of 0.17695427731273614\n",
      "The 23510 th iteration gives loss of 0.17695089052465243\n",
      "The 23511 th iteration gives loss of 0.17694750406422682\n",
      "The 23512 th iteration gives loss of 0.17694411793139903\n",
      "The 23513 th iteration gives loss of 0.1769407321261218\n",
      "The 23514 th iteration gives loss of 0.17693734664834196\n",
      "The 23515 th iteration gives loss of 0.17693396149801563\n",
      "The 23516 th iteration gives loss of 0.17693057667507928\n",
      "The 23517 th iteration gives loss of 0.17692719217950498\n",
      "The 23518 th iteration gives loss of 0.17692380801122223\n",
      "The 23519 th iteration gives loss of 0.17692042417019563\n",
      "The 23520 th iteration gives loss of 0.17691704065637265\n",
      "The 23521 th iteration gives loss of 0.17691365746969054\n",
      "The 23522 th iteration gives loss of 0.17691027461010334\n",
      "The 23523 th iteration gives loss of 0.1769068920775623\n",
      "The 23524 th iteration gives loss of 0.17690350987203213\n",
      "The 23525 th iteration gives loss of 0.17690012799344573\n",
      "The 23526 th iteration gives loss of 0.17689674644175366\n",
      "The 23527 th iteration gives loss of 0.17689336521692156\n",
      "The 23528 th iteration gives loss of 0.17688998431888078\n",
      "The 23529 th iteration gives loss of 0.17688660374758028\n",
      "The 23530 th iteration gives loss of 0.17688322350298982\n",
      "The 23531 th iteration gives loss of 0.1768798435850398\n",
      "The 23532 th iteration gives loss of 0.17687646399368773\n",
      "The 23533 th iteration gives loss of 0.17687308472888438\n",
      "The 23534 th iteration gives loss of 0.17686970579057676\n",
      "The 23535 th iteration gives loss of 0.1768663271787149\n",
      "The 23536 th iteration gives loss of 0.17686294889325674\n",
      "The 23537 th iteration gives loss of 0.17685957093414412\n",
      "The 23538 th iteration gives loss of 0.17685619330131888\n",
      "The 23539 th iteration gives loss of 0.17685281599475342\n",
      "The 23540 th iteration gives loss of 0.1768494390143848\n",
      "The 23541 th iteration gives loss of 0.17684606236015565\n",
      "The 23542 th iteration gives loss of 0.17684268603202485\n",
      "The 23543 th iteration gives loss of 0.1768393100299416\n",
      "The 23544 th iteration gives loss of 0.1768359343538528\n",
      "The 23545 th iteration gives loss of 0.1768325590037153\n",
      "The 23546 th iteration gives loss of 0.17682918397946942\n",
      "The 23547 th iteration gives loss of 0.17682580928107597\n",
      "The 23548 th iteration gives loss of 0.17682243490848729\n",
      "The 23549 th iteration gives loss of 0.17681906086162918\n",
      "The 23550 th iteration gives loss of 0.17681568714048063\n",
      "The 23551 th iteration gives loss of 0.17681231374497844\n",
      "The 23552 th iteration gives loss of 0.17680894067506586\n",
      "The 23553 th iteration gives loss of 0.17680556793070434\n",
      "The 23554 th iteration gives loss of 0.17680219551184412\n",
      "The 23555 th iteration gives loss of 0.17679882341842118\n",
      "The 23556 th iteration gives loss of 0.17679545165040622\n",
      "The 23557 th iteration gives loss of 0.17679208020773302\n",
      "The 23558 th iteration gives loss of 0.1767887090903677\n",
      "The 23559 th iteration gives loss of 0.17678533829823898\n",
      "The 23560 th iteration gives loss of 0.17678196783131161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 23561 th iteration gives loss of 0.17677859768953386\n",
      "The 23562 th iteration gives loss of 0.17677522787285865\n",
      "The 23563 th iteration gives loss of 0.17677185838122828\n",
      "The 23564 th iteration gives loss of 0.17676848921459387\n",
      "The 23565 th iteration gives loss of 0.17676512037290656\n",
      "The 23566 th iteration gives loss of 0.17676175185612206\n",
      "The 23567 th iteration gives loss of 0.17675838366418595\n",
      "The 23568 th iteration gives loss of 0.176755015797055\n",
      "The 23569 th iteration gives loss of 0.1767516482546656\n",
      "The 23570 th iteration gives loss of 0.17674828103697748\n",
      "The 23571 th iteration gives loss of 0.17674491414394314\n",
      "The 23572 th iteration gives loss of 0.1767415475755031\n",
      "The 23573 th iteration gives loss of 0.17673818133162042\n",
      "The 23574 th iteration gives loss of 0.17673481541223024\n",
      "The 23575 th iteration gives loss of 0.17673144981729755\n",
      "The 23576 th iteration gives loss of 0.17672808454676278\n",
      "The 23577 th iteration gives loss of 0.17672471960058125\n",
      "The 23578 th iteration gives loss of 0.17672135497870511\n",
      "The 23579 th iteration gives loss of 0.17671799068106844\n",
      "The 23580 th iteration gives loss of 0.1767146267076402\n",
      "The 23581 th iteration gives loss of 0.17671126305836982\n",
      "The 23582 th iteration gives loss of 0.17670789973320394\n",
      "The 23583 th iteration gives loss of 0.1767045367320833\n",
      "The 23584 th iteration gives loss of 0.17670117405496577\n",
      "The 23585 th iteration gives loss of 0.1766978117018108\n",
      "The 23586 th iteration gives loss of 0.17669444967255543\n",
      "The 23587 th iteration gives loss of 0.17669108796715624\n",
      "The 23588 th iteration gives loss of 0.17668772658555817\n",
      "The 23589 th iteration gives loss of 0.17668436552771488\n",
      "The 23590 th iteration gives loss of 0.17668100479358181\n",
      "The 23591 th iteration gives loss of 0.1766776443831052\n",
      "The 23592 th iteration gives loss of 0.1766742842962227\n",
      "The 23593 th iteration gives loss of 0.17667092453290467\n",
      "The 23594 th iteration gives loss of 0.17666756509308984\n",
      "The 23595 th iteration gives loss of 0.17666420597673563\n",
      "The 23596 th iteration gives loss of 0.17666084718379862\n",
      "The 23597 th iteration gives loss of 0.1766574887142141\n",
      "The 23598 th iteration gives loss of 0.1766541305679309\n",
      "The 23599 th iteration gives loss of 0.17665077274490895\n",
      "The 23600 th iteration gives loss of 0.1766474152451015\n",
      "The 23601 th iteration gives loss of 0.17664405806845004\n",
      "The 23602 th iteration gives loss of 0.1766407012149125\n",
      "The 23603 th iteration gives loss of 0.17663734468443618\n",
      "The 23604 th iteration gives loss of 0.17663398847696815\n",
      "The 23605 th iteration gives loss of 0.176630632592465\n",
      "The 23606 th iteration gives loss of 0.17662727703086992\n",
      "The 23607 th iteration gives loss of 0.17662392179214265\n",
      "The 23608 th iteration gives loss of 0.17662056687622196\n",
      "The 23609 th iteration gives loss of 0.17661721228307142\n",
      "The 23610 th iteration gives loss of 0.17661385801263116\n",
      "The 23611 th iteration gives loss of 0.1766105040648574\n",
      "The 23612 th iteration gives loss of 0.17660715043969835\n",
      "The 23613 th iteration gives loss of 0.17660379713710367\n",
      "The 23614 th iteration gives loss of 0.17660044415703363\n",
      "The 23615 th iteration gives loss of 0.17659709149942746\n",
      "The 23616 th iteration gives loss of 0.17659373916423157\n",
      "The 23617 th iteration gives loss of 0.17659038715140576\n",
      "The 23618 th iteration gives loss of 0.17658703546090362\n",
      "The 23619 th iteration gives loss of 0.17658368409267042\n",
      "The 23620 th iteration gives loss of 0.17658033304665832\n",
      "The 23621 th iteration gives loss of 0.17657698232281677\n",
      "The 23622 th iteration gives loss of 0.17657363192109338\n",
      "The 23623 th iteration gives loss of 0.17657028184144338\n",
      "The 23624 th iteration gives loss of 0.17656693208381247\n",
      "The 23625 th iteration gives loss of 0.17656358264815392\n",
      "The 23626 th iteration gives loss of 0.1765602335344213\n",
      "The 23627 th iteration gives loss of 0.176556884742563\n",
      "The 23628 th iteration gives loss of 0.17655353627253115\n",
      "The 23629 th iteration gives loss of 0.17655018812427134\n",
      "The 23630 th iteration gives loss of 0.1765468402977375\n",
      "The 23631 th iteration gives loss of 0.1765434927928837\n",
      "The 23632 th iteration gives loss of 0.17654014560966522\n",
      "The 23633 th iteration gives loss of 0.1765367987480177\n",
      "The 23634 th iteration gives loss of 0.17653345220790145\n",
      "The 23635 th iteration gives loss of 0.17653010598925808\n",
      "The 23636 th iteration gives loss of 0.1765267600920458\n",
      "The 23637 th iteration gives loss of 0.17652341451622353\n",
      "The 23638 th iteration gives loss of 0.17652006926172056\n",
      "The 23639 th iteration gives loss of 0.1765167243285136\n",
      "The 23640 th iteration gives loss of 0.17651337971653386\n",
      "The 23641 th iteration gives loss of 0.1765100354257381\n",
      "The 23642 th iteration gives loss of 0.17650669145608053\n",
      "The 23643 th iteration gives loss of 0.17650334780750068\n",
      "The 23644 th iteration gives loss of 0.17650000447997224\n",
      "The 23645 th iteration gives loss of 0.17649666147342094\n",
      "The 23646 th iteration gives loss of 0.1764933187878119\n",
      "The 23647 th iteration gives loss of 0.17648997642307937\n",
      "The 23648 th iteration gives loss of 0.1764866343791974\n",
      "The 23649 th iteration gives loss of 0.17648329265610482\n",
      "The 23650 th iteration gives loss of 0.17647995125374805\n",
      "The 23651 th iteration gives loss of 0.1764766101720915\n",
      "The 23652 th iteration gives loss of 0.1764732694110785\n",
      "The 23653 th iteration gives loss of 0.17646992897065167\n",
      "The 23654 th iteration gives loss of 0.17646658885077576\n",
      "The 23655 th iteration gives loss of 0.17646324905139696\n",
      "The 23656 th iteration gives loss of 0.17645990957246405\n",
      "The 23657 th iteration gives loss of 0.17645657041392704\n",
      "The 23658 th iteration gives loss of 0.17645323157573153\n",
      "The 23659 th iteration gives loss of 0.17644989305783765\n",
      "The 23660 th iteration gives loss of 0.17644655486019833\n",
      "The 23661 th iteration gives loss of 0.17644321698276022\n",
      "The 23662 th iteration gives loss of 0.17643987942547146\n",
      "The 23663 th iteration gives loss of 0.17643654218828977\n",
      "The 23664 th iteration gives loss of 0.17643320527116066\n",
      "The 23665 th iteration gives loss of 0.17642986867403407\n",
      "The 23666 th iteration gives loss of 0.1764265323968613\n",
      "The 23667 th iteration gives loss of 0.17642319643959534\n",
      "The 23668 th iteration gives loss of 0.17641986080218533\n",
      "The 23669 th iteration gives loss of 0.17641652548458764\n",
      "The 23670 th iteration gives loss of 0.1764131904867499\n",
      "The 23671 th iteration gives loss of 0.17640985580862234\n",
      "The 23672 th iteration gives loss of 0.1764065214501538\n",
      "The 23673 th iteration gives loss of 0.17640318741129898\n",
      "The 23674 th iteration gives loss of 0.17639985369200908\n",
      "The 23675 th iteration gives loss of 0.17639652029223096\n",
      "The 23676 th iteration gives loss of 0.17639318721192185\n",
      "The 23677 th iteration gives loss of 0.17638985445102998\n",
      "The 23678 th iteration gives loss of 0.17638652200950758\n",
      "The 23679 th iteration gives loss of 0.1763831898872984\n",
      "The 23680 th iteration gives loss of 0.17637985808436335\n",
      "The 23681 th iteration gives loss of 0.17637652660064468\n",
      "The 23682 th iteration gives loss of 0.17637319543609098\n",
      "The 23683 th iteration gives loss of 0.1763698645906747\n",
      "The 23684 th iteration gives loss of 0.17636653406432434\n",
      "The 23685 th iteration gives loss of 0.17636320385700804\n",
      "The 23686 th iteration gives loss of 0.17635987396866273\n",
      "The 23687 th iteration gives loss of 0.17635654439923928\n",
      "The 23688 th iteration gives loss of 0.17635321514869345\n",
      "The 23689 th iteration gives loss of 0.17634988621698708\n",
      "The 23690 th iteration gives loss of 0.1763465576040557\n",
      "The 23691 th iteration gives loss of 0.17634322930986035\n",
      "The 23692 th iteration gives loss of 0.1763399013343379\n",
      "The 23693 th iteration gives loss of 0.17633657367746255\n",
      "The 23694 th iteration gives loss of 0.17633324633917483\n",
      "The 23695 th iteration gives loss of 0.17632991931941408\n",
      "The 23696 th iteration gives loss of 0.17632659261813322\n",
      "The 23697 th iteration gives loss of 0.17632326623530525\n",
      "The 23698 th iteration gives loss of 0.17631994017085728\n",
      "The 23699 th iteration gives loss of 0.1763166144247684\n",
      "The 23700 th iteration gives loss of 0.1763132889969533\n",
      "The 23701 th iteration gives loss of 0.1763099638873899\n",
      "The 23702 th iteration gives loss of 0.17630663909601935\n",
      "The 23703 th iteration gives loss of 0.17630331462279833\n",
      "The 23704 th iteration gives loss of 0.17629999046767741\n",
      "The 23705 th iteration gives loss of 0.17629666663059848\n",
      "The 23706 th iteration gives loss of 0.17629334311151798\n",
      "The 23707 th iteration gives loss of 0.17629001991039248\n",
      "The 23708 th iteration gives loss of 0.17628669702716884\n",
      "The 23709 th iteration gives loss of 0.17628337446179776\n",
      "The 23710 th iteration gives loss of 0.17628005221422985\n",
      "The 23711 th iteration gives loss of 0.17627673028442306\n",
      "The 23712 th iteration gives loss of 0.17627340867233174\n",
      "The 23713 th iteration gives loss of 0.17627008737788083\n",
      "The 23714 th iteration gives loss of 0.17626676640105518\n",
      "The 23715 th iteration gives loss of 0.1762634457417822\n",
      "The 23716 th iteration gives loss of 0.17626012540002475\n",
      "The 23717 th iteration gives loss of 0.17625680537573624\n",
      "The 23718 th iteration gives loss of 0.17625348566885626\n",
      "The 23719 th iteration gives loss of 0.17625016627934761\n",
      "The 23720 th iteration gives loss of 0.17624684720716138\n",
      "The 23721 th iteration gives loss of 0.17624352845223942\n",
      "The 23722 th iteration gives loss of 0.17624021001453638\n",
      "The 23723 th iteration gives loss of 0.1762368918940134\n",
      "The 23724 th iteration gives loss of 0.17623357409061335\n",
      "The 23725 th iteration gives loss of 0.17623025660427885\n",
      "The 23726 th iteration gives loss of 0.17622693943497753\n",
      "The 23727 th iteration gives loss of 0.17622362258265117\n",
      "The 23728 th iteration gives loss of 0.17622030604726158\n",
      "The 23729 th iteration gives loss of 0.1762169898287522\n",
      "The 23730 th iteration gives loss of 0.17621367392706294\n",
      "The 23731 th iteration gives loss of 0.1762103583421699\n",
      "The 23732 th iteration gives loss of 0.17620704307400673\n",
      "The 23733 th iteration gives loss of 0.1762037281225374\n",
      "The 23734 th iteration gives loss of 0.1762004134877004\n",
      "The 23735 th iteration gives loss of 0.1761970991694616\n",
      "The 23736 th iteration gives loss of 0.17619378516774772\n",
      "The 23737 th iteration gives loss of 0.1761904714825422\n",
      "The 23738 th iteration gives loss of 0.17618715811377297\n",
      "The 23739 th iteration gives loss of 0.17618384506140408\n",
      "The 23740 th iteration gives loss of 0.1761805323253808\n",
      "The 23741 th iteration gives loss of 0.17617721990565213\n",
      "The 23742 th iteration gives loss of 0.17617390780216644\n",
      "The 23743 th iteration gives loss of 0.17617059601489485\n",
      "The 23744 th iteration gives loss of 0.17616728454377686\n",
      "The 23745 th iteration gives loss of 0.17616397338876835\n",
      "The 23746 th iteration gives loss of 0.17616066254981305\n",
      "The 23747 th iteration gives loss of 0.17615735202685667\n",
      "The 23748 th iteration gives loss of 0.1761540418198688\n",
      "The 23749 th iteration gives loss of 0.1761507319287885\n",
      "The 23750 th iteration gives loss of 0.17614742235357692\n",
      "The 23751 th iteration gives loss of 0.17614411309417283\n",
      "The 23752 th iteration gives loss of 0.17614080415053185\n",
      "The 23753 th iteration gives loss of 0.1761374955226142\n",
      "The 23754 th iteration gives loss of 0.17613418721036364\n",
      "The 23755 th iteration gives loss of 0.17613087921374118\n",
      "The 23756 th iteration gives loss of 0.17612757153268835\n",
      "The 23757 th iteration gives loss of 0.176124264167159\n",
      "The 23758 th iteration gives loss of 0.17612095711710188\n",
      "The 23759 th iteration gives loss of 0.1761176503824777\n",
      "The 23760 th iteration gives loss of 0.17611434396322037\n",
      "The 23761 th iteration gives loss of 0.17611103785929905\n",
      "The 23762 th iteration gives loss of 0.17610773207067207\n",
      "The 23763 th iteration gives loss of 0.1761044265972765\n",
      "The 23764 th iteration gives loss of 0.17610112143906076\n",
      "The 23765 th iteration gives loss of 0.17609781659598422\n",
      "The 23766 th iteration gives loss of 0.1760945120680035\n",
      "The 23767 th iteration gives loss of 0.17609120785505214\n",
      "The 23768 th iteration gives loss of 0.1760879039570999\n",
      "The 23769 th iteration gives loss of 0.17608460037409485\n",
      "The 23770 th iteration gives loss of 0.1760812971059798\n",
      "The 23771 th iteration gives loss of 0.17607799415271877\n",
      "The 23772 th iteration gives loss of 0.17607469151425775\n",
      "The 23773 th iteration gives loss of 0.17607138919053786\n",
      "The 23774 th iteration gives loss of 0.176068087181536\n",
      "The 23775 th iteration gives loss of 0.1760647854871819\n",
      "The 23776 th iteration gives loss of 0.17606148410743613\n",
      "The 23777 th iteration gives loss of 0.17605818304224777\n",
      "The 23778 th iteration gives loss of 0.1760548822915742\n",
      "The 23779 th iteration gives loss of 0.17605158185535896\n",
      "The 23780 th iteration gives loss of 0.17604828173355003\n",
      "The 23781 th iteration gives loss of 0.17604498192612456\n",
      "The 23782 th iteration gives loss of 0.17604168243301083\n",
      "The 23783 th iteration gives loss of 0.17603838325416196\n",
      "The 23784 th iteration gives loss of 0.1760350843895356\n",
      "The 23785 th iteration gives loss of 0.1760317858390841\n",
      "The 23786 th iteration gives loss of 0.1760284876027628\n",
      "The 23787 th iteration gives loss of 0.1760251896805141\n",
      "The 23788 th iteration gives loss of 0.17602189207229638\n",
      "The 23789 th iteration gives loss of 0.17601859477805978\n",
      "The 23790 th iteration gives loss of 0.17601529779775915\n",
      "The 23791 th iteration gives loss of 0.17601200113133522\n",
      "The 23792 th iteration gives loss of 0.1760087047787502\n",
      "The 23793 th iteration gives loss of 0.17600540873995427\n",
      "The 23794 th iteration gives loss of 0.17600211301490576\n",
      "The 23795 th iteration gives loss of 0.17599881760353703\n",
      "The 23796 th iteration gives loss of 0.17599552250582096\n",
      "The 23797 th iteration gives loss of 0.1759922277217044\n",
      "The 23798 th iteration gives loss of 0.17598893325113\n",
      "The 23799 th iteration gives loss of 0.17598563909405168\n",
      "The 23800 th iteration gives loss of 0.1759823452504331\n",
      "The 23801 th iteration gives loss of 0.17597905172021724\n",
      "The 23802 th iteration gives loss of 0.17597575850335784\n",
      "The 23803 th iteration gives loss of 0.17597246559980978\n",
      "The 23804 th iteration gives loss of 0.1759691730095226\n",
      "The 23805 th iteration gives loss of 0.17596588073244498\n",
      "The 23806 th iteration gives loss of 0.17596258876852877\n",
      "The 23807 th iteration gives loss of 0.17595929711773073\n",
      "The 23808 th iteration gives loss of 0.17595600577999862\n",
      "The 23809 th iteration gives loss of 0.1759527147552967\n",
      "The 23810 th iteration gives loss of 0.17594942404355887\n",
      "The 23811 th iteration gives loss of 0.17594613364474745\n",
      "The 23812 th iteration gives loss of 0.1759428435588081\n",
      "The 23813 th iteration gives loss of 0.17593955378570053\n",
      "The 23814 th iteration gives loss of 0.17593626432537643\n",
      "The 23815 th iteration gives loss of 0.1759329751777838\n",
      "The 23816 th iteration gives loss of 0.17592968634287606\n",
      "The 23817 th iteration gives loss of 0.17592639782060676\n",
      "The 23818 th iteration gives loss of 0.17592310961092508\n",
      "The 23819 th iteration gives loss of 0.17591982171379006\n",
      "The 23820 th iteration gives loss of 0.17591653412914013\n",
      "The 23821 th iteration gives loss of 0.17591324685693407\n",
      "The 23822 th iteration gives loss of 0.17590995989713193\n",
      "The 23823 th iteration gives loss of 0.1759066732496804\n",
      "The 23824 th iteration gives loss of 0.17590338691452403\n",
      "The 23825 th iteration gives loss of 0.1759001008916256\n",
      "The 23826 th iteration gives loss of 0.17589681518093828\n",
      "The 23827 th iteration gives loss of 0.17589352978239997\n",
      "The 23828 th iteration gives loss of 0.1758902446959707\n",
      "The 23829 th iteration gives loss of 0.17588695992161765\n",
      "The 23830 th iteration gives loss of 0.17588367545927724\n",
      "The 23831 th iteration gives loss of 0.17588039130889357\n",
      "The 23832 th iteration gives loss of 0.17587710747043453\n",
      "The 23833 th iteration gives loss of 0.17587382394384538\n",
      "The 23834 th iteration gives loss of 0.17587054072908626\n",
      "The 23835 th iteration gives loss of 0.1758672578261064\n",
      "The 23836 th iteration gives loss of 0.17586397523484415\n",
      "The 23837 th iteration gives loss of 0.17586069295526305\n",
      "The 23838 th iteration gives loss of 0.17585741098731666\n",
      "The 23839 th iteration gives loss of 0.17585412933096045\n",
      "The 23840 th iteration gives loss of 0.17585084798613587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 23841 th iteration gives loss of 0.17584756695280077\n",
      "The 23842 th iteration gives loss of 0.17584428623091786\n",
      "The 23843 th iteration gives loss of 0.17584100582042153\n",
      "The 23844 th iteration gives loss of 0.17583772572127312\n",
      "The 23845 th iteration gives loss of 0.17583444593342804\n",
      "The 23846 th iteration gives loss of 0.1758311664568216\n",
      "The 23847 th iteration gives loss of 0.1758278872914283\n",
      "The 23848 th iteration gives loss of 0.1758246084371879\n",
      "The 23849 th iteration gives loss of 0.17582132989404847\n",
      "The 23850 th iteration gives loss of 0.17581805166198144\n",
      "The 23851 th iteration gives loss of 0.17581477374093352\n",
      "The 23852 th iteration gives loss of 0.17581149613083902\n",
      "The 23853 th iteration gives loss of 0.1758082188316574\n",
      "The 23854 th iteration gives loss of 0.1758049418433513\n",
      "The 23855 th iteration gives loss of 0.17580166516587564\n",
      "The 23856 th iteration gives loss of 0.17579838879916612\n",
      "The 23857 th iteration gives loss of 0.175795112743192\n",
      "The 23858 th iteration gives loss of 0.17579183699789788\n",
      "The 23859 th iteration gives loss of 0.17578856156322445\n",
      "The 23860 th iteration gives loss of 0.1757852864391404\n",
      "The 23861 th iteration gives loss of 0.1757820116255891\n",
      "The 23862 th iteration gives loss of 0.17577873712252925\n",
      "The 23863 th iteration gives loss of 0.17577546292991258\n",
      "The 23864 th iteration gives loss of 0.17577218904769903\n",
      "The 23865 th iteration gives loss of 0.1757689154758235\n",
      "The 23866 th iteration gives loss of 0.17576564221424532\n",
      "The 23867 th iteration gives loss of 0.17576236926292163\n",
      "The 23868 th iteration gives loss of 0.1757590966218039\n",
      "The 23869 th iteration gives loss of 0.17575582429084155\n",
      "The 23870 th iteration gives loss of 0.17575255226999348\n",
      "The 23871 th iteration gives loss of 0.17574928055919742\n",
      "The 23872 th iteration gives loss of 0.17574600915841923\n",
      "The 23873 th iteration gives loss of 0.17574273806760804\n",
      "The 23874 th iteration gives loss of 0.17573946728672124\n",
      "The 23875 th iteration gives loss of 0.17573619681569838\n",
      "The 23876 th iteration gives loss of 0.17573292665450255\n",
      "The 23877 th iteration gives loss of 0.17572965680308408\n",
      "The 23878 th iteration gives loss of 0.17572638726139417\n",
      "The 23879 th iteration gives loss of 0.175723118029394\n",
      "The 23880 th iteration gives loss of 0.17571984910702557\n",
      "The 23881 th iteration gives loss of 0.17571658049423217\n",
      "The 23882 th iteration gives loss of 0.17571331219099073\n",
      "The 23883 th iteration gives loss of 0.17571004419723796\n",
      "The 23884 th iteration gives loss of 0.17570677651293137\n",
      "The 23885 th iteration gives loss of 0.175703509138027\n",
      "The 23886 th iteration gives loss of 0.1757002420724615\n",
      "The 23887 th iteration gives loss of 0.17569697531620804\n",
      "The 23888 th iteration gives loss of 0.175693708869205\n",
      "The 23889 th iteration gives loss of 0.1756904427314172\n",
      "The 23890 th iteration gives loss of 0.17568717690278243\n",
      "The 23891 th iteration gives loss of 0.17568391138325784\n",
      "The 23892 th iteration gives loss of 0.17568064617280937\n",
      "The 23893 th iteration gives loss of 0.17567738127137367\n",
      "The 23894 th iteration gives loss of 0.17567411667891056\n",
      "The 23895 th iteration gives loss of 0.1756708523953706\n",
      "The 23896 th iteration gives loss of 0.17566758842070868\n",
      "The 23897 th iteration gives loss of 0.1756643247548729\n",
      "The 23898 th iteration gives loss of 0.17566106139783078\n",
      "The 23899 th iteration gives loss of 0.17565779834951334\n",
      "The 23900 th iteration gives loss of 0.17565453560988153\n",
      "The 23901 th iteration gives loss of 0.1756512731789015\n",
      "The 23902 th iteration gives loss of 0.17564801105649808\n",
      "The 23903 th iteration gives loss of 0.17564474924265452\n",
      "The 23904 th iteration gives loss of 0.1756414877373054\n",
      "The 23905 th iteration gives loss of 0.17563822654040936\n",
      "The 23906 th iteration gives loss of 0.17563496565191372\n",
      "The 23907 th iteration gives loss of 0.17563170507177728\n",
      "The 23908 th iteration gives loss of 0.17562844479995274\n",
      "The 23909 th iteration gives loss of 0.17562518483638517\n",
      "The 23910 th iteration gives loss of 0.17562192518103384\n",
      "The 23911 th iteration gives loss of 0.17561866583385116\n",
      "The 23912 th iteration gives loss of 0.17561540679479662\n",
      "The 23913 th iteration gives loss of 0.17561214806380965\n",
      "The 23914 th iteration gives loss of 0.17560888964084245\n",
      "The 23915 th iteration gives loss of 0.17560563152586572\n",
      "The 23916 th iteration gives loss of 0.1756023737188184\n",
      "The 23917 th iteration gives loss of 0.1755991162196608\n",
      "The 23918 th iteration gives loss of 0.17559585902833025\n",
      "The 23919 th iteration gives loss of 0.17559260214479186\n",
      "The 23920 th iteration gives loss of 0.1755893455690083\n",
      "The 23921 th iteration gives loss of 0.17558608930090883\n",
      "The 23922 th iteration gives loss of 0.1755828333404638\n",
      "The 23923 th iteration gives loss of 0.17557957768762256\n",
      "The 23924 th iteration gives loss of 0.1755763223423433\n",
      "The 23925 th iteration gives loss of 0.1755730673045612\n",
      "The 23926 th iteration gives loss of 0.17556981257423915\n",
      "The 23927 th iteration gives loss of 0.1755665581513357\n",
      "The 23928 th iteration gives loss of 0.1755633040358047\n",
      "The 23929 th iteration gives loss of 0.17556005022758492\n",
      "The 23930 th iteration gives loss of 0.17555679672664212\n",
      "The 23931 th iteration gives loss of 0.17555354353292535\n",
      "The 23932 th iteration gives loss of 0.17555029064638583\n",
      "The 23933 th iteration gives loss of 0.17554703806697589\n",
      "The 23934 th iteration gives loss of 0.17554378579465188\n",
      "The 23935 th iteration gives loss of 0.1755405338293734\n",
      "The 23936 th iteration gives loss of 0.1755372821710774\n",
      "The 23937 th iteration gives loss of 0.17553403081972793\n",
      "The 23938 th iteration gives loss of 0.17553077977527276\n",
      "The 23939 th iteration gives loss of 0.17552752903767255\n",
      "The 23940 th iteration gives loss of 0.17552427860686973\n",
      "The 23941 th iteration gives loss of 0.17552102848282272\n",
      "The 23942 th iteration gives loss of 0.17551777866548074\n",
      "The 23943 th iteration gives loss of 0.17551452915480495\n",
      "The 23944 th iteration gives loss of 0.17551127995074287\n",
      "The 23945 th iteration gives loss of 0.17550803105325088\n",
      "The 23946 th iteration gives loss of 0.17550478246228315\n",
      "The 23947 th iteration gives loss of 0.17550153417778788\n",
      "The 23948 th iteration gives loss of 0.1754982861997147\n",
      "The 23949 th iteration gives loss of 0.17549503852801926\n",
      "The 23950 th iteration gives loss of 0.1754917911626604\n",
      "The 23951 th iteration gives loss of 0.17548854410359363\n",
      "The 23952 th iteration gives loss of 0.17548529735076399\n",
      "The 23953 th iteration gives loss of 0.17548205090412403\n",
      "The 23954 th iteration gives loss of 0.1754788047636356\n",
      "The 23955 th iteration gives loss of 0.17547555892924302\n",
      "The 23956 th iteration gives loss of 0.17547231340089722\n",
      "The 23957 th iteration gives loss of 0.17546906817856467\n",
      "The 23958 th iteration gives loss of 0.17546582326219515\n",
      "The 23959 th iteration gives loss of 0.17546257865172118\n",
      "The 23960 th iteration gives loss of 0.17545933434712077\n",
      "The 23961 th iteration gives loss of 0.1754560903483313\n",
      "The 23962 th iteration gives loss of 0.1754528466553247\n",
      "The 23963 th iteration gives loss of 0.17544960326803433\n",
      "The 23964 th iteration gives loss of 0.17544636018641543\n",
      "The 23965 th iteration gives loss of 0.17544311741043875\n",
      "The 23966 th iteration gives loss of 0.1754398749400422\n",
      "The 23967 th iteration gives loss of 0.17543663277518887\n",
      "The 23968 th iteration gives loss of 0.17543339091581858\n",
      "The 23969 th iteration gives loss of 0.17543014936189008\n",
      "The 23970 th iteration gives loss of 0.17542690811336165\n",
      "The 23971 th iteration gives loss of 0.1754236671701788\n",
      "The 23972 th iteration gives loss of 0.17542042653230358\n",
      "The 23973 th iteration gives loss of 0.1754171861996863\n",
      "The 23974 th iteration gives loss of 0.17541394617227624\n",
      "The 23975 th iteration gives loss of 0.17541070645003026\n",
      "The 23976 th iteration gives loss of 0.17540746703290544\n",
      "The 23977 th iteration gives loss of 0.17540422792084126\n",
      "The 23978 th iteration gives loss of 0.17540098911380908\n",
      "The 23979 th iteration gives loss of 0.17539775061174764\n",
      "The 23980 th iteration gives loss of 0.1753945124146181\n",
      "The 23981 th iteration gives loss of 0.1753912745223664\n",
      "The 23982 th iteration gives loss of 0.17538803693495358\n",
      "The 23983 th iteration gives loss of 0.17538479965233555\n",
      "The 23984 th iteration gives loss of 0.1753815626744536\n",
      "The 23985 th iteration gives loss of 0.17537832600126746\n",
      "The 23986 th iteration gives loss of 0.17537508963273465\n",
      "The 23987 th iteration gives loss of 0.1753718535688029\n",
      "The 23988 th iteration gives loss of 0.1753686178094276\n",
      "The 23989 th iteration gives loss of 0.17536538235456203\n",
      "The 23990 th iteration gives loss of 0.17536214720416343\n",
      "The 23991 th iteration gives loss of 0.17535891235818205\n",
      "The 23992 th iteration gives loss of 0.17535567781655925\n",
      "The 23993 th iteration gives loss of 0.17535244357926644\n",
      "The 23994 th iteration gives loss of 0.1753492096462547\n",
      "The 23995 th iteration gives loss of 0.1753459760174717\n",
      "The 23996 th iteration gives loss of 0.17534274269287586\n",
      "The 23997 th iteration gives loss of 0.17533950967240733\n",
      "The 23998 th iteration gives loss of 0.17533627695604057\n",
      "The 23999 th iteration gives loss of 0.17533304454371057\n",
      "The 24000 th iteration gives loss of 0.17532981243538404\n",
      "The 24001 th iteration gives loss of 0.17532658063100015\n",
      "The 24002 th iteration gives loss of 0.17532334913052194\n",
      "The 24003 th iteration gives loss of 0.17532011793390292\n",
      "The 24004 th iteration gives loss of 0.1753168870410937\n",
      "The 24005 th iteration gives loss of 0.1753136564520484\n",
      "The 24006 th iteration gives loss of 0.17531042616672562\n",
      "The 24007 th iteration gives loss of 0.17530719618507565\n",
      "The 24008 th iteration gives loss of 0.1753039665070571\n",
      "The 24009 th iteration gives loss of 0.17530073713260252\n",
      "The 24010 th iteration gives loss of 0.17529750806168803\n",
      "The 24011 th iteration gives loss of 0.17529427929426084\n",
      "The 24012 th iteration gives loss of 0.17529105083027027\n",
      "The 24013 th iteration gives loss of 0.17528782266967644\n",
      "The 24014 th iteration gives loss of 0.17528459481242795\n",
      "The 24015 th iteration gives loss of 0.17528136725848054\n",
      "The 24016 th iteration gives loss of 0.1752781400077776\n",
      "The 24017 th iteration gives loss of 0.17527491306029086\n",
      "The 24018 th iteration gives loss of 0.17527168641597096\n",
      "The 24019 th iteration gives loss of 0.17526846007476346\n",
      "The 24020 th iteration gives loss of 0.17526523403661526\n",
      "The 24021 th iteration gives loss of 0.17526200830148822\n",
      "The 24022 th iteration gives loss of 0.17525878286934443\n",
      "The 24023 th iteration gives loss of 0.17525555774012347\n",
      "The 24024 th iteration gives loss of 0.17525233291378206\n",
      "The 24025 th iteration gives loss of 0.17524910839028582\n",
      "The 24026 th iteration gives loss of 0.17524588416957798\n",
      "The 24027 th iteration gives loss of 0.17524266025161164\n",
      "The 24028 th iteration gives loss of 0.17523943663634303\n",
      "The 24029 th iteration gives loss of 0.17523621332373152\n",
      "The 24030 th iteration gives loss of 0.17523299031372233\n",
      "The 24031 th iteration gives loss of 0.17522976760626818\n",
      "The 24032 th iteration gives loss of 0.17522654520132358\n",
      "The 24033 th iteration gives loss of 0.17522332309885127\n",
      "The 24034 th iteration gives loss of 0.1752201012987964\n",
      "The 24035 th iteration gives loss of 0.17521687980111447\n",
      "The 24036 th iteration gives loss of 0.17521365860575275\n",
      "The 24037 th iteration gives loss of 0.17521043771267322\n",
      "The 24038 th iteration gives loss of 0.17520721712183898\n",
      "The 24039 th iteration gives loss of 0.1752039968331852\n",
      "The 24040 th iteration gives loss of 0.17520077684668148\n",
      "The 24041 th iteration gives loss of 0.17519755716226548\n",
      "The 24042 th iteration gives loss of 0.17519433777989607\n",
      "The 24043 th iteration gives loss of 0.1751911186995405\n",
      "The 24044 th iteration gives loss of 0.17518789992113792\n",
      "The 24045 th iteration gives loss of 0.17518468144463942\n",
      "The 24046 th iteration gives loss of 0.175181463270011\n",
      "The 24047 th iteration gives loss of 0.17517824539719729\n",
      "The 24048 th iteration gives loss of 0.17517502782615765\n",
      "The 24049 th iteration gives loss of 0.175171810556849\n",
      "The 24050 th iteration gives loss of 0.1751685935892198\n",
      "The 24051 th iteration gives loss of 0.1751653769232251\n",
      "The 24052 th iteration gives loss of 0.1751621605588146\n",
      "The 24053 th iteration gives loss of 0.17515894449594457\n",
      "The 24054 th iteration gives loss of 0.1751557287345677\n",
      "The 24055 th iteration gives loss of 0.17515251327464165\n",
      "The 24056 th iteration gives loss of 0.1751492981161196\n",
      "The 24057 th iteration gives loss of 0.175146083258948\n",
      "The 24058 th iteration gives loss of 0.17514286870309403\n",
      "The 24059 th iteration gives loss of 0.1751396544485053\n",
      "The 24060 th iteration gives loss of 0.17513644049513527\n",
      "The 24061 th iteration gives loss of 0.17513322684293528\n",
      "The 24062 th iteration gives loss of 0.17513001349186866\n",
      "The 24063 th iteration gives loss of 0.17512680044187187\n",
      "The 24064 th iteration gives loss of 0.17512358769291422\n",
      "The 24065 th iteration gives loss of 0.17512037524494398\n",
      "The 24066 th iteration gives loss of 0.1751171630979123\n",
      "The 24067 th iteration gives loss of 0.1751139512517827\n",
      "The 24068 th iteration gives loss of 0.1751107397065001\n",
      "The 24069 th iteration gives loss of 0.17510752846202204\n",
      "The 24070 th iteration gives loss of 0.17510431751830546\n",
      "The 24071 th iteration gives loss of 0.17510110687529765\n",
      "The 24072 th iteration gives loss of 0.1750978965329625\n",
      "The 24073 th iteration gives loss of 0.1750946864912377\n",
      "The 24074 th iteration gives loss of 0.1750914767500934\n",
      "The 24075 th iteration gives loss of 0.17508826730946764\n",
      "The 24076 th iteration gives loss of 0.1750850581693355\n",
      "The 24077 th iteration gives loss of 0.17508184932962947\n",
      "The 24078 th iteration gives loss of 0.17507864079031796\n",
      "The 24079 th iteration gives loss of 0.1750754325513536\n",
      "The 24080 th iteration gives loss of 0.17507222461268704\n",
      "The 24081 th iteration gives loss of 0.175069016974263\n",
      "The 24082 th iteration gives loss of 0.17506580963604632\n",
      "The 24083 th iteration gives loss of 0.1750626025980004\n",
      "The 24084 th iteration gives loss of 0.17505939586006414\n",
      "The 24085 th iteration gives loss of 0.17505618942219547\n",
      "The 24086 th iteration gives loss of 0.17505298328435168\n",
      "The 24087 th iteration gives loss of 0.1750497774464799\n",
      "The 24088 th iteration gives loss of 0.17504657190853806\n",
      "The 24089 th iteration gives loss of 0.17504336667048645\n",
      "The 24090 th iteration gives loss of 0.17504016173227016\n",
      "The 24091 th iteration gives loss of 0.17503695709385003\n",
      "The 24092 th iteration gives loss of 0.17503375275518063\n",
      "The 24093 th iteration gives loss of 0.17503054871620433\n",
      "The 24094 th iteration gives loss of 0.17502734497688496\n",
      "The 24095 th iteration gives loss of 0.1750241415371798\n",
      "The 24096 th iteration gives loss of 0.1750209383970339\n",
      "The 24097 th iteration gives loss of 0.175017735556409\n",
      "The 24098 th iteration gives loss of 0.1750145330152614\n",
      "The 24099 th iteration gives loss of 0.1750113307735286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 24100 th iteration gives loss of 0.17500812883118078\n",
      "The 24101 th iteration gives loss of 0.17500492718816585\n",
      "The 24102 th iteration gives loss of 0.1750017258444409\n",
      "The 24103 th iteration gives loss of 0.17499852479995948\n",
      "The 24104 th iteration gives loss of 0.17499532405468474\n",
      "The 24105 th iteration gives loss of 0.17499212360854985\n",
      "The 24106 th iteration gives loss of 0.17498892346152883\n",
      "The 24107 th iteration gives loss of 0.1749857236135581\n",
      "The 24108 th iteration gives loss of 0.17498252406459938\n",
      "The 24109 th iteration gives loss of 0.17497932481462927\n",
      "The 24110 th iteration gives loss of 0.17497612586356562\n",
      "The 24111 th iteration gives loss of 0.17497292721138755\n",
      "The 24112 th iteration gives loss of 0.1749697288580339\n",
      "The 24113 th iteration gives loss of 0.1749665308034695\n",
      "The 24114 th iteration gives loss of 0.17496333304764275\n",
      "The 24115 th iteration gives loss of 0.1749601355905129\n",
      "The 24116 th iteration gives loss of 0.17495693843203147\n",
      "The 24117 th iteration gives loss of 0.1749537415721504\n",
      "The 24118 th iteration gives loss of 0.17495054501082624\n",
      "The 24119 th iteration gives loss of 0.17494734874801468\n",
      "The 24120 th iteration gives loss of 0.17494415278366898\n",
      "The 24121 th iteration gives loss of 0.17494095711773952\n",
      "The 24122 th iteration gives loss of 0.17493776175019013\n",
      "The 24123 th iteration gives loss of 0.17493456668096702\n",
      "The 24124 th iteration gives loss of 0.17493137191002484\n",
      "The 24125 th iteration gives loss of 0.17492817743732822\n",
      "The 24126 th iteration gives loss of 0.17492498326281508\n",
      "The 24127 th iteration gives loss of 0.17492178938645336\n",
      "The 24128 th iteration gives loss of 0.174918595808195\n",
      "The 24129 th iteration gives loss of 0.17491540252798932\n",
      "The 24130 th iteration gives loss of 0.1749122095457902\n",
      "The 24131 th iteration gives loss of 0.17490901686155808\n",
      "The 24132 th iteration gives loss of 0.1749058244752454\n",
      "The 24133 th iteration gives loss of 0.1749026323868055\n",
      "The 24134 th iteration gives loss of 0.17489944059618717\n",
      "The 24135 th iteration gives loss of 0.17489624910334942\n",
      "The 24136 th iteration gives loss of 0.17489305790825826\n",
      "The 24137 th iteration gives loss of 0.17488986701085757\n",
      "The 24138 th iteration gives loss of 0.17488667641109326\n",
      "The 24139 th iteration gives loss of 0.1748834861089275\n",
      "The 24140 th iteration gives loss of 0.17488029610432032\n",
      "The 24141 th iteration gives loss of 0.1748771063972224\n",
      "The 24142 th iteration gives loss of 0.17487391698758536\n",
      "The 24143 th iteration gives loss of 0.1748707278753662\n",
      "The 24144 th iteration gives loss of 0.1748675390605223\n",
      "The 24145 th iteration gives loss of 0.17486435054300525\n",
      "The 24146 th iteration gives loss of 0.17486116232275592\n",
      "The 24147 th iteration gives loss of 0.1748579743997552\n",
      "The 24148 th iteration gives loss of 0.1748547867739447\n",
      "The 24149 th iteration gives loss of 0.1748515994452813\n",
      "The 24150 th iteration gives loss of 0.1748484124137042\n",
      "The 24151 th iteration gives loss of 0.174845225679194\n",
      "The 24152 th iteration gives loss of 0.1748420392416856\n",
      "The 24153 th iteration gives loss of 0.17483885310114153\n",
      "The 24154 th iteration gives loss of 0.17483566725751595\n",
      "The 24155 th iteration gives loss of 0.17483248171075694\n",
      "The 24156 th iteration gives loss of 0.17482929646083545\n",
      "The 24157 th iteration gives loss of 0.17482611150768543\n",
      "The 24158 th iteration gives loss of 0.17482292685126916\n",
      "The 24159 th iteration gives loss of 0.1748197424915571\n",
      "The 24160 th iteration gives loss of 0.17481655842848076\n",
      "The 24161 th iteration gives loss of 0.1748133746619982\n",
      "The 24162 th iteration gives loss of 0.17481019119208147\n",
      "The 24163 th iteration gives loss of 0.17480700801866658\n",
      "The 24164 th iteration gives loss of 0.17480382514171688\n",
      "The 24165 th iteration gives loss of 0.17480064256118746\n",
      "The 24166 th iteration gives loss of 0.174797460277027\n",
      "The 24167 th iteration gives loss of 0.17479427828919933\n",
      "The 24168 th iteration gives loss of 0.1747910965976465\n",
      "The 24169 th iteration gives loss of 0.17478791520233813\n",
      "The 24170 th iteration gives loss of 0.1747847341032169\n",
      "The 24171 th iteration gives loss of 0.17478155330024578\n",
      "The 24172 th iteration gives loss of 0.17477837279337377\n",
      "The 24173 th iteration gives loss of 0.17477519258255464\n",
      "The 24174 th iteration gives loss of 0.17477201266774936\n",
      "The 24175 th iteration gives loss of 0.1747688330489146\n",
      "The 24176 th iteration gives loss of 0.17476565372598984\n",
      "The 24177 th iteration gives loss of 0.1747624746989466\n",
      "The 24178 th iteration gives loss of 0.1747592959677259\n",
      "The 24179 th iteration gives loss of 0.17475611753229803\n",
      "The 24180 th iteration gives loss of 0.1747529393926069\n",
      "The 24181 th iteration gives loss of 0.17474976154860442\n",
      "The 24182 th iteration gives loss of 0.17474658400025928\n",
      "The 24183 th iteration gives loss of 0.17474340674751124\n",
      "The 24184 th iteration gives loss of 0.17474022979031995\n",
      "The 24185 th iteration gives loss of 0.1747370531286474\n",
      "The 24186 th iteration gives loss of 0.17473387676243446\n",
      "The 24187 th iteration gives loss of 0.1747307006916457\n",
      "The 24188 th iteration gives loss of 0.17472752491623347\n",
      "The 24189 th iteration gives loss of 0.17472434943615664\n",
      "The 24190 th iteration gives loss of 0.17472117425136252\n",
      "The 24191 th iteration gives loss of 0.1747179993618195\n",
      "The 24192 th iteration gives loss of 0.17471482476747188\n",
      "The 24193 th iteration gives loss of 0.17471165046826864\n",
      "The 24194 th iteration gives loss of 0.1747084764641774\n",
      "The 24195 th iteration gives loss of 0.1747053027551449\n",
      "The 24196 th iteration gives loss of 0.17470212934112772\n",
      "The 24197 th iteration gives loss of 0.17469895622208206\n",
      "The 24198 th iteration gives loss of 0.174695783397965\n",
      "The 24199 th iteration gives loss of 0.17469261086873172\n",
      "The 24200 th iteration gives loss of 0.17468943863433006\n",
      "The 24201 th iteration gives loss of 0.17468626669472\n",
      "The 24202 th iteration gives loss of 0.17468309504985383\n",
      "The 24203 th iteration gives loss of 0.17467992369969004\n",
      "The 24204 th iteration gives loss of 0.17467675264417823\n",
      "The 24205 th iteration gives loss of 0.17467358188328438\n",
      "The 24206 th iteration gives loss of 0.17467041141694384\n",
      "The 24207 th iteration gives loss of 0.1746672412451273\n",
      "The 24208 th iteration gives loss of 0.17466407136779388\n",
      "The 24209 th iteration gives loss of 0.17466090178488877\n",
      "The 24210 th iteration gives loss of 0.17465773249636668\n",
      "The 24211 th iteration gives loss of 0.17465456350218073\n",
      "The 24212 th iteration gives loss of 0.1746513948022946\n",
      "The 24213 th iteration gives loss of 0.17464822639665178\n",
      "The 24214 th iteration gives loss of 0.17464505828521495\n",
      "The 24215 th iteration gives loss of 0.17464189046794099\n",
      "The 24216 th iteration gives loss of 0.1746387229447956\n",
      "The 24217 th iteration gives loss of 0.17463555571570424\n",
      "The 24218 th iteration gives loss of 0.17463238878064088\n",
      "The 24219 th iteration gives loss of 0.17462922213956594\n",
      "The 24220 th iteration gives loss of 0.17462605579241638\n",
      "The 24221 th iteration gives loss of 0.1746228897391636\n",
      "The 24222 th iteration gives loss of 0.17461972397975653\n",
      "The 24223 th iteration gives loss of 0.17461655851414162\n",
      "The 24224 th iteration gives loss of 0.17461339334229417\n",
      "The 24225 th iteration gives loss of 0.17461022846414556\n",
      "The 24226 th iteration gives loss of 0.1746070638796655\n",
      "The 24227 th iteration gives loss of 0.17460389958881065\n",
      "The 24228 th iteration gives loss of 0.17460073559153003\n",
      "The 24229 th iteration gives loss of 0.17459757188778346\n",
      "The 24230 th iteration gives loss of 0.17459440847752233\n",
      "The 24231 th iteration gives loss of 0.17459124536069523\n",
      "The 24232 th iteration gives loss of 0.17458808253727545\n",
      "The 24233 th iteration gives loss of 0.17458492000719358\n",
      "The 24234 th iteration gives loss of 0.1745817577704307\n",
      "The 24235 th iteration gives loss of 0.17457859582692825\n",
      "The 24236 th iteration gives loss of 0.17457543417663626\n",
      "The 24237 th iteration gives loss of 0.174572272819524\n",
      "The 24238 th iteration gives loss of 0.17456911175553322\n",
      "The 24239 th iteration gives loss of 0.17456595098462857\n",
      "The 24240 th iteration gives loss of 0.1745627905067613\n",
      "The 24241 th iteration gives loss of 0.17455963032188235\n",
      "The 24242 th iteration gives loss of 0.17455647042996272\n",
      "The 24243 th iteration gives loss of 0.17455331083094072\n",
      "The 24244 th iteration gives loss of 0.17455015152477923\n",
      "The 24245 th iteration gives loss of 0.17454699251143074\n",
      "The 24246 th iteration gives loss of 0.17454383379083996\n",
      "The 24247 th iteration gives loss of 0.1745406753629815\n",
      "The 24248 th iteration gives loss of 0.17453751722780123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 24249 th iteration gives loss of 0.1745343593852595\n",
      "The 24250 th iteration gives loss of 0.17453120183531165\n",
      "The 24251 th iteration gives loss of 0.17452804457790486\n",
      "The 24252 th iteration gives loss of 0.1745248876129999\n",
      "The 24253 th iteration gives loss of 0.17452173094055032\n",
      "The 24254 th iteration gives loss of 0.17451857456051223\n",
      "The 24255 th iteration gives loss of 0.17451541847283916\n",
      "The 24256 th iteration gives loss of 0.17451226267749054\n",
      "The 24257 th iteration gives loss of 0.17450910717441537\n",
      "The 24258 th iteration gives loss of 0.17450595196357557\n",
      "The 24259 th iteration gives loss of 0.17450279704491486\n",
      "The 24260 th iteration gives loss of 0.17449964241841012\n",
      "The 24261 th iteration gives loss of 0.1744964880840019\n",
      "The 24262 th iteration gives loss of 0.1744933340416396\n",
      "The 24263 th iteration gives loss of 0.17449018029129099\n",
      "The 24264 th iteration gives loss of 0.17448702683290213\n",
      "The 24265 th iteration gives loss of 0.1744838736664333\n",
      "The 24266 th iteration gives loss of 0.17448072079184504\n",
      "The 24267 th iteration gives loss of 0.1744775682090862\n",
      "The 24268 th iteration gives loss of 0.1744744159181166\n",
      "The 24269 th iteration gives loss of 0.17447126391888146\n",
      "The 24270 th iteration gives loss of 0.17446811221134462\n",
      "The 24271 th iteration gives loss of 0.17446496079546037\n",
      "The 24272 th iteration gives loss of 0.1744618096711815\n",
      "The 24273 th iteration gives loss of 0.17445865883847272\n",
      "The 24274 th iteration gives loss of 0.17445550829727446\n",
      "The 24275 th iteration gives loss of 0.17445235804755774\n",
      "The 24276 th iteration gives loss of 0.17444920808926934\n",
      "The 24277 th iteration gives loss of 0.1744460584223674\n",
      "The 24278 th iteration gives loss of 0.1744429090467961\n",
      "The 24279 th iteration gives loss of 0.1744397599625285\n",
      "The 24280 th iteration gives loss of 0.1744366111695071\n",
      "The 24281 th iteration gives loss of 0.17443346266768975\n",
      "The 24282 th iteration gives loss of 0.17443031445703952\n",
      "The 24283 th iteration gives loss of 0.1744271665375087\n",
      "The 24284 th iteration gives loss of 0.17442401890905054\n",
      "The 24285 th iteration gives loss of 0.1744208715716207\n",
      "The 24286 th iteration gives loss of 0.17441772452517476\n",
      "The 24287 th iteration gives loss of 0.17441457776966598\n",
      "The 24288 th iteration gives loss of 0.1744114313050486\n",
      "The 24289 th iteration gives loss of 0.17440828513128864\n",
      "The 24290 th iteration gives loss of 0.1744051392483318\n",
      "The 24291 th iteration gives loss of 0.17440199365614203\n",
      "The 24292 th iteration gives loss of 0.1743988483546606\n",
      "The 24293 th iteration gives loss of 0.17439570334385968\n",
      "The 24294 th iteration gives loss of 0.1743925586236895\n",
      "The 24295 th iteration gives loss of 0.1743894141941006\n",
      "The 24296 th iteration gives loss of 0.17438627005504326\n",
      "The 24297 th iteration gives loss of 0.17438312620649496\n",
      "The 24298 th iteration gives loss of 0.17437998264837878\n",
      "The 24299 th iteration gives loss of 0.17437683938068282\n",
      "The 24300 th iteration gives loss of 0.1743736964033523\n",
      "The 24301 th iteration gives loss of 0.1743705537163276\n",
      "The 24302 th iteration gives loss of 0.17436741131958075\n",
      "The 24303 th iteration gives loss of 0.17436426921306977\n",
      "The 24304 th iteration gives loss of 0.1743611273967421\n",
      "The 24305 th iteration gives loss of 0.17435798587054777\n",
      "The 24306 th iteration gives loss of 0.1743548446344496\n",
      "The 24307 th iteration gives loss of 0.1743517036884081\n",
      "The 24308 th iteration gives loss of 0.17434856303236843\n",
      "The 24309 th iteration gives loss of 0.1743454226662952\n",
      "The 24310 th iteration gives loss of 0.17434228259013757\n",
      "The 24311 th iteration gives loss of 0.17433914280384913\n",
      "The 24312 th iteration gives loss of 0.17433600330740204\n",
      "The 24313 th iteration gives loss of 0.1743328641007361\n",
      "The 24314 th iteration gives loss of 0.1743297251838097\n",
      "The 24315 th iteration gives loss of 0.17432658655657948\n",
      "The 24316 th iteration gives loss of 0.17432344821900173\n",
      "The 24317 th iteration gives loss of 0.17432031017103858\n",
      "The 24318 th iteration gives loss of 0.1743171724126399\n",
      "The 24319 th iteration gives loss of 0.1743140349437548\n",
      "The 24320 th iteration gives loss of 0.17431089776435266\n",
      "The 24321 th iteration gives loss of 0.17430776087437327\n",
      "The 24322 th iteration gives loss of 0.17430462427378593\n",
      "The 24323 th iteration gives loss of 0.17430148796254008\n",
      "The 24324 th iteration gives loss of 0.1742983519405858\n",
      "The 24325 th iteration gives loss of 0.1742952162079015\n",
      "The 24326 th iteration gives loss of 0.1742920807644197\n",
      "The 24327 th iteration gives loss of 0.1742889456101073\n",
      "The 24328 th iteration gives loss of 0.17428581074491795\n",
      "The 24329 th iteration gives loss of 0.17428267616879917\n",
      "The 24330 th iteration gives loss of 0.17427954188172073\n",
      "The 24331 th iteration gives loss of 0.174276407883626\n",
      "The 24332 th iteration gives loss of 0.17427327417448374\n",
      "The 24333 th iteration gives loss of 0.17427014075423253\n",
      "The 24334 th iteration gives loss of 0.1742670076228406\n",
      "The 24335 th iteration gives loss of 0.17426387478025887\n",
      "The 24336 th iteration gives loss of 0.17426074222646026\n",
      "The 24337 th iteration gives loss of 0.17425760996137443\n",
      "The 24338 th iteration gives loss of 0.1742544779849645\n",
      "The 24339 th iteration gives loss of 0.17425134629720496\n",
      "The 24340 th iteration gives loss of 0.17424821489803174\n",
      "The 24341 th iteration gives loss of 0.17424508378740608\n",
      "The 24342 th iteration gives loss of 0.17424195296528203\n",
      "The 24343 th iteration gives loss of 0.17423882243161815\n",
      "The 24344 th iteration gives loss of 0.1742356921863779\n",
      "The 24345 th iteration gives loss of 0.17423256222950423\n",
      "The 24346 th iteration gives loss of 0.17422943256095946\n",
      "The 24347 th iteration gives loss of 0.17422630318070095\n",
      "The 24348 th iteration gives loss of 0.17422317408867555\n",
      "The 24349 th iteration gives loss of 0.17422004528485127\n",
      "The 24350 th iteration gives loss of 0.17421691676917708\n",
      "The 24351 th iteration gives loss of 0.17421378854159972\n",
      "The 24352 th iteration gives loss of 0.17421066060210016\n",
      "The 24353 th iteration gives loss of 0.1742075329506114\n",
      "The 24354 th iteration gives loss of 0.17420440558709507\n",
      "The 24355 th iteration gives loss of 0.17420127851151704\n",
      "The 24356 th iteration gives loss of 0.17419815172381517\n",
      "The 24357 th iteration gives loss of 0.1741950252239739\n",
      "The 24358 th iteration gives loss of 0.17419189901192078\n",
      "The 24359 th iteration gives loss of 0.17418877308762512\n",
      "The 24360 th iteration gives loss of 0.17418564745105058\n",
      "The 24361 th iteration gives loss of 0.1741825221021346\n",
      "The 24362 th iteration gives loss of 0.1741793970408431\n",
      "The 24363 th iteration gives loss of 0.1741762722671344\n",
      "The 24364 th iteration gives loss of 0.1741731477809511\n",
      "The 24365 th iteration gives loss of 0.17417002358227504\n",
      "The 24366 th iteration gives loss of 0.17416689967103774\n",
      "The 24367 th iteration gives loss of 0.1741637760472046\n",
      "The 24368 th iteration gives loss of 0.17416065271072353\n",
      "The 24369 th iteration gives loss of 0.17415752966156853\n",
      "The 24370 th iteration gives loss of 0.17415440689969056\n",
      "The 24371 th iteration gives loss of 0.1741512844250291\n",
      "The 24372 th iteration gives loss of 0.17414816223755017\n",
      "The 24373 th iteration gives loss of 0.17414504033722153\n",
      "The 24374 th iteration gives loss of 0.17414191872399273\n",
      "The 24375 th iteration gives loss of 0.17413879739781168\n",
      "The 24376 th iteration gives loss of 0.17413567635863655\n",
      "The 24377 th iteration gives loss of 0.1741325556064282\n",
      "The 24378 th iteration gives loss of 0.1741294351411377\n",
      "The 24379 th iteration gives loss of 0.17412631496272965\n",
      "The 24380 th iteration gives loss of 0.1741231950711464\n",
      "The 24381 th iteration gives loss of 0.17412007546636427\n",
      "The 24382 th iteration gives loss of 0.17411695614832579\n",
      "The 24383 th iteration gives loss of 0.17411383711699194\n",
      "The 24384 th iteration gives loss of 0.17411071837230732\n",
      "The 24385 th iteration gives loss of 0.1741075999142342\n",
      "The 24386 th iteration gives loss of 0.17410448174274676\n",
      "The 24387 th iteration gives loss of 0.17410136385777475\n",
      "The 24388 th iteration gives loss of 0.17409824625928844\n",
      "The 24389 th iteration gives loss of 0.1740951289472438\n",
      "The 24390 th iteration gives loss of 0.1740920119215884\n",
      "The 24391 th iteration gives loss of 0.17408889518228865\n",
      "The 24392 th iteration gives loss of 0.17408577872930328\n",
      "The 24393 th iteration gives loss of 0.17408266256258037\n",
      "The 24394 th iteration gives loss of 0.17407954668206735\n",
      "The 24395 th iteration gives loss of 0.1740764310877399\n",
      "The 24396 th iteration gives loss of 0.17407331577954085\n",
      "The 24397 th iteration gives loss of 0.17407020075744048\n",
      "The 24398 th iteration gives loss of 0.17406708602137486\n",
      "The 24399 th iteration gives loss of 0.17406397157131656\n",
      "The 24400 th iteration gives loss of 0.1740608574072136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 24401 th iteration gives loss of 0.1740577435290326\n",
      "The 24402 th iteration gives loss of 0.17405462993671186\n",
      "The 24403 th iteration gives loss of 0.1740515166302236\n",
      "The 24404 th iteration gives loss of 0.17404840360952492\n",
      "The 24405 th iteration gives loss of 0.17404529087455545\n",
      "The 24406 th iteration gives loss of 0.17404217842529168\n",
      "The 24407 th iteration gives loss of 0.174039066261678\n",
      "The 24408 th iteration gives loss of 0.1740359543836645\n",
      "The 24409 th iteration gives loss of 0.17403284279122405\n",
      "The 24410 th iteration gives loss of 0.17402973148430415\n",
      "The 24411 th iteration gives loss of 0.17402662046286252\n",
      "The 24412 th iteration gives loss of 0.1740235097268548\n",
      "The 24413 th iteration gives loss of 0.1740203992762337\n",
      "The 24414 th iteration gives loss of 0.1740172891109691\n",
      "The 24415 th iteration gives loss of 0.1740141792309998\n",
      "The 24416 th iteration gives loss of 0.17401106963629667\n",
      "The 24417 th iteration gives loss of 0.17400796032681093\n",
      "The 24418 th iteration gives loss of 0.1740048513024966\n",
      "The 24419 th iteration gives loss of 0.17400174256331016\n",
      "The 24420 th iteration gives loss of 0.17399863410920363\n",
      "The 24421 th iteration gives loss of 0.17399552594014614\n",
      "The 24422 th iteration gives loss of 0.1739924180560796\n",
      "The 24423 th iteration gives loss of 0.17398931045698784\n",
      "The 24424 th iteration gives loss of 0.17398620314278734\n",
      "The 24425 th iteration gives loss of 0.17398309611346416\n",
      "The 24426 th iteration gives loss of 0.17397998936896375\n",
      "The 24427 th iteration gives loss of 0.17397688290924943\n",
      "The 24428 th iteration gives loss of 0.17397377673427103\n",
      "The 24429 th iteration gives loss of 0.1739706708439753\n",
      "The 24430 th iteration gives loss of 0.17396756523833695\n",
      "The 24431 th iteration gives loss of 0.1739644599173109\n",
      "The 24432 th iteration gives loss of 0.1739613548808437\n",
      "The 24433 th iteration gives loss of 0.173958250128893\n",
      "The 24434 th iteration gives loss of 0.1739551456614345\n",
      "The 24435 th iteration gives loss of 0.17395204147839996\n",
      "The 24436 th iteration gives loss of 0.17394893757975854\n",
      "The 24437 th iteration gives loss of 0.17394583396545532\n",
      "The 24438 th iteration gives loss of 0.17394273063546734\n",
      "The 24439 th iteration gives loss of 0.17393962758972217\n",
      "The 24440 th iteration gives loss of 0.1739365248282036\n",
      "The 24441 th iteration gives loss of 0.17393342235085157\n",
      "The 24442 th iteration gives loss of 0.1739303201576319\n",
      "The 24443 th iteration gives loss of 0.173927218248506\n",
      "The 24444 th iteration gives loss of 0.17392411662341575\n",
      "The 24445 th iteration gives loss of 0.17392101528232495\n",
      "The 24446 th iteration gives loss of 0.17391791422518657\n",
      "The 24447 th iteration gives loss of 0.17391481345196064\n",
      "The 24448 th iteration gives loss of 0.17391171296261043\n",
      "The 24449 th iteration gives loss of 0.17390861275708033\n",
      "The 24450 th iteration gives loss of 0.17390551283533687\n",
      "The 24451 th iteration gives loss of 0.17390241319733196\n",
      "The 24452 th iteration gives loss of 0.17389931384302068\n",
      "The 24453 th iteration gives loss of 0.17389621477235587\n",
      "The 24454 th iteration gives loss of 0.17389311598530904\n",
      "The 24455 th iteration gives loss of 0.1738900174818244\n",
      "The 24456 th iteration gives loss of 0.17388691926186492\n",
      "The 24457 th iteration gives loss of 0.17388382132537833\n",
      "The 24458 th iteration gives loss of 0.1738807236723329\n",
      "The 24459 th iteration gives loss of 0.17387762630267958\n",
      "The 24460 th iteration gives loss of 0.17387452921636723\n",
      "The 24461 th iteration gives loss of 0.1738714324133672\n",
      "The 24462 th iteration gives loss of 0.1738683358936295\n",
      "The 24463 th iteration gives loss of 0.17386523965711406\n",
      "The 24464 th iteration gives loss of 0.17386214370377603\n",
      "The 24465 th iteration gives loss of 0.1738590480335688\n",
      "The 24466 th iteration gives loss of 0.17385595264645198\n",
      "The 24467 th iteration gives loss of 0.17385285754237698\n",
      "The 24468 th iteration gives loss of 0.17384976272130884\n",
      "The 24469 th iteration gives loss of 0.17384666818319613\n",
      "The 24470 th iteration gives loss of 0.17384357392800726\n",
      "The 24471 th iteration gives loss of 0.1738404799556909\n",
      "The 24472 th iteration gives loss of 0.17383738626619435\n",
      "The 24473 th iteration gives loss of 0.17383429285949353\n",
      "The 24474 th iteration gives loss of 0.17383119973553002\n",
      "The 24475 th iteration gives loss of 0.17382810689427564\n",
      "The 24476 th iteration gives loss of 0.17382501433567224\n",
      "The 24477 th iteration gives loss of 0.1738219220596903\n",
      "The 24478 th iteration gives loss of 0.1738188300662682\n",
      "The 24479 th iteration gives loss of 0.1738157383553901\n",
      "The 24480 th iteration gives loss of 0.1738126469269894\n",
      "The 24481 th iteration gives loss of 0.1738095557810309\n",
      "The 24482 th iteration gives loss of 0.17380646491746862\n",
      "The 24483 th iteration gives loss of 0.1738033743362621\n",
      "The 24484 th iteration gives loss of 0.17380028403737385\n",
      "The 24485 th iteration gives loss of 0.1737971940207533\n",
      "The 24486 th iteration gives loss of 0.17379410428635783\n",
      "The 24487 th iteration gives loss of 0.17379101483414447\n",
      "The 24488 th iteration gives loss of 0.17378792566406254\n",
      "The 24489 th iteration gives loss of 0.17378483677608594\n",
      "The 24490 th iteration gives loss of 0.17378174817016456\n",
      "The 24491 th iteration gives loss of 0.17377865984624585\n",
      "The 24492 th iteration gives loss of 0.17377557180430164\n",
      "The 24493 th iteration gives loss of 0.17377248404428378\n",
      "The 24494 th iteration gives loss of 0.1737693965661422\n",
      "The 24495 th iteration gives loss of 0.17376630936984436\n",
      "The 24496 th iteration gives loss of 0.17376322245534204\n",
      "The 24497 th iteration gives loss of 0.17376013582258978\n",
      "The 24498 th iteration gives loss of 0.1737570494715408\n",
      "The 24499 th iteration gives loss of 0.1737539634021648\n",
      "The 24500 th iteration gives loss of 0.17375087761441482\n",
      "The 24501 th iteration gives loss of 0.17374779210823757\n",
      "The 24502 th iteration gives loss of 0.17374470688360433\n",
      "The 24503 th iteration gives loss of 0.1737416219404603\n",
      "The 24504 th iteration gives loss of 0.17373853727877092\n",
      "The 24505 th iteration gives loss of 0.1737354528984961\n",
      "The 24506 th iteration gives loss of 0.1737323687995747\n",
      "The 24507 th iteration gives loss of 0.17372928498198437\n",
      "The 24508 th iteration gives loss of 0.17372620144566445\n",
      "The 24509 th iteration gives loss of 0.1737231181905893\n",
      "The 24510 th iteration gives loss of 0.17372003521670462\n",
      "The 24511 th iteration gives loss of 0.17371695252397823\n",
      "The 24512 th iteration gives loss of 0.1737138701123573\n",
      "The 24513 th iteration gives loss of 0.1737107879817974\n",
      "The 24514 th iteration gives loss of 0.17370770613225384\n",
      "The 24515 th iteration gives loss of 0.173704624563697\n",
      "The 24516 th iteration gives loss of 0.1737015432760717\n",
      "The 24517 th iteration gives loss of 0.17369846226934435\n",
      "The 24518 th iteration gives loss of 0.173695381543466\n",
      "The 24519 th iteration gives loss of 0.1736923010983911\n",
      "The 24520 th iteration gives loss of 0.17368922093408648\n",
      "The 24521 th iteration gives loss of 0.17368614105050242\n",
      "The 24522 th iteration gives loss of 0.17368306144759563\n",
      "The 24523 th iteration gives loss of 0.17367998212532154\n",
      "The 24524 th iteration gives loss of 0.17367690308365064\n",
      "The 24525 th iteration gives loss of 0.17367382432251957\n",
      "The 24526 th iteration gives loss of 0.1736707458418994\n",
      "The 24527 th iteration gives loss of 0.17366766764174518\n",
      "The 24528 th iteration gives loss of 0.17366458972201157\n",
      "The 24529 th iteration gives loss of 0.17366151208266067\n",
      "The 24530 th iteration gives loss of 0.17365843472364526\n",
      "The 24531 th iteration gives loss of 0.17365535764491413\n",
      "The 24532 th iteration gives loss of 0.1736522808464393\n",
      "The 24533 th iteration gives loss of 0.17364920432818334\n",
      "The 24534 th iteration gives loss of 0.17364612809007282\n",
      "The 24535 th iteration gives loss of 0.17364305213209694\n",
      "The 24536 th iteration gives loss of 0.17363997645420398\n",
      "The 24537 th iteration gives loss of 0.17363690105633933\n",
      "The 24538 th iteration gives loss of 0.1736338259384703\n",
      "The 24539 th iteration gives loss of 0.17363075110055567\n",
      "The 24540 th iteration gives loss of 0.17362767654255037\n",
      "The 24541 th iteration gives loss of 0.17362460226440696\n",
      "The 24542 th iteration gives loss of 0.1736215282660871\n",
      "The 24543 th iteration gives loss of 0.17361845454754443\n",
      "The 24544 th iteration gives loss of 0.17361538110874383\n",
      "The 24545 th iteration gives loss of 0.173612307949645\n",
      "The 24546 th iteration gives loss of 0.17360923507018036\n",
      "The 24547 th iteration gives loss of 0.17360616247033783\n",
      "The 24548 th iteration gives loss of 0.17360309015006173\n",
      "The 24549 th iteration gives loss of 0.17360001810930428\n",
      "The 24550 th iteration gives loss of 0.1735969463480299\n",
      "The 24551 th iteration gives loss of 0.17359387486619826\n",
      "The 24552 th iteration gives loss of 0.1735908036637594\n",
      "The 24553 th iteration gives loss of 0.1735877327406731\n",
      "The 24554 th iteration gives loss of 0.17358466209690224\n",
      "The 24555 th iteration gives loss of 0.17358159173240623\n",
      "The 24556 th iteration gives loss of 0.17357852164711898\n",
      "The 24557 th iteration gives loss of 0.17357545184102138\n",
      "The 24558 th iteration gives loss of 0.17357238231406721\n",
      "The 24559 th iteration gives loss of 0.17356931306620443\n",
      "The 24560 th iteration gives loss of 0.17356624409740729\n",
      "The 24561 th iteration gives loss of 0.17356317540761537\n",
      "The 24562 th iteration gives loss of 0.1735601069967913\n",
      "The 24563 th iteration gives loss of 0.1735570388649003\n",
      "The 24564 th iteration gives loss of 0.1735539710118951\n",
      "The 24565 th iteration gives loss of 0.17355090343772905\n",
      "The 24566 th iteration gives loss of 0.17354783614236002\n",
      "The 24567 th iteration gives loss of 0.17354476912575223\n",
      "The 24568 th iteration gives loss of 0.1735417023878557\n",
      "The 24569 th iteration gives loss of 0.1735386359286354\n",
      "The 24570 th iteration gives loss of 0.17353556974804413\n",
      "The 24571 th iteration gives loss of 0.1735325038460332\n",
      "The 24572 th iteration gives loss of 0.1735294382225667\n",
      "The 24573 th iteration gives loss of 0.17352637287760753\n",
      "The 24574 th iteration gives loss of 0.1735233078111061\n",
      "The 24575 th iteration gives loss of 0.17352024302302044\n",
      "The 24576 th iteration gives loss of 0.1735171785133112\n",
      "The 24577 th iteration gives loss of 0.17351411428193614\n",
      "The 24578 th iteration gives loss of 0.17351105032884698\n",
      "The 24579 th iteration gives loss of 0.17350798665400366\n",
      "The 24580 th iteration gives loss of 0.17350492325736502\n",
      "The 24581 th iteration gives loss of 0.17350186013888586\n",
      "The 24582 th iteration gives loss of 0.17349879729853018\n",
      "The 24583 th iteration gives loss of 0.17349573473624602\n",
      "The 24584 th iteration gives loss of 0.1734926724520022\n",
      "The 24585 th iteration gives loss of 0.1734896104457574\n",
      "The 24586 th iteration gives loss of 0.17348654871745087\n",
      "The 24587 th iteration gives loss of 0.17348348726705337\n",
      "The 24588 th iteration gives loss of 0.17348042609451814\n",
      "The 24589 th iteration gives loss of 0.1734773651998165\n",
      "The 24590 th iteration gives loss of 0.17347430458288515\n",
      "The 24591 th iteration gives loss of 0.1734712442436944\n",
      "The 24592 th iteration gives loss of 0.17346818418219617\n",
      "The 24593 th iteration gives loss of 0.17346512439835315\n",
      "The 24594 th iteration gives loss of 0.17346206489212238\n",
      "The 24595 th iteration gives loss of 0.1734590056634514\n",
      "The 24596 th iteration gives loss of 0.17345594671231818\n",
      "The 24597 th iteration gives loss of 0.17345288803866651\n",
      "The 24598 th iteration gives loss of 0.17344982964244596\n",
      "The 24599 th iteration gives loss of 0.17344677152363605\n",
      "The 24600 th iteration gives loss of 0.1734437136821772\n",
      "The 24601 th iteration gives loss of 0.17344065611803386\n",
      "The 24602 th iteration gives loss of 0.17343759883116042\n",
      "The 24603 th iteration gives loss of 0.17343454182152301\n",
      "The 24604 th iteration gives loss of 0.17343148508905956\n",
      "The 24605 th iteration gives loss of 0.17342842863374563\n",
      "The 24606 th iteration gives loss of 0.17342537245553413\n",
      "The 24607 th iteration gives loss of 0.17342231655438883\n",
      "The 24608 th iteration gives loss of 0.17341926093025742\n",
      "The 24609 th iteration gives loss of 0.17341620558310059\n",
      "The 24610 th iteration gives loss of 0.17341315051288236\n",
      "The 24611 th iteration gives loss of 0.17341009571954785\n",
      "The 24612 th iteration gives loss of 0.17340704120306336\n",
      "The 24613 th iteration gives loss of 0.1734039869633961\n",
      "The 24614 th iteration gives loss of 0.1734009330004774\n",
      "The 24615 th iteration gives loss of 0.17339787931428216\n",
      "The 24616 th iteration gives loss of 0.1733948259047778\n",
      "The 24617 th iteration gives loss of 0.17339177277190748\n",
      "The 24618 th iteration gives loss of 0.17338871991563032\n",
      "The 24619 th iteration gives loss of 0.17338566733590785\n",
      "The 24620 th iteration gives loss of 0.17338261503269778\n",
      "The 24621 th iteration gives loss of 0.17337956300595217\n",
      "The 24622 th iteration gives loss of 0.1733765112556301\n",
      "The 24623 th iteration gives loss of 0.17337345978169563\n",
      "The 24624 th iteration gives loss of 0.17337040858411015\n",
      "The 24625 th iteration gives loss of 0.1733673576628263\n",
      "The 24626 th iteration gives loss of 0.17336430701779149\n",
      "The 24627 th iteration gives loss of 0.17336125664896868\n",
      "The 24628 th iteration gives loss of 0.17335820655633072\n",
      "The 24629 th iteration gives loss of 0.17335515673981336\n",
      "The 24630 th iteration gives loss of 0.17335210719939942\n",
      "The 24631 th iteration gives loss of 0.17334905793501984\n",
      "The 24632 th iteration gives loss of 0.17334600894665503\n",
      "The 24633 th iteration gives loss of 0.17334296023424037\n",
      "The 24634 th iteration gives loss of 0.17333991179775388\n",
      "The 24635 th iteration gives loss of 0.17333686363714443\n",
      "The 24636 th iteration gives loss of 0.17333381575237328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 24637 th iteration gives loss of 0.1733307681433981\n",
      "The 24638 th iteration gives loss of 0.1733277208101762\n",
      "The 24639 th iteration gives loss of 0.17332467375266755\n",
      "The 24640 th iteration gives loss of 0.17332162697082354\n",
      "The 24641 th iteration gives loss of 0.17331858046460152\n",
      "The 24642 th iteration gives loss of 0.17331553423396465\n",
      "The 24643 th iteration gives loss of 0.17331248827886922\n",
      "The 24644 th iteration gives loss of 0.17330944259928038\n",
      "The 24645 th iteration gives loss of 0.17330639719513982\n",
      "The 24646 th iteration gives loss of 0.17330335206642683\n",
      "The 24647 th iteration gives loss of 0.1733003072130867\n",
      "The 24648 th iteration gives loss of 0.1732972626350747\n",
      "The 24649 th iteration gives loss of 0.17329421833235384\n",
      "The 24650 th iteration gives loss of 0.17329117430487465\n",
      "The 24651 th iteration gives loss of 0.17328813055260203\n",
      "The 24652 th iteration gives loss of 0.1732850870755029\n",
      "The 24653 th iteration gives loss of 0.17328204387351911\n",
      "The 24654 th iteration gives loss of 0.17327900094661514\n",
      "The 24655 th iteration gives loss of 0.17327595829475134\n",
      "The 24656 th iteration gives loss of 0.17327291591787766\n",
      "The 24657 th iteration gives loss of 0.17326987381595957\n",
      "The 24658 th iteration gives loss of 0.17326683198896034\n",
      "The 24659 th iteration gives loss of 0.17326379043682746\n",
      "The 24660 th iteration gives loss of 0.17326074915952608\n",
      "The 24661 th iteration gives loss of 0.17325770815701166\n",
      "The 24662 th iteration gives loss of 0.17325466742923024\n",
      "The 24663 th iteration gives loss of 0.1732516269761617\n",
      "The 24664 th iteration gives loss of 0.17324858679775254\n",
      "The 24665 th iteration gives loss of 0.17324554689396648\n",
      "The 24666 th iteration gives loss of 0.1732425072647505\n",
      "The 24667 th iteration gives loss of 0.17323946791007028\n",
      "The 24668 th iteration gives loss of 0.17323642882987791\n",
      "The 24669 th iteration gives loss of 0.17323339002414365\n",
      "The 24670 th iteration gives loss of 0.17323035149282148\n",
      "The 24671 th iteration gives loss of 0.17322731323585822\n",
      "The 24672 th iteration gives loss of 0.17322427525321857\n",
      "The 24673 th iteration gives loss of 0.17322123754487112\n",
      "The 24674 th iteration gives loss of 0.17321820011076744\n",
      "The 24675 th iteration gives loss of 0.17321516295085254\n",
      "The 24676 th iteration gives loss of 0.1732121260651064\n",
      "The 24677 th iteration gives loss of 0.173209089453473\n",
      "The 24678 th iteration gives loss of 0.17320605311591517\n",
      "The 24679 th iteration gives loss of 0.17320301705238528\n",
      "The 24680 th iteration gives loss of 0.1731999812628527\n",
      "The 24681 th iteration gives loss of 0.17319694574726366\n",
      "The 24682 th iteration gives loss of 0.17319391050558602\n",
      "The 24683 th iteration gives loss of 0.17319087553777418\n",
      "The 24684 th iteration gives loss of 0.17318784084377853\n",
      "The 24685 th iteration gives loss of 0.17318480642357353\n",
      "The 24686 th iteration gives loss of 0.17318177227710763\n",
      "The 24687 th iteration gives loss of 0.17317873840433243\n",
      "The 24688 th iteration gives loss of 0.1731757048052205\n",
      "The 24689 th iteration gives loss of 0.17317267147972293\n",
      "The 24690 th iteration gives loss of 0.1731696384277962\n",
      "The 24691 th iteration gives loss of 0.17316660564940348\n",
      "The 24692 th iteration gives loss of 0.17316357314450187\n",
      "The 24693 th iteration gives loss of 0.17316054091304867\n",
      "The 24694 th iteration gives loss of 0.17315750895499668\n",
      "The 24695 th iteration gives loss of 0.1731544772703067\n",
      "The 24696 th iteration gives loss of 0.17315144585894712\n",
      "The 24697 th iteration gives loss of 0.17314841472086462\n",
      "The 24698 th iteration gives loss of 0.17314538385602365\n",
      "The 24699 th iteration gives loss of 0.1731423532643788\n",
      "The 24700 th iteration gives loss of 0.17313932294588652\n",
      "The 24701 th iteration gives loss of 0.17313629290051308\n",
      "The 24702 th iteration gives loss of 0.17313326312821506\n",
      "The 24703 th iteration gives loss of 0.17313023362894459\n",
      "The 24704 th iteration gives loss of 0.17312720440266477\n",
      "The 24705 th iteration gives loss of 0.17312417544933323\n",
      "The 24706 th iteration gives loss of 0.1731211467689061\n",
      "The 24707 th iteration gives loss of 0.17311811836134158\n",
      "The 24708 th iteration gives loss of 0.17311509022660038\n",
      "The 24709 th iteration gives loss of 0.17311206236464152\n",
      "The 24710 th iteration gives loss of 0.1731090347754252\n",
      "The 24711 th iteration gives loss of 0.17310600745890392\n",
      "The 24712 th iteration gives loss of 0.1731029804150518\n",
      "The 24713 th iteration gives loss of 0.17309995364380285\n",
      "The 24714 th iteration gives loss of 0.17309692714512342\n",
      "The 24715 th iteration gives loss of 0.17309390091897464\n",
      "The 24716 th iteration gives loss of 0.1730908749653265\n",
      "The 24717 th iteration gives loss of 0.1730878492841161\n",
      "The 24718 th iteration gives loss of 0.17308482387531604\n",
      "The 24719 th iteration gives loss of 0.1730817987388842\n",
      "The 24720 th iteration gives loss of 0.17307877387477896\n",
      "The 24721 th iteration gives loss of 0.1730757492829554\n",
      "The 24722 th iteration gives loss of 0.17307272496337137\n",
      "The 24723 th iteration gives loss of 0.17306970091598217\n",
      "The 24724 th iteration gives loss of 0.1730666771407552\n",
      "The 24725 th iteration gives loss of 0.17306365363763837\n",
      "The 24726 th iteration gives loss of 0.17306063040660247\n",
      "The 24727 th iteration gives loss of 0.17305760744759704\n",
      "The 24728 th iteration gives loss of 0.17305458476059438\n",
      "The 24729 th iteration gives loss of 0.17305156234553223\n",
      "The 24730 th iteration gives loss of 0.17304854020237775\n",
      "The 24731 th iteration gives loss of 0.17304551833110127\n",
      "The 24732 th iteration gives loss of 0.17304249673163957\n",
      "The 24733 th iteration gives loss of 0.1730394754039667\n",
      "The 24734 th iteration gives loss of 0.17303645434803758\n",
      "The 24735 th iteration gives loss of 0.17303343356380474\n",
      "The 24736 th iteration gives loss of 0.17303041305123934\n",
      "The 24737 th iteration gives loss of 0.17302739281028648\n",
      "The 24738 th iteration gives loss of 0.173024372840913\n",
      "The 24739 th iteration gives loss of 0.1730213531430745\n",
      "The 24740 th iteration gives loss of 0.17301833371673433\n",
      "The 24741 th iteration gives loss of 0.17301531456183886\n",
      "The 24742 th iteration gives loss of 0.17301229567836857\n",
      "The 24743 th iteration gives loss of 0.17300927706625885\n",
      "The 24744 th iteration gives loss of 0.1730062587254784\n",
      "The 24745 th iteration gives loss of 0.17300324065598577\n",
      "The 24746 th iteration gives loss of 0.17300022285774222\n",
      "The 24747 th iteration gives loss of 0.1729972053307052\n",
      "The 24748 th iteration gives loss of 0.17299418807482114\n",
      "The 24749 th iteration gives loss of 0.1729911710900669\n",
      "The 24750 th iteration gives loss of 0.17298815437639098\n",
      "The 24751 th iteration gives loss of 0.17298513793375367\n",
      "The 24752 th iteration gives loss of 0.17298212176211622\n",
      "The 24753 th iteration gives loss of 0.17297910586143705\n",
      "The 24754 th iteration gives loss of 0.17297609023167493\n",
      "The 24755 th iteration gives loss of 0.1729730748727831\n",
      "The 24756 th iteration gives loss of 0.1729700597847265\n",
      "The 24757 th iteration gives loss of 0.17296704496746115\n",
      "The 24758 th iteration gives loss of 0.1729640304209431\n",
      "The 24759 th iteration gives loss of 0.17296101614512674\n",
      "The 24760 th iteration gives loss of 0.17295800213999302\n",
      "The 24761 th iteration gives loss of 0.17295498840547577\n",
      "The 24762 th iteration gives loss of 0.1729519749415452\n",
      "The 24763 th iteration gives loss of 0.17294896174816043\n",
      "The 24764 th iteration gives loss of 0.17294594882528042\n",
      "The 24765 th iteration gives loss of 0.17294293617285356\n",
      "The 24766 th iteration gives loss of 0.17293992379085002\n",
      "The 24767 th iteration gives loss of 0.1729369116792314\n",
      "The 24768 th iteration gives loss of 0.17293389983794535\n",
      "The 24769 th iteration gives loss of 0.17293088826695696\n",
      "The 24770 th iteration gives loss of 0.17292787696622178\n",
      "The 24771 th iteration gives loss of 0.1729248659357019\n",
      "The 24772 th iteration gives loss of 0.1729218551753584\n",
      "The 24773 th iteration gives loss of 0.1729188446851476\n",
      "The 24774 th iteration gives loss of 0.17291583446500938\n",
      "The 24775 th iteration gives loss of 0.1729128245149375\n",
      "The 24776 th iteration gives loss of 0.1729098148348731\n",
      "The 24777 th iteration gives loss of 0.17290680542476936\n",
      "The 24778 th iteration gives loss of 0.17290379628459523\n",
      "The 24779 th iteration gives loss of 0.17290078741430245\n",
      "The 24780 th iteration gives loss of 0.17289777881384957\n",
      "The 24781 th iteration gives loss of 0.1728947704831984\n",
      "The 24782 th iteration gives loss of 0.17289176242231186\n",
      "The 24783 th iteration gives loss of 0.17288875463114917\n",
      "The 24784 th iteration gives loss of 0.1728857471096593\n",
      "The 24785 th iteration gives loss of 0.17288273985780786\n",
      "The 24786 th iteration gives loss of 0.17287973287555625\n",
      "The 24787 th iteration gives loss of 0.17287672616285685\n",
      "The 24788 th iteration gives loss of 0.1728737197196812\n",
      "The 24789 th iteration gives loss of 0.17287071354596756\n",
      "The 24790 th iteration gives loss of 0.17286770764168818\n",
      "The 24791 th iteration gives loss of 0.1728647020068087\n",
      "The 24792 th iteration gives loss of 0.1728616966412693\n",
      "The 24793 th iteration gives loss of 0.17285869154504208\n",
      "The 24794 th iteration gives loss of 0.17285568671808357\n",
      "The 24795 th iteration gives loss of 0.1728526821603426\n",
      "The 24796 th iteration gives loss of 0.17284967787179564\n",
      "The 24797 th iteration gives loss of 0.1728466738523913\n",
      "The 24798 th iteration gives loss of 0.17284367010209217\n",
      "The 24799 th iteration gives loss of 0.1728406666208616\n",
      "The 24800 th iteration gives loss of 0.17283766340864667\n",
      "The 24801 th iteration gives loss of 0.17283466046540832\n",
      "The 24802 th iteration gives loss of 0.17283165779110998\n",
      "The 24803 th iteration gives loss of 0.17282865538571565\n",
      "The 24804 th iteration gives loss of 0.17282565324917465\n",
      "The 24805 th iteration gives loss of 0.17282265138145417\n",
      "The 24806 th iteration gives loss of 0.1728196497825047\n",
      "The 24807 th iteration gives loss of 0.17281664845229297\n",
      "The 24808 th iteration gives loss of 0.17281364739077892\n",
      "The 24809 th iteration gives loss of 0.17281064659790765\n",
      "The 24810 th iteration gives loss of 0.1728076460736502\n",
      "The 24811 th iteration gives loss of 0.17280464581796912\n",
      "The 24812 th iteration gives loss of 0.1728016458308144\n",
      "The 24813 th iteration gives loss of 0.17279864611214724\n",
      "The 24814 th iteration gives loss of 0.17279564666192354\n",
      "The 24815 th iteration gives loss of 0.1727926474801195\n",
      "The 24816 th iteration gives loss of 0.172789648566683\n",
      "The 24817 th iteration gives loss of 0.17278664992156303\n",
      "The 24818 th iteration gives loss of 0.17278365154472428\n",
      "The 24819 th iteration gives loss of 0.1727806534361301\n",
      "The 24820 th iteration gives loss of 0.17277765559574088\n",
      "The 24821 th iteration gives loss of 0.17277465802351566\n",
      "The 24822 th iteration gives loss of 0.17277166071940928\n",
      "The 24823 th iteration gives loss of 0.172768663683386\n",
      "The 24824 th iteration gives loss of 0.17276566691539688\n",
      "The 24825 th iteration gives loss of 0.17276267041540194\n",
      "The 24826 th iteration gives loss of 0.1727596741833725\n",
      "The 24827 th iteration gives loss of 0.17275667821926158\n",
      "The 24828 th iteration gives loss of 0.1727536825230219\n",
      "The 24829 th iteration gives loss of 0.1727506870946172\n",
      "The 24830 th iteration gives loss of 0.17274769193399747\n",
      "The 24831 th iteration gives loss of 0.17274469704113923\n",
      "The 24832 th iteration gives loss of 0.17274170241599232\n",
      "The 24833 th iteration gives loss of 0.17273870805852026\n",
      "The 24834 th iteration gives loss of 0.1727357139686784\n",
      "The 24835 th iteration gives loss of 0.17273272014641944\n",
      "The 24836 th iteration gives loss of 0.17272972659171676\n",
      "The 24837 th iteration gives loss of 0.17272673330452068\n",
      "The 24838 th iteration gives loss of 0.17272374028479423\n",
      "The 24839 th iteration gives loss of 0.1727207475324825\n",
      "The 24840 th iteration gives loss of 0.17271775504756723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 24841 th iteration gives loss of 0.1727147628299951\n",
      "The 24842 th iteration gives loss of 0.17271177087972966\n",
      "The 24843 th iteration gives loss of 0.17270877919672256\n",
      "The 24844 th iteration gives loss of 0.17270578778094453\n",
      "The 24845 th iteration gives loss of 0.1727027966323439\n",
      "The 24846 th iteration gives loss of 0.1726998057508912\n",
      "The 24847 th iteration gives loss of 0.17269681513653448\n",
      "The 24848 th iteration gives loss of 0.17269382478924175\n",
      "The 24849 th iteration gives loss of 0.17269083470896204\n",
      "The 24850 th iteration gives loss of 0.1726878448956635\n",
      "The 24851 th iteration gives loss of 0.17268485534929726\n",
      "The 24852 th iteration gives loss of 0.17268186606983368\n",
      "The 24853 th iteration gives loss of 0.17267887705723425\n",
      "The 24854 th iteration gives loss of 0.17267588831144448\n",
      "The 24855 th iteration gives loss of 0.17267289983242656\n",
      "The 24856 th iteration gives loss of 0.17266991162014383\n",
      "The 24857 th iteration gives loss of 0.17266692367455247\n",
      "The 24858 th iteration gives loss of 0.17266393599562313\n",
      "The 24859 th iteration gives loss of 0.17266094858329686\n",
      "The 24860 th iteration gives loss of 0.17265796143755388\n",
      "The 24861 th iteration gives loss of 0.17265497455834086\n",
      "The 24862 th iteration gives loss of 0.17265198794561437\n",
      "The 24863 th iteration gives loss of 0.17264900159933513\n",
      "The 24864 th iteration gives loss of 0.17264601551946912\n",
      "The 24865 th iteration gives loss of 0.17264302970596612\n",
      "The 24866 th iteration gives loss of 0.17264004415879083\n",
      "The 24867 th iteration gives loss of 0.17263705887791878\n",
      "The 24868 th iteration gives loss of 0.17263407386327892\n",
      "The 24869 th iteration gives loss of 0.17263108911485753\n",
      "The 24870 th iteration gives loss of 0.172628104632594\n",
      "The 24871 th iteration gives loss of 0.17262512041645864\n",
      "The 24872 th iteration gives loss of 0.17262213646640745\n",
      "The 24873 th iteration gives loss of 0.17261915278240145\n",
      "The 24874 th iteration gives loss of 0.17261616936439844\n",
      "The 24875 th iteration gives loss of 0.1726131862123527\n",
      "The 24876 th iteration gives loss of 0.1726102033262346\n",
      "The 24877 th iteration gives loss of 0.1726072207060014\n",
      "The 24878 th iteration gives loss of 0.1726042383516044\n",
      "The 24879 th iteration gives loss of 0.17260125626301445\n",
      "The 24880 th iteration gives loss of 0.17259827444017728\n",
      "The 24881 th iteration gives loss of 0.17259529288306874\n",
      "The 24882 th iteration gives loss of 0.17259231159163624\n",
      "The 24883 th iteration gives loss of 0.1725893305658432\n",
      "The 24884 th iteration gives loss of 0.1725863498056444\n",
      "The 24885 th iteration gives loss of 0.17258336931100496\n",
      "The 24886 th iteration gives loss of 0.17258038908188714\n",
      "The 24887 th iteration gives loss of 0.172577409118246\n",
      "The 24888 th iteration gives loss of 0.1725744294200376\n",
      "The 24889 th iteration gives loss of 0.17257144998723323\n",
      "The 24890 th iteration gives loss of 0.17256847081978002\n",
      "The 24891 th iteration gives loss of 0.17256549191764448\n",
      "The 24892 th iteration gives loss of 0.1725625132807772\n",
      "The 24893 th iteration gives loss of 0.17255953490915282\n",
      "The 24894 th iteration gives loss of 0.17255655680271825\n",
      "The 24895 th iteration gives loss of 0.17255357896143553\n",
      "The 24896 th iteration gives loss of 0.1725506013852682\n",
      "The 24897 th iteration gives loss of 0.17254762407418098\n",
      "The 24898 th iteration gives loss of 0.17254464702811756\n",
      "The 24899 th iteration gives loss of 0.1725416702470469\n",
      "The 24900 th iteration gives loss of 0.17253869373093111\n",
      "The 24901 th iteration gives loss of 0.17253571747972787\n",
      "The 24902 th iteration gives loss of 0.17253274149339495\n",
      "The 24903 th iteration gives loss of 0.17252976577189327\n",
      "The 24904 th iteration gives loss of 0.17252679031517865\n",
      "The 24905 th iteration gives loss of 0.17252381512321202\n",
      "The 24906 th iteration gives loss of 0.17252084019596148\n",
      "The 24907 th iteration gives loss of 0.17251786553337353\n",
      "The 24908 th iteration gives loss of 0.1725148911354189\n",
      "The 24909 th iteration gives loss of 0.1725119170020565\n",
      "The 24910 th iteration gives loss of 0.17250894313323847\n",
      "The 24911 th iteration gives loss of 0.17250596952892605\n",
      "The 24912 th iteration gives loss of 0.17250299618908263\n",
      "The 24913 th iteration gives loss of 0.17250002311366736\n",
      "The 24914 th iteration gives loss of 0.17249705030263632\n",
      "The 24915 th iteration gives loss of 0.17249407775595635\n",
      "The 24916 th iteration gives loss of 0.172491105473586\n",
      "The 24917 th iteration gives loss of 0.17248813345547584\n",
      "The 24918 th iteration gives loss of 0.17248516170159311\n",
      "The 24919 th iteration gives loss of 0.17248219021189348\n",
      "The 24920 th iteration gives loss of 0.17247921898634863\n",
      "The 24921 th iteration gives loss of 0.17247624802490633\n",
      "The 24922 th iteration gives loss of 0.1724732773275206\n",
      "The 24923 th iteration gives loss of 0.17247030689416679\n",
      "The 24924 th iteration gives loss of 0.1724673367247911\n",
      "The 24925 th iteration gives loss of 0.17246436681935934\n",
      "The 24926 th iteration gives loss of 0.17246139717783232\n",
      "The 24927 th iteration gives loss of 0.1724584278001849\n",
      "The 24928 th iteration gives loss of 0.17245545868634896\n",
      "The 24929 th iteration gives loss of 0.17245248983630013\n",
      "The 24930 th iteration gives loss of 0.1724495212499809\n",
      "The 24931 th iteration gives loss of 0.17244655292737607\n",
      "The 24932 th iteration gives loss of 0.1724435848684367\n",
      "The 24933 th iteration gives loss of 0.17244061707312303\n",
      "The 24934 th iteration gives loss of 0.17243764954138074\n",
      "The 24935 th iteration gives loss of 0.17243468227318992\n",
      "The 24936 th iteration gives loss of 0.1724317152684894\n",
      "The 24937 th iteration gives loss of 0.17242874852725765\n",
      "The 24938 th iteration gives loss of 0.17242578204945103\n",
      "The 24939 th iteration gives loss of 0.17242281583502328\n",
      "The 24940 th iteration gives loss of 0.1724198498839436\n",
      "The 24941 th iteration gives loss of 0.1724168841961632\n",
      "The 24942 th iteration gives loss of 0.17241391877164527\n",
      "The 24943 th iteration gives loss of 0.17241095361034237\n",
      "The 24944 th iteration gives loss of 0.1724079887122201\n",
      "The 24945 th iteration gives loss of 0.1724050240772495\n",
      "The 24946 th iteration gives loss of 0.17240205970537525\n",
      "The 24947 th iteration gives loss of 0.17239909559656086\n",
      "The 24948 th iteration gives loss of 0.17239613175076335\n",
      "The 24949 th iteration gives loss of 0.1723931681679463\n",
      "The 24950 th iteration gives loss of 0.17239020484807885\n",
      "The 24951 th iteration gives loss of 0.1723872417911064\n",
      "The 24952 th iteration gives loss of 0.17238427899699113\n",
      "The 24953 th iteration gives loss of 0.17238131646570745\n",
      "The 24954 th iteration gives loss of 0.17237835419719813\n",
      "The 24955 th iteration gives loss of 0.17237539219143225\n",
      "The 24956 th iteration gives loss of 0.17237243044836903\n",
      "The 24957 th iteration gives loss of 0.17236946896796038\n",
      "The 24958 th iteration gives loss of 0.17236650775017234\n",
      "The 24959 th iteration gives loss of 0.1723635467949674\n",
      "The 24960 th iteration gives loss of 0.17236058610230104\n",
      "The 24961 th iteration gives loss of 0.17235762567213112\n",
      "The 24962 th iteration gives loss of 0.17235466550443065\n",
      "The 24963 th iteration gives loss of 0.17235170559914195\n",
      "The 24964 th iteration gives loss of 0.1723487459562374\n",
      "The 24965 th iteration gives loss of 0.17234578657567862\n",
      "The 24966 th iteration gives loss of 0.1723428274574143\n",
      "The 24967 th iteration gives loss of 0.17233986860141384\n",
      "The 24968 th iteration gives loss of 0.17233691000763188\n",
      "The 24969 th iteration gives loss of 0.17233395167603432\n",
      "The 24970 th iteration gives loss of 0.17233099360658274\n",
      "The 24971 th iteration gives loss of 0.17232803579922348\n",
      "The 24972 th iteration gives loss of 0.17232507825392612\n",
      "The 24973 th iteration gives loss of 0.1723221209706487\n",
      "The 24974 th iteration gives loss of 0.17231916394935373\n",
      "The 24975 th iteration gives loss of 0.17231620719000743\n",
      "The 24976 th iteration gives loss of 0.1723132506925577\n",
      "The 24977 th iteration gives loss of 0.17231029445696588\n",
      "The 24978 th iteration gives loss of 0.17230733848320398\n",
      "The 24979 th iteration gives loss of 0.17230438277122162\n",
      "The 24980 th iteration gives loss of 0.17230142732097592\n",
      "The 24981 th iteration gives loss of 0.17229847213242966\n",
      "The 24982 th iteration gives loss of 0.1722955172055473\n",
      "The 24983 th iteration gives loss of 0.172292562540294\n",
      "The 24984 th iteration gives loss of 0.1722896081366142\n",
      "The 24985 th iteration gives loss of 0.17228665399448248\n",
      "The 24986 th iteration gives loss of 0.17228370011385313\n",
      "The 24987 th iteration gives loss of 0.17228074649469158\n",
      "The 24988 th iteration gives loss of 0.17227779313695463\n",
      "The 24989 th iteration gives loss of 0.1722748400405846\n",
      "The 24990 th iteration gives loss of 0.1722718872055673\n",
      "The 24991 th iteration gives loss of 0.17226893463185583\n",
      "The 24992 th iteration gives loss of 0.1722659823194038\n",
      "The 24993 th iteration gives loss of 0.17226303026818374\n",
      "The 24994 th iteration gives loss of 0.17226007847813635\n",
      "The 24995 th iteration gives loss of 0.1722571269492446\n",
      "The 24996 th iteration gives loss of 0.17225417568145762\n",
      "The 24997 th iteration gives loss of 0.17225122467472906\n",
      "The 24998 th iteration gives loss of 0.17224827392903289\n",
      "The 24999 th iteration gives loss of 0.17224532344431426\n",
      "The 25000 th iteration gives loss of 0.17224237322054148\n",
      "The 25001 th iteration gives loss of 0.1722394232576776\n",
      "The 25002 th iteration gives loss of 0.1722364735556822\n",
      "The 25003 th iteration gives loss of 0.17223352411451243\n",
      "The 25004 th iteration gives loss of 0.17223057493412833\n",
      "The 25005 th iteration gives loss of 0.17222762601448785\n",
      "The 25006 th iteration gives loss of 0.1722246773555605\n",
      "The 25007 th iteration gives loss of 0.17222172895730012\n",
      "The 25008 th iteration gives loss of 0.1722187808196699\n",
      "The 25009 th iteration gives loss of 0.17221583294262585\n",
      "The 25010 th iteration gives loss of 0.17221288532612344\n",
      "The 25011 th iteration gives loss of 0.17220993797013867\n",
      "The 25012 th iteration gives loss of 0.1722069908746178\n",
      "The 25013 th iteration gives loss of 0.1722040440395254\n",
      "The 25014 th iteration gives loss of 0.17220109746482987\n",
      "The 25015 th iteration gives loss of 0.172198151150483\n",
      "The 25016 th iteration gives loss of 0.17219520509643876\n",
      "The 25017 th iteration gives loss of 0.17219225930266838\n",
      "The 25018 th iteration gives loss of 0.17218931376913646\n",
      "The 25019 th iteration gives loss of 0.1721863684957943\n",
      "The 25020 th iteration gives loss of 0.172183423482604\n",
      "The 25021 th iteration gives loss of 0.172180478729528\n",
      "The 25022 th iteration gives loss of 0.17217753423651946\n",
      "The 25023 th iteration gives loss of 0.17217459000354016\n",
      "The 25024 th iteration gives loss of 0.17217164603056762\n",
      "The 25025 th iteration gives loss of 0.17216870231753853\n",
      "The 25026 th iteration gives loss of 0.17216575886442617\n",
      "The 25027 th iteration gives loss of 0.1721628156711946\n",
      "The 25028 th iteration gives loss of 0.17215987273778877\n",
      "The 25029 th iteration gives loss of 0.17215693006418228\n",
      "The 25030 th iteration gives loss of 0.17215398765033654\n",
      "The 25031 th iteration gives loss of 0.1721510454962033\n",
      "The 25032 th iteration gives loss of 0.17214810360174918\n",
      "The 25033 th iteration gives loss of 0.17214516196692603\n",
      "The 25034 th iteration gives loss of 0.17214222059170728\n",
      "The 25035 th iteration gives loss of 0.17213927947605\n",
      "The 25036 th iteration gives loss of 0.1721363386199088\n",
      "The 25037 th iteration gives loss of 0.1721333980232489\n",
      "The 25038 th iteration gives loss of 0.17213045768602886\n",
      "The 25039 th iteration gives loss of 0.1721275176082048\n",
      "The 25040 th iteration gives loss of 0.1721245777897412\n",
      "The 25041 th iteration gives loss of 0.17212163823059665\n",
      "The 25042 th iteration gives loss of 0.17211869893074574\n",
      "The 25043 th iteration gives loss of 0.17211575989012723\n",
      "The 25044 th iteration gives loss of 0.1721128211087153\n",
      "The 25045 th iteration gives loss of 0.17210988258646934\n",
      "The 25046 th iteration gives loss of 0.17210694432334275\n",
      "The 25047 th iteration gives loss of 0.1721040063193013\n",
      "The 25048 th iteration gives loss of 0.17210106857431345\n",
      "The 25049 th iteration gives loss of 0.17209813108831798\n",
      "The 25050 th iteration gives loss of 0.1720951938612967\n",
      "The 25051 th iteration gives loss of 0.17209225689320395\n",
      "The 25052 th iteration gives loss of 0.17208932018399245\n",
      "The 25053 th iteration gives loss of 0.1720863837336294\n",
      "The 25054 th iteration gives loss of 0.17208344754208346\n",
      "The 25055 th iteration gives loss of 0.17208051160930377\n",
      "The 25056 th iteration gives loss of 0.17207757593524423\n",
      "The 25057 th iteration gives loss of 0.17207464051988386\n",
      "The 25058 th iteration gives loss of 0.17207170536317115\n",
      "The 25059 th iteration gives loss of 0.1720687704650766\n",
      "The 25060 th iteration gives loss of 0.17206583582554988\n",
      "The 25061 th iteration gives loss of 0.17206290144455375\n",
      "The 25062 th iteration gives loss of 0.17205996732205048\n",
      "The 25063 th iteration gives loss of 0.1720570334580059\n",
      "The 25064 th iteration gives loss of 0.1720540998523742\n",
      "The 25065 th iteration gives loss of 0.1720511665051265\n",
      "The 25066 th iteration gives loss of 0.1720482334162015\n",
      "The 25067 th iteration gives loss of 0.1720453005855764\n",
      "The 25068 th iteration gives loss of 0.17204236801321202\n",
      "The 25069 th iteration gives loss of 0.17203943569907154\n",
      "The 25070 th iteration gives loss of 0.17203650364309675\n",
      "The 25071 th iteration gives loss of 0.1720335718452687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 25072 th iteration gives loss of 0.17203064030554574\n",
      "The 25073 th iteration gives loss of 0.17202770902387557\n",
      "The 25074 th iteration gives loss of 0.17202477800022814\n",
      "The 25075 th iteration gives loss of 0.17202184723456887\n",
      "The 25076 th iteration gives loss of 0.17201891672684635\n",
      "The 25077 th iteration gives loss of 0.17201598647703695\n",
      "The 25078 th iteration gives loss of 0.17201305648508491\n",
      "The 25079 th iteration gives loss of 0.17201012675096297\n",
      "The 25080 th iteration gives loss of 0.17200719727461958\n",
      "The 25081 th iteration gives loss of 0.17200426805602909\n",
      "The 25082 th iteration gives loss of 0.17200133909513943\n",
      "The 25083 th iteration gives loss of 0.1719984103919221\n",
      "The 25084 th iteration gives loss of 0.17199548194633402\n",
      "The 25085 th iteration gives loss of 0.17199255375834366\n",
      "The 25086 th iteration gives loss of 0.17198962582789418\n",
      "The 25087 th iteration gives loss of 0.17198669815495188\n",
      "The 25088 th iteration gives loss of 0.17198377073949253\n",
      "The 25089 th iteration gives loss of 0.1719808435814641\n",
      "The 25090 th iteration gives loss of 0.17197791668083032\n",
      "The 25091 th iteration gives loss of 0.17197499003754374\n",
      "The 25092 th iteration gives loss of 0.17197206365157935\n",
      "The 25093 th iteration gives loss of 0.1719691375228919\n",
      "The 25094 th iteration gives loss of 0.17196621165143705\n",
      "The 25095 th iteration gives loss of 0.17196328603719277\n",
      "The 25096 th iteration gives loss of 0.17196036068010154\n",
      "The 25097 th iteration gives loss of 0.17195743558012372\n",
      "The 25098 th iteration gives loss of 0.17195451073722398\n",
      "The 25099 th iteration gives loss of 0.17195158615137496\n",
      "The 25100 th iteration gives loss of 0.17194866182252844\n",
      "The 25101 th iteration gives loss of 0.17194573775064353\n",
      "The 25102 th iteration gives loss of 0.17194281393567962\n",
      "The 25103 th iteration gives loss of 0.17193989037760435\n",
      "The 25104 th iteration gives loss of 0.17193696707637546\n",
      "The 25105 th iteration gives loss of 0.17193404403195428\n",
      "The 25106 th iteration gives loss of 0.17193112124430185\n",
      "The 25107 th iteration gives loss of 0.17192819871336817\n",
      "The 25108 th iteration gives loss of 0.17192527643912847\n",
      "The 25109 th iteration gives loss of 0.17192235442154677\n",
      "The 25110 th iteration gives loss of 0.1719194326605658\n",
      "The 25111 th iteration gives loss of 0.17191651115616402\n",
      "The 25112 th iteration gives loss of 0.17191358990829178\n",
      "The 25113 th iteration gives loss of 0.17191066891691575\n",
      "The 25114 th iteration gives loss of 0.17190774818200377\n",
      "The 25115 th iteration gives loss of 0.1719048277034966\n",
      "The 25116 th iteration gives loss of 0.17190190748137021\n",
      "The 25117 th iteration gives loss of 0.17189898751558083\n",
      "The 25118 th iteration gives loss of 0.1718960678060889\n",
      "The 25119 th iteration gives loss of 0.17189314835285993\n",
      "The 25120 th iteration gives loss of 0.1718902291558554\n",
      "The 25121 th iteration gives loss of 0.17188731021503\n",
      "The 25122 th iteration gives loss of 0.17188439153034904\n",
      "The 25123 th iteration gives loss of 0.1718814731017686\n",
      "The 25124 th iteration gives loss of 0.17187855492925652\n",
      "The 25125 th iteration gives loss of 0.17187563701276942\n",
      "The 25126 th iteration gives loss of 0.17187271935227819\n",
      "The 25127 th iteration gives loss of 0.17186980194773468\n",
      "The 25128 th iteration gives loss of 0.17186688479908044\n",
      "The 25129 th iteration gives loss of 0.17186396790631636\n",
      "The 25130 th iteration gives loss of 0.17186105126937457\n",
      "The 25131 th iteration gives loss of 0.17185813488822949\n",
      "The 25132 th iteration gives loss of 0.17185521876283727\n",
      "The 25133 th iteration gives loss of 0.17185230289316292\n",
      "The 25134 th iteration gives loss of 0.17184938727915905\n",
      "The 25135 th iteration gives loss of 0.17184647192079006\n",
      "The 25136 th iteration gives loss of 0.17184355681802937\n",
      "The 25137 th iteration gives loss of 0.171840641970822\n",
      "The 25138 th iteration gives loss of 0.1718377273791316\n",
      "The 25139 th iteration gives loss of 0.17183481304293277\n",
      "The 25140 th iteration gives loss of 0.1718318989621692\n",
      "The 25141 th iteration gives loss of 0.17182898513680306\n",
      "The 25142 th iteration gives loss of 0.17182607156680785\n",
      "The 25143 th iteration gives loss of 0.17182315825214492\n",
      "The 25144 th iteration gives loss of 0.17182024519276176\n",
      "The 25145 th iteration gives loss of 0.17181733238862576\n",
      "The 25146 th iteration gives loss of 0.17181441983970114\n",
      "The 25147 th iteration gives loss of 0.17181150754594463\n",
      "The 25148 th iteration gives loss of 0.17180859550732883\n",
      "The 25149 th iteration gives loss of 0.17180568372380245\n",
      "The 25150 th iteration gives loss of 0.17180277219532356\n",
      "The 25151 th iteration gives loss of 0.17179986092186486\n",
      "The 25152 th iteration gives loss of 0.17179694990338282\n",
      "The 25153 th iteration gives loss of 0.17179403913983704\n",
      "The 25154 th iteration gives loss of 0.17179112863118956\n",
      "The 25155 th iteration gives loss of 0.17178821837740274\n",
      "The 25156 th iteration gives loss of 0.17178530837843697\n",
      "The 25157 th iteration gives loss of 0.17178239863425748\n",
      "The 25158 th iteration gives loss of 0.17177948914481977\n",
      "The 25159 th iteration gives loss of 0.17177657991007328\n",
      "The 25160 th iteration gives loss of 0.17177367093000456\n",
      "The 25161 th iteration gives loss of 0.17177076220457094\n",
      "The 25162 th iteration gives loss of 0.17176785373371872\n",
      "The 25163 th iteration gives loss of 0.17176494551741986\n",
      "The 25164 th iteration gives loss of 0.17176203755561734\n",
      "The 25165 th iteration gives loss of 0.1717591298483096\n",
      "The 25166 th iteration gives loss of 0.1717562223954205\n",
      "The 25167 th iteration gives loss of 0.17175331519693107\n",
      "The 25168 th iteration gives loss of 0.1717504082527999\n",
      "The 25169 th iteration gives loss of 0.17174750156298474\n",
      "The 25170 th iteration gives loss of 0.1717445951274463\n",
      "The 25171 th iteration gives loss of 0.171741688946153\n",
      "The 25172 th iteration gives loss of 0.17173878301905196\n",
      "The 25173 th iteration gives loss of 0.17173587734612553\n",
      "The 25174 th iteration gives loss of 0.1717329719273202\n",
      "The 25175 th iteration gives loss of 0.17173006676259378\n",
      "The 25176 th iteration gives loss of 0.1717271618519234\n",
      "The 25177 th iteration gives loss of 0.17172425719525145\n",
      "The 25178 th iteration gives loss of 0.17172135279255824\n",
      "The 25179 th iteration gives loss of 0.17171844864378522\n",
      "The 25180 th iteration gives loss of 0.17171554474891385\n",
      "The 25181 th iteration gives loss of 0.17171264110789664\n",
      "The 25182 th iteration gives loss of 0.17170973772069018\n",
      "The 25183 th iteration gives loss of 0.1717068345872662\n",
      "The 25184 th iteration gives loss of 0.17170393170757678\n",
      "The 25185 th iteration gives loss of 0.17170102908157822\n",
      "The 25186 th iteration gives loss of 0.17169812670925075\n",
      "The 25187 th iteration gives loss of 0.1716952245905422\n",
      "The 25188 th iteration gives loss of 0.17169232272541946\n",
      "The 25189 th iteration gives loss of 0.17168942111383154\n",
      "The 25190 th iteration gives loss of 0.17168651975575708\n",
      "The 25191 th iteration gives loss of 0.17168361865115453\n",
      "The 25192 th iteration gives loss of 0.17168071779997857\n",
      "The 25193 th iteration gives loss of 0.1716778172021904\n",
      "The 25194 th iteration gives loss of 0.17167491685776062\n",
      "The 25195 th iteration gives loss of 0.17167201676664032\n",
      "The 25196 th iteration gives loss of 0.1716691169287995\n",
      "The 25197 th iteration gives loss of 0.17166621734418494\n",
      "The 25198 th iteration gives loss of 0.17166331801277254\n",
      "The 25199 th iteration gives loss of 0.1716604189345206\n",
      "The 25200 th iteration gives loss of 0.1716575201093876\n",
      "The 25201 th iteration gives loss of 0.17165462153734473\n",
      "The 25202 th iteration gives loss of 0.17165172321834252\n",
      "The 25203 th iteration gives loss of 0.1716488251523479\n",
      "The 25204 th iteration gives loss of 0.17164592733931547\n",
      "The 25205 th iteration gives loss of 0.17164302977921064\n",
      "The 25206 th iteration gives loss of 0.17164013247199547\n",
      "The 25207 th iteration gives loss of 0.1716372354176303\n",
      "The 25208 th iteration gives loss of 0.1716343386160873\n",
      "The 25209 th iteration gives loss of 0.17163144206731404\n",
      "The 25210 th iteration gives loss of 0.1716285457712739\n",
      "The 25211 th iteration gives loss of 0.17162564972792893\n",
      "The 25212 th iteration gives loss of 0.17162275393725704\n",
      "The 25213 th iteration gives loss of 0.17161985839919536\n",
      "The 25214 th iteration gives loss of 0.17161696311371927\n",
      "The 25215 th iteration gives loss of 0.17161406808078286\n",
      "The 25216 th iteration gives loss of 0.1716111733003562\n",
      "The 25217 th iteration gives loss of 0.1716082787723948\n",
      "The 25218 th iteration gives loss of 0.17160538449686757\n",
      "The 25219 th iteration gives loss of 0.1716024904737192\n",
      "The 25220 th iteration gives loss of 0.17159959670293531\n",
      "The 25221 th iteration gives loss of 0.17159670318445155\n",
      "The 25222 th iteration gives loss of 0.17159380991825443\n",
      "The 25223 th iteration gives loss of 0.1715909169042886\n",
      "The 25224 th iteration gives loss of 0.17158802414252336\n",
      "The 25225 th iteration gives loss of 0.17158513163292077\n",
      "The 25226 th iteration gives loss of 0.17158223937543626\n",
      "The 25227 th iteration gives loss of 0.17157934737003858\n",
      "The 25228 th iteration gives loss of 0.1715764556166838\n",
      "The 25229 th iteration gives loss of 0.1715735641153286\n",
      "The 25230 th iteration gives loss of 0.17157067286595107\n",
      "The 25231 th iteration gives loss of 0.17156778186850105\n",
      "The 25232 th iteration gives loss of 0.1715648911229415\n",
      "The 25233 th iteration gives loss of 0.17156200062924465\n",
      "The 25234 th iteration gives loss of 0.17155911038735222\n",
      "The 25235 th iteration gives loss of 0.17155622039723567\n",
      "The 25236 th iteration gives loss of 0.17155333065886244\n",
      "The 25237 th iteration gives loss of 0.17155044117218565\n",
      "The 25238 th iteration gives loss of 0.17154755193717652\n",
      "The 25239 th iteration gives loss of 0.17154466295378495\n",
      "The 25240 th iteration gives loss of 0.1715417742219856\n",
      "The 25241 th iteration gives loss of 0.17153888574172987\n",
      "The 25242 th iteration gives loss of 0.17153599751298015\n",
      "The 25243 th iteration gives loss of 0.17153310953570575\n",
      "The 25244 th iteration gives loss of 0.17153022180986294\n",
      "The 25245 th iteration gives loss of 0.1715273343354119\n",
      "The 25246 th iteration gives loss of 0.17152444711232426\n",
      "The 25247 th iteration gives loss of 0.17152156014054512\n",
      "The 25248 th iteration gives loss of 0.17151867342005092\n",
      "The 25249 th iteration gives loss of 0.17151578695079187\n",
      "The 25250 th iteration gives loss of 0.17151290073273937\n",
      "The 25251 th iteration gives loss of 0.1715100147658559\n",
      "The 25252 th iteration gives loss of 0.17150712905009274\n",
      "The 25253 th iteration gives loss of 0.17150424358542393\n",
      "The 25254 th iteration gives loss of 0.17150135837180294\n",
      "The 25255 th iteration gives loss of 0.1714984734091924\n",
      "The 25256 th iteration gives loss of 0.17149558869755097\n",
      "The 25257 th iteration gives loss of 0.1714927042368545\n",
      "The 25258 th iteration gives loss of 0.17148982002705493\n",
      "The 25259 th iteration gives loss of 0.17148693606811435\n",
      "The 25260 th iteration gives loss of 0.1714840523599923\n",
      "The 25261 th iteration gives loss of 0.1714811689026549\n",
      "The 25262 th iteration gives loss of 0.1714782856960639\n",
      "The 25263 th iteration gives loss of 0.17147540274017356\n",
      "The 25264 th iteration gives loss of 0.17147252003495436\n",
      "The 25265 th iteration gives loss of 0.17146963758037007\n",
      "The 25266 th iteration gives loss of 0.17146675537637265\n",
      "The 25267 th iteration gives loss of 0.171463873422939\n",
      "The 25268 th iteration gives loss of 0.17146099172001633\n",
      "The 25269 th iteration gives loss of 0.17145811026757007\n",
      "The 25270 th iteration gives loss of 0.17145522906556632\n",
      "The 25271 th iteration gives loss of 0.17145234811396343\n",
      "The 25272 th iteration gives loss of 0.1714494674127193\n",
      "The 25273 th iteration gives loss of 0.17144658696180948\n",
      "The 25274 th iteration gives loss of 0.17144370676117954\n",
      "The 25275 th iteration gives loss of 0.17144082681080655\n",
      "The 25276 th iteration gives loss of 0.17143794711064272\n",
      "The 25277 th iteration gives loss of 0.1714350676606518\n",
      "The 25278 th iteration gives loss of 0.17143218846079458\n",
      "The 25279 th iteration gives loss of 0.17142930951103486\n",
      "The 25280 th iteration gives loss of 0.17142643081134032\n",
      "The 25281 th iteration gives loss of 0.1714235523616684\n",
      "The 25282 th iteration gives loss of 0.1714206741619706\n",
      "The 25283 th iteration gives loss of 0.1714177962122268\n",
      "The 25284 th iteration gives loss of 0.17141491851238655\n",
      "The 25285 th iteration gives loss of 0.1714120410624169\n",
      "The 25286 th iteration gives loss of 0.171409163862278\n",
      "The 25287 th iteration gives loss of 0.17140628691193188\n",
      "The 25288 th iteration gives loss of 0.17140341021135147\n",
      "The 25289 th iteration gives loss of 0.1714005337604784\n",
      "The 25290 th iteration gives loss of 0.17139765755928202\n",
      "The 25291 th iteration gives loss of 0.17139478160773633\n",
      "The 25292 th iteration gives loss of 0.17139190590578846\n",
      "The 25293 th iteration gives loss of 0.17138903045341083\n",
      "The 25294 th iteration gives loss of 0.1713861552505582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 25295 th iteration gives loss of 0.1713832802971911\n",
      "The 25296 th iteration gives loss of 0.17138040559328377\n",
      "The 25297 th iteration gives loss of 0.17137753113878185\n",
      "The 25298 th iteration gives loss of 0.1713746569336679\n",
      "The 25299 th iteration gives loss of 0.1713717829778881\n",
      "The 25300 th iteration gives loss of 0.171368909271402\n",
      "The 25301 th iteration gives loss of 0.17136603581417928\n",
      "The 25302 th iteration gives loss of 0.17136316260618226\n",
      "The 25303 th iteration gives loss of 0.17136028964737562\n",
      "The 25304 th iteration gives loss of 0.17135741693771936\n",
      "The 25305 th iteration gives loss of 0.17135454447716345\n",
      "The 25306 th iteration gives loss of 0.1713516722656903\n",
      "The 25307 th iteration gives loss of 0.17134880030324498\n",
      "The 25308 th iteration gives loss of 0.17134592858979855\n",
      "The 25309 th iteration gives loss of 0.17134305712530784\n",
      "The 25310 th iteration gives loss of 0.1713401859097518\n",
      "The 25311 th iteration gives loss of 0.17133731494307577\n",
      "The 25312 th iteration gives loss of 0.1713344442252437\n",
      "The 25313 th iteration gives loss of 0.17133157375620706\n",
      "The 25314 th iteration gives loss of 0.17132870353595467\n",
      "The 25315 th iteration gives loss of 0.17132583356443234\n",
      "The 25316 th iteration gives loss of 0.17132296384160678\n",
      "The 25317 th iteration gives loss of 0.1713200943674355\n",
      "The 25318 th iteration gives loss of 0.171317225141885\n",
      "The 25319 th iteration gives loss of 0.17131435616491372\n",
      "The 25320 th iteration gives loss of 0.17131148743648578\n",
      "The 25321 th iteration gives loss of 0.17130861895656077\n",
      "The 25322 th iteration gives loss of 0.17130575072510926\n",
      "The 25323 th iteration gives loss of 0.17130288274208288\n",
      "The 25324 th iteration gives loss of 0.17130001500745068\n",
      "The 25325 th iteration gives loss of 0.17129714752116473\n",
      "The 25326 th iteration gives loss of 0.17129428028320665\n",
      "The 25327 th iteration gives loss of 0.17129141329352215\n",
      "The 25328 th iteration gives loss of 0.17128854655208253\n",
      "The 25329 th iteration gives loss of 0.17128568005884104\n",
      "The 25330 th iteration gives loss of 0.17128281381376662\n",
      "The 25331 th iteration gives loss of 0.17127994781682582\n",
      "The 25332 th iteration gives loss of 0.17127708206797257\n",
      "The 25333 th iteration gives loss of 0.17127421656717404\n",
      "The 25334 th iteration gives loss of 0.17127135131439214\n",
      "The 25335 th iteration gives loss of 0.17126848630958028\n",
      "The 25336 th iteration gives loss of 0.1712656215527109\n",
      "The 25337 th iteration gives loss of 0.171262757043738\n",
      "The 25338 th iteration gives loss of 0.17125989278264042\n",
      "The 25339 th iteration gives loss of 0.1712570287693677\n",
      "The 25340 th iteration gives loss of 0.1712541650038768\n",
      "The 25341 th iteration gives loss of 0.17125130148614187\n",
      "The 25342 th iteration gives loss of 0.17124843821611085\n",
      "The 25343 th iteration gives loss of 0.1712455751937618\n",
      "The 25344 th iteration gives loss of 0.17124271241904804\n",
      "The 25345 th iteration gives loss of 0.17123984989194518\n",
      "The 25346 th iteration gives loss of 0.1712369876124041\n",
      "The 25347 th iteration gives loss of 0.1712341255803814\n",
      "The 25348 th iteration gives loss of 0.17123126379584563\n",
      "The 25349 th iteration gives loss of 0.17122840225876776\n",
      "The 25350 th iteration gives loss of 0.1712255409690903\n",
      "The 25351 th iteration gives loss of 0.17122267992679888\n",
      "The 25352 th iteration gives loss of 0.1712198191318342\n",
      "The 25353 th iteration gives loss of 0.17121695858418362\n",
      "The 25354 th iteration gives loss of 0.17121409828379014\n",
      "The 25355 th iteration gives loss of 0.17121123823061876\n",
      "The 25356 th iteration gives loss of 0.1712083784246353\n",
      "The 25357 th iteration gives loss of 0.17120551886579588\n",
      "The 25358 th iteration gives loss of 0.17120265955407254\n",
      "The 25359 th iteration gives loss of 0.17119980048942213\n",
      "The 25360 th iteration gives loss of 0.17119694167180424\n",
      "The 25361 th iteration gives loss of 0.17119408310119122\n",
      "The 25362 th iteration gives loss of 0.17119122477753929\n",
      "The 25363 th iteration gives loss of 0.17118836670081217\n",
      "The 25364 th iteration gives loss of 0.17118550887097125\n",
      "The 25365 th iteration gives loss of 0.17118265128798027\n",
      "The 25366 th iteration gives loss of 0.17117979395179547\n",
      "The 25367 th iteration gives loss of 0.17117693686238702\n",
      "The 25368 th iteration gives loss of 0.1711740800197159\n",
      "The 25369 th iteration gives loss of 0.1711712234237489\n",
      "The 25370 th iteration gives loss of 0.171168367074437\n",
      "The 25371 th iteration gives loss of 0.17116551097175428\n",
      "The 25372 th iteration gives loss of 0.17116265511565218\n",
      "The 25373 th iteration gives loss of 0.17115979950609758\n",
      "The 25374 th iteration gives loss of 0.17115694414306398\n",
      "The 25375 th iteration gives loss of 0.17115408902649776\n",
      "The 25376 th iteration gives loss of 0.1711512341563593\n",
      "The 25377 th iteration gives loss of 0.17114837953263456\n",
      "The 25378 th iteration gives loss of 0.17114552515526704\n",
      "The 25379 th iteration gives loss of 0.1711426710242292\n",
      "The 25380 th iteration gives loss of 0.17113981713947754\n",
      "The 25381 th iteration gives loss of 0.1711369635009669\n",
      "The 25382 th iteration gives loss of 0.17113411010866908\n",
      "The 25383 th iteration gives loss of 0.17113125696254583\n",
      "The 25384 th iteration gives loss of 0.1711284040625649\n",
      "The 25385 th iteration gives loss of 0.17112555140867922\n",
      "The 25386 th iteration gives loss of 0.17112269900085827\n",
      "The 25387 th iteration gives loss of 0.17111984683906364\n",
      "The 25388 th iteration gives loss of 0.17111699492326024\n",
      "The 25389 th iteration gives loss of 0.17111414325340307\n",
      "The 25390 th iteration gives loss of 0.1711112918294548\n",
      "The 25391 th iteration gives loss of 0.17110844065138894\n",
      "The 25392 th iteration gives loss of 0.17110558971915266\n",
      "The 25393 th iteration gives loss of 0.17110273903272288\n",
      "The 25394 th iteration gives loss of 0.1710998885920541\n",
      "The 25395 th iteration gives loss of 0.1710970383971166\n",
      "The 25396 th iteration gives loss of 0.17109418844786384\n",
      "The 25397 th iteration gives loss of 0.17109133874426677\n",
      "The 25398 th iteration gives loss of 0.17108848928627662\n",
      "The 25399 th iteration gives loss of 0.17108564007386248\n",
      "The 25400 th iteration gives loss of 0.17108279110699895\n",
      "The 25401 th iteration gives loss of 0.17107994238562835\n",
      "The 25402 th iteration gives loss of 0.17107709390972875\n",
      "The 25403 th iteration gives loss of 0.17107424567924925\n",
      "The 25404 th iteration gives loss of 0.17107139769416488\n",
      "The 25405 th iteration gives loss of 0.17106854995443202\n",
      "The 25406 th iteration gives loss of 0.17106570246002112\n",
      "The 25407 th iteration gives loss of 0.1710628552108776\n",
      "The 25408 th iteration gives loss of 0.17106000820698425\n",
      "The 25409 th iteration gives loss of 0.17105716144828725\n",
      "The 25410 th iteration gives loss of 0.17105431493476173\n",
      "The 25411 th iteration gives loss of 0.17105146866636195\n",
      "The 25412 th iteration gives loss of 0.17104862264306203\n",
      "The 25413 th iteration gives loss of 0.17104577686480696\n",
      "The 25414 th iteration gives loss of 0.17104293133157322\n",
      "The 25415 th iteration gives loss of 0.1710400860433242\n",
      "The 25416 th iteration gives loss of 0.17103724100001488\n",
      "The 25417 th iteration gives loss of 0.17103439620160826\n",
      "The 25418 th iteration gives loss of 0.1710315516480754\n",
      "The 25419 th iteration gives loss of 0.17102870733937398\n",
      "The 25420 th iteration gives loss of 0.1710258632754778\n",
      "The 25421 th iteration gives loss of 0.17102301945632348\n",
      "The 25422 th iteration gives loss of 0.17102017588189544\n",
      "The 25423 th iteration gives loss of 0.17101733255214763\n",
      "The 25424 th iteration gives loss of 0.17101448946704467\n",
      "The 25425 th iteration gives loss of 0.1710116466265566\n",
      "The 25426 th iteration gives loss of 0.17100880403062893\n",
      "The 25427 th iteration gives loss of 0.17100596167924323\n",
      "The 25428 th iteration gives loss of 0.17100311957234837\n",
      "The 25429 th iteration gives loss of 0.17100027770992157\n",
      "The 25430 th iteration gives loss of 0.17099743609191367\n",
      "The 25431 th iteration gives loss of 0.17099459471829037\n",
      "The 25432 th iteration gives loss of 0.17099175358901697\n",
      "The 25433 th iteration gives loss of 0.17098891270404878\n",
      "The 25434 th iteration gives loss of 0.1709860720633632\n",
      "The 25435 th iteration gives loss of 0.17098323166691487\n",
      "The 25436 th iteration gives loss of 0.1709803915146634\n",
      "The 25437 th iteration gives loss of 0.17097755160657963\n",
      "The 25438 th iteration gives loss of 0.17097471194261182\n",
      "The 25439 th iteration gives loss of 0.1709718725227459\n",
      "The 25440 th iteration gives loss of 0.17096903334692123\n",
      "The 25441 th iteration gives loss of 0.17096619441511446\n",
      "The 25442 th iteration gives loss of 0.17096335572728813\n",
      "The 25443 th iteration gives loss of 0.17096051728339637\n",
      "The 25444 th iteration gives loss of 0.1709576790834135\n",
      "The 25445 th iteration gives loss of 0.17095484112729337\n",
      "The 25446 th iteration gives loss of 0.1709520034150076\n",
      "The 25447 th iteration gives loss of 0.17094916594651222\n",
      "The 25448 th iteration gives loss of 0.17094632872177326\n",
      "The 25449 th iteration gives loss of 0.1709434917407413\n",
      "The 25450 th iteration gives loss of 0.17094065500340147\n",
      "The 25451 th iteration gives loss of 0.17093781850970374\n",
      "The 25452 th iteration gives loss of 0.17093498225961773\n",
      "The 25453 th iteration gives loss of 0.17093214625309947\n",
      "The 25454 th iteration gives loss of 0.17092931049010884\n",
      "The 25455 th iteration gives loss of 0.17092647497061694\n",
      "The 25456 th iteration gives loss of 0.17092363969458008\n",
      "The 25457 th iteration gives loss of 0.17092080466197496\n",
      "The 25458 th iteration gives loss of 0.17091796987275287\n",
      "The 25459 th iteration gives loss of 0.17091513532687583\n",
      "The 25460 th iteration gives loss of 0.17091230102431373\n",
      "The 25461 th iteration gives loss of 0.1709094669650202\n",
      "The 25462 th iteration gives loss of 0.17090663314896493\n",
      "The 25463 th iteration gives loss of 0.1709037995761153\n",
      "The 25464 th iteration gives loss of 0.17090096624642104\n",
      "The 25465 th iteration gives loss of 0.17089813315986543\n",
      "The 25466 th iteration gives loss of 0.17089530031638675\n",
      "The 25467 th iteration gives loss of 0.17089246771596814\n",
      "The 25468 th iteration gives loss of 0.17088963535856438\n",
      "The 25469 th iteration gives loss of 0.1708868032441411\n",
      "The 25470 th iteration gives loss of 0.17088397137266478\n",
      "The 25471 th iteration gives loss of 0.17088113974408256\n",
      "The 25472 th iteration gives loss of 0.1708783083583753\n",
      "The 25473 th iteration gives loss of 0.17087547721549137\n",
      "The 25474 th iteration gives loss of 0.1708726463154109\n",
      "The 25475 th iteration gives loss of 0.1708698156580944\n",
      "The 25476 th iteration gives loss of 0.17086698524348629\n",
      "The 25477 th iteration gives loss of 0.17086415507156466\n",
      "The 25478 th iteration gives loss of 0.17086132514228936\n",
      "The 25479 th iteration gives loss of 0.17085849545562326\n",
      "The 25480 th iteration gives loss of 0.17085566601153715\n",
      "The 25481 th iteration gives loss of 0.17085283680998614\n",
      "The 25482 th iteration gives loss of 0.17085000785093052\n",
      "The 25483 th iteration gives loss of 0.17084717913433559\n",
      "The 25484 th iteration gives loss of 0.17084435066016976\n",
      "The 25485 th iteration gives loss of 0.1708415224283992\n",
      "The 25486 th iteration gives loss of 0.17083869443897573\n",
      "The 25487 th iteration gives loss of 0.17083586669186587\n",
      "The 25488 th iteration gives loss of 0.1708330391870396\n",
      "The 25489 th iteration gives loss of 0.17083021192444683\n",
      "The 25490 th iteration gives loss of 0.1708273849040699\n",
      "The 25491 th iteration gives loss of 0.17082455812585767\n",
      "The 25492 th iteration gives loss of 0.1708217315897709\n",
      "The 25493 th iteration gives loss of 0.17081890529579336\n",
      "The 25494 th iteration gives loss of 0.17081607924386255\n",
      "The 25495 th iteration gives loss of 0.17081325343394965\n",
      "The 25496 th iteration gives loss of 0.17081042786602785\n",
      "The 25497 th iteration gives loss of 0.17080760254004831\n",
      "The 25498 th iteration gives loss of 0.17080477745598088\n",
      "The 25499 th iteration gives loss of 0.1708019526137952\n",
      "The 25500 th iteration gives loss of 0.17079912801344507\n",
      "The 25501 th iteration gives loss of 0.17079630365489507\n",
      "The 25502 th iteration gives loss of 0.1707934795381016\n",
      "The 25503 th iteration gives loss of 0.1707906556630456\n",
      "The 25504 th iteration gives loss of 0.17078783202967598\n",
      "The 25505 th iteration gives loss of 0.17078500863796695\n",
      "The 25506 th iteration gives loss of 0.17078218548787003\n",
      "The 25507 th iteration gives loss of 0.17077936257935086\n",
      "The 25508 th iteration gives loss of 0.1707765399123776\n",
      "The 25509 th iteration gives loss of 0.17077371748691117\n",
      "The 25510 th iteration gives loss of 0.17077089530291756\n",
      "The 25511 th iteration gives loss of 0.17076807336035324\n",
      "The 25512 th iteration gives loss of 0.17076525165919273\n",
      "The 25513 th iteration gives loss of 0.170762430199389\n",
      "The 25514 th iteration gives loss of 0.17075960898091355\n",
      "The 25515 th iteration gives loss of 0.1707567880037245\n",
      "The 25516 th iteration gives loss of 0.1707539672677824\n",
      "The 25517 th iteration gives loss of 0.17075114677305908\n",
      "The 25518 th iteration gives loss of 0.17074832651950891\n",
      "The 25519 th iteration gives loss of 0.17074550650709874\n",
      "The 25520 th iteration gives loss of 0.17074268673579554\n",
      "The 25521 th iteration gives loss of 0.17073986720556153\n",
      "The 25522 th iteration gives loss of 0.17073704791636227\n",
      "The 25523 th iteration gives loss of 0.1707342288681427\n",
      "The 25524 th iteration gives loss of 0.1707314100608969\n",
      "The 25525 th iteration gives loss of 0.1707285914945648\n",
      "The 25526 th iteration gives loss of 0.1707257731691223\n",
      "The 25527 th iteration gives loss of 0.1707229550845219\n",
      "The 25528 th iteration gives loss of 0.17072013724073742\n",
      "The 25529 th iteration gives loss of 0.17071731963772552\n",
      "The 25530 th iteration gives loss of 0.17071450227545146\n",
      "The 25531 th iteration gives loss of 0.1707116851538774\n",
      "The 25532 th iteration gives loss of 0.17070886827297385\n",
      "The 25533 th iteration gives loss of 0.17070605163269098\n",
      "The 25534 th iteration gives loss of 0.17070323523301156\n",
      "The 25535 th iteration gives loss of 0.1707004190738815\n",
      "The 25536 th iteration gives loss of 0.17069760315527266\n",
      "The 25537 th iteration gives loss of 0.17069478747714883\n",
      "The 25538 th iteration gives loss of 0.1706919720394628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 25539 th iteration gives loss of 0.1706891568421962\n",
      "The 25540 th iteration gives loss of 0.17068634188529627\n",
      "The 25541 th iteration gives loss of 0.17068352716873736\n",
      "The 25542 th iteration gives loss of 0.170680712692474\n",
      "The 25543 th iteration gives loss of 0.17067789845648215\n",
      "The 25544 th iteration gives loss of 0.17067508446071072\n",
      "The 25545 th iteration gives loss of 0.17067227070513138\n",
      "The 25546 th iteration gives loss of 0.17066945718970666\n",
      "The 25547 th iteration gives loss of 0.17066664391440606\n",
      "The 25548 th iteration gives loss of 0.17066383087917641\n",
      "The 25549 th iteration gives loss of 0.17066101808400544\n",
      "The 25550 th iteration gives loss of 0.17065820552883107\n",
      "The 25551 th iteration gives loss of 0.1706553932136266\n",
      "The 25552 th iteration gives loss of 0.17065258113836246\n",
      "The 25553 th iteration gives loss of 0.17064976930299938\n",
      "The 25554 th iteration gives loss of 0.17064695770749985\n",
      "The 25555 th iteration gives loss of 0.17064414635182226\n",
      "The 25556 th iteration gives loss of 0.17064133523593372\n",
      "The 25557 th iteration gives loss of 0.17063852435980348\n",
      "The 25558 th iteration gives loss of 0.17063571372339456\n",
      "The 25559 th iteration gives loss of 0.17063290332665915\n",
      "The 25560 th iteration gives loss of 0.17063009316957306\n",
      "The 25561 th iteration gives loss of 0.17062728325208742\n",
      "The 25562 th iteration gives loss of 0.17062447357417662\n",
      "The 25563 th iteration gives loss of 0.17062166413580485\n",
      "The 25564 th iteration gives loss of 0.1706188549369325\n",
      "The 25565 th iteration gives loss of 0.17061604597751726\n",
      "The 25566 th iteration gives loss of 0.17061323725753522\n",
      "The 25567 th iteration gives loss of 0.1706104287769405\n",
      "The 25568 th iteration gives loss of 0.17060762053569428\n",
      "The 25569 th iteration gives loss of 0.17060481253377088\n",
      "The 25570 th iteration gives loss of 0.1706020047711292\n",
      "The 25571 th iteration gives loss of 0.17059919724772848\n",
      "The 25572 th iteration gives loss of 0.17059638996354132\n",
      "The 25573 th iteration gives loss of 0.17059358291851748\n",
      "The 25574 th iteration gives loss of 0.17059077611263254\n",
      "The 25575 th iteration gives loss of 0.1705879695458552\n",
      "The 25576 th iteration gives loss of 0.1705851632181318\n",
      "The 25577 th iteration gives loss of 0.1705823571294395\n",
      "The 25578 th iteration gives loss of 0.1705795512797381\n",
      "The 25579 th iteration gives loss of 0.170576745668985\n",
      "The 25580 th iteration gives loss of 0.17057394029716075\n",
      "The 25581 th iteration gives loss of 0.17057113516420588\n",
      "The 25582 th iteration gives loss of 0.17056833027010398\n",
      "The 25583 th iteration gives loss of 0.1705655256148078\n",
      "The 25584 th iteration gives loss of 0.17056272119829022\n",
      "The 25585 th iteration gives loss of 0.17055991702050413\n",
      "The 25586 th iteration gives loss of 0.17055711308141677\n",
      "The 25587 th iteration gives loss of 0.17055430938099853\n",
      "The 25588 th iteration gives loss of 0.1705515059192099\n",
      "The 25589 th iteration gives loss of 0.1705487026960074\n",
      "The 25590 th iteration gives loss of 0.1705458997113639\n",
      "The 25591 th iteration gives loss of 0.17054309696523978\n",
      "The 25592 th iteration gives loss of 0.17054029445759955\n",
      "The 25593 th iteration gives loss of 0.1705374921884044\n",
      "The 25594 th iteration gives loss of 0.17053469015761835\n",
      "The 25595 th iteration gives loss of 0.1705318883652108\n",
      "The 25596 th iteration gives loss of 0.17052908681114035\n",
      "The 25597 th iteration gives loss of 0.17052628549536566\n",
      "The 25598 th iteration gives loss of 0.17052348441786716\n",
      "The 25599 th iteration gives loss of 0.17052068357859443\n",
      "The 25600 th iteration gives loss of 0.17051788297751427\n",
      "The 25601 th iteration gives loss of 0.1705150826145937\n",
      "The 25602 th iteration gives loss of 0.17051228248980005\n",
      "The 25603 th iteration gives loss of 0.17050948260308613\n",
      "The 25604 th iteration gives loss of 0.17050668295442598\n",
      "The 25605 th iteration gives loss of 0.17050388354377244\n",
      "The 25606 th iteration gives loss of 0.17050108437109415\n",
      "The 25607 th iteration gives loss of 0.17049828543636125\n",
      "The 25608 th iteration gives loss of 0.17049548673953543\n",
      "The 25609 th iteration gives loss of 0.17049268828057804\n",
      "The 25610 th iteration gives loss of 0.17048989005944726\n",
      "The 25611 th iteration gives loss of 0.1704870920761172\n",
      "The 25612 th iteration gives loss of 0.17048429433054763\n",
      "The 25613 th iteration gives loss of 0.17048149682270686\n",
      "The 25614 th iteration gives loss of 0.1704786995525539\n",
      "The 25615 th iteration gives loss of 0.17047590252004\n",
      "The 25616 th iteration gives loss of 0.1704731057251554\n",
      "The 25617 th iteration gives loss of 0.17047030916785075\n",
      "The 25618 th iteration gives loss of 0.17046751284808975\n",
      "The 25619 th iteration gives loss of 0.17046471676583053\n",
      "The 25620 th iteration gives loss of 0.1704619209210446\n",
      "The 25621 th iteration gives loss of 0.17045912531370197\n",
      "The 25622 th iteration gives loss of 0.17045632994375082\n",
      "The 25623 th iteration gives loss of 0.1704535348111686\n",
      "The 25624 th iteration gives loss of 0.170450739915912\n",
      "The 25625 th iteration gives loss of 0.170447945257949\n",
      "The 25626 th iteration gives loss of 0.17044515083724143\n",
      "The 25627 th iteration gives loss of 0.1704423566537526\n",
      "The 25628 th iteration gives loss of 0.1704395627074476\n",
      "The 25629 th iteration gives loss of 0.17043676899828678\n",
      "The 25630 th iteration gives loss of 0.17043397552624018\n",
      "The 25631 th iteration gives loss of 0.17043118229126863\n",
      "The 25632 th iteration gives loss of 0.17042838929334156\n",
      "The 25633 th iteration gives loss of 0.17042559653242131\n",
      "The 25634 th iteration gives loss of 0.17042280400846213\n",
      "The 25635 th iteration gives loss of 0.17042001172143498\n",
      "The 25636 th iteration gives loss of 0.17041721967130685\n",
      "The 25637 th iteration gives loss of 0.17041442785804484\n",
      "The 25638 th iteration gives loss of 0.1704116362815879\n",
      "The 25639 th iteration gives loss of 0.1704088449419401\n",
      "The 25640 th iteration gives loss of 0.17040605383903798\n",
      "The 25641 th iteration gives loss of 0.1704032629728498\n",
      "The 25642 th iteration gives loss of 0.17040047234334438\n",
      "The 25643 th iteration gives loss of 0.17039768195048152\n",
      "The 25644 th iteration gives loss of 0.1703948917942318\n",
      "The 25645 th iteration gives loss of 0.1703921018745489\n",
      "The 25646 th iteration gives loss of 0.17038931219140166\n",
      "The 25647 th iteration gives loss of 0.17038652274476443\n",
      "The 25648 th iteration gives loss of 0.17038373353459016\n",
      "The 25649 th iteration gives loss of 0.17038094456083627\n",
      "The 25650 th iteration gives loss of 0.17037815582348598\n",
      "The 25651 th iteration gives loss of 0.17037536732248473\n",
      "The 25652 th iteration gives loss of 0.17037257905781722\n",
      "The 25653 th iteration gives loss of 0.1703697910294272\n",
      "The 25654 th iteration gives loss of 0.17036700323728646\n",
      "The 25655 th iteration gives loss of 0.17036421568136134\n",
      "The 25656 th iteration gives loss of 0.17036142836161763\n",
      "The 25657 th iteration gives loss of 0.17035864127801273\n",
      "The 25658 th iteration gives loss of 0.1703558544305126\n",
      "The 25659 th iteration gives loss of 0.1703530678190839\n",
      "The 25660 th iteration gives loss of 0.17035028144369455\n",
      "The 25661 th iteration gives loss of 0.1703474953043027\n",
      "The 25662 th iteration gives loss of 0.17034470940088212\n",
      "The 25663 th iteration gives loss of 0.17034192373337778\n",
      "The 25664 th iteration gives loss of 0.17033913830176706\n",
      "The 25665 th iteration gives loss of 0.17033635310602335\n",
      "The 25666 th iteration gives loss of 0.1703335681460879\n",
      "The 25667 th iteration gives loss of 0.17033078342193758\n",
      "The 25668 th iteration gives loss of 0.17032799893354442\n",
      "The 25669 th iteration gives loss of 0.17032521468085352\n",
      "The 25670 th iteration gives loss of 0.17032243066384423\n",
      "The 25671 th iteration gives loss of 0.17031964688247933\n",
      "The 25672 th iteration gives loss of 0.17031686333671545\n",
      "The 25673 th iteration gives loss of 0.1703140800265244\n",
      "The 25674 th iteration gives loss of 0.1703112969518635\n",
      "The 25675 th iteration gives loss of 0.17030851411271056\n",
      "The 25676 th iteration gives loss of 0.17030573150901746\n",
      "The 25677 th iteration gives loss of 0.1703029491407394\n",
      "The 25678 th iteration gives loss of 0.1703001670078617\n",
      "The 25679 th iteration gives loss of 0.17029738511034986\n",
      "The 25680 th iteration gives loss of 0.17029460344814853\n",
      "The 25681 th iteration gives loss of 0.17029182202122814\n",
      "The 25682 th iteration gives loss of 0.17028904082956012\n",
      "The 25683 th iteration gives loss of 0.17028625987310678\n",
      "The 25684 th iteration gives loss of 0.17028347915182662\n",
      "The 25685 th iteration gives loss of 0.17028069866568657\n",
      "The 25686 th iteration gives loss of 0.17027791841465723\n",
      "The 25687 th iteration gives loss of 0.17027513839869352\n",
      "The 25688 th iteration gives loss of 0.17027235861776827\n",
      "The 25689 th iteration gives loss of 0.1702695790718444\n",
      "The 25690 th iteration gives loss of 0.1702667997608768\n",
      "The 25691 th iteration gives loss of 0.17026402068484156\n",
      "The 25692 th iteration gives loss of 0.1702612418437022\n",
      "The 25693 th iteration gives loss of 0.17025846323741908\n",
      "The 25694 th iteration gives loss of 0.17025568486595433\n",
      "The 25695 th iteration gives loss of 0.17025290672927654\n",
      "The 25696 th iteration gives loss of 0.17025012882734536\n",
      "The 25697 th iteration gives loss of 0.1702473511601264\n",
      "The 25698 th iteration gives loss of 0.17024457372758675\n",
      "The 25699 th iteration gives loss of 0.17024179652969063\n",
      "The 25700 th iteration gives loss of 0.17023901956639953\n",
      "The 25701 th iteration gives loss of 0.17023624283768754\n",
      "The 25702 th iteration gives loss of 0.17023346634350722\n",
      "The 25703 th iteration gives loss of 0.1702306900838257\n",
      "The 25704 th iteration gives loss of 0.17022791405860566\n",
      "The 25705 th iteration gives loss of 0.170225138267818\n",
      "The 25706 th iteration gives loss of 0.17022236271143174\n",
      "The 25707 th iteration gives loss of 0.17021958738939302\n",
      "The 25708 th iteration gives loss of 0.17021681230168276\n",
      "The 25709 th iteration gives loss of 0.17021403744825914\n",
      "The 25710 th iteration gives loss of 0.17021126282908636\n",
      "The 25711 th iteration gives loss of 0.17020848844413491\n",
      "The 25712 th iteration gives loss of 0.1702057142933552\n",
      "The 25713 th iteration gives loss of 0.1702029403767297\n",
      "The 25714 th iteration gives loss of 0.1702001666942063\n",
      "The 25715 th iteration gives loss of 0.17019739324575814\n",
      "The 25716 th iteration gives loss of 0.1701946200313546\n",
      "The 25717 th iteration gives loss of 0.17019184705094537\n",
      "The 25718 th iteration gives loss of 0.17018907430450161\n",
      "The 25719 th iteration gives loss of 0.17018630179200447\n",
      "The 25720 th iteration gives loss of 0.17018352951339785\n",
      "The 25721 th iteration gives loss of 0.17018075746864766\n",
      "The 25722 th iteration gives loss of 0.17017798565772987\n",
      "The 25723 th iteration gives loss of 0.17017521408059766\n",
      "The 25724 th iteration gives loss of 0.17017244273722293\n",
      "The 25725 th iteration gives loss of 0.17016967162755858\n",
      "The 25726 th iteration gives loss of 0.17016690075159288\n",
      "The 25727 th iteration gives loss of 0.17016413010926523\n",
      "The 25728 th iteration gives loss of 0.17016135970055155\n",
      "The 25729 th iteration gives loss of 0.17015858952542076\n",
      "The 25730 th iteration gives loss of 0.17015581958383078\n",
      "The 25731 th iteration gives loss of 0.170153049875741\n",
      "The 25732 th iteration gives loss of 0.1701502804011283\n",
      "The 25733 th iteration gives loss of 0.17014751115995796\n",
      "The 25734 th iteration gives loss of 0.17014474215217684\n",
      "The 25735 th iteration gives loss of 0.1701419733777652\n",
      "The 25736 th iteration gives loss of 0.17013920483668402\n",
      "The 25737 th iteration gives loss of 0.17013643652889973\n",
      "The 25738 th iteration gives loss of 0.17013366845437206\n",
      "The 25739 th iteration gives loss of 0.17013090061307126\n",
      "The 25740 th iteration gives loss of 0.17012813300495502\n",
      "The 25741 th iteration gives loss of 0.17012536562998923\n",
      "The 25742 th iteration gives loss of 0.17012259848814723\n",
      "The 25743 th iteration gives loss of 0.17011983157938296\n",
      "The 25744 th iteration gives loss of 0.17011706490367243\n",
      "The 25745 th iteration gives loss of 0.17011429846096696\n",
      "The 25746 th iteration gives loss of 0.17011153225123188\n",
      "The 25747 th iteration gives loss of 0.17010876627444746\n",
      "The 25748 th iteration gives loss of 0.17010600053056957\n",
      "The 25749 th iteration gives loss of 0.17010323501955907\n",
      "The 25750 th iteration gives loss of 0.17010046974137863\n",
      "The 25751 th iteration gives loss of 0.17009770469600313\n",
      "The 25752 th iteration gives loss of 0.170094939883397\n",
      "The 25753 th iteration gives loss of 0.17009217530350929\n",
      "The 25754 th iteration gives loss of 0.17008941095631727\n",
      "The 25755 th iteration gives loss of 0.17008664684179103\n",
      "The 25756 th iteration gives loss of 0.1700838829598836\n",
      "The 25757 th iteration gives loss of 0.1700811193105703\n",
      "The 25758 th iteration gives loss of 0.17007835589380466\n",
      "The 25759 th iteration gives loss of 0.17007559270955755\n",
      "The 25760 th iteration gives loss of 0.1700728297577902\n",
      "The 25761 th iteration gives loss of 0.17007006703846797\n",
      "The 25762 th iteration gives loss of 0.17006730455156505\n",
      "The 25763 th iteration gives loss of 0.17006454229703652\n",
      "The 25764 th iteration gives loss of 0.17006178027484672\n",
      "The 25765 th iteration gives loss of 0.17005901848496416\n",
      "The 25766 th iteration gives loss of 0.17005625692735113\n",
      "The 25767 th iteration gives loss of 0.17005349560198313\n",
      "The 25768 th iteration gives loss of 0.17005073450880212\n",
      "The 25769 th iteration gives loss of 0.17004797364779037\n",
      "The 25770 th iteration gives loss of 0.17004521301890868\n",
      "The 25771 th iteration gives loss of 0.17004245262212306\n",
      "The 25772 th iteration gives loss of 0.17003969245740452\n",
      "The 25773 th iteration gives loss of 0.17003693252470103\n",
      "The 25774 th iteration gives loss of 0.1700341728239905\n",
      "The 25775 th iteration gives loss of 0.1700314133552338\n",
      "The 25776 th iteration gives loss of 0.17002865411839405\n",
      "The 25777 th iteration gives loss of 0.17002589511343877\n",
      "The 25778 th iteration gives loss of 0.17002313634033342\n",
      "The 25779 th iteration gives loss of 0.17002037779904577\n",
      "The 25780 th iteration gives loss of 0.17001761948953054\n",
      "The 25781 th iteration gives loss of 0.17001486141175215\n",
      "The 25782 th iteration gives loss of 0.17001210356570037\n",
      "The 25783 th iteration gives loss of 0.1700093459513106\n",
      "The 25784 th iteration gives loss of 0.17000658856856032\n",
      "The 25785 th iteration gives loss of 0.1700038314174107\n",
      "The 25786 th iteration gives loss of 0.17000107449782814\n",
      "The 25787 th iteration gives loss of 0.16999831780978014\n",
      "The 25788 th iteration gives loss of 0.16999556135323599\n",
      "The 25789 th iteration gives loss of 0.16999280512814263\n",
      "The 25790 th iteration gives loss of 0.16999004913448118\n",
      "The 25791 th iteration gives loss of 0.16998729337221438\n",
      "The 25792 th iteration gives loss of 0.16998453784130763\n",
      "The 25793 th iteration gives loss of 0.1699817825417197\n",
      "The 25794 th iteration gives loss of 0.1699790274734219\n",
      "The 25795 th iteration gives loss of 0.16997627263637088\n",
      "The 25796 th iteration gives loss of 0.16997351803053773\n",
      "The 25797 th iteration gives loss of 0.16997076365588784\n",
      "The 25798 th iteration gives loss of 0.16996800951238586\n",
      "The 25799 th iteration gives loss of 0.16996525559999548\n",
      "The 25800 th iteration gives loss of 0.16996250191867718\n",
      "The 25801 th iteration gives loss of 0.1699597484684075\n",
      "The 25802 th iteration gives loss of 0.16995699524913388\n",
      "The 25803 th iteration gives loss of 0.169954242260853\n",
      "The 25804 th iteration gives loss of 0.16995148950349093\n",
      "The 25805 th iteration gives loss of 0.16994873697703422\n",
      "The 25806 th iteration gives loss of 0.1699459846814469\n",
      "The 25807 th iteration gives loss of 0.1699432326166915\n",
      "The 25808 th iteration gives loss of 0.16994048078273466\n",
      "The 25809 th iteration gives loss of 0.16993772917953684\n",
      "The 25810 th iteration gives loss of 0.16993497780706115\n",
      "The 25811 th iteration gives loss of 0.16993222666528454\n",
      "The 25812 th iteration gives loss of 0.1699294757541623\n",
      "The 25813 th iteration gives loss of 0.1699267250736644\n",
      "The 25814 th iteration gives loss of 0.1699239746237492\n",
      "The 25815 th iteration gives loss of 0.16992122440439014\n",
      "The 25816 th iteration gives loss of 0.16991847441554725\n",
      "The 25817 th iteration gives loss of 0.16991572465718535\n",
      "The 25818 th iteration gives loss of 0.1699129751292663\n",
      "The 25819 th iteration gives loss of 0.16991022583176674\n",
      "The 25820 th iteration gives loss of 0.16990747676464882\n",
      "The 25821 th iteration gives loss of 0.16990472792786523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 25822 th iteration gives loss of 0.16990197932138418\n",
      "The 25823 th iteration gives loss of 0.16989923094517909\n",
      "The 25824 th iteration gives loss of 0.1698964827992202\n",
      "The 25825 th iteration gives loss of 0.16989373488345785\n",
      "The 25826 th iteration gives loss of 0.16989098719786644\n",
      "The 25827 th iteration gives loss of 0.16988823974241293\n",
      "The 25828 th iteration gives loss of 0.169885492517047\n",
      "The 25829 th iteration gives loss of 0.16988274552175547\n",
      "The 25830 th iteration gives loss of 0.1698799987564795\n",
      "The 25831 th iteration gives loss of 0.1698772522212044\n",
      "The 25832 th iteration gives loss of 0.1698745059158847\n",
      "The 25833 th iteration gives loss of 0.16987175984049876\n",
      "The 25834 th iteration gives loss of 0.16986901399499307\n",
      "The 25835 th iteration gives loss of 0.16986626837935054\n",
      "The 25836 th iteration gives loss of 0.16986352299351595\n",
      "The 25837 th iteration gives loss of 0.16986077783747044\n",
      "The 25838 th iteration gives loss of 0.1698580329111776\n",
      "The 25839 th iteration gives loss of 0.16985528821459708\n",
      "The 25840 th iteration gives loss of 0.1698525437477002\n",
      "The 25841 th iteration gives loss of 0.16984979951044235\n",
      "The 25842 th iteration gives loss of 0.16984705550279375\n",
      "The 25843 th iteration gives loss of 0.16984431172472983\n",
      "The 25844 th iteration gives loss of 0.16984156817620236\n",
      "The 25845 th iteration gives loss of 0.16983882485718826\n",
      "The 25846 th iteration gives loss of 0.1698360817676436\n",
      "The 25847 th iteration gives loss of 0.16983333890752927\n",
      "The 25848 th iteration gives loss of 0.1698305962768188\n",
      "The 25849 th iteration gives loss of 0.1698278538754733\n",
      "The 25850 th iteration gives loss of 0.16982511170346848\n",
      "The 25851 th iteration gives loss of 0.1698223697607532\n",
      "The 25852 th iteration gives loss of 0.1698196280473052\n",
      "The 25853 th iteration gives loss of 0.16981688656308183\n",
      "The 25854 th iteration gives loss of 0.16981414530805647\n",
      "The 25855 th iteration gives loss of 0.16981140428218675\n",
      "The 25856 th iteration gives loss of 0.16980866348544613\n",
      "The 25857 th iteration gives loss of 0.1698059229177898\n",
      "The 25858 th iteration gives loss of 0.16980318257918425\n",
      "The 25859 th iteration gives loss of 0.16980044246960233\n",
      "The 25860 th iteration gives loss of 0.16979770258901494\n",
      "The 25861 th iteration gives loss of 0.16979496293736415\n",
      "The 25862 th iteration gives loss of 0.16979222351463685\n",
      "The 25863 th iteration gives loss of 0.16978948432078167\n",
      "The 25864 th iteration gives loss of 0.16978674535579302\n",
      "The 25865 th iteration gives loss of 0.16978400661960652\n",
      "The 25866 th iteration gives loss of 0.16978126811219008\n",
      "The 25867 th iteration gives loss of 0.1697785298335238\n",
      "The 25868 th iteration gives loss of 0.16977579178356367\n",
      "The 25869 th iteration gives loss of 0.16977305396227527\n",
      "The 25870 th iteration gives loss of 0.1697703163696283\n",
      "The 25871 th iteration gives loss of 0.16976757900558676\n",
      "The 25872 th iteration gives loss of 0.16976484187010465\n",
      "The 25873 th iteration gives loss of 0.16976210496316138\n",
      "The 25874 th iteration gives loss of 0.1697593682847217\n",
      "The 25875 th iteration gives loss of 0.16975663183474737\n",
      "The 25876 th iteration gives loss of 0.1697538956131995\n",
      "The 25877 th iteration gives loss of 0.16975115962005305\n",
      "The 25878 th iteration gives loss of 0.16974842385526534\n",
      "The 25879 th iteration gives loss of 0.16974568831880454\n",
      "The 25880 th iteration gives loss of 0.1697429530106341\n",
      "The 25881 th iteration gives loss of 0.1697402179307316\n",
      "The 25882 th iteration gives loss of 0.16973748307903821\n",
      "The 25883 th iteration gives loss of 0.16973474845554806\n",
      "The 25884 th iteration gives loss of 0.1697320140602014\n",
      "The 25885 th iteration gives loss of 0.16972927989297423\n",
      "The 25886 th iteration gives loss of 0.1697265459538414\n",
      "The 25887 th iteration gives loss of 0.16972381224275307\n",
      "The 25888 th iteration gives loss of 0.16972107875968004\n",
      "The 25889 th iteration gives loss of 0.16971834550458625\n",
      "The 25890 th iteration gives loss of 0.1697156124774465\n",
      "The 25891 th iteration gives loss of 0.1697128796782161\n",
      "The 25892 th iteration gives loss of 0.1697101471068654\n",
      "The 25893 th iteration gives loss of 0.16970741476335385\n",
      "The 25894 th iteration gives loss of 0.169704682647647\n",
      "The 25895 th iteration gives loss of 0.1697019507597192\n",
      "The 25896 th iteration gives loss of 0.16969921909953192\n",
      "The 25897 th iteration gives loss of 0.16969648766705153\n",
      "The 25898 th iteration gives loss of 0.16969375646224066\n",
      "The 25899 th iteration gives loss of 0.16969102548506623\n",
      "The 25900 th iteration gives loss of 0.16968829473549776\n",
      "The 25901 th iteration gives loss of 0.1696855642134881\n",
      "The 25902 th iteration gives loss of 0.1696828339190193\n",
      "The 25903 th iteration gives loss of 0.16968010385204282\n",
      "The 25904 th iteration gives loss of 0.16967737401253521\n",
      "The 25905 th iteration gives loss of 0.16967464440046046\n",
      "The 25906 th iteration gives loss of 0.16967191501577394\n",
      "The 25907 th iteration gives loss of 0.16966918585844207\n",
      "The 25908 th iteration gives loss of 0.1696664569284482\n",
      "The 25909 th iteration gives loss of 0.16966372822573658\n",
      "The 25910 th iteration gives loss of 0.16966099975028875\n",
      "The 25911 th iteration gives loss of 0.16965827150206142\n",
      "The 25912 th iteration gives loss of 0.16965554348102532\n",
      "The 25913 th iteration gives loss of 0.16965281568714732\n",
      "The 25914 th iteration gives loss of 0.16965008812037613\n",
      "The 25915 th iteration gives loss of 0.16964736078070305\n",
      "The 25916 th iteration gives loss of 0.16964463366808247\n",
      "The 25917 th iteration gives loss of 0.16964190678246394\n",
      "The 25918 th iteration gives loss of 0.16963918012383838\n",
      "The 25919 th iteration gives loss of 0.16963645369215216\n",
      "The 25920 th iteration gives loss of 0.16963372748738828\n",
      "The 25921 th iteration gives loss of 0.16963100150949975\n",
      "The 25922 th iteration gives loss of 0.16962827575846268\n",
      "The 25923 th iteration gives loss of 0.16962555023422524\n",
      "The 25924 th iteration gives loss of 0.16962282493677305\n",
      "The 25925 th iteration gives loss of 0.16962009986605492\n",
      "The 25926 th iteration gives loss of 0.1696173750220478\n",
      "The 25927 th iteration gives loss of 0.16961465040471552\n",
      "The 25928 th iteration gives loss of 0.1696119260140152\n",
      "The 25929 th iteration gives loss of 0.1696092018499253\n",
      "The 25930 th iteration gives loss of 0.16960647791240097\n",
      "The 25931 th iteration gives loss of 0.1696037542014178\n",
      "The 25932 th iteration gives loss of 0.16960103071693605\n",
      "The 25933 th iteration gives loss of 0.16959830745891502\n",
      "The 25934 th iteration gives loss of 0.16959558442732764\n",
      "The 25935 th iteration gives loss of 0.1695928616221467\n",
      "The 25936 th iteration gives loss of 0.16959013904332831\n",
      "The 25937 th iteration gives loss of 0.1695874166908378\n",
      "The 25938 th iteration gives loss of 0.16958469456463374\n",
      "The 25939 th iteration gives loss of 0.1695819726647036\n",
      "The 25940 th iteration gives loss of 0.1695792509909934\n",
      "The 25941 th iteration gives loss of 0.169576529543479\n",
      "The 25942 th iteration gives loss of 0.1695738083221256\n",
      "The 25943 th iteration gives loss of 0.16957108732689688\n",
      "The 25944 th iteration gives loss of 0.1695683665577531\n",
      "The 25945 th iteration gives loss of 0.169565646014661\n",
      "The 25946 th iteration gives loss of 0.1695629256976091\n",
      "The 25947 th iteration gives loss of 0.1695602056065255\n",
      "The 25948 th iteration gives loss of 0.16955748574140878\n",
      "The 25949 th iteration gives loss of 0.16955476610219752\n",
      "The 25950 th iteration gives loss of 0.16955204668887988\n",
      "The 25951 th iteration gives loss of 0.16954932750141674\n",
      "The 25952 th iteration gives loss of 0.16954660853976744\n",
      "The 25953 th iteration gives loss of 0.16954388980389903\n",
      "The 25954 th iteration gives loss of 0.16954117129377444\n",
      "The 25955 th iteration gives loss of 0.16953845300936438\n",
      "The 25956 th iteration gives loss of 0.16953573495063565\n",
      "The 25957 th iteration gives loss of 0.16953301711755944\n",
      "The 25958 th iteration gives loss of 0.16953029951008755\n",
      "The 25959 th iteration gives loss of 0.16952758212819521\n",
      "The 25960 th iteration gives loss of 0.16952486497185085\n",
      "The 25961 th iteration gives loss of 0.16952214804100993\n",
      "The 25962 th iteration gives loss of 0.1695194313356442\n",
      "The 25963 th iteration gives loss of 0.16951671485571126\n",
      "The 25964 th iteration gives loss of 0.16951399860119887\n",
      "The 25965 th iteration gives loss of 0.16951128257205075\n",
      "The 25966 th iteration gives loss of 0.16950856676823675\n",
      "The 25967 th iteration gives loss of 0.16950585118973488\n",
      "The 25968 th iteration gives loss of 0.1695031358365103\n",
      "The 25969 th iteration gives loss of 0.16950042070851237\n",
      "The 25970 th iteration gives loss of 0.1694977058057183\n",
      "The 25971 th iteration gives loss of 0.16949499112808428\n",
      "The 25972 th iteration gives loss of 0.16949227667559782\n",
      "The 25973 th iteration gives loss of 0.16948956244819985\n",
      "The 25974 th iteration gives loss of 0.16948684844586925\n",
      "The 25975 th iteration gives loss of 0.16948413466857118\n",
      "The 25976 th iteration gives loss of 0.16948142111627196\n",
      "The 25977 th iteration gives loss of 0.16947870778893803\n",
      "The 25978 th iteration gives loss of 0.1694759946865287\n",
      "The 25979 th iteration gives loss of 0.16947328180901974\n",
      "The 25980 th iteration gives loss of 0.1694705691563675\n",
      "The 25981 th iteration gives loss of 0.1694678567285391\n",
      "The 25982 th iteration gives loss of 0.16946514452551015\n",
      "The 25983 th iteration gives loss of 0.1694624325472372\n",
      "The 25984 th iteration gives loss of 0.1694597207936918\n",
      "The 25985 th iteration gives loss of 0.1694570092648377\n",
      "The 25986 th iteration gives loss of 0.16945429796064107\n",
      "The 25987 th iteration gives loss of 0.16945158688106543\n",
      "The 25988 th iteration gives loss of 0.16944887602608144\n",
      "The 25989 th iteration gives loss of 0.16944616539564766\n",
      "The 25990 th iteration gives loss of 0.16944345498973346\n",
      "The 25991 th iteration gives loss of 0.16944074480830879\n",
      "The 25992 th iteration gives loss of 0.16943803485134026\n",
      "The 25993 th iteration gives loss of 0.16943532511878767\n",
      "The 25994 th iteration gives loss of 0.16943261561063042\n",
      "The 25995 th iteration gives loss of 0.1694299063268138\n",
      "The 25996 th iteration gives loss of 0.16942719726731525\n",
      "The 25997 th iteration gives loss of 0.16942448843209618\n",
      "The 25998 th iteration gives loss of 0.1694217798211344\n",
      "The 25999 th iteration gives loss of 0.16941907143438545\n",
      "The 26000 th iteration gives loss of 0.16941636327181464\n",
      "The 26001 th iteration gives loss of 0.16941365533339328\n",
      "The 26002 th iteration gives loss of 0.1694109476190864\n",
      "The 26003 th iteration gives loss of 0.16940824012885555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 26004 th iteration gives loss of 0.1694055328626801\n",
      "The 26005 th iteration gives loss of 0.16940282582051283\n",
      "The 26006 th iteration gives loss of 0.16940011900231985\n",
      "The 26007 th iteration gives loss of 0.16939741240806788\n",
      "The 26008 th iteration gives loss of 0.16939470603773066\n",
      "The 26009 th iteration gives loss of 0.16939199989126927\n",
      "The 26010 th iteration gives loss of 0.1693892939686486\n",
      "The 26011 th iteration gives loss of 0.16938658826984188\n",
      "The 26012 th iteration gives loss of 0.1693838827948057\n",
      "The 26013 th iteration gives loss of 0.16938117754351067\n",
      "The 26014 th iteration gives loss of 0.16937847251591856\n",
      "The 26015 th iteration gives loss of 0.16937576771199944\n",
      "The 26016 th iteration gives loss of 0.16937306313172892\n",
      "The 26017 th iteration gives loss of 0.1693703587750573\n",
      "The 26018 th iteration gives loss of 0.16936765464195752\n",
      "The 26019 th iteration gives loss of 0.1693649507323972\n",
      "The 26020 th iteration gives loss of 0.16936224704634062\n",
      "The 26021 th iteration gives loss of 0.16935954358374936\n",
      "The 26022 th iteration gives loss of 0.16935684034459886\n",
      "The 26023 th iteration gives loss of 0.16935413732884785\n",
      "The 26024 th iteration gives loss of 0.16935143453646756\n",
      "The 26025 th iteration gives loss of 0.1693487319674213\n",
      "The 26026 th iteration gives loss of 0.16934602962167505\n",
      "The 26027 th iteration gives loss of 0.16934332749919898\n",
      "The 26028 th iteration gives loss of 0.16934062559995627\n",
      "The 26029 th iteration gives loss of 0.1693379239239049\n",
      "The 26030 th iteration gives loss of 0.16933522247102534\n",
      "The 26031 th iteration gives loss of 0.1693325212412845\n",
      "The 26032 th iteration gives loss of 0.16932982023463153\n",
      "The 26033 th iteration gives loss of 0.1693271194510481\n",
      "The 26034 th iteration gives loss of 0.16932441889049238\n",
      "The 26035 th iteration gives loss of 0.16932171855293313\n",
      "The 26036 th iteration gives loss of 0.16931901843834418\n",
      "The 26037 th iteration gives loss of 0.1693163185466832\n",
      "The 26038 th iteration gives loss of 0.16931361887791369\n",
      "The 26039 th iteration gives loss of 0.16931091943199716\n",
      "The 26040 th iteration gives loss of 0.16930822020892602\n",
      "The 26041 th iteration gives loss of 0.16930552120864262\n",
      "The 26042 th iteration gives loss of 0.16930282243111994\n",
      "The 26043 th iteration gives loss of 0.1693001238763227\n",
      "The 26044 th iteration gives loss of 0.1692974255442195\n",
      "The 26045 th iteration gives loss of 0.1692947274347737\n",
      "The 26046 th iteration gives loss of 0.16929202954796815\n",
      "The 26047 th iteration gives loss of 0.1692893318837437\n",
      "The 26048 th iteration gives loss of 0.16928663444207398\n",
      "The 26049 th iteration gives loss of 0.16928393722293575\n",
      "The 26050 th iteration gives loss of 0.16928124022628988\n",
      "The 26051 th iteration gives loss of 0.1692785434520965\n",
      "The 26052 th iteration gives loss of 0.16927584690032826\n",
      "The 26053 th iteration gives loss of 0.169273150570951\n",
      "The 26054 th iteration gives loss of 0.16927045446392788\n",
      "The 26055 th iteration gives loss of 0.16926775857923076\n",
      "The 26056 th iteration gives loss of 0.1692650629168308\n",
      "The 26057 th iteration gives loss of 0.16926236747667728\n",
      "The 26058 th iteration gives loss of 0.16925967225874758\n",
      "The 26059 th iteration gives loss of 0.169256977263007\n",
      "The 26060 th iteration gives loss of 0.1692542824894243\n",
      "The 26061 th iteration gives loss of 0.169251587937963\n",
      "The 26062 th iteration gives loss of 0.16924889360858186\n",
      "The 26063 th iteration gives loss of 0.16924619950125938\n",
      "The 26064 th iteration gives loss of 0.1692435056159651\n",
      "The 26065 th iteration gives loss of 0.16924081195265325\n",
      "The 26066 th iteration gives loss of 0.1692381185112819\n",
      "The 26067 th iteration gives loss of 0.1692354252918499\n",
      "The 26068 th iteration gives loss of 0.1692327322942936\n",
      "The 26069 th iteration gives loss of 0.1692300395185934\n",
      "The 26070 th iteration gives loss of 0.1692273469647053\n",
      "The 26071 th iteration gives loss of 0.1692246546326122\n",
      "The 26072 th iteration gives loss of 0.16922196252226585\n",
      "The 26073 th iteration gives loss of 0.16921927063363695\n",
      "The 26074 th iteration gives loss of 0.16921657896668746\n",
      "The 26075 th iteration gives loss of 0.16921388752139843\n",
      "The 26076 th iteration gives loss of 0.16921119629773\n",
      "The 26077 th iteration gives loss of 0.16920850529565004\n",
      "The 26078 th iteration gives loss of 0.16920581451510608\n",
      "The 26079 th iteration gives loss of 0.1692031239560838\n",
      "The 26080 th iteration gives loss of 0.16920043361854878\n",
      "The 26081 th iteration gives loss of 0.1691977435024634\n",
      "The 26082 th iteration gives loss of 0.16919505360779005\n",
      "The 26083 th iteration gives loss of 0.16919236393450515\n",
      "The 26084 th iteration gives loss of 0.16918967448256483\n",
      "The 26085 th iteration gives loss of 0.1691869852519461\n",
      "The 26086 th iteration gives loss of 0.16918429624261158\n",
      "The 26087 th iteration gives loss of 0.16918160745451696\n",
      "The 26088 th iteration gives loss of 0.16917891888764658\n",
      "The 26089 th iteration gives loss of 0.16917623054196015\n",
      "The 26090 th iteration gives loss of 0.16917354241741897\n",
      "The 26091 th iteration gives loss of 0.16917085451398875\n",
      "The 26092 th iteration gives loss of 0.16916816683164781\n",
      "The 26093 th iteration gives loss of 0.1691654793703496\n",
      "The 26094 th iteration gives loss of 0.16916279213007335\n",
      "The 26095 th iteration gives loss of 0.16916010511077312\n",
      "The 26096 th iteration gives loss of 0.16915741831242445\n",
      "The 26097 th iteration gives loss of 0.16915473173499174\n",
      "The 26098 th iteration gives loss of 0.16915204537843906\n",
      "The 26099 th iteration gives loss of 0.1691493592427324\n",
      "The 26100 th iteration gives loss of 0.1691466733278445\n",
      "The 26101 th iteration gives loss of 0.16914398763373095\n",
      "The 26102 th iteration gives loss of 0.1691413021603748\n",
      "The 26103 th iteration gives loss of 0.16913861690772328\n",
      "The 26104 th iteration gives loss of 0.16913593187575895\n",
      "The 26105 th iteration gives loss of 0.16913324706443916\n",
      "The 26106 th iteration gives loss of 0.16913056247373454\n",
      "The 26107 th iteration gives loss of 0.16912787810361335\n",
      "The 26108 th iteration gives loss of 0.1691251939540398\n",
      "The 26109 th iteration gives loss of 0.16912251002498413\n",
      "The 26110 th iteration gives loss of 0.16911982631640166\n",
      "The 26111 th iteration gives loss of 0.1691171428282688\n",
      "The 26112 th iteration gives loss of 0.169114459560547\n",
      "The 26113 th iteration gives loss of 0.16911177651320478\n",
      "The 26114 th iteration gives loss of 0.1691090936862221\n",
      "The 26115 th iteration gives loss of 0.16910641107954402\n",
      "The 26116 th iteration gives loss of 0.16910372869314763\n",
      "The 26117 th iteration gives loss of 0.16910104652700447\n",
      "The 26118 th iteration gives loss of 0.16909836458107444\n",
      "The 26119 th iteration gives loss of 0.16909568285531862\n",
      "The 26120 th iteration gives loss of 0.16909300134971195\n",
      "The 26121 th iteration gives loss of 0.16909032006422403\n",
      "The 26122 th iteration gives loss of 0.16908763899881987\n",
      "The 26123 th iteration gives loss of 0.16908495815345992\n",
      "The 26124 th iteration gives loss of 0.16908227752812088\n",
      "The 26125 th iteration gives loss of 0.16907959712275006\n",
      "The 26126 th iteration gives loss of 0.1690769169373298\n",
      "The 26127 th iteration gives loss of 0.1690742369718286\n",
      "The 26128 th iteration gives loss of 0.16907155722619943\n",
      "The 26129 th iteration gives loss of 0.16906887770042825\n",
      "The 26130 th iteration gives loss of 0.16906619839447865\n",
      "The 26131 th iteration gives loss of 0.16906351930829783\n",
      "The 26132 th iteration gives loss of 0.16906084044187666\n",
      "The 26133 th iteration gives loss of 0.1690581617951638\n",
      "The 26134 th iteration gives loss of 0.1690554833681338\n",
      "The 26135 th iteration gives loss of 0.1690528051607555\n",
      "The 26136 th iteration gives loss of 0.16905012717298953\n",
      "The 26137 th iteration gives loss of 0.16904744940480138\n",
      "The 26138 th iteration gives loss of 0.16904477185616193\n",
      "The 26139 th iteration gives loss of 0.16904209452704935\n",
      "The 26140 th iteration gives loss of 0.16903941741741776\n",
      "The 26141 th iteration gives loss of 0.16903674052722706\n",
      "The 26142 th iteration gives loss of 0.1690340638564619\n",
      "The 26143 th iteration gives loss of 0.1690313874050726\n",
      "The 26144 th iteration gives loss of 0.16902871117304594\n",
      "The 26145 th iteration gives loss of 0.16902603516032155\n",
      "The 26146 th iteration gives loss of 0.16902335936688195\n",
      "The 26147 th iteration gives loss of 0.16902068379269716\n",
      "The 26148 th iteration gives loss of 0.16901800843773152\n",
      "The 26149 th iteration gives loss of 0.16901533330194968\n",
      "The 26150 th iteration gives loss of 0.1690126583853189\n",
      "The 26151 th iteration gives loss of 0.1690099836878018\n",
      "The 26152 th iteration gives loss of 0.1690073092093735\n",
      "The 26153 th iteration gives loss of 0.1690046349499952\n",
      "The 26154 th iteration gives loss of 0.16900196090963515\n",
      "The 26155 th iteration gives loss of 0.16899928708826445\n",
      "The 26156 th iteration gives loss of 0.16899661348585188\n",
      "The 26157 th iteration gives loss of 0.16899394010234295\n",
      "The 26158 th iteration gives loss of 0.16899126693772135\n",
      "The 26159 th iteration gives loss of 0.16898859399196628\n",
      "The 26160 th iteration gives loss of 0.1689859212650212\n",
      "The 26161 th iteration gives loss of 0.16898324875687096\n",
      "The 26162 th iteration gives loss of 0.16898057646747167\n",
      "The 26163 th iteration gives loss of 0.16897790439679136\n",
      "The 26164 th iteration gives loss of 0.16897523254479876\n",
      "The 26165 th iteration gives loss of 0.16897256091145912\n",
      "The 26166 th iteration gives loss of 0.16896988949675118\n",
      "The 26167 th iteration gives loss of 0.16896721830061884\n",
      "The 26168 th iteration gives loss of 0.1689645473230485\n",
      "The 26169 th iteration gives loss of 0.16896187656399614\n",
      "The 26170 th iteration gives loss of 0.168959206023433\n",
      "The 26171 th iteration gives loss of 0.1689565357013285\n",
      "The 26172 th iteration gives loss of 0.16895386559765416\n",
      "The 26173 th iteration gives loss of 0.16895119571235878\n",
      "The 26174 th iteration gives loss of 0.1689485260454308\n",
      "The 26175 th iteration gives loss of 0.16894585659682065\n",
      "The 26176 th iteration gives loss of 0.16894318736650296\n",
      "The 26177 th iteration gives loss of 0.16894051835444465\n",
      "The 26178 th iteration gives loss of 0.16893784956060903\n",
      "The 26179 th iteration gives loss of 0.168935180984967\n",
      "The 26180 th iteration gives loss of 0.16893251262748368\n",
      "The 26181 th iteration gives loss of 0.16892984448813597\n",
      "The 26182 th iteration gives loss of 0.16892717656687037\n",
      "The 26183 th iteration gives loss of 0.16892450886366925\n",
      "The 26184 th iteration gives loss of 0.16892184137849292\n",
      "The 26185 th iteration gives loss of 0.16891917411130827\n",
      "The 26186 th iteration gives loss of 0.16891650706209468\n",
      "The 26187 th iteration gives loss of 0.16891384023079542\n",
      "The 26188 th iteration gives loss of 0.16891117361740765\n",
      "The 26189 th iteration gives loss of 0.1689085072218767\n",
      "The 26190 th iteration gives loss of 0.16890584104417591\n",
      "The 26191 th iteration gives loss of 0.16890317508427297\n",
      "The 26192 th iteration gives loss of 0.16890050934213688\n",
      "The 26193 th iteration gives loss of 0.1688978438177223\n",
      "The 26194 th iteration gives loss of 0.16889517851101765\n",
      "The 26195 th iteration gives loss of 0.16889251342196396\n",
      "The 26196 th iteration gives loss of 0.16888984855055175\n",
      "The 26197 th iteration gives loss of 0.1688871838967372\n",
      "The 26198 th iteration gives loss of 0.16888451946049116\n",
      "The 26199 th iteration gives loss of 0.1688818552417798\n",
      "The 26200 th iteration gives loss of 0.16887919124056114\n",
      "The 26201 th iteration gives loss of 0.16887652745681284\n",
      "The 26202 th iteration gives loss of 0.1688738638905042\n",
      "The 26203 th iteration gives loss of 0.16887120054159158\n",
      "The 26204 th iteration gives loss of 0.1688685374100534\n",
      "The 26205 th iteration gives loss of 0.16886587449584806\n",
      "The 26206 th iteration gives loss of 0.1688632117989597\n",
      "The 26207 th iteration gives loss of 0.16886054931933056\n",
      "The 26208 th iteration gives loss of 0.16885788705693713\n",
      "The 26209 th iteration gives loss of 0.168855225011743\n",
      "The 26210 th iteration gives loss of 0.16885256318373418\n",
      "The 26211 th iteration gives loss of 0.16884990157286076\n",
      "The 26212 th iteration gives loss of 0.1688472401790969\n",
      "The 26213 th iteration gives loss of 0.1688445790024036\n",
      "The 26214 th iteration gives loss of 0.16884191804275106\n",
      "The 26215 th iteration gives loss of 0.1688392573001026\n",
      "The 26216 th iteration gives loss of 0.16883659677443705\n",
      "The 26217 th iteration gives loss of 0.16883393646570324\n",
      "The 26218 th iteration gives loss of 0.16883127637388873\n",
      "The 26219 th iteration gives loss of 0.16882861649894976\n",
      "The 26220 th iteration gives loss of 0.1688259568408524\n",
      "The 26221 th iteration gives loss of 0.1688232973995779\n",
      "The 26222 th iteration gives loss of 0.16882063817506782\n",
      "The 26223 th iteration gives loss of 0.16881797916730526\n",
      "The 26224 th iteration gives loss of 0.16881532037626204\n",
      "The 26225 th iteration gives loss of 0.16881266180190063\n",
      "The 26226 th iteration gives loss of 0.16881000344418454\n",
      "The 26227 th iteration gives loss of 0.1688073453030788\n",
      "The 26228 th iteration gives loss of 0.16880468737855797\n",
      "The 26229 th iteration gives loss of 0.1688020296705853\n",
      "The 26230 th iteration gives loss of 0.16879937217912985\n",
      "The 26231 th iteration gives loss of 0.1687967149041588\n",
      "The 26232 th iteration gives loss of 0.16879405784563747\n",
      "The 26233 th iteration gives loss of 0.16879140100353274\n",
      "The 26234 th iteration gives loss of 0.16878874437781755\n",
      "The 26235 th iteration gives loss of 0.16878608796845396\n",
      "The 26236 th iteration gives loss of 0.16878343177541105\n",
      "The 26237 th iteration gives loss of 0.16878077579866133\n",
      "The 26238 th iteration gives loss of 0.1687781200381709\n",
      "The 26239 th iteration gives loss of 0.16877546449389547\n",
      "The 26240 th iteration gives loss of 0.16877280916580645\n",
      "The 26241 th iteration gives loss of 0.16877015405388462\n",
      "The 26242 th iteration gives loss of 0.1687674991580706\n",
      "The 26243 th iteration gives loss of 0.1687648444783579\n",
      "The 26244 th iteration gives loss of 0.16876219001470938\n",
      "The 26245 th iteration gives loss of 0.1687595357670836\n",
      "The 26246 th iteration gives loss of 0.16875688173544487\n",
      "The 26247 th iteration gives loss of 0.16875422791977607\n",
      "The 26248 th iteration gives loss of 0.16875157432003443\n",
      "The 26249 th iteration gives loss of 0.1687489209361863\n",
      "The 26250 th iteration gives loss of 0.16874626776821208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 26251 th iteration gives loss of 0.16874361481605488\n",
      "The 26252 th iteration gives loss of 0.16874096207969555\n",
      "The 26253 th iteration gives loss of 0.16873830955910277\n",
      "The 26254 th iteration gives loss of 0.16873565725424522\n",
      "The 26255 th iteration gives loss of 0.16873300516509449\n",
      "The 26256 th iteration gives loss of 0.1687303532916069\n",
      "The 26257 th iteration gives loss of 0.1687277016337597\n",
      "The 26258 th iteration gives loss of 0.1687250501915072\n",
      "The 26259 th iteration gives loss of 0.1687223989648227\n",
      "The 26260 th iteration gives loss of 0.16871974795367992\n",
      "The 26261 th iteration gives loss of 0.16871709715803945\n",
      "The 26262 th iteration gives loss of 0.16871444657788176\n",
      "The 26263 th iteration gives loss of 0.16871179621315677\n",
      "The 26264 th iteration gives loss of 0.16870914606382553\n",
      "The 26265 th iteration gives loss of 0.16870649612988603\n",
      "The 26266 th iteration gives loss of 0.1687038464112785\n",
      "The 26267 th iteration gives loss of 0.16870119690799185\n",
      "The 26268 th iteration gives loss of 0.16869854761997918\n",
      "The 26269 th iteration gives loss of 0.16869589854720865\n",
      "The 26270 th iteration gives loss of 0.16869324968964988\n",
      "The 26271 th iteration gives loss of 0.16869060104726644\n",
      "The 26272 th iteration gives loss of 0.16868795262003308\n",
      "The 26273 th iteration gives loss of 0.1686853044079166\n",
      "The 26274 th iteration gives loss of 0.16868265641088473\n",
      "The 26275 th iteration gives loss of 0.168680008628891\n",
      "The 26276 th iteration gives loss of 0.16867736106192657\n",
      "The 26277 th iteration gives loss of 0.1686747137099341\n",
      "The 26278 th iteration gives loss of 0.1686720665729054\n",
      "The 26279 th iteration gives loss of 0.16866941965079751\n",
      "The 26280 th iteration gives loss of 0.16866677294357166\n",
      "The 26281 th iteration gives loss of 0.16866412645119994\n",
      "The 26282 th iteration gives loss of 0.16866148017364982\n",
      "The 26283 th iteration gives loss of 0.16865883411088928\n",
      "The 26284 th iteration gives loss of 0.16865618826289247\n",
      "The 26285 th iteration gives loss of 0.16865354262961207\n",
      "The 26286 th iteration gives loss of 0.16865089721102147\n",
      "The 26287 th iteration gives loss of 0.168648252007104\n",
      "The 26288 th iteration gives loss of 0.16864560701780237\n",
      "The 26289 th iteration gives loss of 0.16864296224309772\n",
      "The 26290 th iteration gives loss of 0.1686403176829591\n",
      "The 26291 th iteration gives loss of 0.16863767333735105\n",
      "The 26292 th iteration gives loss of 0.16863502920624335\n",
      "The 26293 th iteration gives loss of 0.16863238528959132\n",
      "The 26294 th iteration gives loss of 0.16862974158737692\n",
      "The 26295 th iteration gives loss of 0.16862709809956591\n",
      "The 26296 th iteration gives loss of 0.16862445482612903\n",
      "The 26297 th iteration gives loss of 0.16862181176701743\n",
      "The 26298 th iteration gives loss of 0.16861916892221587\n",
      "The 26299 th iteration gives loss of 0.16861652629167925\n",
      "The 26300 th iteration gives loss of 0.16861388387538517\n",
      "The 26301 th iteration gives loss of 0.16861124167330319\n",
      "The 26302 th iteration gives loss of 0.1686085996853852\n",
      "The 26303 th iteration gives loss of 0.16860595791161842\n",
      "The 26304 th iteration gives loss of 0.1686033163519564\n",
      "The 26305 th iteration gives loss of 0.16860067500636824\n",
      "The 26306 th iteration gives loss of 0.16859803387482777\n",
      "The 26307 th iteration gives loss of 0.16859539295730347\n",
      "The 26308 th iteration gives loss of 0.16859275225375625\n",
      "The 26309 th iteration gives loss of 0.16859011176415603\n",
      "The 26310 th iteration gives loss of 0.16858747148846429\n",
      "The 26311 th iteration gives loss of 0.16858483142667297\n",
      "The 26312 th iteration gives loss of 0.16858219157872123\n",
      "The 26313 th iteration gives loss of 0.1685795519445886\n",
      "The 26314 th iteration gives loss of 0.16857691252424137\n",
      "The 26315 th iteration gives loss of 0.16857427331765537\n",
      "The 26316 th iteration gives loss of 0.1685716343247849\n",
      "The 26317 th iteration gives loss of 0.16856899554560634\n",
      "The 26318 th iteration gives loss of 0.16856635698008451\n",
      "The 26319 th iteration gives loss of 0.16856371862818362\n",
      "The 26320 th iteration gives loss of 0.16856108048987595\n",
      "The 26321 th iteration gives loss of 0.16855844256513858\n",
      "The 26322 th iteration gives loss of 0.16855580485392224\n",
      "The 26323 th iteration gives loss of 0.16855316735619447\n",
      "The 26324 th iteration gives loss of 0.16855053007193724\n",
      "The 26325 th iteration gives loss of 0.1685478930011105\n",
      "The 26326 th iteration gives loss of 0.16854525614368626\n",
      "The 26327 th iteration gives loss of 0.16854261949962532\n",
      "The 26328 th iteration gives loss of 0.16853998306890366\n",
      "The 26329 th iteration gives loss of 0.16853734685147964\n",
      "The 26330 th iteration gives loss of 0.16853471084733224\n",
      "The 26331 th iteration gives loss of 0.16853207505641402\n",
      "The 26332 th iteration gives loss of 0.16852943947871457\n",
      "The 26333 th iteration gives loss of 0.16852680411417417\n",
      "The 26334 th iteration gives loss of 0.1685241689627854\n",
      "The 26335 th iteration gives loss of 0.16852153402450318\n",
      "The 26336 th iteration gives loss of 0.16851889929929065\n",
      "The 26337 th iteration gives loss of 0.16851626478713155\n",
      "The 26338 th iteration gives loss of 0.1685136304879819\n",
      "The 26339 th iteration gives loss of 0.16851099640181988\n",
      "The 26340 th iteration gives loss of 0.16850836252860135\n",
      "The 26341 th iteration gives loss of 0.16850572886829968\n",
      "The 26342 th iteration gives loss of 0.16850309542087832\n",
      "The 26343 th iteration gives loss of 0.1685004621863133\n",
      "The 26344 th iteration gives loss of 0.16849782916456643\n",
      "The 26345 th iteration gives loss of 0.16849519635560695\n",
      "The 26346 th iteration gives loss of 0.16849256375940727\n",
      "The 26347 th iteration gives loss of 0.16848993137592436\n",
      "The 26348 th iteration gives loss of 0.1684872992051306\n",
      "The 26349 th iteration gives loss of 0.16848466724700584\n",
      "The 26350 th iteration gives loss of 0.1684820355015041\n",
      "The 26351 th iteration gives loss of 0.1684794039685988\n",
      "The 26352 th iteration gives loss of 0.1684767726482584\n",
      "The 26353 th iteration gives loss of 0.16847414154044238\n",
      "The 26354 th iteration gives loss of 0.16847151064513038\n",
      "The 26355 th iteration gives loss of 0.16846887996228166\n",
      "The 26356 th iteration gives loss of 0.16846624949187422\n",
      "The 26357 th iteration gives loss of 0.16846361923386188\n",
      "The 26358 th iteration gives loss of 0.16846098918822083\n",
      "The 26359 th iteration gives loss of 0.16845835935492315\n",
      "The 26360 th iteration gives loss of 0.16845572973392808\n",
      "The 26361 th iteration gives loss of 0.1684531003252057\n",
      "The 26362 th iteration gives loss of 0.1684504711287257\n",
      "The 26363 th iteration gives loss of 0.16844784214445788\n",
      "The 26364 th iteration gives loss of 0.16844521337236662\n",
      "The 26365 th iteration gives loss of 0.1684425848124296\n",
      "The 26366 th iteration gives loss of 0.16843995646459925\n",
      "The 26367 th iteration gives loss of 0.16843732832885214\n",
      "The 26368 th iteration gives loss of 0.168434700405159\n",
      "The 26369 th iteration gives loss of 0.1684320726934753\n",
      "The 26370 th iteration gives loss of 0.16842944519378114\n",
      "The 26371 th iteration gives loss of 0.16842681790604283\n",
      "The 26372 th iteration gives loss of 0.16842419083022891\n",
      "The 26373 th iteration gives loss of 0.16842156396630348\n",
      "The 26374 th iteration gives loss of 0.1684189373142309\n",
      "The 26375 th iteration gives loss of 0.16841631087398606\n",
      "The 26376 th iteration gives loss of 0.16841368464553727\n",
      "The 26377 th iteration gives loss of 0.16841105862885233\n",
      "The 26378 th iteration gives loss of 0.16840843282389614\n",
      "The 26379 th iteration gives loss of 0.1684058072306314\n",
      "The 26380 th iteration gives loss of 0.16840318184903777\n",
      "The 26381 th iteration gives loss of 0.1684005566790926\n",
      "The 26382 th iteration gives loss of 0.16839793172073206\n",
      "The 26383 th iteration gives loss of 0.16839530697394164\n",
      "The 26384 th iteration gives loss of 0.16839268243869956\n",
      "The 26385 th iteration gives loss of 0.16839005811496124\n",
      "The 26386 th iteration gives loss of 0.16838743400270026\n",
      "The 26387 th iteration gives loss of 0.16838481010187725\n",
      "The 26388 th iteration gives loss of 0.16838218641246613\n",
      "The 26389 th iteration gives loss of 0.1683795629344342\n",
      "The 26390 th iteration gives loss of 0.16837693966775336\n",
      "The 26391 th iteration gives loss of 0.16837431661238722\n",
      "The 26392 th iteration gives loss of 0.168371693768293\n",
      "The 26393 th iteration gives loss of 0.1683690711354613\n",
      "The 26394 th iteration gives loss of 0.16836644871384418\n",
      "The 26395 th iteration gives loss of 0.1683638265034134\n",
      "The 26396 th iteration gives loss of 0.16836120450413533\n",
      "The 26397 th iteration gives loss of 0.16835858271599258\n",
      "The 26398 th iteration gives loss of 0.16835596113894\n",
      "The 26399 th iteration gives loss of 0.16835333977294173\n",
      "The 26400 th iteration gives loss of 0.16835071861796877\n",
      "The 26401 th iteration gives loss of 0.16834809767399672\n",
      "The 26402 th iteration gives loss of 0.16834547694098545\n",
      "The 26403 th iteration gives loss of 0.16834285641890565\n",
      "The 26404 th iteration gives loss of 0.16834023610773385\n",
      "The 26405 th iteration gives loss of 0.16833761600743313\n",
      "The 26406 th iteration gives loss of 0.16833499611795963\n",
      "The 26407 th iteration gives loss of 0.16833237643930243\n",
      "The 26408 th iteration gives loss of 0.1683297569714155\n",
      "The 26409 th iteration gives loss of 0.16832713771426908\n",
      "The 26410 th iteration gives loss of 0.16832451866782877\n",
      "The 26411 th iteration gives loss of 0.1683218998320712\n",
      "The 26412 th iteration gives loss of 0.1683192812069568\n",
      "The 26413 th iteration gives loss of 0.1683166627924533\n",
      "The 26414 th iteration gives loss of 0.1683140445885413\n",
      "The 26415 th iteration gives loss of 0.16831142659517054\n",
      "The 26416 th iteration gives loss of 0.16830880881233082\n",
      "The 26417 th iteration gives loss of 0.16830619123996787\n",
      "The 26418 th iteration gives loss of 0.16830357387806244\n",
      "The 26419 th iteration gives loss of 0.16830095672658402\n",
      "The 26420 th iteration gives loss of 0.16829833978549685\n",
      "The 26421 th iteration gives loss of 0.16829572305477322\n",
      "The 26422 th iteration gives loss of 0.16829310653437787\n",
      "The 26423 th iteration gives loss of 0.16829049022427475\n",
      "The 26424 th iteration gives loss of 0.1682878741244408\n",
      "The 26425 th iteration gives loss of 0.16828525823483503\n",
      "The 26426 th iteration gives loss of 0.16828264255543768\n",
      "The 26427 th iteration gives loss of 0.16828002708620374\n",
      "The 26428 th iteration gives loss of 0.16827741182711195\n",
      "The 26429 th iteration gives loss of 0.16827479677812715\n",
      "The 26430 th iteration gives loss of 0.1682721819392151\n",
      "The 26431 th iteration gives loss of 0.16826956731034748\n",
      "The 26432 th iteration gives loss of 0.16826695289148563\n",
      "The 26433 th iteration gives loss of 0.16826433868261523\n",
      "The 26434 th iteration gives loss of 0.16826172468368833\n",
      "The 26435 th iteration gives loss of 0.16825911089468223\n",
      "The 26436 th iteration gives loss of 0.168256497315547\n",
      "The 26437 th iteration gives loss of 0.16825388394627241\n",
      "The 26438 th iteration gives loss of 0.16825127078682495\n",
      "The 26439 th iteration gives loss of 0.16824865783716367\n",
      "The 26440 th iteration gives loss of 0.16824604509725616\n",
      "The 26441 th iteration gives loss of 0.16824343256708277\n",
      "The 26442 th iteration gives loss of 0.16824082024659526\n",
      "The 26443 th iteration gives loss of 0.1682382081357771\n",
      "The 26444 th iteration gives loss of 0.16823559623458673\n",
      "The 26445 th iteration gives loss of 0.16823298454299943\n",
      "The 26446 th iteration gives loss of 0.16823037306097424\n",
      "The 26447 th iteration gives loss of 0.1682277617884919\n",
      "The 26448 th iteration gives loss of 0.16822515072551222\n",
      "The 26449 th iteration gives loss of 0.16822253987200878\n",
      "The 26450 th iteration gives loss of 0.16821992922794662\n",
      "The 26451 th iteration gives loss of 0.16821731879329632\n",
      "The 26452 th iteration gives loss of 0.16821470856801657\n",
      "The 26453 th iteration gives loss of 0.1682120985520908\n",
      "The 26454 th iteration gives loss of 0.16820948874547864\n",
      "The 26455 th iteration gives loss of 0.1682068791481507\n",
      "The 26456 th iteration gives loss of 0.1682042697600776\n",
      "The 26457 th iteration gives loss of 0.16820166058121766\n",
      "The 26458 th iteration gives loss of 0.16819905161155257\n",
      "The 26459 th iteration gives loss of 0.16819644285103996\n",
      "The 26460 th iteration gives loss of 0.1681938342996581\n",
      "The 26461 th iteration gives loss of 0.16819122595737507\n",
      "The 26462 th iteration gives loss of 0.16818861782414068\n",
      "The 26463 th iteration gives loss of 0.16818600989995464\n",
      "The 26464 th iteration gives loss of 0.16818340218475877\n",
      "The 26465 th iteration gives loss of 0.16818079467852953\n",
      "The 26466 th iteration gives loss of 0.1681781873812477\n",
      "The 26467 th iteration gives loss of 0.16817558029286478\n",
      "The 26468 th iteration gives loss of 0.16817297341335338\n",
      "The 26469 th iteration gives loss of 0.16817036674268754\n",
      "The 26470 th iteration gives loss of 0.1681677602808276\n",
      "The 26471 th iteration gives loss of 0.16816515402775062\n",
      "The 26472 th iteration gives loss of 0.16816254798342412\n",
      "The 26473 th iteration gives loss of 0.1681599421478069\n",
      "The 26474 th iteration gives loss of 0.16815733652088233\n",
      "The 26475 th iteration gives loss of 0.16815473110261064\n",
      "The 26476 th iteration gives loss of 0.16815212589295764\n",
      "The 26477 th iteration gives loss of 0.1681495208918977\n",
      "The 26478 th iteration gives loss of 0.16814691609939358\n",
      "The 26479 th iteration gives loss of 0.16814431151541875\n",
      "The 26480 th iteration gives loss of 0.1681417071399326\n",
      "The 26481 th iteration gives loss of 0.16813910297292203\n",
      "The 26482 th iteration gives loss of 0.16813649901433464\n",
      "The 26483 th iteration gives loss of 0.1681338952641528\n",
      "The 26484 th iteration gives loss of 0.1681312917223485\n",
      "The 26485 th iteration gives loss of 0.16812868838887662\n",
      "The 26486 th iteration gives loss of 0.16812608526370998\n",
      "The 26487 th iteration gives loss of 0.16812348234682092\n",
      "The 26488 th iteration gives loss of 0.1681208796381782\n",
      "The 26489 th iteration gives loss of 0.16811827713774202\n",
      "The 26490 th iteration gives loss of 0.168115674845493\n",
      "The 26491 th iteration gives loss of 0.16811307276139534\n",
      "The 26492 th iteration gives loss of 0.1681104708854152\n",
      "The 26493 th iteration gives loss of 0.16810786921752313\n",
      "The 26494 th iteration gives loss of 0.16810526775769163\n",
      "The 26495 th iteration gives loss of 0.1681026665058805\n",
      "The 26496 th iteration gives loss of 0.1681000654620619\n",
      "The 26497 th iteration gives loss of 0.16809746462620925\n",
      "The 26498 th iteration gives loss of 0.16809486399828236\n",
      "The 26499 th iteration gives loss of 0.1680922635782595\n",
      "The 26500 th iteration gives loss of 0.16808966336610587\n",
      "The 26501 th iteration gives loss of 0.16808706336178908\n",
      "The 26502 th iteration gives loss of 0.16808446356526943\n",
      "The 26503 th iteration gives loss of 0.16808186397652647\n",
      "The 26504 th iteration gives loss of 0.168079264595529\n",
      "The 26505 th iteration gives loss of 0.16807666542224478\n",
      "The 26506 th iteration gives loss of 0.1680740664566305\n",
      "The 26507 th iteration gives loss of 0.16807146769867148\n",
      "The 26508 th iteration gives loss of 0.16806886914833047\n",
      "The 26509 th iteration gives loss of 0.16806627080557504\n",
      "The 26510 th iteration gives loss of 0.16806367267037542\n",
      "The 26511 th iteration gives loss of 0.1680610747426937\n",
      "The 26512 th iteration gives loss of 0.16805847702250587\n",
      "The 26513 th iteration gives loss of 0.1680558795097819\n",
      "The 26514 th iteration gives loss of 0.1680532822044882\n",
      "The 26515 th iteration gives loss of 0.16805068510659382\n",
      "The 26516 th iteration gives loss of 0.16804808821606254\n",
      "The 26517 th iteration gives loss of 0.16804549153287138\n",
      "The 26518 th iteration gives loss of 0.16804289505697773\n",
      "The 26519 th iteration gives loss of 0.16804029878836488\n",
      "The 26520 th iteration gives loss of 0.16803770272699167\n",
      "The 26521 th iteration gives loss of 0.16803510687282988\n",
      "The 26522 th iteration gives loss of 0.1680325112258373\n",
      "The 26523 th iteration gives loss of 0.16802991578599785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 26524 th iteration gives loss of 0.1680273205532824\n",
      "The 26525 th iteration gives loss of 0.16802472552765438\n",
      "The 26526 th iteration gives loss of 0.1680221307090734\n",
      "The 26527 th iteration gives loss of 0.1680195360975138\n",
      "The 26528 th iteration gives loss of 0.16801694169294795\n",
      "The 26529 th iteration gives loss of 0.1680143474953463\n",
      "The 26530 th iteration gives loss of 0.16801175350466926\n",
      "The 26531 th iteration gives loss of 0.1680091597208963\n",
      "The 26532 th iteration gives loss of 0.16800656614398704\n",
      "The 26533 th iteration gives loss of 0.16800397277391282\n",
      "The 26534 th iteration gives loss of 0.16800137961064804\n",
      "The 26535 th iteration gives loss of 0.1679987866541523\n",
      "The 26536 th iteration gives loss of 0.1679961939043983\n",
      "The 26537 th iteration gives loss of 0.1679936013613579\n",
      "The 26538 th iteration gives loss of 0.16799100902499978\n",
      "The 26539 th iteration gives loss of 0.1679884168952924\n",
      "The 26540 th iteration gives loss of 0.1679858249721962\n",
      "The 26541 th iteration gives loss of 0.16798323325569078\n",
      "The 26542 th iteration gives loss of 0.16798064174574298\n",
      "The 26543 th iteration gives loss of 0.16797805044231345\n",
      "The 26544 th iteration gives loss of 0.16797545934538596\n",
      "The 26545 th iteration gives loss of 0.167972868454909\n",
      "The 26546 th iteration gives loss of 0.16797027777087312\n",
      "The 26547 th iteration gives loss of 0.16796768729322847\n",
      "The 26548 th iteration gives loss of 0.16796509702195606\n",
      "The 26549 th iteration gives loss of 0.16796250695702414\n",
      "The 26550 th iteration gives loss of 0.16795991709838823\n",
      "The 26551 th iteration gives loss of 0.16795732744603611\n",
      "The 26552 th iteration gives loss of 0.167954737999929\n",
      "The 26553 th iteration gives loss of 0.16795214876003645\n",
      "The 26554 th iteration gives loss of 0.1679495597263274\n",
      "The 26555 th iteration gives loss of 0.16794697089875918\n",
      "The 26556 th iteration gives loss of 0.16794438227731984\n",
      "The 26557 th iteration gives loss of 0.16794179386197058\n",
      "The 26558 th iteration gives loss of 0.16793920565267736\n",
      "The 26559 th iteration gives loss of 0.16793661764940582\n",
      "The 26560 th iteration gives loss of 0.167934029852134\n",
      "The 26561 th iteration gives loss of 0.1679314422608258\n",
      "The 26562 th iteration gives loss of 0.16792885487545453\n",
      "The 26563 th iteration gives loss of 0.16792626769597835\n",
      "The 26564 th iteration gives loss of 0.1679236807223856\n",
      "The 26565 th iteration gives loss of 0.16792109395462668\n",
      "The 26566 th iteration gives loss of 0.16791850739267683\n",
      "The 26567 th iteration gives loss of 0.16791592103650244\n",
      "The 26568 th iteration gives loss of 0.16791333488607568\n",
      "The 26569 th iteration gives loss of 0.16791074894137417\n",
      "The 26570 th iteration gives loss of 0.16790816320234242\n",
      "The 26571 th iteration gives loss of 0.16790557766897443\n",
      "The 26572 th iteration gives loss of 0.16790299234123307\n",
      "The 26573 th iteration gives loss of 0.16790040721908273\n",
      "The 26574 th iteration gives loss of 0.1678978223024898\n",
      "The 26575 th iteration gives loss of 0.16789523759143107\n",
      "The 26576 th iteration gives loss of 0.16789265308586895\n",
      "The 26577 th iteration gives loss of 0.1678900687857751\n",
      "The 26578 th iteration gives loss of 0.16788748469112266\n",
      "The 26579 th iteration gives loss of 0.1678849008018799\n",
      "The 26580 th iteration gives loss of 0.16788231711800458\n",
      "The 26581 th iteration gives loss of 0.16787973363947295\n",
      "The 26582 th iteration gives loss of 0.16787715036626127\n",
      "The 26583 th iteration gives loss of 0.16787456729832587\n",
      "The 26584 th iteration gives loss of 0.16787198443565027\n",
      "The 26585 th iteration gives loss of 0.16786940177818674\n",
      "The 26586 th iteration gives loss of 0.16786681932591346\n",
      "The 26587 th iteration gives loss of 0.1678642370788059\n",
      "The 26588 th iteration gives loss of 0.16786165503682424\n",
      "The 26589 th iteration gives loss of 0.16785907319993848\n",
      "The 26590 th iteration gives loss of 0.16785649156812052\n",
      "The 26591 th iteration gives loss of 0.1678539101413292\n",
      "The 26592 th iteration gives loss of 0.16785132891955867\n",
      "The 26593 th iteration gives loss of 0.16784874790275178\n",
      "The 26594 th iteration gives loss of 0.1678461670908799\n",
      "The 26595 th iteration gives loss of 0.16784358648393255\n",
      "The 26596 th iteration gives loss of 0.1678410060818609\n",
      "The 26597 th iteration gives loss of 0.1678384258846417\n",
      "The 26598 th iteration gives loss of 0.16783584589223496\n",
      "The 26599 th iteration gives loss of 0.1678332661046233\n",
      "The 26600 th iteration gives loss of 0.16783068652177302\n",
      "The 26601 th iteration gives loss of 0.1678281071436411\n",
      "The 26602 th iteration gives loss of 0.16782552797021508\n",
      "The 26603 th iteration gives loss of 0.1678229490014455\n",
      "The 26604 th iteration gives loss of 0.1678203702373103\n",
      "The 26605 th iteration gives loss of 0.16781779167776767\n",
      "The 26606 th iteration gives loss of 0.16781521332281277\n",
      "The 26607 th iteration gives loss of 0.16781263517239156\n",
      "The 26608 th iteration gives loss of 0.16781005722648284\n",
      "The 26609 th iteration gives loss of 0.16780747948505545\n",
      "The 26610 th iteration gives loss of 0.16780490194807726\n",
      "The 26611 th iteration gives loss of 0.16780232461551578\n",
      "The 26612 th iteration gives loss of 0.16779974748734036\n",
      "The 26613 th iteration gives loss of 0.16779717056352722\n",
      "The 26614 th iteration gives loss of 0.16779459384403186\n",
      "The 26615 th iteration gives loss of 0.16779201732884313\n",
      "The 26616 th iteration gives loss of 0.1677894410179133\n",
      "The 26617 th iteration gives loss of 0.16778686491120748\n",
      "The 26618 th iteration gives loss of 0.1677842890087226\n",
      "The 26619 th iteration gives loss of 0.16778171331039402\n",
      "The 26620 th iteration gives loss of 0.16777913781620926\n",
      "The 26621 th iteration gives loss of 0.16777656252613096\n",
      "The 26622 th iteration gives loss of 0.16777398744014704\n",
      "The 26623 th iteration gives loss of 0.1677714125582021\n",
      "The 26624 th iteration gives loss of 0.1677688378802753\n",
      "The 26625 th iteration gives loss of 0.16776626340634207\n",
      "The 26626 th iteration gives loss of 0.16776368913635517\n",
      "The 26627 th iteration gives loss of 0.16776111507029487\n",
      "The 26628 th iteration gives loss of 0.16775854120814354\n",
      "The 26629 th iteration gives loss of 0.16775596754984318\n",
      "The 26630 th iteration gives loss of 0.1677533940953851\n",
      "The 26631 th iteration gives loss of 0.16775082084472934\n",
      "The 26632 th iteration gives loss of 0.1677482477978392\n",
      "The 26633 th iteration gives loss of 0.16774567495469733\n",
      "The 26634 th iteration gives loss of 0.16774310231526113\n",
      "The 26635 th iteration gives loss of 0.16774052987950655\n",
      "The 26636 th iteration gives loss of 0.16773795764740637\n",
      "The 26637 th iteration gives loss of 0.16773538561892337\n",
      "The 26638 th iteration gives loss of 0.1677328137940258\n",
      "The 26639 th iteration gives loss of 0.16773024217268095\n",
      "The 26640 th iteration gives loss of 0.1677276707548706\n",
      "The 26641 th iteration gives loss of 0.16772509954055395\n",
      "The 26642 th iteration gives loss of 0.16772252852970035\n",
      "The 26643 th iteration gives loss of 0.1677199577222826\n",
      "The 26644 th iteration gives loss of 0.16771738711827824\n",
      "The 26645 th iteration gives loss of 0.167714816717642\n",
      "The 26646 th iteration gives loss of 0.16771224652033945\n",
      "The 26647 th iteration gives loss of 0.1677096765263558\n",
      "The 26648 th iteration gives loss of 0.16770710673565623\n",
      "The 26649 th iteration gives loss of 0.1677045371482059\n",
      "The 26650 th iteration gives loss of 0.1677019677639734\n",
      "The 26651 th iteration gives loss of 0.1676993985829282\n",
      "The 26652 th iteration gives loss of 0.16769682960504792\n",
      "The 26653 th iteration gives loss of 0.16769426083029884\n",
      "The 26654 th iteration gives loss of 0.16769169225865418\n",
      "The 26655 th iteration gives loss of 0.16768912389005905\n",
      "The 26656 th iteration gives loss of 0.16768655572451507\n",
      "The 26657 th iteration gives loss of 0.16768398776196863\n",
      "The 26658 th iteration gives loss of 0.16768142000240294\n",
      "The 26659 th iteration gives loss of 0.16767885244577763\n",
      "The 26660 th iteration gives loss of 0.16767628509206575\n",
      "The 26661 th iteration gives loss of 0.16767371794124683\n",
      "The 26662 th iteration gives loss of 0.1676711509932689\n",
      "The 26663 th iteration gives loss of 0.1676685842481217\n",
      "The 26664 th iteration gives loss of 0.16766601770576997\n",
      "The 26665 th iteration gives loss of 0.1676634513661761\n",
      "The 26666 th iteration gives loss of 0.167660885229307\n",
      "The 26667 th iteration gives loss of 0.1676583192951467\n",
      "The 26668 th iteration gives loss of 0.16765575356365908\n",
      "The 26669 th iteration gives loss of 0.16765318803480364\n",
      "The 26670 th iteration gives loss of 0.16765062270856504\n",
      "The 26671 th iteration gives loss of 0.167648057584892\n",
      "The 26672 th iteration gives loss of 0.16764549266377426\n",
      "The 26673 th iteration gives loss of 0.1676429279451752\n",
      "The 26674 th iteration gives loss of 0.1676403634290635\n",
      "The 26675 th iteration gives loss of 0.1676377991154098\n",
      "The 26676 th iteration gives loss of 0.16763523500418326\n",
      "The 26677 th iteration gives loss of 0.16763267109534447\n",
      "The 26678 th iteration gives loss of 0.16763010738887982\n",
      "The 26679 th iteration gives loss of 0.16762754388474632\n",
      "The 26680 th iteration gives loss of 0.16762498058291067\n",
      "The 26681 th iteration gives loss of 0.16762241748336115\n",
      "The 26682 th iteration gives loss of 0.16761985458604467\n",
      "The 26683 th iteration gives loss of 0.1676172918909374\n",
      "The 26684 th iteration gives loss of 0.16761472939803118\n",
      "The 26685 th iteration gives loss of 0.1676121671072629\n",
      "The 26686 th iteration gives loss of 0.16760960501861538\n",
      "The 26687 th iteration gives loss of 0.1676070431320537\n",
      "The 26688 th iteration gives loss of 0.16760448144756357\n",
      "The 26689 th iteration gives loss of 0.16760191996509696\n",
      "The 26690 th iteration gives loss of 0.16759935868463632\n",
      "The 26691 th iteration gives loss of 0.1675967976061333\n",
      "The 26692 th iteration gives loss of 0.16759423672958085\n",
      "The 26693 th iteration gives loss of 0.16759167605493655\n",
      "The 26694 th iteration gives loss of 0.1675891155821649\n",
      "The 26695 th iteration gives loss of 0.16758655531124175\n",
      "The 26696 th iteration gives loss of 0.167583995242138\n",
      "The 26697 th iteration gives loss of 0.16758143537481776\n",
      "The 26698 th iteration gives loss of 0.16757887570925697\n",
      "The 26699 th iteration gives loss of 0.16757631624542477\n",
      "The 26700 th iteration gives loss of 0.1675737569832776\n",
      "The 26701 th iteration gives loss of 0.1675711979228076\n",
      "The 26702 th iteration gives loss of 0.1675686390639679\n",
      "The 26703 th iteration gives loss of 0.1675660804067311\n",
      "The 26704 th iteration gives loss of 0.16756352195106922\n",
      "The 26705 th iteration gives loss of 0.16756096369695236\n",
      "The 26706 th iteration gives loss of 0.16755840564434793\n",
      "The 26707 th iteration gives loss of 0.16755584779322996\n",
      "The 26708 th iteration gives loss of 0.1675532901435606\n",
      "The 26709 th iteration gives loss of 0.16755073269531046\n",
      "The 26710 th iteration gives loss of 0.16754817544845205\n",
      "The 26711 th iteration gives loss of 0.167545618402964\n",
      "The 26712 th iteration gives loss of 0.1675430615588051\n",
      "The 26713 th iteration gives loss of 0.1675405049159455\n",
      "The 26714 th iteration gives loss of 0.1675379484743617\n",
      "The 26715 th iteration gives loss of 0.16753539223401562\n",
      "The 26716 th iteration gives loss of 0.1675328361948704\n",
      "The 26717 th iteration gives loss of 0.1675302803569218\n",
      "The 26718 th iteration gives loss of 0.16752772472010935\n",
      "The 26719 th iteration gives loss of 0.16752516928442468\n",
      "The 26720 th iteration gives loss of 0.1675226140498276\n",
      "The 26721 th iteration gives loss of 0.16752005901628575\n",
      "The 26722 th iteration gives loss of 0.16751750418376868\n",
      "The 26723 th iteration gives loss of 0.16751494955225302\n",
      "The 26724 th iteration gives loss of 0.1675123951217137\n",
      "The 26725 th iteration gives loss of 0.16750984089210466\n",
      "The 26726 th iteration gives loss of 0.16750728686340255\n",
      "The 26727 th iteration gives loss of 0.1675047330355809\n",
      "The 26728 th iteration gives loss of 0.1675021794086006\n",
      "The 26729 th iteration gives loss of 0.16749962598243803\n",
      "The 26730 th iteration gives loss of 0.16749707275706732\n",
      "The 26731 th iteration gives loss of 0.16749451973245116\n",
      "The 26732 th iteration gives loss of 0.1674919669085692\n",
      "The 26733 th iteration gives loss of 0.16748941428537206\n",
      "The 26734 th iteration gives loss of 0.1674868618628393\n",
      "The 26735 th iteration gives loss of 0.1674843096409436\n",
      "The 26736 th iteration gives loss of 0.16748175761965997\n",
      "The 26737 th iteration gives loss of 0.16747920579894732\n",
      "The 26738 th iteration gives loss of 0.16747665417877422\n",
      "The 26739 th iteration gives loss of 0.1674741027591219\n",
      "The 26740 th iteration gives loss of 0.16747155153995774\n",
      "The 26741 th iteration gives loss of 0.1674690005212421\n",
      "The 26742 th iteration gives loss of 0.16746644970295313\n",
      "The 26743 th iteration gives loss of 0.16746389908505388\n",
      "The 26744 th iteration gives loss of 0.16746134866752022\n",
      "The 26745 th iteration gives loss of 0.16745879845032\n",
      "The 26746 th iteration gives loss of 0.16745624843342385\n",
      "The 26747 th iteration gives loss of 0.16745369861679957\n",
      "The 26748 th iteration gives loss of 0.1674511490004216\n",
      "The 26749 th iteration gives loss of 0.1674485995842557\n",
      "The 26750 th iteration gives loss of 0.16744605036827306\n",
      "The 26751 th iteration gives loss of 0.16744350135244598\n",
      "The 26752 th iteration gives loss of 0.16744095253672947\n",
      "The 26753 th iteration gives loss of 0.1674384039211165\n",
      "The 26754 th iteration gives loss of 0.1674358555055652\n",
      "The 26755 th iteration gives loss of 0.16743330729004297\n",
      "The 26756 th iteration gives loss of 0.16743075927452114\n",
      "The 26757 th iteration gives loss of 0.16742821145897158\n",
      "The 26758 th iteration gives loss of 0.16742566384336613\n",
      "The 26759 th iteration gives loss of 0.16742311642767238\n",
      "The 26760 th iteration gives loss of 0.1674205692118608\n",
      "The 26761 th iteration gives loss of 0.1674180221958986\n",
      "The 26762 th iteration gives loss of 0.16741547537976137\n",
      "The 26763 th iteration gives loss of 0.16741292876342245\n",
      "The 26764 th iteration gives loss of 0.16741038234683217\n",
      "The 26765 th iteration gives loss of 0.1674078361299821\n",
      "The 26766 th iteration gives loss of 0.16740529011282515\n",
      "The 26767 th iteration gives loss of 0.16740274429534246\n",
      "The 26768 th iteration gives loss of 0.167400198677507\n",
      "The 26769 th iteration gives loss of 0.16739765325927278\n",
      "The 26770 th iteration gives loss of 0.167395108040626\n",
      "The 26771 th iteration gives loss of 0.16739256302152453\n",
      "The 26772 th iteration gives loss of 0.16739001820195037\n",
      "The 26773 th iteration gives loss of 0.16738747358186218\n",
      "The 26774 th iteration gives loss of 0.16738492916124117\n",
      "The 26775 th iteration gives loss of 0.16738238494004623\n",
      "The 26776 th iteration gives loss of 0.16737984091825872\n",
      "The 26777 th iteration gives loss of 0.16737729709583982\n",
      "The 26778 th iteration gives loss of 0.16737475347275652\n",
      "The 26779 th iteration gives loss of 0.16737221004898412\n",
      "The 26780 th iteration gives loss of 0.1673696668244948\n",
      "The 26781 th iteration gives loss of 0.16736712379926325\n",
      "The 26782 th iteration gives loss of 0.16736458097324539\n",
      "The 26783 th iteration gives loss of 0.16736203834642302\n",
      "The 26784 th iteration gives loss of 0.1673594959187489\n",
      "The 26785 th iteration gives loss of 0.16735695369021844\n",
      "The 26786 th iteration gives loss of 0.16735441166078877\n",
      "The 26787 th iteration gives loss of 0.16735186983042713\n",
      "The 26788 th iteration gives loss of 0.16734932819910925\n",
      "The 26789 th iteration gives loss of 0.16734678676680162\n",
      "The 26790 th iteration gives loss of 0.1673442455334664\n",
      "The 26791 th iteration gives loss of 0.16734170449909375\n",
      "The 26792 th iteration gives loss of 0.16733916366364002\n",
      "The 26793 th iteration gives loss of 0.1673366230270751\n",
      "The 26794 th iteration gives loss of 0.1673340825893757\n",
      "The 26795 th iteration gives loss of 0.16733154235050365\n",
      "The 26796 th iteration gives loss of 0.16732900231043368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 26797 th iteration gives loss of 0.16732646246913604\n",
      "The 26798 th iteration gives loss of 0.16732392282658398\n",
      "The 26799 th iteration gives loss of 0.16732138338272887\n",
      "The 26800 th iteration gives loss of 0.167318844137573\n",
      "The 26801 th iteration gives loss of 0.16731630509106615\n",
      "The 26802 th iteration gives loss of 0.1673137662431832\n",
      "The 26803 th iteration gives loss of 0.16731122759388267\n",
      "The 26804 th iteration gives loss of 0.16730868914315347\n",
      "The 26805 th iteration gives loss of 0.16730615089095072\n",
      "The 26806 th iteration gives loss of 0.16730361283725964\n",
      "The 26807 th iteration gives loss of 0.16730107498203334\n",
      "The 26808 th iteration gives loss of 0.16729853732525193\n",
      "The 26809 th iteration gives loss of 0.16729599986688304\n",
      "The 26810 th iteration gives loss of 0.16729346260690092\n",
      "The 26811 th iteration gives loss of 0.16729092554526132\n",
      "The 26812 th iteration gives loss of 0.16728838868195772\n",
      "The 26813 th iteration gives loss of 0.16728585201694376\n",
      "The 26814 th iteration gives loss of 0.16728331555019765\n",
      "The 26815 th iteration gives loss of 0.16728077928167873\n",
      "The 26816 th iteration gives loss of 0.16727824321136503\n",
      "The 26817 th iteration gives loss of 0.16727570733922606\n",
      "The 26818 th iteration gives loss of 0.1672731716652302\n",
      "The 26819 th iteration gives loss of 0.1672706361893549\n",
      "The 26820 th iteration gives loss of 0.16726810091156327\n",
      "The 26821 th iteration gives loss of 0.16726556583182606\n",
      "The 26822 th iteration gives loss of 0.1672630309501195\n",
      "The 26823 th iteration gives loss of 0.16726049626640427\n",
      "The 26824 th iteration gives loss of 0.16725796178065294\n",
      "The 26825 th iteration gives loss of 0.16725542749284053\n",
      "The 26826 th iteration gives loss of 0.16725289340292757\n",
      "The 26827 th iteration gives loss of 0.16725035951089662\n",
      "The 26828 th iteration gives loss of 0.16724782581671002\n",
      "The 26829 th iteration gives loss of 0.16724529232034552\n",
      "The 26830 th iteration gives loss of 0.16724275902176297\n",
      "The 26831 th iteration gives loss of 0.16724022592093735\n",
      "The 26832 th iteration gives loss of 0.16723769301783697\n",
      "The 26833 th iteration gives loss of 0.16723516031243982\n",
      "The 26834 th iteration gives loss of 0.16723262780471126\n",
      "The 26835 th iteration gives loss of 0.16723009549461967\n",
      "The 26836 th iteration gives loss of 0.16722756338214032\n",
      "The 26837 th iteration gives loss of 0.1672250314672349\n",
      "The 26838 th iteration gives loss of 0.16722249974988435\n",
      "The 26839 th iteration gives loss of 0.1672199682300468\n",
      "The 26840 th iteration gives loss of 0.16721743690769778\n",
      "The 26841 th iteration gives loss of 0.16721490578281306\n",
      "The 26842 th iteration gives loss of 0.16721237485536183\n",
      "The 26843 th iteration gives loss of 0.1672098441253122\n",
      "The 26844 th iteration gives loss of 0.16720731359262897\n",
      "The 26845 th iteration gives loss of 0.16720478325728236\n",
      "The 26846 th iteration gives loss of 0.16720225311925946\n",
      "The 26847 th iteration gives loss of 0.1671997231785086\n",
      "The 26848 th iteration gives loss of 0.16719719343501274\n",
      "The 26849 th iteration gives loss of 0.16719466388874282\n",
      "The 26850 th iteration gives loss of 0.16719213453966134\n",
      "The 26851 th iteration gives loss of 0.16718960538774713\n",
      "The 26852 th iteration gives loss of 0.16718707643296418\n",
      "The 26853 th iteration gives loss of 0.16718454767528207\n",
      "The 26854 th iteration gives loss of 0.16718201911468597\n",
      "The 26855 th iteration gives loss of 0.16717949075112645\n",
      "The 26856 th iteration gives loss of 0.16717696258458473\n",
      "The 26857 th iteration gives loss of 0.16717443461503007\n",
      "The 26858 th iteration gives loss of 0.16717190684242492\n",
      "The 26859 th iteration gives loss of 0.1671693792667435\n",
      "The 26860 th iteration gives loss of 0.1671668518879713\n",
      "The 26861 th iteration gives loss of 0.1671643247060574\n",
      "The 26862 th iteration gives loss of 0.16716179772098197\n",
      "The 26863 th iteration gives loss of 0.16715927093271057\n",
      "The 26864 th iteration gives loss of 0.16715674434122457\n",
      "The 26865 th iteration gives loss of 0.16715421794647725\n",
      "The 26866 th iteration gives loss of 0.16715169174846659\n",
      "The 26867 th iteration gives loss of 0.16714916574713437\n",
      "The 26868 th iteration gives loss of 0.16714663994246584\n",
      "The 26869 th iteration gives loss of 0.16714411433442147\n",
      "The 26870 th iteration gives loss of 0.16714158892297867\n",
      "The 26871 th iteration gives loss of 0.1671390637080989\n",
      "The 26872 th iteration gives loss of 0.16713653868977507\n",
      "The 26873 th iteration gives loss of 0.16713401386796142\n",
      "The 26874 th iteration gives loss of 0.16713148924262614\n",
      "The 26875 th iteration gives loss of 0.16712896481374723\n",
      "The 26876 th iteration gives loss of 0.16712644058128695\n",
      "The 26877 th iteration gives loss of 0.16712391654522316\n",
      "The 26878 th iteration gives loss of 0.16712139270552495\n",
      "The 26879 th iteration gives loss of 0.16711886906216072\n",
      "The 26880 th iteration gives loss of 0.1671163456151019\n",
      "The 26881 th iteration gives loss of 0.16711382236432276\n",
      "The 26882 th iteration gives loss of 0.1671112993097836\n",
      "The 26883 th iteration gives loss of 0.16710877645146022\n",
      "The 26884 th iteration gives loss of 0.16710625378932709\n",
      "The 26885 th iteration gives loss of 0.16710373132335327\n",
      "The 26886 th iteration gives loss of 0.16710120905350956\n",
      "The 26887 th iteration gives loss of 0.16709868697975502\n",
      "The 26888 th iteration gives loss of 0.16709616510207426\n",
      "The 26889 th iteration gives loss of 0.167093643420437\n",
      "The 26890 th iteration gives loss of 0.1670911219348105\n",
      "The 26891 th iteration gives loss of 0.16708860064515757\n",
      "The 26892 th iteration gives loss of 0.16708607955146795\n",
      "The 26893 th iteration gives loss of 0.16708355865368985\n",
      "The 26894 th iteration gives loss of 0.16708103795180765\n",
      "The 26895 th iteration gives loss of 0.16707851744579325\n",
      "The 26896 th iteration gives loss of 0.16707599713560323\n",
      "The 26897 th iteration gives loss of 0.1670734770212195\n",
      "The 26898 th iteration gives loss of 0.1670709571026051\n",
      "The 26899 th iteration gives loss of 0.1670684373797464\n",
      "The 26900 th iteration gives loss of 0.16706591785260155\n",
      "The 26901 th iteration gives loss of 0.16706339852114163\n",
      "The 26902 th iteration gives loss of 0.16706087938533606\n",
      "The 26903 th iteration gives loss of 0.16705836044515804\n",
      "The 26904 th iteration gives loss of 0.1670558417005796\n",
      "The 26905 th iteration gives loss of 0.1670533231515651\n",
      "The 26906 th iteration gives loss of 0.16705080479809442\n",
      "The 26907 th iteration gives loss of 0.16704828664013416\n",
      "The 26908 th iteration gives loss of 0.16704576867765242\n",
      "The 26909 th iteration gives loss of 0.16704325091063005\n",
      "The 26910 th iteration gives loss of 0.16704073333902392\n",
      "The 26911 th iteration gives loss of 0.16703821596280743\n",
      "The 26912 th iteration gives loss of 0.16703569878195823\n",
      "The 26913 th iteration gives loss of 0.16703318179643703\n",
      "The 26914 th iteration gives loss of 0.1670306650062238\n",
      "The 26915 th iteration gives loss of 0.16702814841128877\n",
      "The 26916 th iteration gives loss of 0.1670256320115972\n",
      "The 26917 th iteration gives loss of 0.16702311580711818\n",
      "The 26918 th iteration gives loss of 0.16702059979782582\n",
      "The 26919 th iteration gives loss of 0.16701808398369045\n",
      "The 26920 th iteration gives loss of 0.16701556836468398\n",
      "The 26921 th iteration gives loss of 0.1670130529407748\n",
      "The 26922 th iteration gives loss of 0.167010537711933\n",
      "The 26923 th iteration gives loss of 0.16700802267814358\n",
      "The 26924 th iteration gives loss of 0.167005507839366\n",
      "The 26925 th iteration gives loss of 0.16700299319556092\n",
      "The 26926 th iteration gives loss of 0.16700047874670282\n",
      "The 26927 th iteration gives loss of 0.16699796449277465\n",
      "The 26928 th iteration gives loss of 0.16699545043373878\n",
      "The 26929 th iteration gives loss of 0.16699293656956654\n",
      "The 26930 th iteration gives loss of 0.16699042290022698\n",
      "The 26931 th iteration gives loss of 0.1669879094257065\n",
      "The 26932 th iteration gives loss of 0.16698539614594987\n",
      "The 26933 th iteration gives loss of 0.16698288306093992\n",
      "The 26934 th iteration gives loss of 0.16698037017064832\n",
      "The 26935 th iteration gives loss of 0.1669778574750504\n",
      "The 26936 th iteration gives loss of 0.16697534497411387\n",
      "The 26937 th iteration gives loss of 0.16697283266780646\n",
      "The 26938 th iteration gives loss of 0.16697032055609795\n",
      "The 26939 th iteration gives loss of 0.16696780863895747\n",
      "The 26940 th iteration gives loss of 0.1669652969163606\n",
      "The 26941 th iteration gives loss of 0.16696278538827516\n",
      "The 26942 th iteration gives loss of 0.16696027405467445\n",
      "The 26943 th iteration gives loss of 0.1669577629155264\n",
      "The 26944 th iteration gives loss of 0.16695525197081043\n",
      "The 26945 th iteration gives loss of 0.1669527412204899\n",
      "The 26946 th iteration gives loss of 0.166950230664536\n",
      "The 26947 th iteration gives loss of 0.1669477203029149\n",
      "The 26948 th iteration gives loss of 0.16694521013560593\n",
      "The 26949 th iteration gives loss of 0.16694270016257567\n",
      "The 26950 th iteration gives loss of 0.16694019038379662\n",
      "The 26951 th iteration gives loss of 0.16693768079923774\n",
      "The 26952 th iteration gives loss of 0.16693517140886824\n",
      "The 26953 th iteration gives loss of 0.16693266221266417\n",
      "The 26954 th iteration gives loss of 0.16693015321059268\n",
      "The 26955 th iteration gives loss of 0.16692764440261734\n",
      "The 26956 th iteration gives loss of 0.16692513578874094\n",
      "The 26957 th iteration gives loss of 0.16692262736888747\n",
      "The 26958 th iteration gives loss of 0.16692011914305768\n",
      "The 26959 th iteration gives loss of 0.16691761111121758\n",
      "The 26960 th iteration gives loss of 0.1669151032733339\n",
      "The 26961 th iteration gives loss of 0.16691259562937905\n",
      "The 26962 th iteration gives loss of 0.16691008817932265\n",
      "The 26963 th iteration gives loss of 0.16690758092313718\n",
      "The 26964 th iteration gives loss of 0.1669050738607976\n",
      "The 26965 th iteration gives loss of 0.166902566992269\n",
      "The 26966 th iteration gives loss of 0.16690006031752927\n",
      "The 26967 th iteration gives loss of 0.16689755383653218\n",
      "The 26968 th iteration gives loss of 0.16689504754926654\n",
      "The 26969 th iteration gives loss of 0.16689254145569976\n",
      "The 26970 th iteration gives loss of 0.1668900355558029\n",
      "The 26971 th iteration gives loss of 0.16688752984954097\n",
      "The 26972 th iteration gives loss of 0.16688502433688293\n",
      "The 26973 th iteration gives loss of 0.16688251901781698\n",
      "The 26974 th iteration gives loss of 0.16688001389228624\n",
      "The 26975 th iteration gives loss of 0.166877508960281\n",
      "The 26976 th iteration gives loss of 0.16687500422177934\n",
      "The 26977 th iteration gives loss of 0.16687249967673182\n",
      "The 26978 th iteration gives loss of 0.16686999532511518\n",
      "The 26979 th iteration gives loss of 0.16686749116691418\n",
      "The 26980 th iteration gives loss of 0.16686498720208268\n",
      "The 26981 th iteration gives loss of 0.1668624834306015\n",
      "The 26982 th iteration gives loss of 0.16685997985243423\n",
      "The 26983 th iteration gives loss of 0.16685747646756294\n",
      "The 26984 th iteration gives loss of 0.16685497327595009\n",
      "The 26985 th iteration gives loss of 0.1668524702775713\n",
      "The 26986 th iteration gives loss of 0.1668499674723821\n",
      "The 26987 th iteration gives loss of 0.16684746486037336\n",
      "The 26988 th iteration gives loss of 0.16684496244150948\n",
      "The 26989 th iteration gives loss of 0.16684246021575885\n",
      "The 26990 th iteration gives loss of 0.16683995818309957\n",
      "The 26991 th iteration gives loss of 0.16683745634349245\n",
      "The 26992 th iteration gives loss of 0.16683495469690912\n",
      "The 26993 th iteration gives loss of 0.1668324532433232\n",
      "The 26994 th iteration gives loss of 0.16682995198271686\n",
      "The 26995 th iteration gives loss of 0.1668274509150499\n",
      "The 26996 th iteration gives loss of 0.16682495004029896\n",
      "The 26997 th iteration gives loss of 0.16682244935842447\n",
      "The 26998 th iteration gives loss of 0.16681994886941512\n",
      "The 26999 th iteration gives loss of 0.16681744857321826\n",
      "The 27000 th iteration gives loss of 0.1668149484698184\n",
      "The 27001 th iteration gives loss of 0.1668124485591914\n",
      "The 27002 th iteration gives loss of 0.16680994884129316\n",
      "The 27003 th iteration gives loss of 0.16680744931611255\n",
      "The 27004 th iteration gives loss of 0.16680494998361065\n",
      "The 27005 th iteration gives loss of 0.16680245084375322\n",
      "The 27006 th iteration gives loss of 0.16679995189653068\n",
      "The 27007 th iteration gives loss of 0.16679745314188738\n",
      "The 27008 th iteration gives loss of 0.16679495457981275\n",
      "The 27009 th iteration gives loss of 0.1667924562102854\n",
      "The 27010 th iteration gives loss of 0.16678995803325306\n",
      "The 27011 th iteration gives loss of 0.1667874600487004\n",
      "The 27012 th iteration gives loss of 0.16678496225659734\n",
      "The 27013 th iteration gives loss of 0.16678246465691662\n",
      "The 27014 th iteration gives loss of 0.16677996724962615\n",
      "The 27015 th iteration gives loss of 0.16677747003469673\n",
      "The 27016 th iteration gives loss of 0.16677497301209515\n",
      "The 27017 th iteration gives loss of 0.16677247618181004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 27018 th iteration gives loss of 0.16676997954379658\n",
      "The 27019 th iteration gives loss of 0.16676748309802095\n",
      "The 27020 th iteration gives loss of 0.16676498684446905\n",
      "The 27021 th iteration gives loss of 0.16676249078310565\n",
      "The 27022 th iteration gives loss of 0.16675999491390223\n",
      "The 27023 th iteration gives loss of 0.16675749923683156\n",
      "The 27024 th iteration gives loss of 0.16675500375186067\n",
      "The 27025 th iteration gives loss of 0.16675250845896444\n",
      "The 27026 th iteration gives loss of 0.1667500133581154\n",
      "The 27027 th iteration gives loss of 0.1667475184492853\n",
      "The 27028 th iteration gives loss of 0.16674502373242522\n",
      "The 27029 th iteration gives loss of 0.16674252920753507\n",
      "The 27030 th iteration gives loss of 0.16674003487457634\n",
      "The 27031 th iteration gives loss of 0.16673754073351524\n",
      "The 27032 th iteration gives loss of 0.16673504678432252\n",
      "The 27033 th iteration gives loss of 0.16673255302698384\n",
      "The 27034 th iteration gives loss of 0.16673005946144034\n",
      "The 27035 th iteration gives loss of 0.16672756608770095\n",
      "The 27036 th iteration gives loss of 0.16672507290570543\n",
      "The 27037 th iteration gives loss of 0.16672257991544204\n",
      "The 27038 th iteration gives loss of 0.1667200871168765\n",
      "The 27039 th iteration gives loss of 0.16671759450998172\n",
      "The 27040 th iteration gives loss of 0.16671510209472262\n",
      "The 27041 th iteration gives loss of 0.1667126098710892\n",
      "The 27042 th iteration gives loss of 0.16671011783902984\n",
      "The 27043 th iteration gives loss of 0.1667076259985221\n",
      "The 27044 th iteration gives loss of 0.16670513434954556\n",
      "The 27045 th iteration gives loss of 0.16670264289206885\n",
      "The 27046 th iteration gives loss of 0.16670015162604973\n",
      "The 27047 th iteration gives loss of 0.1666976605514812\n",
      "The 27048 th iteration gives loss of 0.16669516966832554\n",
      "The 27049 th iteration gives loss of 0.16669267897654383\n",
      "The 27050 th iteration gives loss of 0.1666901884761149\n",
      "The 27051 th iteration gives loss of 0.16668769816701182\n",
      "The 27052 th iteration gives loss of 0.1666852080492132\n",
      "The 27053 th iteration gives loss of 0.16668271812267746\n",
      "The 27054 th iteration gives loss of 0.16668022838737945\n",
      "The 27055 th iteration gives loss of 0.16667773884328885\n",
      "The 27056 th iteration gives loss of 0.16667524949038007\n",
      "The 27057 th iteration gives loss of 0.16667276032862646\n",
      "The 27058 th iteration gives loss of 0.166670271358003\n",
      "The 27059 th iteration gives loss of 0.16666778257846257\n",
      "The 27060 th iteration gives loss of 0.16666529398998572\n",
      "The 27061 th iteration gives loss of 0.16666280559256313\n",
      "The 27062 th iteration gives loss of 0.16666031738613718\n",
      "The 27063 th iteration gives loss of 0.1666578293707012\n",
      "The 27064 th iteration gives loss of 0.16665534154620618\n",
      "The 27065 th iteration gives loss of 0.16665285391264303\n",
      "The 27066 th iteration gives loss of 0.16665036646996656\n",
      "The 27067 th iteration gives loss of 0.1666478792181578\n",
      "The 27068 th iteration gives loss of 0.16664539215719204\n",
      "The 27069 th iteration gives loss of 0.16664290528703202\n",
      "The 27070 th iteration gives loss of 0.16664041860764756\n",
      "The 27071 th iteration gives loss of 0.1666379321190217\n",
      "The 27072 th iteration gives loss of 0.16663544582110745\n",
      "The 27073 th iteration gives loss of 0.1666329597138943\n",
      "The 27074 th iteration gives loss of 0.16663047379734225\n",
      "The 27075 th iteration gives loss of 0.1666279880714372\n",
      "The 27076 th iteration gives loss of 0.16662550253612693\n",
      "The 27077 th iteration gives loss of 0.16662301719140238\n",
      "The 27078 th iteration gives loss of 0.16662053203722474\n",
      "The 27079 th iteration gives loss of 0.1666180470735716\n",
      "The 27080 th iteration gives loss of 0.16661556230041435\n",
      "The 27081 th iteration gives loss of 0.16661307771771214\n",
      "The 27082 th iteration gives loss of 0.1666105933254528\n",
      "The 27083 th iteration gives loss of 0.16660810912359986\n",
      "The 27084 th iteration gives loss of 0.1666056251121291\n",
      "The 27085 th iteration gives loss of 0.16660314129101808\n",
      "The 27086 th iteration gives loss of 0.16660065766022114\n",
      "The 27087 th iteration gives loss of 0.16659817421970868\n",
      "The 27088 th iteration gives loss of 0.16659569096946786\n",
      "The 27089 th iteration gives loss of 0.16659320790946672\n",
      "The 27090 th iteration gives loss of 0.16659072503967498\n",
      "The 27091 th iteration gives loss of 0.16658824236005476\n",
      "The 27092 th iteration gives loss of 0.16658575987058724\n",
      "The 27093 th iteration gives loss of 0.1665832775712387\n",
      "The 27094 th iteration gives loss of 0.1665807954619874\n",
      "The 27095 th iteration gives loss of 0.16657831354279912\n",
      "The 27096 th iteration gives loss of 0.16657583181365093\n",
      "The 27097 th iteration gives loss of 0.16657335027451037\n",
      "The 27098 th iteration gives loss of 0.1665708689253469\n",
      "The 27099 th iteration gives loss of 0.1665683877661314\n",
      "The 27100 th iteration gives loss of 0.16656590679684305\n",
      "The 27101 th iteration gives loss of 0.16656342601744079\n",
      "The 27102 th iteration gives loss of 0.16656094542791622\n",
      "The 27103 th iteration gives loss of 0.16655846502822352\n",
      "The 27104 th iteration gives loss of 0.16655598481833897\n",
      "The 27105 th iteration gives loss of 0.16655350479823788\n",
      "The 27106 th iteration gives loss of 0.16655102496788726\n",
      "The 27107 th iteration gives loss of 0.16654854532725397\n",
      "The 27108 th iteration gives loss of 0.16654606587631862\n",
      "The 27109 th iteration gives loss of 0.16654358661504606\n",
      "The 27110 th iteration gives loss of 0.1665411075434115\n",
      "The 27111 th iteration gives loss of 0.16653862866139152\n",
      "The 27112 th iteration gives loss of 0.16653614996894767\n",
      "The 27113 th iteration gives loss of 0.16653367146605413\n",
      "The 27114 th iteration gives loss of 0.16653119315268328\n",
      "The 27115 th iteration gives loss of 0.1665287150288193\n",
      "The 27116 th iteration gives loss of 0.16652623709440517\n",
      "The 27117 th iteration gives loss of 0.16652375934944058\n",
      "The 27118 th iteration gives loss of 0.1665212817938842\n",
      "The 27119 th iteration gives loss of 0.1665188044277054\n",
      "The 27120 th iteration gives loss of 0.1665163272508791\n",
      "The 27121 th iteration gives loss of 0.16651385026337867\n",
      "The 27122 th iteration gives loss of 0.16651137346518163\n",
      "The 27123 th iteration gives loss of 0.1665088968562513\n",
      "The 27124 th iteration gives loss of 0.16650642043655145\n",
      "The 27125 th iteration gives loss of 0.1665039442060704\n",
      "The 27126 th iteration gives loss of 0.16650146816476985\n",
      "The 27127 th iteration gives loss of 0.1664989923126221\n",
      "The 27128 th iteration gives loss of 0.16649651664959864\n",
      "The 27129 th iteration gives loss of 0.16649404117567773\n",
      "The 27130 th iteration gives loss of 0.16649156589082525\n",
      "The 27131 th iteration gives loss of 0.16648909079500526\n",
      "The 27132 th iteration gives loss of 0.1664866158882077\n",
      "The 27133 th iteration gives loss of 0.16648414117039773\n",
      "The 27134 th iteration gives loss of 0.16648166664152922\n",
      "The 27135 th iteration gives loss of 0.16647919230159655\n",
      "The 27136 th iteration gives loss of 0.16647671815056142\n",
      "The 27137 th iteration gives loss of 0.1664742441883969\n",
      "The 27138 th iteration gives loss of 0.16647177041507516\n",
      "The 27139 th iteration gives loss of 0.1664692968305668\n",
      "The 27140 th iteration gives loss of 0.16646682343484573\n",
      "The 27141 th iteration gives loss of 0.16646435022787495\n",
      "The 27142 th iteration gives loss of 0.16646187720964284\n",
      "The 27143 th iteration gives loss of 0.16645940438010248\n",
      "The 27144 th iteration gives loss of 0.16645693173923914\n",
      "The 27145 th iteration gives loss of 0.1664544592870126\n",
      "The 27146 th iteration gives loss of 0.16645198702341452\n",
      "The 27147 th iteration gives loss of 0.16644951494840035\n",
      "The 27148 th iteration gives loss of 0.16644704306193808\n",
      "The 27149 th iteration gives loss of 0.16644457136401278\n",
      "The 27150 th iteration gives loss of 0.166442099854592\n",
      "The 27151 th iteration gives loss of 0.16643962853363953\n",
      "The 27152 th iteration gives loss of 0.16643715740113574\n",
      "The 27153 th iteration gives loss of 0.16643468645704845\n",
      "The 27154 th iteration gives loss of 0.16643221570134648\n",
      "The 27155 th iteration gives loss of 0.16642974513400932\n",
      "The 27156 th iteration gives loss of 0.16642727475500657\n",
      "The 27157 th iteration gives loss of 0.16642480456430986\n",
      "The 27158 th iteration gives loss of 0.16642233456188776\n",
      "The 27159 th iteration gives loss of 0.1664198647477146\n",
      "The 27160 th iteration gives loss of 0.1664173951217568\n",
      "The 27161 th iteration gives loss of 0.16641492568399535\n",
      "The 27162 th iteration gives loss of 0.16641245643439645\n",
      "The 27163 th iteration gives loss of 0.16640998737293725\n",
      "The 27164 th iteration gives loss of 0.1664075184995699\n",
      "The 27165 th iteration gives loss of 0.16640504981429644\n",
      "The 27166 th iteration gives loss of 0.1664025813170717\n",
      "The 27167 th iteration gives loss of 0.16640011300786678\n",
      "The 27168 th iteration gives loss of 0.16639764488665576\n",
      "The 27169 th iteration gives loss of 0.16639517695341244\n",
      "The 27170 th iteration gives loss of 0.16639270920809943\n",
      "The 27171 th iteration gives loss of 0.16639024165069674\n",
      "The 27172 th iteration gives loss of 0.1663877742811797\n",
      "The 27173 th iteration gives loss of 0.16638530709951396\n",
      "The 27174 th iteration gives loss of 0.16638284010567578\n",
      "The 27175 th iteration gives loss of 0.1663803732996344\n",
      "The 27176 th iteration gives loss of 0.16637790668136027\n",
      "The 27177 th iteration gives loss of 0.16637544025082535\n",
      "The 27178 th iteration gives loss of 0.16637297400800646\n",
      "The 27179 th iteration gives loss of 0.16637050795286518\n",
      "The 27180 th iteration gives loss of 0.16636804208537784\n",
      "The 27181 th iteration gives loss of 0.1663655764055313\n",
      "The 27182 th iteration gives loss of 0.16636311091328082\n",
      "The 27183 th iteration gives loss of 0.16636064560859634\n",
      "The 27184 th iteration gives loss of 0.16635818049145482\n",
      "The 27185 th iteration gives loss of 0.16635571556183487\n",
      "The 27186 th iteration gives loss of 0.16635325081969843\n",
      "The 27187 th iteration gives loss of 0.1663507862650154\n",
      "The 27188 th iteration gives loss of 0.16634832189777235\n",
      "The 27189 th iteration gives loss of 0.1663458577179258\n",
      "The 27190 th iteration gives loss of 0.16634339372545787\n",
      "The 27191 th iteration gives loss of 0.1663409299203384\n",
      "The 27192 th iteration gives loss of 0.16633846630253235\n",
      "The 27193 th iteration gives loss of 0.16633600287201136\n",
      "The 27194 th iteration gives loss of 0.1663335396287573\n",
      "The 27195 th iteration gives loss of 0.16633107657274027\n",
      "The 27196 th iteration gives loss of 0.16632861370392737\n",
      "The 27197 th iteration gives loss of 0.1663261510222967\n",
      "The 27198 th iteration gives loss of 0.16632368852781712\n",
      "The 27199 th iteration gives loss of 0.1663212262204503\n",
      "The 27200 th iteration gives loss of 0.16631876410018473\n",
      "The 27201 th iteration gives loss of 0.16631630216698298\n",
      "The 27202 th iteration gives loss of 0.1663138404208161\n",
      "The 27203 th iteration gives loss of 0.1663113788616599\n",
      "The 27204 th iteration gives loss of 0.166308917489487\n",
      "The 27205 th iteration gives loss of 0.16630645630426302\n",
      "The 27206 th iteration gives loss of 0.1663039953059676\n",
      "The 27207 th iteration gives loss of 0.1663015344945741\n",
      "The 27208 th iteration gives loss of 0.166299073870049\n",
      "The 27209 th iteration gives loss of 0.16629661343235871\n",
      "The 27210 th iteration gives loss of 0.16629415318148502\n",
      "The 27211 th iteration gives loss of 0.16629169311740466\n",
      "The 27212 th iteration gives loss of 0.16628923324007366\n",
      "The 27213 th iteration gives loss of 0.16628677354946794\n",
      "The 27214 th iteration gives loss of 0.16628431404557587\n",
      "The 27215 th iteration gives loss of 0.16628185472835172\n",
      "The 27216 th iteration gives loss of 0.1662793955977728\n",
      "The 27217 th iteration gives loss of 0.16627693665380341\n",
      "The 27218 th iteration gives loss of 0.16627447789642968\n",
      "The 27219 th iteration gives loss of 0.16627201932561583\n",
      "The 27220 th iteration gives loss of 0.16626956094133677\n",
      "The 27221 th iteration gives loss of 0.16626710274357007\n",
      "The 27222 th iteration gives loss of 0.16626464473226732\n",
      "The 27223 th iteration gives loss of 0.16626218690742348\n",
      "The 27224 th iteration gives loss of 0.1662597292690015\n",
      "The 27225 th iteration gives loss of 0.16625727181696762\n",
      "The 27226 th iteration gives loss of 0.16625481455130275\n",
      "The 27227 th iteration gives loss of 0.16625235747197453\n",
      "The 27228 th iteration gives loss of 0.16624990057895958\n",
      "The 27229 th iteration gives loss of 0.16624744387221776\n",
      "The 27230 th iteration gives loss of 0.16624498735174714\n",
      "The 27231 th iteration gives loss of 0.16624253101748926\n",
      "The 27232 th iteration gives loss of 0.16624007486942574\n",
      "The 27233 th iteration gives loss of 0.1662376189075441\n",
      "The 27234 th iteration gives loss of 0.166235163131797\n",
      "The 27235 th iteration gives loss of 0.16623270754216155\n",
      "The 27236 th iteration gives loss of 0.16623025213862108\n",
      "The 27237 th iteration gives loss of 0.16622779692113918\n",
      "The 27238 th iteration gives loss of 0.16622534188968036\n",
      "The 27239 th iteration gives loss of 0.16622288704422866\n",
      "The 27240 th iteration gives loss of 0.1662204323847524\n",
      "The 27241 th iteration gives loss of 0.16621797791121445\n",
      "The 27242 th iteration gives loss of 0.16621552362360548\n",
      "The 27243 th iteration gives loss of 0.16621306952188214\n",
      "The 27244 th iteration gives loss of 0.16621061560602782\n",
      "The 27245 th iteration gives loss of 0.16620816187600876\n",
      "The 27246 th iteration gives loss of 0.16620570833179718\n",
      "The 27247 th iteration gives loss of 0.16620325497336316\n",
      "The 27248 th iteration gives loss of 0.16620080180067626\n",
      "The 27249 th iteration gives loss of 0.16619834881372766\n",
      "The 27250 th iteration gives loss of 0.16619589601245977\n",
      "The 27251 th iteration gives loss of 0.16619344339687084\n",
      "The 27252 th iteration gives loss of 0.1661909909669142\n",
      "The 27253 th iteration gives loss of 0.16618853872257286\n",
      "The 27254 th iteration gives loss of 0.1661860866638232\n",
      "The 27255 th iteration gives loss of 0.16618363479062004\n",
      "The 27256 th iteration gives loss of 0.16618118310295915\n",
      "The 27257 th iteration gives loss of 0.16617873160078678\n",
      "The 27258 th iteration gives loss of 0.16617628028409204\n",
      "The 27259 th iteration gives loss of 0.16617382915284323\n",
      "The 27260 th iteration gives loss of 0.16617137820701616\n",
      "The 27261 th iteration gives loss of 0.16616892744657308\n",
      "The 27262 th iteration gives loss of 0.16616647687149486\n",
      "The 27263 th iteration gives loss of 0.1661640264817553\n",
      "The 27264 th iteration gives loss of 0.1661615762773182\n",
      "The 27265 th iteration gives loss of 0.1661591262581667\n",
      "The 27266 th iteration gives loss of 0.16615667642425982\n",
      "The 27267 th iteration gives loss of 0.16615422677557795\n",
      "The 27268 th iteration gives loss of 0.16615177731209946\n",
      "The 27269 th iteration gives loss of 0.1661493280337762\n",
      "The 27270 th iteration gives loss of 0.16614687894060065\n",
      "The 27271 th iteration gives loss of 0.16614443003253795\n",
      "The 27272 th iteration gives loss of 0.16614198130955743\n",
      "The 27273 th iteration gives loss of 0.1661395327716348\n",
      "The 27274 th iteration gives loss of 0.1661370844187378\n",
      "The 27275 th iteration gives loss of 0.16613463625085106\n",
      "The 27276 th iteration gives loss of 0.1661321882679302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 27277 th iteration gives loss of 0.16612974046996007\n",
      "The 27278 th iteration gives loss of 0.1661272928569137\n",
      "The 27279 th iteration gives loss of 0.16612484542874456\n",
      "The 27280 th iteration gives loss of 0.16612239818543972\n",
      "The 27281 th iteration gives loss of 0.1661199511269822\n",
      "The 27282 th iteration gives loss of 0.16611750425332772\n",
      "The 27283 th iteration gives loss of 0.16611505756444506\n",
      "The 27284 th iteration gives loss of 0.1661126110603233\n",
      "The 27285 th iteration gives loss of 0.16611016474091714\n",
      "The 27286 th iteration gives loss of 0.16610771860621856\n",
      "The 27287 th iteration gives loss of 0.16610527265618244\n",
      "The 27288 th iteration gives loss of 0.16610282689079678\n",
      "The 27289 th iteration gives loss of 0.166100381310019\n",
      "The 27290 th iteration gives loss of 0.16609793591382147\n",
      "The 27291 th iteration gives loss of 0.16609549070219046\n",
      "The 27292 th iteration gives loss of 0.16609304567508015\n",
      "The 27293 th iteration gives loss of 0.16609060083248217\n",
      "The 27294 th iteration gives loss of 0.16608815617435965\n",
      "The 27295 th iteration gives loss of 0.16608571170068429\n",
      "The 27296 th iteration gives loss of 0.166083267411433\n",
      "The 27297 th iteration gives loss of 0.16608082330657178\n",
      "The 27298 th iteration gives loss of 0.16607837938607184\n",
      "The 27299 th iteration gives loss of 0.16607593564990705\n",
      "The 27300 th iteration gives loss of 0.1660734920980591\n",
      "The 27301 th iteration gives loss of 0.16607104873048248\n",
      "The 27302 th iteration gives loss of 0.16606860554716976\n",
      "The 27303 th iteration gives loss of 0.16606616254808387\n",
      "The 27304 th iteration gives loss of 0.1660637197331941\n",
      "The 27305 th iteration gives loss of 0.16606127710247212\n",
      "The 27306 th iteration gives loss of 0.16605883465589943\n",
      "The 27307 th iteration gives loss of 0.16605639239344577\n",
      "The 27308 th iteration gives loss of 0.16605395031507775\n",
      "The 27309 th iteration gives loss of 0.16605150842076785\n",
      "The 27310 th iteration gives loss of 0.1660490667104962\n",
      "The 27311 th iteration gives loss of 0.16604662518422486\n",
      "The 27312 th iteration gives loss of 0.16604418384193922\n",
      "The 27313 th iteration gives loss of 0.1660417426836031\n",
      "The 27314 th iteration gives loss of 0.16603930170919298\n",
      "The 27315 th iteration gives loss of 0.16603686091867403\n",
      "The 27316 th iteration gives loss of 0.16603442031202087\n",
      "The 27317 th iteration gives loss of 0.16603197988921395\n",
      "The 27318 th iteration gives loss of 0.16602953965021952\n",
      "The 27319 th iteration gives loss of 0.16602709959501383\n",
      "The 27320 th iteration gives loss of 0.16602465972355795\n",
      "The 27321 th iteration gives loss of 0.16602222003584188\n",
      "The 27322 th iteration gives loss of 0.16601978053181998\n",
      "The 27323 th iteration gives loss of 0.16601734121147876\n",
      "The 27324 th iteration gives loss of 0.16601490207478184\n",
      "The 27325 th iteration gives loss of 0.1660124631217105\n",
      "The 27326 th iteration gives loss of 0.16601002435223516\n",
      "The 27327 th iteration gives loss of 0.16600758576632144\n",
      "The 27328 th iteration gives loss of 0.1660051473639414\n",
      "The 27329 th iteration gives loss of 0.16600270914506993\n",
      "The 27330 th iteration gives loss of 0.16600027110968885\n",
      "The 27331 th iteration gives loss of 0.1659978332577642\n",
      "The 27332 th iteration gives loss of 0.16599539558926021\n",
      "The 27333 th iteration gives loss of 0.16599295810416215\n",
      "The 27334 th iteration gives loss of 0.16599052080243715\n",
      "The 27335 th iteration gives loss of 0.16598808368405338\n",
      "The 27336 th iteration gives loss of 0.16598564674899094\n",
      "The 27337 th iteration gives loss of 0.1659832099972227\n",
      "The 27338 th iteration gives loss of 0.16598077342871087\n",
      "The 27339 th iteration gives loss of 0.16597833704344112\n",
      "The 27340 th iteration gives loss of 0.16597590084137787\n",
      "The 27341 th iteration gives loss of 0.1659734648224974\n",
      "The 27342 th iteration gives loss of 0.1659710289867675\n",
      "The 27343 th iteration gives loss of 0.1659685933341642\n",
      "The 27344 th iteration gives loss of 0.16596615786465513\n",
      "The 27345 th iteration gives loss of 0.1659637225782216\n",
      "The 27346 th iteration gives loss of 0.16596128747483188\n",
      "The 27347 th iteration gives loss of 0.16595885255445422\n",
      "The 27348 th iteration gives loss of 0.16595641781707934\n",
      "The 27349 th iteration gives loss of 0.165953983262659\n",
      "The 27350 th iteration gives loss of 0.1659515488911675\n",
      "The 27351 th iteration gives loss of 0.16594911470258855\n",
      "The 27352 th iteration gives loss of 0.16594668069687707\n",
      "The 27353 th iteration gives loss of 0.16594424687403717\n",
      "The 27354 th iteration gives loss of 0.1659418132340075\n",
      "The 27355 th iteration gives loss of 0.1659393797767728\n",
      "The 27356 th iteration gives loss of 0.1659369465023107\n",
      "The 27357 th iteration gives loss of 0.16593451341060086\n",
      "The 27358 th iteration gives loss of 0.16593208050159652\n",
      "The 27359 th iteration gives loss of 0.16592964777527905\n",
      "The 27360 th iteration gives loss of 0.16592721523162743\n",
      "The 27361 th iteration gives loss of 0.16592478287061063\n",
      "The 27362 th iteration gives loss of 0.16592235069219322\n",
      "The 27363 th iteration gives loss of 0.16591991869635989\n",
      "The 27364 th iteration gives loss of 0.16591748688307165\n",
      "The 27365 th iteration gives loss of 0.16591505525231315\n",
      "The 27366 th iteration gives loss of 0.16591262380404145\n",
      "The 27367 th iteration gives loss of 0.1659101925382473\n",
      "The 27368 th iteration gives loss of 0.16590776145489664\n",
      "The 27369 th iteration gives loss of 0.1659053305539444\n",
      "The 27370 th iteration gives loss of 0.16590289983539264\n",
      "The 27371 th iteration gives loss of 0.16590046929919186\n",
      "The 27372 th iteration gives loss of 0.1658980389453358\n",
      "The 27373 th iteration gives loss of 0.16589560877378154\n",
      "The 27374 th iteration gives loss of 0.16589317878449586\n",
      "The 27375 th iteration gives loss of 0.16589074897746742\n",
      "The 27376 th iteration gives loss of 0.1658883193526618\n",
      "The 27377 th iteration gives loss of 0.16588588991005512\n",
      "The 27378 th iteration gives loss of 0.16588346064961315\n",
      "The 27379 th iteration gives loss of 0.1658810315713134\n",
      "The 27380 th iteration gives loss of 0.16587860267512697\n",
      "The 27381 th iteration gives loss of 0.1658761739610289\n",
      "The 27382 th iteration gives loss of 0.1658737454289876\n",
      "The 27383 th iteration gives loss of 0.1658713170789833\n",
      "The 27384 th iteration gives loss of 0.165868888910973\n",
      "The 27385 th iteration gives loss of 0.16586646092494461\n",
      "The 27386 th iteration gives loss of 0.16586403312086986\n",
      "The 27387 th iteration gives loss of 0.1658616054987173\n",
      "The 27388 th iteration gives loss of 0.1658591780584663\n",
      "The 27389 th iteration gives loss of 0.16585675080007622\n",
      "The 27390 th iteration gives loss of 0.1658543237235279\n",
      "The 27391 th iteration gives loss of 0.16585189682879753\n",
      "The 27392 th iteration gives loss of 0.16584947011584933\n",
      "The 27393 th iteration gives loss of 0.16584704358466656\n",
      "The 27394 th iteration gives loss of 0.16584461723521846\n",
      "The 27395 th iteration gives loss of 0.16584219106746825\n",
      "The 27396 th iteration gives loss of 0.16583976508140535\n",
      "The 27397 th iteration gives loss of 0.1658373392769833\n",
      "The 27398 th iteration gives loss of 0.1658349136541869\n",
      "The 27399 th iteration gives loss of 0.1658324882129926\n",
      "The 27400 th iteration gives loss of 0.1658300629533635\n",
      "The 27401 th iteration gives loss of 0.16582763787527963\n",
      "The 27402 th iteration gives loss of 0.16582521297870836\n",
      "The 27403 th iteration gives loss of 0.16582278826362318\n",
      "The 27404 th iteration gives loss of 0.16582036373000672\n",
      "The 27405 th iteration gives loss of 0.16581793937781628\n",
      "The 27406 th iteration gives loss of 0.16581551520703416\n",
      "The 27407 th iteration gives loss of 0.16581309121763363\n",
      "The 27408 th iteration gives loss of 0.16581066740958236\n",
      "The 27409 th iteration gives loss of 0.16580824378285758\n",
      "The 27410 th iteration gives loss of 0.1658058203374218\n",
      "The 27411 th iteration gives loss of 0.16580339707326208\n",
      "The 27412 th iteration gives loss of 0.16580097399035312\n",
      "The 27413 th iteration gives loss of 0.16579855108865865\n",
      "The 27414 th iteration gives loss of 0.16579612836815003\n",
      "The 27415 th iteration gives loss of 0.16579370582879632\n",
      "The 27416 th iteration gives loss of 0.1657912834705943\n",
      "The 27417 th iteration gives loss of 0.16578886129348577\n",
      "The 27418 th iteration gives loss of 0.16578643929746556\n",
      "The 27419 th iteration gives loss of 0.1657840174824935\n",
      "The 27420 th iteration gives loss of 0.16578159584855512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 27421 th iteration gives loss of 0.16577917439560913\n",
      "The 27422 th iteration gives loss of 0.16577675312363355\n",
      "The 27423 th iteration gives loss of 0.16577433203261296\n",
      "The 27424 th iteration gives loss of 0.16577191112250345\n",
      "The 27425 th iteration gives loss of 0.16576949039328273\n",
      "The 27426 th iteration gives loss of 0.16576706984493325\n",
      "The 27427 th iteration gives loss of 0.16576464947741384\n",
      "The 27428 th iteration gives loss of 0.16576222929070838\n",
      "The 27429 th iteration gives loss of 0.16575980928478856\n",
      "The 27430 th iteration gives loss of 0.16575738945962004\n",
      "The 27431 th iteration gives loss of 0.16575496981518195\n",
      "The 27432 th iteration gives loss of 0.16575255035144595\n",
      "The 27433 th iteration gives loss of 0.16575013106838454\n",
      "The 27434 th iteration gives loss of 0.16574771196596916\n",
      "The 27435 th iteration gives loss of 0.16574529304417662\n",
      "The 27436 th iteration gives loss of 0.16574287430297546\n",
      "The 27437 th iteration gives loss of 0.16574045574234025\n",
      "The 27438 th iteration gives loss of 0.16573803736224882\n",
      "The 27439 th iteration gives loss of 0.16573561916266108\n",
      "The 27440 th iteration gives loss of 0.16573320114356416\n",
      "The 27441 th iteration gives loss of 0.1657307833049262\n",
      "The 27442 th iteration gives loss of 0.1657283656467186\n",
      "The 27443 th iteration gives loss of 0.1657259481689179\n",
      "The 27444 th iteration gives loss of 0.16572353087149072\n",
      "The 27445 th iteration gives loss of 0.16572111375441967\n",
      "The 27446 th iteration gives loss of 0.16571869681766413\n",
      "The 27447 th iteration gives loss of 0.16571628006120848\n",
      "The 27448 th iteration gives loss of 0.16571386348502218\n",
      "The 27449 th iteration gives loss of 0.16571144708908203\n",
      "The 27450 th iteration gives loss of 0.16570903087335753\n",
      "The 27451 th iteration gives loss of 0.1657066148378102\n",
      "The 27452 th iteration gives loss of 0.1657041989824358\n",
      "The 27453 th iteration gives loss of 0.16570178330719054\n",
      "The 27454 th iteration gives loss of 0.1656993678120576\n",
      "The 27455 th iteration gives loss of 0.1656969524969958\n",
      "The 27456 th iteration gives loss of 0.16569453736200015\n",
      "The 27457 th iteration gives loss of 0.16569212240702202\n",
      "The 27458 th iteration gives loss of 0.16568970763204652\n",
      "The 27459 th iteration gives loss of 0.16568729303704283\n",
      "The 27460 th iteration gives loss of 0.16568487862198375\n",
      "The 27461 th iteration gives loss of 0.1656824643868475\n",
      "The 27462 th iteration gives loss of 0.16568005033161132\n",
      "The 27463 th iteration gives loss of 0.1656776364562257\n",
      "The 27464 th iteration gives loss of 0.1656752227606851\n",
      "The 27465 th iteration gives loss of 0.16567280924495587\n",
      "The 27466 th iteration gives loss of 0.16567039590901414\n",
      "The 27467 th iteration gives loss of 0.16566798275282255\n",
      "The 27468 th iteration gives loss of 0.1656655697763624\n",
      "The 27469 th iteration gives loss of 0.16566315697961204\n",
      "The 27470 th iteration gives loss of 0.1656607443625357\n",
      "The 27471 th iteration gives loss of 0.16565833192510682\n",
      "The 27472 th iteration gives loss of 0.16565591966730325\n",
      "The 27473 th iteration gives loss of 0.16565350758909111\n",
      "The 27474 th iteration gives loss of 0.16565109569045225\n",
      "The 27475 th iteration gives loss of 0.1656486839713622\n",
      "The 27476 th iteration gives loss of 0.16564627243177688\n",
      "The 27477 th iteration gives loss of 0.16564386107168141\n",
      "The 27478 th iteration gives loss of 0.16564144989106117\n",
      "The 27479 th iteration gives loss of 0.16563903888986795\n",
      "The 27480 th iteration gives loss of 0.1656366280680802\n",
      "The 27481 th iteration gives loss of 0.16563421742567205\n",
      "The 27482 th iteration gives loss of 0.1656318069626218\n",
      "The 27483 th iteration gives loss of 0.16562939667889984\n",
      "The 27484 th iteration gives loss of 0.1656269865744768\n",
      "The 27485 th iteration gives loss of 0.16562457664932767\n",
      "The 27486 th iteration gives loss of 0.16562216690341738\n",
      "The 27487 th iteration gives loss of 0.1656197573367479\n",
      "The 27488 th iteration gives loss of 0.16561734794926541\n",
      "The 27489 th iteration gives loss of 0.16561493874094393\n",
      "The 27490 th iteration gives loss of 0.16561252971176532\n",
      "The 27491 th iteration gives loss of 0.1656101208616983\n",
      "The 27492 th iteration gives loss of 0.16560771219072093\n",
      "The 27493 th iteration gives loss of 0.16560530369879883\n",
      "The 27494 th iteration gives loss of 0.1656028953859175\n",
      "The 27495 th iteration gives loss of 0.1656004872520237\n",
      "The 27496 th iteration gives loss of 0.16559807929712483\n",
      "The 27497 th iteration gives loss of 0.16559567152116758\n",
      "The 27498 th iteration gives loss of 0.16559326392414764\n",
      "The 27499 th iteration gives loss of 0.16559085650602906\n",
      "The 27500 th iteration gives loss of 0.16558844926677202\n",
      "The 27501 th iteration gives loss of 0.16558604220636627\n",
      "The 27502 th iteration gives loss of 0.16558363532477346\n",
      "The 27503 th iteration gives loss of 0.16558122862198482\n",
      "The 27504 th iteration gives loss of 0.16557882209795322\n",
      "The 27505 th iteration gives loss of 0.16557641575265636\n",
      "The 27506 th iteration gives loss of 0.16557400958607044\n",
      "The 27507 th iteration gives loss of 0.16557160359817005\n",
      "The 27508 th iteration gives loss of 0.16556919778893606\n",
      "The 27509 th iteration gives loss of 0.16556679215832487\n",
      "The 27510 th iteration gives loss of 0.16556438670632667\n",
      "The 27511 th iteration gives loss of 0.16556198143290027\n",
      "The 27512 th iteration gives loss of 0.16555957633803273\n",
      "The 27513 th iteration gives loss of 0.16555717142167584\n",
      "The 27514 th iteration gives loss of 0.1655547666838238\n",
      "The 27515 th iteration gives loss of 0.1655523621244556\n",
      "The 27516 th iteration gives loss of 0.16554995774352085\n",
      "The 27517 th iteration gives loss of 0.16554755354100822\n",
      "The 27518 th iteration gives loss of 0.16554514951687418\n",
      "The 27519 th iteration gives loss of 0.1655427456711226\n",
      "The 27520 th iteration gives loss of 0.16554034200370346\n",
      "The 27521 th iteration gives loss of 0.16553793851458895\n",
      "The 27522 th iteration gives loss of 0.16553553520375336\n",
      "The 27523 th iteration gives loss of 0.16553313207118217\n",
      "The 27524 th iteration gives loss of 0.16553072911684844\n",
      "The 27525 th iteration gives loss of 0.16552832634071782\n",
      "The 27526 th iteration gives loss of 0.165525923742763\n",
      "The 27527 th iteration gives loss of 0.16552352132295506\n",
      "The 27528 th iteration gives loss of 0.1655211190812806\n",
      "The 27529 th iteration gives loss of 0.1655187170177079\n",
      "The 27530 th iteration gives loss of 0.16551631513219686\n",
      "The 27531 th iteration gives loss of 0.16551391342472102\n",
      "The 27532 th iteration gives loss of 0.1655115118952749\n",
      "The 27533 th iteration gives loss of 0.1655091105438127\n",
      "The 27534 th iteration gives loss of 0.16550670937032633\n",
      "The 27535 th iteration gives loss of 0.1655043083747752\n",
      "The 27536 th iteration gives loss of 0.16550190755713384\n",
      "The 27537 th iteration gives loss of 0.16549950691736995\n",
      "The 27538 th iteration gives loss of 0.16549710645547697\n",
      "The 27539 th iteration gives loss of 0.16549470617141063\n",
      "The 27540 th iteration gives loss of 0.16549230606513943\n",
      "The 27541 th iteration gives loss of 0.16548990613665931\n",
      "The 27542 th iteration gives loss of 0.16548750638593954\n",
      "The 27543 th iteration gives loss of 0.1654851068129383\n",
      "The 27544 th iteration gives loss of 0.1654827074176242\n",
      "The 27545 th iteration gives loss of 0.16548030819999485\n",
      "The 27546 th iteration gives loss of 0.16547790916000515\n",
      "The 27547 th iteration gives loss of 0.16547551029764138\n",
      "The 27548 th iteration gives loss of 0.16547311161285944\n",
      "The 27549 th iteration gives loss of 0.1654707131056491\n",
      "The 27550 th iteration gives loss of 0.16546831477597715\n",
      "The 27551 th iteration gives loss of 0.16546591662382398\n",
      "The 27552 th iteration gives loss of 0.16546351864915496\n",
      "The 27553 th iteration gives loss of 0.1654611208519464\n",
      "The 27554 th iteration gives loss of 0.16545872323216956\n",
      "The 27555 th iteration gives loss of 0.1654563257897968\n",
      "The 27556 th iteration gives loss of 0.1654539285248037\n",
      "The 27557 th iteration gives loss of 0.16545153143716812\n",
      "The 27558 th iteration gives loss of 0.16544913452685747\n",
      "The 27559 th iteration gives loss of 0.1654467377938526\n",
      "The 27560 th iteration gives loss of 0.16544434123811425\n",
      "The 27561 th iteration gives loss of 0.16544194485963423\n",
      "The 27562 th iteration gives loss of 0.16543954865836474\n",
      "The 27563 th iteration gives loss of 0.1654371526342939\n",
      "The 27564 th iteration gives loss of 0.16543475678739153\n",
      "The 27565 th iteration gives loss of 0.16543236111762968\n",
      "The 27566 th iteration gives loss of 0.165429965624983\n",
      "The 27567 th iteration gives loss of 0.16542757030942823\n",
      "The 27568 th iteration gives loss of 0.16542517517093552\n",
      "The 27569 th iteration gives loss of 0.1654227802094724\n",
      "The 27570 th iteration gives loss of 0.16542038542503115\n",
      "The 27571 th iteration gives loss of 0.16541799081755867\n",
      "The 27572 th iteration gives loss of 0.16541559638705924\n",
      "The 27573 th iteration gives loss of 0.16541320213347668\n",
      "The 27574 th iteration gives loss of 0.1654108080567977\n",
      "The 27575 th iteration gives loss of 0.1654084141569999\n",
      "The 27576 th iteration gives loss of 0.16540602043406086\n",
      "The 27577 th iteration gives loss of 0.16540362688793467\n",
      "The 27578 th iteration gives loss of 0.1654012335186062\n",
      "The 27579 th iteration gives loss of 0.16539884032605487\n",
      "The 27580 th iteration gives loss of 0.16539644731024358\n",
      "The 27581 th iteration gives loss of 0.16539405447115102\n",
      "The 27582 th iteration gives loss of 0.16539166180875065\n",
      "The 27583 th iteration gives loss of 0.16538926932301762\n",
      "The 27584 th iteration gives loss of 0.16538687701392993\n",
      "The 27585 th iteration gives loss of 0.16538448488144858\n",
      "The 27586 th iteration gives loss of 0.16538209292555547\n",
      "The 27587 th iteration gives loss of 0.16537970114622585\n",
      "The 27588 th iteration gives loss of 0.1653773095434239\n",
      "The 27589 th iteration gives loss of 0.16537491811713007\n",
      "The 27590 th iteration gives loss of 0.16537252686731646\n",
      "The 27591 th iteration gives loss of 0.16537013579396617\n",
      "The 27592 th iteration gives loss of 0.1653677448970313\n",
      "The 27593 th iteration gives loss of 0.1653653541765044\n",
      "The 27594 th iteration gives loss of 0.16536296363236072\n",
      "The 27595 th iteration gives loss of 0.16536057326455686\n",
      "The 27596 th iteration gives loss of 0.1653581830730765\n",
      "The 27597 th iteration gives loss of 0.16535579305788636\n",
      "The 27598 th iteration gives loss of 0.1653534032189762\n",
      "The 27599 th iteration gives loss of 0.1653510135563084\n",
      "The 27600 th iteration gives loss of 0.16534862406985962\n",
      "The 27601 th iteration gives loss of 0.1653462347595951\n",
      "The 27602 th iteration gives loss of 0.1653438456255\n",
      "The 27603 th iteration gives loss of 0.16534145666753736\n",
      "The 27604 th iteration gives loss of 0.16533906788569724\n",
      "The 27605 th iteration gives loss of 0.16533667927993534\n",
      "The 27606 th iteration gives loss of 0.16533429085024193\n",
      "The 27607 th iteration gives loss of 0.16533190259656896\n",
      "The 27608 th iteration gives loss of 0.16532951451891267\n",
      "The 27609 th iteration gives loss of 0.16532712661722848\n",
      "The 27610 th iteration gives loss of 0.16532473889150376\n",
      "The 27611 th iteration gives loss of 0.165322351341707\n",
      "The 27612 th iteration gives loss of 0.16531996396781035\n",
      "The 27613 th iteration gives loss of 0.16531757676978942\n",
      "The 27614 th iteration gives loss of 0.16531518974762033\n",
      "The 27615 th iteration gives loss of 0.16531280290127118\n",
      "The 27616 th iteration gives loss of 0.1653104162307147\n",
      "The 27617 th iteration gives loss of 0.1653080297359376\n",
      "The 27618 th iteration gives loss of 0.16530564341690077\n",
      "The 27619 th iteration gives loss of 0.16530325727358197\n",
      "The 27620 th iteration gives loss of 0.16530087130595345\n",
      "The 27621 th iteration gives loss of 0.16529848551398937\n",
      "The 27622 th iteration gives loss of 0.16529609989765762\n",
      "The 27623 th iteration gives loss of 0.16529371445695304\n",
      "The 27624 th iteration gives loss of 0.1652913291918249\n",
      "The 27625 th iteration gives loss of 0.16528894410226305\n",
      "The 27626 th iteration gives loss of 0.16528655918823035\n",
      "The 27627 th iteration gives loss of 0.16528417444970764\n",
      "The 27628 th iteration gives loss of 0.16528178988666956\n",
      "The 27629 th iteration gives loss of 0.16527940549907832\n",
      "The 27630 th iteration gives loss of 0.16527702128692\n",
      "The 27631 th iteration gives loss of 0.16527463725017497\n",
      "The 27632 th iteration gives loss of 0.16527225338879936\n",
      "The 27633 th iteration gives loss of 0.16526986970276\n",
      "The 27634 th iteration gives loss of 0.16526748619206388\n",
      "The 27635 th iteration gives loss of 0.16526510285665608\n",
      "The 27636 th iteration gives loss of 0.1652627196965197\n",
      "The 27637 th iteration gives loss of 0.1652603367116359\n",
      "The 27638 th iteration gives loss of 0.1652579539019744\n",
      "The 27639 th iteration gives loss of 0.1652555712674996\n",
      "The 27640 th iteration gives loss of 0.16525318880819334\n",
      "The 27641 th iteration gives loss of 0.16525080652402604\n",
      "The 27642 th iteration gives loss of 0.1652484244149731\n",
      "The 27643 th iteration gives loss of 0.16524604248100724\n",
      "The 27644 th iteration gives loss of 0.16524366072211197\n",
      "The 27645 th iteration gives loss of 0.16524127913824385\n",
      "The 27646 th iteration gives loss of 0.16523889772939998\n",
      "The 27647 th iteration gives loss of 0.1652365164955262\n",
      "The 27648 th iteration gives loss of 0.16523413543661528\n",
      "The 27649 th iteration gives loss of 0.16523175455263936\n",
      "The 27650 th iteration gives loss of 0.16522937384357025\n",
      "The 27651 th iteration gives loss of 0.16522699330937882\n",
      "The 27652 th iteration gives loss of 0.1652246129500294\n",
      "The 27653 th iteration gives loss of 0.16522223276552048\n",
      "The 27654 th iteration gives loss of 0.1652198527558064\n",
      "The 27655 th iteration gives loss of 0.16521747292087202\n",
      "The 27656 th iteration gives loss of 0.16521509326068629\n",
      "The 27657 th iteration gives loss of 0.16521271377522148\n",
      "The 27658 th iteration gives loss of 0.1652103344644519\n",
      "The 27659 th iteration gives loss of 0.16520795532835814\n",
      "The 27660 th iteration gives loss of 0.16520557636690333\n",
      "The 27661 th iteration gives loss of 0.16520319758006535\n",
      "The 27662 th iteration gives loss of 0.16520081896782518\n",
      "The 27663 th iteration gives loss of 0.16519844053014812\n",
      "The 27664 th iteration gives loss of 0.1651960622670192\n",
      "The 27665 th iteration gives loss of 0.16519368417840083\n",
      "The 27666 th iteration gives loss of 0.16519130626426323\n",
      "The 27667 th iteration gives loss of 0.16518892852459416\n",
      "The 27668 th iteration gives loss of 0.16518655095935464\n",
      "The 27669 th iteration gives loss of 0.16518417356853973\n",
      "The 27670 th iteration gives loss of 0.1651817963521019\n",
      "The 27671 th iteration gives loss of 0.16517941931001784\n",
      "The 27672 th iteration gives loss of 0.16517704244226586\n",
      "The 27673 th iteration gives loss of 0.1651746657488161\n",
      "The 27674 th iteration gives loss of 0.16517228922965413\n",
      "The 27675 th iteration gives loss of 0.1651699128847478\n",
      "The 27676 th iteration gives loss of 0.16516753671406467\n",
      "The 27677 th iteration gives loss of 0.16516516071758094\n",
      "The 27678 th iteration gives loss of 0.16516278489526648\n",
      "The 27679 th iteration gives loss of 0.16516040924711625\n",
      "The 27680 th iteration gives loss of 0.16515803377307953\n",
      "The 27681 th iteration gives loss of 0.16515565847314792\n",
      "The 27682 th iteration gives loss of 0.16515328334728552\n",
      "The 27683 th iteration gives loss of 0.16515090839546348\n",
      "The 27684 th iteration gives loss of 0.16514853361766726\n",
      "The 27685 th iteration gives loss of 0.16514615901386698\n",
      "The 27686 th iteration gives loss of 0.1651437845840285\n",
      "The 27687 th iteration gives loss of 0.1651414103281289\n",
      "The 27688 th iteration gives loss of 0.16513903624615303\n",
      "The 27689 th iteration gives loss of 0.16513666233806265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 27690 th iteration gives loss of 0.16513428860383858\n",
      "The 27691 th iteration gives loss of 0.1651319150434474\n",
      "The 27692 th iteration gives loss of 0.1651295416568739\n",
      "The 27693 th iteration gives loss of 0.1651271684440813\n",
      "The 27694 th iteration gives loss of 0.16512479540505579\n",
      "The 27695 th iteration gives loss of 0.16512242253976211\n",
      "The 27696 th iteration gives loss of 0.1651200498481707\n",
      "The 27697 th iteration gives loss of 0.1651176773302631\n",
      "The 27698 th iteration gives loss of 0.16511530498601495\n",
      "The 27699 th iteration gives loss of 0.16511293281539902\n",
      "The 27700 th iteration gives loss of 0.16511056081838196\n",
      "The 27701 th iteration gives loss of 0.1651081889949446\n",
      "The 27702 th iteration gives loss of 0.16510581734506033\n",
      "The 27703 th iteration gives loss of 0.16510344586870448\n",
      "The 27704 th iteration gives loss of 0.1651010745658476\n",
      "The 27705 th iteration gives loss of 0.16509870343646993\n",
      "The 27706 th iteration gives loss of 0.16509633248053635\n",
      "The 27707 th iteration gives loss of 0.16509396169802754\n",
      "The 27708 th iteration gives loss of 0.16509159108891375\n",
      "The 27709 th iteration gives loss of 0.16508922065317255\n",
      "The 27710 th iteration gives loss of 0.1650868503907795\n",
      "The 27711 th iteration gives loss of 0.16508448030170272\n",
      "The 27712 th iteration gives loss of 0.16508211038591547\n",
      "The 27713 th iteration gives loss of 0.1650797406434014\n",
      "The 27714 th iteration gives loss of 0.16507737107412376\n",
      "The 27715 th iteration gives loss of 0.16507500167807607\n",
      "The 27716 th iteration gives loss of 0.165072632455203\n",
      "The 27717 th iteration gives loss of 0.16507026340550207\n",
      "The 27718 th iteration gives loss of 0.16506789452893894\n",
      "The 27719 th iteration gives loss of 0.16506552582548747\n",
      "The 27720 th iteration gives loss of 0.1650631572951256\n",
      "The 27721 th iteration gives loss of 0.16506078893781828\n",
      "The 27722 th iteration gives loss of 0.1650584207535516\n",
      "The 27723 th iteration gives loss of 0.16505605274228297\n",
      "The 27724 th iteration gives loss of 0.16505368490400488\n",
      "The 27725 th iteration gives loss of 0.16505131723868682\n",
      "The 27726 th iteration gives loss of 0.165048949746299\n",
      "The 27727 th iteration gives loss of 0.1650465824268153\n",
      "The 27728 th iteration gives loss of 0.16504421528021473\n",
      "The 27729 th iteration gives loss of 0.16504184830646823\n",
      "The 27730 th iteration gives loss of 0.16503948150554984\n",
      "The 27731 th iteration gives loss of 0.1650371148774365\n",
      "The 27732 th iteration gives loss of 0.16503474842210528\n",
      "The 27733 th iteration gives loss of 0.16503238213951416\n",
      "The 27734 th iteration gives loss of 0.16503001602964945\n",
      "The 27735 th iteration gives loss of 0.16502765009248788\n",
      "The 27736 th iteration gives loss of 0.16502528432799776\n",
      "The 27737 th iteration gives loss of 0.16502291873615713\n",
      "The 27738 th iteration gives loss of 0.16502055331693938\n",
      "The 27739 th iteration gives loss of 0.16501818807031532\n",
      "The 27740 th iteration gives loss of 0.16501582299626766\n",
      "The 27741 th iteration gives loss of 0.1650134580947573\n",
      "The 27742 th iteration gives loss of 0.16501109336577058\n",
      "The 27743 th iteration gives loss of 0.16500872880927767\n",
      "The 27744 th iteration gives loss of 0.16500636442525013\n",
      "The 27745 th iteration gives loss of 0.1650040002136658\n",
      "The 27746 th iteration gives loss of 0.16500163617450242\n",
      "The 27747 th iteration gives loss of 0.16499927230772155\n",
      "The 27748 th iteration gives loss of 0.16499690861330665\n",
      "The 27749 th iteration gives loss of 0.16499454509123387\n",
      "The 27750 th iteration gives loss of 0.16499218174147381\n",
      "The 27751 th iteration gives loss of 0.1649898185639978\n",
      "The 27752 th iteration gives loss of 0.1649874555587769\n",
      "The 27753 th iteration gives loss of 0.16498509272580658\n",
      "The 27754 th iteration gives loss of 0.16498273006504016\n",
      "The 27755 th iteration gives loss of 0.16498036757646983\n",
      "The 27756 th iteration gives loss of 0.1649780052600454\n",
      "The 27757 th iteration gives loss of 0.16497564311576235\n",
      "The 27758 th iteration gives loss of 0.16497328114358273\n",
      "The 27759 th iteration gives loss of 0.16497091934348165\n",
      "The 27760 th iteration gives loss of 0.16496855771544505\n",
      "The 27761 th iteration gives loss of 0.1649661962594338\n",
      "The 27762 th iteration gives loss of 0.16496383497543032\n",
      "The 27763 th iteration gives loss of 0.16496147386340068\n",
      "The 27764 th iteration gives loss of 0.16495911292333007\n",
      "The 27765 th iteration gives loss of 0.16495675215518493\n",
      "The 27766 th iteration gives loss of 0.16495439155894193\n",
      "The 27767 th iteration gives loss of 0.16495203113457502\n",
      "The 27768 th iteration gives loss of 0.16494967088205925\n",
      "The 27769 th iteration gives loss of 0.16494731080136704\n",
      "The 27770 th iteration gives loss of 0.16494495089248365\n",
      "The 27771 th iteration gives loss of 0.1649425911553671\n",
      "The 27772 th iteration gives loss of 0.16494023159000737\n",
      "The 27773 th iteration gives loss of 0.16493787219635478\n",
      "The 27774 th iteration gives loss of 0.1649355129744087\n",
      "The 27775 th iteration gives loss of 0.1649331539241354\n",
      "The 27776 th iteration gives loss of 0.16493079504550862\n",
      "The 27777 th iteration gives loss of 0.16492843633849963\n",
      "The 27778 th iteration gives loss of 0.16492607780308316\n",
      "The 27779 th iteration gives loss of 0.1649237194392426\n",
      "The 27780 th iteration gives loss of 0.1649213612469386\n",
      "The 27781 th iteration gives loss of 0.16491900322615163\n",
      "The 27782 th iteration gives loss of 0.16491664537685843\n",
      "The 27783 th iteration gives loss of 0.16491428769904115\n",
      "The 27784 th iteration gives loss of 0.16491193019265998\n",
      "The 27785 th iteration gives loss of 0.1649095728576913\n",
      "The 27786 th iteration gives loss of 0.16490721569411362\n",
      "The 27787 th iteration gives loss of 0.16490485870189933\n",
      "The 27788 th iteration gives loss of 0.1649025018810291\n",
      "The 27789 th iteration gives loss of 0.1649001452314684\n",
      "The 27790 th iteration gives loss of 0.16489778875319733\n",
      "The 27791 th iteration gives loss of 0.16489543244618984\n",
      "The 27792 th iteration gives loss of 0.16489307631041858\n",
      "The 27793 th iteration gives loss of 0.16489072034585492\n",
      "The 27794 th iteration gives loss of 0.16488836455247396\n",
      "The 27795 th iteration gives loss of 0.16488600893026106\n",
      "The 27796 th iteration gives loss of 0.16488365347917613\n",
      "The 27797 th iteration gives loss of 0.16488129819920577\n",
      "The 27798 th iteration gives loss of 0.16487894309032\n",
      "The 27799 th iteration gives loss of 0.16487658815248774\n",
      "The 27800 th iteration gives loss of 0.16487423338569046\n",
      "The 27801 th iteration gives loss of 0.16487187878989437\n",
      "The 27802 th iteration gives loss of 0.16486952436508265\n",
      "The 27803 th iteration gives loss of 0.1648671701112232\n",
      "The 27804 th iteration gives loss of 0.16486481602830452\n",
      "The 27805 th iteration gives loss of 0.16486246211628722\n",
      "The 27806 th iteration gives loss of 0.16486010837514578\n",
      "The 27807 th iteration gives loss of 0.16485775480485598\n",
      "The 27808 th iteration gives loss of 0.16485540140539792\n",
      "The 27809 th iteration gives loss of 0.1648530481767437\n",
      "The 27810 th iteration gives loss of 0.16485069511886655\n",
      "The 27811 th iteration gives loss of 0.1648483422317498\n",
      "The 27812 th iteration gives loss of 0.16484598951535753\n",
      "The 27813 th iteration gives loss of 0.16484363696965498\n",
      "The 27814 th iteration gives loss of 0.1648412845946255\n",
      "The 27815 th iteration gives loss of 0.16483893239025463\n",
      "The 27816 th iteration gives loss of 0.16483658035651427\n",
      "The 27817 th iteration gives loss of 0.16483422849337132\n",
      "The 27818 th iteration gives loss of 0.16483187680078737\n",
      "The 27819 th iteration gives loss of 0.16482952527876893\n",
      "The 27820 th iteration gives loss of 0.1648271739272598\n",
      "The 27821 th iteration gives loss of 0.1648248227462596\n",
      "The 27822 th iteration gives loss of 0.16482247173572606\n",
      "The 27823 th iteration gives loss of 0.1648201208956394\n",
      "The 27824 th iteration gives loss of 0.1648177702259732\n",
      "The 27825 th iteration gives loss of 0.16481541972670913\n",
      "The 27826 th iteration gives loss of 0.1648130693978043\n",
      "The 27827 th iteration gives loss of 0.1648107192392545\n",
      "The 27828 th iteration gives loss of 0.16480836925101738\n",
      "The 27829 th iteration gives loss of 0.1648060194330737\n",
      "The 27830 th iteration gives loss of 0.16480366978540242\n",
      "The 27831 th iteration gives loss of 0.16480132030797548\n",
      "The 27832 th iteration gives loss of 0.16479897100076854\n",
      "The 27833 th iteration gives loss of 0.16479662186375557\n",
      "The 27834 th iteration gives loss of 0.1647942728968991\n",
      "The 27835 th iteration gives loss of 0.16479192410019752\n",
      "The 27836 th iteration gives loss of 0.1647895754736055\n",
      "The 27837 th iteration gives loss of 0.16478722701709939\n",
      "The 27838 th iteration gives loss of 0.16478487873066594\n",
      "The 27839 th iteration gives loss of 0.1647825306142732\n",
      "The 27840 th iteration gives loss of 0.16478018266790118\n",
      "The 27841 th iteration gives loss of 0.16477783489151032\n",
      "The 27842 th iteration gives loss of 0.16477548728508273\n",
      "The 27843 th iteration gives loss of 0.1647731398485998\n",
      "The 27844 th iteration gives loss of 0.16477079258202462\n",
      "The 27845 th iteration gives loss of 0.16476844548533884\n",
      "The 27846 th iteration gives loss of 0.16476609855851418\n",
      "The 27847 th iteration gives loss of 0.16476375180153346\n",
      "The 27848 th iteration gives loss of 0.1647614052143614\n",
      "The 27849 th iteration gives loss of 0.164759058796988\n",
      "The 27850 th iteration gives loss of 0.1647567125493623\n",
      "The 27851 th iteration gives loss of 0.1647543664714799\n",
      "The 27852 th iteration gives loss of 0.16475202056330446\n",
      "The 27853 th iteration gives loss of 0.16474967482481173\n",
      "The 27854 th iteration gives loss of 0.16474732925598537\n",
      "The 27855 th iteration gives loss of 0.1647449838567888\n",
      "The 27856 th iteration gives loss of 0.16474263862721497\n",
      "The 27857 th iteration gives loss of 0.16474029356721254\n",
      "The 27858 th iteration gives loss of 0.16473794867677347\n",
      "The 27859 th iteration gives loss of 0.164735603955871\n",
      "The 27860 th iteration gives loss of 0.16473325940447361\n",
      "The 27861 th iteration gives loss of 0.16473091502256088\n",
      "The 27862 th iteration gives loss of 0.16472857081010478\n",
      "The 27863 th iteration gives loss of 0.16472622676708062\n",
      "The 27864 th iteration gives loss of 0.16472388289346582\n",
      "The 27865 th iteration gives loss of 0.16472153918923313\n",
      "The 27866 th iteration gives loss of 0.16471919565436346\n",
      "The 27867 th iteration gives loss of 0.16471685228882332\n",
      "The 27868 th iteration gives loss of 0.16471450909258714\n",
      "The 27869 th iteration gives loss of 0.16471216606563388\n",
      "The 27870 th iteration gives loss of 0.1647098232079338\n",
      "The 27871 th iteration gives loss of 0.16470748051946693\n",
      "The 27872 th iteration gives loss of 0.16470513800020672\n",
      "The 27873 th iteration gives loss of 0.16470279565012036\n",
      "The 27874 th iteration gives loss of 0.16470045346920106\n",
      "The 27875 th iteration gives loss of 0.1646981114574023\n",
      "The 27876 th iteration gives loss of 0.16469576961471025\n",
      "The 27877 th iteration gives loss of 0.16469342794110103\n",
      "The 27878 th iteration gives loss of 0.16469108643654964\n",
      "The 27879 th iteration gives loss of 0.16468874510102186\n",
      "The 27880 th iteration gives loss of 0.16468640393449507\n",
      "The 27881 th iteration gives loss of 0.16468406293695445\n",
      "The 27882 th iteration gives loss of 0.16468172210836468\n",
      "The 27883 th iteration gives loss of 0.16467938144870173\n",
      "The 27884 th iteration gives loss of 0.16467704095794486\n",
      "The 27885 th iteration gives loss of 0.16467470063606218\n",
      "The 27886 th iteration gives loss of 0.16467236048304124\n",
      "The 27887 th iteration gives loss of 0.1646700204988364\n",
      "The 27888 th iteration gives loss of 0.1646676806834422\n",
      "The 27889 th iteration gives loss of 0.1646653410368215\n",
      "The 27890 th iteration gives loss of 0.16466300155895824\n",
      "The 27891 th iteration gives loss of 0.164660662249821\n",
      "The 27892 th iteration gives loss of 0.16465832310938247\n",
      "The 27893 th iteration gives loss of 0.16465598413762184\n",
      "The 27894 th iteration gives loss of 0.1646536453345105\n",
      "The 27895 th iteration gives loss of 0.1646513067000343\n",
      "The 27896 th iteration gives loss of 0.16464896823415354\n",
      "The 27897 th iteration gives loss of 0.1646466299368491\n",
      "The 27898 th iteration gives loss of 0.1646442918080888\n",
      "The 27899 th iteration gives loss of 0.16464195384786282\n",
      "The 27900 th iteration gives loss of 0.16463961605614605\n",
      "The 27901 th iteration gives loss of 0.16463727843289588\n",
      "The 27902 th iteration gives loss of 0.1646349409780947\n",
      "The 27903 th iteration gives loss of 0.16463260369172222\n",
      "The 27904 th iteration gives loss of 0.1646302665737428\n",
      "The 27905 th iteration gives loss of 0.16462792962415215\n",
      "The 27906 th iteration gives loss of 0.16462559284290967\n",
      "The 27907 th iteration gives loss of 0.16462325622998555\n",
      "The 27908 th iteration gives loss of 0.16462091978535534\n",
      "The 27909 th iteration gives loss of 0.16461858350901173\n",
      "The 27910 th iteration gives loss of 0.16461624740091457\n",
      "The 27911 th iteration gives loss of 0.16461391146104698\n",
      "The 27912 th iteration gives loss of 0.16461157568938\n",
      "The 27913 th iteration gives loss of 0.16460924008588576\n",
      "The 27914 th iteration gives loss of 0.16460690465053351\n",
      "The 27915 th iteration gives loss of 0.16460456938331014\n",
      "The 27916 th iteration gives loss of 0.1646022342841859\n",
      "The 27917 th iteration gives loss of 0.16459989935313415\n",
      "The 27918 th iteration gives loss of 0.16459756459014108\n",
      "The 27919 th iteration gives loss of 0.16459522999516885\n",
      "The 27920 th iteration gives loss of 0.16459289556819426\n",
      "The 27921 th iteration gives loss of 0.16459056130919777\n",
      "The 27922 th iteration gives loss of 0.16458822721814934\n",
      "The 27923 th iteration gives loss of 0.16458589329502396\n",
      "The 27924 th iteration gives loss of 0.1645835595397921\n",
      "The 27925 th iteration gives loss of 0.1645812259524445\n",
      "The 27926 th iteration gives loss of 0.16457889253294813\n",
      "The 27927 th iteration gives loss of 0.16457655928126763\n",
      "The 27928 th iteration gives loss of 0.1645742261973832\n",
      "The 27929 th iteration gives loss of 0.16457189328128513\n",
      "The 27930 th iteration gives loss of 0.1645695605329239\n",
      "The 27931 th iteration gives loss of 0.16456722795229903\n",
      "The 27932 th iteration gives loss of 0.16456489553936485\n",
      "The 27933 th iteration gives loss of 0.16456256329410957\n",
      "The 27934 th iteration gives loss of 0.16456023121650812\n",
      "The 27935 th iteration gives loss of 0.16455789930652753\n",
      "The 27936 th iteration gives loss of 0.16455556756413928\n",
      "The 27937 th iteration gives loss of 0.16455323598933172\n",
      "The 27938 th iteration gives loss of 0.16455090458207355\n",
      "The 27939 th iteration gives loss of 0.1645485733423386\n",
      "The 27940 th iteration gives loss of 0.16454624227010123\n",
      "The 27941 th iteration gives loss of 0.16454391136534213\n",
      "The 27942 th iteration gives loss of 0.16454158062802818\n",
      "The 27943 th iteration gives loss of 0.16453925005814424\n",
      "The 27944 th iteration gives loss of 0.16453691965565556\n",
      "The 27945 th iteration gives loss of 0.16453458942054897\n",
      "The 27946 th iteration gives loss of 0.1645322593527901\n",
      "The 27947 th iteration gives loss of 0.16452992945235265\n",
      "The 27948 th iteration gives loss of 0.16452759971921724\n",
      "The 27949 th iteration gives loss of 0.16452527015336246\n",
      "The 27950 th iteration gives loss of 0.16452294075474305\n",
      "The 27951 th iteration gives loss of 0.16452061152336364\n",
      "The 27952 th iteration gives loss of 0.16451828245917083\n",
      "The 27953 th iteration gives loss of 0.16451595356216672\n",
      "The 27954 th iteration gives loss of 0.16451362483231718\n",
      "The 27955 th iteration gives loss of 0.16451129626958147\n",
      "The 27956 th iteration gives loss of 0.16450896787395006\n",
      "The 27957 th iteration gives loss of 0.1645066396453874\n",
      "The 27958 th iteration gives loss of 0.16450431158388323\n",
      "The 27959 th iteration gives loss of 0.1645019836894058\n",
      "The 27960 th iteration gives loss of 0.16449965596192206\n",
      "The 27961 th iteration gives loss of 0.1644973284014246\n",
      "The 27962 th iteration gives loss of 0.16449500100787534\n",
      "The 27963 th iteration gives loss of 0.16449267378125174\n",
      "The 27964 th iteration gives loss of 0.1644903467215308\n",
      "The 27965 th iteration gives loss of 0.16448801982868919\n",
      "The 27966 th iteration gives loss of 0.16448569310269712\n",
      "The 27967 th iteration gives loss of 0.16448336654353515\n",
      "The 27968 th iteration gives loss of 0.16448104015116702\n",
      "The 27969 th iteration gives loss of 0.16447871392558133\n",
      "The 27970 th iteration gives loss of 0.1644763878667467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 27971 th iteration gives loss of 0.1644740619746466\n",
      "The 27972 th iteration gives loss of 0.16447173624924732\n",
      "The 27973 th iteration gives loss of 0.16446941069052526\n",
      "The 27974 th iteration gives loss of 0.16446708529845155\n",
      "The 27975 th iteration gives loss of 0.16446476007301225\n",
      "The 27976 th iteration gives loss of 0.16446243501417396\n",
      "The 27977 th iteration gives loss of 0.16446011012192005\n",
      "The 27978 th iteration gives loss of 0.16445778539621708\n",
      "The 27979 th iteration gives loss of 0.1644554608370373\n",
      "The 27980 th iteration gives loss of 0.1644531364443718\n",
      "The 27981 th iteration gives loss of 0.16445081221818572\n",
      "The 27982 th iteration gives loss of 0.16444848815844773\n",
      "The 27983 th iteration gives loss of 0.16444616426513683\n",
      "The 27984 th iteration gives loss of 0.1644438405382435\n",
      "The 27985 th iteration gives loss of 0.1644415169777238\n",
      "The 27986 th iteration gives loss of 0.16443919358356002\n",
      "The 27987 th iteration gives loss of 0.16443687035572918\n",
      "The 27988 th iteration gives loss of 0.16443454729419912\n",
      "The 27989 th iteration gives loss of 0.16443222439895427\n",
      "The 27990 th iteration gives loss of 0.16442990166997007\n",
      "The 27991 th iteration gives loss of 0.16442757910721556\n",
      "The 27992 th iteration gives loss of 0.16442525671066843\n",
      "The 27993 th iteration gives loss of 0.16442293448030096\n",
      "The 27994 th iteration gives loss of 0.16442061241608866\n",
      "The 27995 th iteration gives loss of 0.16441829051801257\n",
      "The 27996 th iteration gives loss of 0.16441596878605316\n",
      "The 27997 th iteration gives loss of 0.16441364722016952\n",
      "The 27998 th iteration gives loss of 0.16441132582034274\n",
      "The 27999 th iteration gives loss of 0.1644090045865557\n",
      "The 28000 th iteration gives loss of 0.16440668351877363\n",
      "The 28001 th iteration gives loss of 0.16440436261698252\n",
      "The 28002 th iteration gives loss of 0.16440204188114577\n",
      "The 28003 th iteration gives loss of 0.16439972131124653\n",
      "The 28004 th iteration gives loss of 0.16439740090725116\n",
      "The 28005 th iteration gives loss of 0.16439508066915567\n",
      "The 28006 th iteration gives loss of 0.16439276059690522\n",
      "The 28007 th iteration gives loss of 0.16439044069049893\n",
      "The 28008 th iteration gives loss of 0.16438812094990454\n",
      "The 28009 th iteration gives loss of 0.16438580137510211\n",
      "The 28010 th iteration gives loss of 0.16438348196604624\n",
      "The 28011 th iteration gives loss of 0.16438116272274148\n",
      "The 28012 th iteration gives loss of 0.16437884364514746\n",
      "The 28013 th iteration gives loss of 0.1643765247332461\n",
      "The 28014 th iteration gives loss of 0.16437420598700295\n",
      "The 28015 th iteration gives loss of 0.1643718874063986\n",
      "The 28016 th iteration gives loss of 0.16436956899140825\n",
      "The 28017 th iteration gives loss of 0.16436725074200742\n",
      "The 28018 th iteration gives loss of 0.16436493265817542\n",
      "The 28019 th iteration gives loss of 0.1643626147398783\n",
      "The 28020 th iteration gives loss of 0.16436029698711\n",
      "The 28021 th iteration gives loss of 0.1643579793998181\n",
      "The 28022 th iteration gives loss of 0.16435566197800414\n",
      "The 28023 th iteration gives loss of 0.16435334472162144\n",
      "The 28024 th iteration gives loss of 0.16435102763065895\n",
      "The 28025 th iteration gives loss of 0.16434871070509885\n",
      "The 28026 th iteration gives loss of 0.16434639394489678\n",
      "The 28027 th iteration gives loss of 0.16434407735003675\n",
      "The 28028 th iteration gives loss of 0.16434176092049538\n",
      "The 28029 th iteration gives loss of 0.16433944465625158\n",
      "The 28030 th iteration gives loss of 0.16433712855727484\n",
      "The 28031 th iteration gives loss of 0.16433481262354996\n",
      "The 28032 th iteration gives loss of 0.16433249685504087\n",
      "The 28033 th iteration gives loss of 0.16433018125172608\n",
      "The 28034 th iteration gives loss of 0.1643278658135874\n",
      "The 28035 th iteration gives loss of 0.16432555054058978\n",
      "The 28036 th iteration gives loss of 0.1643232354327161\n",
      "The 28037 th iteration gives loss of 0.16432092048993885\n",
      "The 28038 th iteration gives loss of 0.16431860571223117\n",
      "The 28039 th iteration gives loss of 0.1643162910995804\n",
      "The 28040 th iteration gives loss of 0.1643139766519489\n",
      "The 28041 th iteration gives loss of 0.16431166236932107\n",
      "The 28042 th iteration gives loss of 0.16430934825166224\n",
      "The 28043 th iteration gives loss of 0.16430703429895904\n",
      "The 28044 th iteration gives loss of 0.16430472051117764\n",
      "The 28045 th iteration gives loss of 0.16430240688829895\n",
      "The 28046 th iteration gives loss of 0.16430009343029048\n",
      "The 28047 th iteration gives loss of 0.16429778013713728\n",
      "The 28048 th iteration gives loss of 0.16429546700881606\n",
      "The 28049 th iteration gives loss of 0.164293154045294\n",
      "The 28050 th iteration gives loss of 0.16429084124654966\n",
      "The 28051 th iteration gives loss of 0.16428852861255985\n",
      "The 28052 th iteration gives loss of 0.16428621614329694\n",
      "The 28053 th iteration gives loss of 0.16428390383875063\n",
      "The 28054 th iteration gives loss of 0.16428159169886894\n",
      "The 28055 th iteration gives loss of 0.16427927972364853\n",
      "The 28056 th iteration gives loss of 0.16427696791306903\n",
      "The 28057 th iteration gives loss of 0.16427465626708704\n",
      "The 28058 th iteration gives loss of 0.1642723447856847\n",
      "The 28059 th iteration gives loss of 0.16427003346884436\n",
      "The 28060 th iteration gives loss of 0.1642677223165297\n",
      "The 28061 th iteration gives loss of 0.16426541132873612\n",
      "The 28062 th iteration gives loss of 0.16426310050541976\n",
      "The 28063 th iteration gives loss of 0.16426078984656095\n",
      "The 28064 th iteration gives loss of 0.16425847935213875\n",
      "The 28065 th iteration gives loss of 0.16425616902213824\n",
      "The 28066 th iteration gives loss of 0.16425385885651597\n",
      "The 28067 th iteration gives loss of 0.16425154885525184\n",
      "The 28068 th iteration gives loss of 0.16424923901833155\n",
      "The 28069 th iteration gives loss of 0.16424692934572277\n",
      "The 28070 th iteration gives loss of 0.164244619837396\n",
      "The 28071 th iteration gives loss of 0.16424231049334212\n",
      "The 28072 th iteration gives loss of 0.16424000131352545\n",
      "The 28073 th iteration gives loss of 0.16423769229791987\n",
      "The 28074 th iteration gives loss of 0.16423538344651195\n",
      "The 28075 th iteration gives loss of 0.16423307475927038\n",
      "The 28076 th iteration gives loss of 0.164230766236173\n",
      "The 28077 th iteration gives loss of 0.16422845787718296\n",
      "The 28078 th iteration gives loss of 0.16422614968228896\n",
      "The 28079 th iteration gives loss of 0.16422384165147053\n",
      "The 28080 th iteration gives loss of 0.16422153378469223\n",
      "The 28081 th iteration gives loss of 0.16421922608193468\n",
      "The 28082 th iteration gives loss of 0.16421691854317294\n",
      "The 28083 th iteration gives loss of 0.1642146111683777\n",
      "The 28084 th iteration gives loss of 0.1642123039575343\n",
      "The 28085 th iteration gives loss of 0.16420999691062016\n",
      "The 28086 th iteration gives loss of 0.16420769002758753\n",
      "The 28087 th iteration gives loss of 0.1642053833084429\n",
      "The 28088 th iteration gives loss of 0.1642030767531406\n",
      "The 28089 th iteration gives loss of 0.164200770361657\n",
      "The 28090 th iteration gives loss of 0.16419846413398076\n",
      "The 28091 th iteration gives loss of 0.16419615807008137\n",
      "The 28092 th iteration gives loss of 0.16419385216993598\n",
      "The 28093 th iteration gives loss of 0.1641915464335097\n",
      "The 28094 th iteration gives loss of 0.16418924086079445\n",
      "The 28095 th iteration gives loss of 0.16418693545175367\n",
      "The 28096 th iteration gives loss of 0.16418463020637183\n",
      "The 28097 th iteration gives loss of 0.16418232512461695\n",
      "The 28098 th iteration gives loss of 0.16418002020646377\n",
      "The 28099 th iteration gives loss of 0.16417771545189735\n",
      "The 28100 th iteration gives loss of 0.16417541086088605\n",
      "The 28101 th iteration gives loss of 0.16417310643340588\n",
      "The 28102 th iteration gives loss of 0.16417080216943988\n",
      "The 28103 th iteration gives loss of 0.1641684980689547\n",
      "The 28104 th iteration gives loss of 0.16416619413193045\n",
      "The 28105 th iteration gives loss of 0.16416389035833776\n",
      "The 28106 th iteration gives loss of 0.16416158674816253\n",
      "The 28107 th iteration gives loss of 0.16415928330136115\n",
      "The 28108 th iteration gives loss of 0.16415698001793316\n",
      "The 28109 th iteration gives loss of 0.1641546768978453\n",
      "The 28110 th iteration gives loss of 0.16415237394106486\n",
      "The 28111 th iteration gives loss of 0.16415007114757424\n",
      "The 28112 th iteration gives loss of 0.16414776851735025\n",
      "The 28113 th iteration gives loss of 0.16414546605037428\n",
      "The 28114 th iteration gives loss of 0.16414316374660262\n",
      "The 28115 th iteration gives loss of 0.1641408616060297\n",
      "The 28116 th iteration gives loss of 0.16413855962862764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 28117 th iteration gives loss of 0.16413625781436236\n",
      "The 28118 th iteration gives loss of 0.16413395616322432\n",
      "The 28119 th iteration gives loss of 0.16413165467517785\n",
      "The 28120 th iteration gives loss of 0.1641293533502087\n",
      "The 28121 th iteration gives loss of 0.1641270521882793\n",
      "The 28122 th iteration gives loss of 0.16412475118937347\n",
      "The 28123 th iteration gives loss of 0.1641224503534676\n",
      "The 28124 th iteration gives loss of 0.16412014968053387\n",
      "The 28125 th iteration gives loss of 0.16411784917054875\n",
      "The 28126 th iteration gives loss of 0.16411554882349172\n",
      "The 28127 th iteration gives loss of 0.1641132486393423\n",
      "The 28128 th iteration gives loss of 0.16411094861805928\n",
      "The 28129 th iteration gives loss of 0.16410864875963121\n",
      "The 28130 th iteration gives loss of 0.1641063490640439\n",
      "The 28131 th iteration gives loss of 0.1641040495312509\n",
      "The 28132 th iteration gives loss of 0.16410175016123352\n",
      "The 28133 th iteration gives loss of 0.16409945095398018\n",
      "The 28134 th iteration gives loss of 0.16409715190946136\n",
      "The 28135 th iteration gives loss of 0.16409485302764265\n",
      "The 28136 th iteration gives loss of 0.16409255430850728\n",
      "The 28137 th iteration gives loss of 0.1640902557520405\n",
      "The 28138 th iteration gives loss of 0.16408795735820378\n",
      "The 28139 th iteration gives loss of 0.16408565912697867\n",
      "The 28140 th iteration gives loss of 0.16408336105833726\n",
      "The 28141 th iteration gives loss of 0.16408106315225834\n",
      "The 28142 th iteration gives loss of 0.16407876540872252\n",
      "The 28143 th iteration gives loss of 0.16407646782769694\n",
      "The 28144 th iteration gives loss of 0.16407417040916686\n",
      "The 28145 th iteration gives loss of 0.16407187315310448\n",
      "The 28146 th iteration gives loss of 0.16406957605947395\n",
      "The 28147 th iteration gives loss of 0.16406727912827165\n",
      "The 28148 th iteration gives loss of 0.16406498235945732\n",
      "The 28149 th iteration gives loss of 0.16406268575301228\n",
      "The 28150 th iteration gives loss of 0.16406038930891328\n",
      "The 28151 th iteration gives loss of 0.16405809302713675\n",
      "The 28152 th iteration gives loss of 0.16405579690765337\n",
      "The 28153 th iteration gives loss of 0.16405350095044557\n",
      "The 28154 th iteration gives loss of 0.16405120515548696\n",
      "The 28155 th iteration gives loss of 0.1640489095227572\n",
      "The 28156 th iteration gives loss of 0.16404661405222085\n",
      "The 28157 th iteration gives loss of 0.1640443187438595\n",
      "The 28158 th iteration gives loss of 0.16404202359765577\n",
      "The 28159 th iteration gives loss of 0.16403972861357957\n",
      "The 28160 th iteration gives loss of 0.1640374337916095\n",
      "The 28161 th iteration gives loss of 0.16403513913171927\n",
      "The 28162 th iteration gives loss of 0.16403284463388082\n",
      "The 28163 th iteration gives loss of 0.16403055029807734\n",
      "The 28164 th iteration gives loss of 0.16402825612428118\n",
      "The 28165 th iteration gives loss of 0.16402596211247308\n",
      "The 28166 th iteration gives loss of 0.16402366826261386\n",
      "The 28167 th iteration gives loss of 0.16402137457469326\n",
      "The 28168 th iteration gives loss of 0.16401908104868745\n",
      "The 28169 th iteration gives loss of 0.16401678768457373\n",
      "The 28170 th iteration gives loss of 0.16401449448231648\n",
      "The 28171 th iteration gives loss of 0.16401220144189788\n",
      "The 28172 th iteration gives loss of 0.16400990856329556\n",
      "The 28173 th iteration gives loss of 0.16400761584648155\n",
      "The 28174 th iteration gives loss of 0.16400532329143477\n",
      "The 28175 th iteration gives loss of 0.16400303089814408\n",
      "The 28176 th iteration gives loss of 0.16400073866656226\n",
      "The 28177 th iteration gives loss of 0.16399844659667337\n",
      "The 28178 th iteration gives loss of 0.16399615468846396\n",
      "The 28179 th iteration gives loss of 0.16399386294188845\n",
      "The 28180 th iteration gives loss of 0.16399157135694023\n",
      "The 28181 th iteration gives loss of 0.1639892799335973\n",
      "The 28182 th iteration gives loss of 0.1639869886718242\n",
      "The 28183 th iteration gives loss of 0.16398469757160028\n",
      "The 28184 th iteration gives loss of 0.1639824066329058\n",
      "The 28185 th iteration gives loss of 0.16398011585571998\n",
      "The 28186 th iteration gives loss of 0.16397782523999993\n",
      "The 28187 th iteration gives loss of 0.1639755347857508\n",
      "The 28188 th iteration gives loss of 0.16397324449292663\n",
      "The 28189 th iteration gives loss of 0.163970954361506\n",
      "The 28190 th iteration gives loss of 0.1639686643914626\n",
      "The 28191 th iteration gives loss of 0.16396637458278399\n",
      "The 28192 th iteration gives loss of 0.16396408493544573\n",
      "The 28193 th iteration gives loss of 0.16396179544941064\n",
      "The 28194 th iteration gives loss of 0.16395950612467017\n",
      "The 28195 th iteration gives loss of 0.1639572169611822\n",
      "The 28196 th iteration gives loss of 0.16395492795893687\n",
      "The 28197 th iteration gives loss of 0.16395263911791313\n",
      "The 28198 th iteration gives loss of 0.16395035043807274\n",
      "The 28199 th iteration gives loss of 0.16394806191940064\n",
      "The 28200 th iteration gives loss of 0.16394577356187554\n",
      "The 28201 th iteration gives loss of 0.16394348536547165\n",
      "The 28202 th iteration gives loss of 0.1639411973301631\n",
      "The 28203 th iteration gives loss of 0.16393890945592188\n",
      "The 28204 th iteration gives loss of 0.16393662174272067\n",
      "The 28205 th iteration gives loss of 0.16393433419055575\n",
      "The 28206 th iteration gives loss of 0.1639320467993855\n",
      "The 28207 th iteration gives loss of 0.1639297595691899\n",
      "The 28208 th iteration gives loss of 0.1639274724999462\n",
      "The 28209 th iteration gives loss of 0.1639251855916308\n",
      "The 28210 th iteration gives loss of 0.16392289884422412\n",
      "The 28211 th iteration gives loss of 0.16392061225769464\n",
      "The 28212 th iteration gives loss of 0.16391832583201607\n",
      "The 28213 th iteration gives loss of 0.16391603956717177\n",
      "The 28214 th iteration gives loss of 0.16391375346314094\n",
      "The 28215 th iteration gives loss of 0.16391146751989533\n",
      "The 28216 th iteration gives loss of 0.1639091817373988\n",
      "The 28217 th iteration gives loss of 0.16390689611564407\n",
      "The 28218 th iteration gives loss of 0.16390461065460388\n",
      "The 28219 th iteration gives loss of 0.16390232535425275\n",
      "The 28220 th iteration gives loss of 0.16390004021456792\n",
      "The 28221 th iteration gives loss of 0.16389775523552397\n",
      "The 28222 th iteration gives loss of 0.16389547041709793\n",
      "The 28223 th iteration gives loss of 0.1638931857592654\n",
      "The 28224 th iteration gives loss of 0.16389090126199687\n",
      "The 28225 th iteration gives loss of 0.16388861692527315\n",
      "The 28226 th iteration gives loss of 0.1638863327490754\n",
      "The 28227 th iteration gives loss of 0.16388404873337037\n",
      "The 28228 th iteration gives loss of 0.1638817648781554\n",
      "The 28229 th iteration gives loss of 0.16387948118337337\n",
      "The 28230 th iteration gives loss of 0.16387719764902542\n",
      "The 28231 th iteration gives loss of 0.16387491427508918\n",
      "The 28232 th iteration gives loss of 0.16387263106151687\n",
      "The 28233 th iteration gives loss of 0.16387034800831132\n",
      "The 28234 th iteration gives loss of 0.16386806511543353\n",
      "The 28235 th iteration gives loss of 0.16386578238285637\n",
      "The 28236 th iteration gives loss of 0.16386349981056522\n",
      "The 28237 th iteration gives loss of 0.16386121739853815\n",
      "The 28238 th iteration gives loss of 0.163858935146743\n",
      "The 28239 th iteration gives loss of 0.16385665305515817\n",
      "The 28240 th iteration gives loss of 0.16385437112376702\n",
      "The 28241 th iteration gives loss of 0.16385208935253753\n",
      "The 28242 th iteration gives loss of 0.16384980774145114\n",
      "The 28243 th iteration gives loss of 0.1638475262904814\n",
      "The 28244 th iteration gives loss of 0.16384524499960051\n",
      "The 28245 th iteration gives loss of 0.16384296386878738\n",
      "The 28246 th iteration gives loss of 0.1638406828980283\n",
      "The 28247 th iteration gives loss of 0.16383840208728614\n",
      "The 28248 th iteration gives loss of 0.1638361214365451\n",
      "The 28249 th iteration gives loss of 0.16383384094577783\n",
      "The 28250 th iteration gives loss of 0.16383156061496124\n",
      "The 28251 th iteration gives loss of 0.16382928044406067\n",
      "The 28252 th iteration gives loss of 0.1638270004330718\n",
      "The 28253 th iteration gives loss of 0.1638247205819645\n",
      "The 28254 th iteration gives loss of 0.16382244089071754\n",
      "The 28255 th iteration gives loss of 0.16382016135929756\n",
      "The 28256 th iteration gives loss of 0.16381788198768413\n",
      "The 28257 th iteration gives loss of 0.16381560277586293\n",
      "The 28258 th iteration gives loss of 0.16381332372378932\n",
      "The 28259 th iteration gives loss of 0.1638110448314601\n",
      "The 28260 th iteration gives loss of 0.16380876609884326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 28261 th iteration gives loss of 0.16380648752591292\n",
      "The 28262 th iteration gives loss of 0.1638042091126528\n",
      "The 28263 th iteration gives loss of 0.1638019308590276\n",
      "The 28264 th iteration gives loss of 0.16379965276502484\n",
      "The 28265 th iteration gives loss of 0.16379737483061854\n",
      "The 28266 th iteration gives loss of 0.1637950970557829\n",
      "The 28267 th iteration gives loss of 0.16379281944049778\n",
      "The 28268 th iteration gives loss of 0.16379054198472504\n",
      "The 28269 th iteration gives loss of 0.16378826468845914\n",
      "The 28270 th iteration gives loss of 0.1637859875516711\n",
      "The 28271 th iteration gives loss of 0.16378371057433638\n",
      "The 28272 th iteration gives loss of 0.16378143375642185\n",
      "The 28273 th iteration gives loss of 0.16377915709791796\n",
      "The 28274 th iteration gives loss of 0.16377688059879816\n",
      "The 28275 th iteration gives loss of 0.1637746042590434\n",
      "The 28276 th iteration gives loss of 0.16377232807860723\n",
      "The 28277 th iteration gives loss of 0.1637700520574882\n",
      "The 28278 th iteration gives loss of 0.16376777619564986\n",
      "The 28279 th iteration gives loss of 0.16376550049307803\n",
      "The 28280 th iteration gives loss of 0.16376322494975173\n",
      "The 28281 th iteration gives loss of 0.1637609495656401\n",
      "The 28282 th iteration gives loss of 0.16375867434071611\n",
      "The 28283 th iteration gives loss of 0.1637563992749658\n",
      "The 28284 th iteration gives loss of 0.16375412436835832\n",
      "The 28285 th iteration gives loss of 0.16375184962087166\n",
      "The 28286 th iteration gives loss of 0.16374957503247917\n",
      "The 28287 th iteration gives loss of 0.1637473006031678\n",
      "The 28288 th iteration gives loss of 0.1637450263329023\n",
      "The 28289 th iteration gives loss of 0.16374275222166548\n",
      "The 28290 th iteration gives loss of 0.16374047826943133\n",
      "The 28291 th iteration gives loss of 0.16373820447617707\n",
      "The 28292 th iteration gives loss of 0.1637359308418791\n",
      "The 28293 th iteration gives loss of 0.16373365736651682\n",
      "The 28294 th iteration gives loss of 0.16373138405005155\n",
      "The 28295 th iteration gives loss of 0.163729110892488\n",
      "The 28296 th iteration gives loss of 0.16372683789377088\n",
      "The 28297 th iteration gives loss of 0.16372456505390412\n",
      "The 28298 th iteration gives loss of 0.16372229237284763\n",
      "The 28299 th iteration gives loss of 0.1637200198505801\n",
      "The 28300 th iteration gives loss of 0.16371774748708723\n",
      "The 28301 th iteration gives loss of 0.16371547528232805\n",
      "The 28302 th iteration gives loss of 0.16371320323629004\n",
      "The 28303 th iteration gives loss of 0.16371093134894746\n",
      "The 28304 th iteration gives loss of 0.16370865962028516\n",
      "The 28305 th iteration gives loss of 0.16370638805027574\n",
      "The 28306 th iteration gives loss of 0.16370411663888526\n",
      "The 28307 th iteration gives loss of 0.16370184538609728\n",
      "The 28308 th iteration gives loss of 0.1636995742918887\n",
      "The 28309 th iteration gives loss of 0.16369730335623925\n",
      "The 28310 th iteration gives loss of 0.1636950325791163\n",
      "The 28311 th iteration gives loss of 0.16369276196050492\n",
      "The 28312 th iteration gives loss of 0.1636904915003752\n",
      "The 28313 th iteration gives loss of 0.16368822119871348\n",
      "The 28314 th iteration gives loss of 0.16368595105548273\n",
      "The 28315 th iteration gives loss of 0.16368368107066913\n",
      "The 28316 th iteration gives loss of 0.16368141124424349\n",
      "The 28317 th iteration gives loss of 0.16367914157618346\n",
      "The 28318 th iteration gives loss of 0.16367687206647388\n",
      "The 28319 th iteration gives loss of 0.1636746027150778\n",
      "The 28320 th iteration gives loss of 0.16367233352198504\n",
      "The 28321 th iteration gives loss of 0.16367006448716606\n",
      "The 28322 th iteration gives loss of 0.1636677956105942\n",
      "The 28323 th iteration gives loss of 0.16366552689224836\n",
      "The 28324 th iteration gives loss of 0.16366325833210912\n",
      "The 28325 th iteration gives loss of 0.1636609899301411\n",
      "The 28326 th iteration gives loss of 0.16365872168633522\n",
      "The 28327 th iteration gives loss of 0.16365645360066094\n",
      "The 28328 th iteration gives loss of 0.16365418567308618\n",
      "The 28329 th iteration gives loss of 0.16365191790360933\n",
      "The 28330 th iteration gives loss of 0.16364965029219214\n",
      "The 28331 th iteration gives loss of 0.16364738283880131\n",
      "The 28332 th iteration gives loss of 0.16364511554344152\n",
      "The 28333 th iteration gives loss of 0.16364284840606824\n",
      "The 28334 th iteration gives loss of 0.16364058142665813\n",
      "The 28335 th iteration gives loss of 0.163638314605197\n",
      "The 28336 th iteration gives loss of 0.16363604794165906\n",
      "The 28337 th iteration gives loss of 0.163633781436018\n",
      "The 28338 th iteration gives loss of 0.1636315150882498\n",
      "The 28339 th iteration gives loss of 0.1636292488983302\n",
      "The 28340 th iteration gives loss of 0.16362698286624852\n",
      "The 28341 th iteration gives loss of 0.16362471699195658\n",
      "The 28342 th iteration gives loss of 0.16362245127545672\n",
      "The 28343 th iteration gives loss of 0.1636201857167067\n",
      "The 28344 th iteration gives loss of 0.16361792031569608\n",
      "The 28345 th iteration gives loss of 0.16361565507239723\n",
      "The 28346 th iteration gives loss of 0.16361338998678027\n",
      "The 28347 th iteration gives loss of 0.16361112505882064\n",
      "The 28348 th iteration gives loss of 0.16360886028851349\n",
      "The 28349 th iteration gives loss of 0.16360659567582\n",
      "The 28350 th iteration gives loss of 0.16360433122071924\n",
      "The 28351 th iteration gives loss of 0.16360206692319162\n",
      "The 28352 th iteration gives loss of 0.16359980278321057\n",
      "The 28353 th iteration gives loss of 0.16359753880074926\n",
      "The 28354 th iteration gives loss of 0.1635952749757884\n",
      "The 28355 th iteration gives loss of 0.1635930113083034\n",
      "The 28356 th iteration gives loss of 0.1635907477982762\n",
      "The 28357 th iteration gives loss of 0.16358848444567603\n",
      "The 28358 th iteration gives loss of 0.16358622125047392\n",
      "The 28359 th iteration gives loss of 0.16358395821266933\n",
      "The 28360 th iteration gives loss of 0.1635816953322195\n",
      "The 28361 th iteration gives loss of 0.16357943260910482\n",
      "The 28362 th iteration gives loss of 0.16357717004330685\n",
      "The 28363 th iteration gives loss of 0.16357490763478935\n",
      "The 28364 th iteration gives loss of 0.16357264538354918\n",
      "The 28365 th iteration gives loss of 0.16357038328954293\n",
      "The 28366 th iteration gives loss of 0.1635681213527702\n",
      "The 28367 th iteration gives loss of 0.16356585957318526\n",
      "The 28368 th iteration gives loss of 0.1635635979507686\n",
      "The 28369 th iteration gives loss of 0.1635613364855129\n",
      "The 28370 th iteration gives loss of 0.16355907517737325\n",
      "The 28371 th iteration gives loss of 0.16355681402634215\n",
      "The 28372 th iteration gives loss of 0.16355455303238212\n",
      "The 28373 th iteration gives loss of 0.16355229219550035\n",
      "The 28374 th iteration gives loss of 0.16355003151563813\n",
      "The 28375 th iteration gives loss of 0.16354777099278806\n",
      "The 28376 th iteration gives loss of 0.16354551062692163\n",
      "The 28377 th iteration gives loss of 0.16354325041802528\n",
      "The 28378 th iteration gives loss of 0.16354099036606268\n",
      "The 28379 th iteration gives loss of 0.16353873047102432\n",
      "The 28380 th iteration gives loss of 0.16353647073286873\n",
      "The 28381 th iteration gives loss of 0.16353421115158567\n",
      "The 28382 th iteration gives loss of 0.16353195172715332\n",
      "The 28383 th iteration gives loss of 0.1635296924595483\n",
      "The 28384 th iteration gives loss of 0.1635274333487386\n",
      "The 28385 th iteration gives loss of 0.1635251743947083\n",
      "The 28386 th iteration gives loss of 0.16352291559743168\n",
      "The 28387 th iteration gives loss of 0.16352065695689438\n",
      "The 28388 th iteration gives loss of 0.16351839847305563\n",
      "The 28389 th iteration gives loss of 0.16351614014589422\n",
      "The 28390 th iteration gives loss of 0.1635138819754027\n",
      "The 28391 th iteration gives loss of 0.16351162396155328\n",
      "The 28392 th iteration gives loss of 0.16350936610431385\n",
      "The 28393 th iteration gives loss of 0.16350710840366386\n",
      "The 28394 th iteration gives loss of 0.16350485085957978\n",
      "The 28395 th iteration gives loss of 0.16350259347204762\n",
      "The 28396 th iteration gives loss of 0.16350033624103283\n",
      "The 28397 th iteration gives loss of 0.16349807916651785\n",
      "The 28398 th iteration gives loss of 0.16349582224847592\n",
      "The 28399 th iteration gives loss of 0.16349356548689534\n",
      "The 28400 th iteration gives loss of 0.1634913088817341\n",
      "The 28401 th iteration gives loss of 0.16348905243298756\n",
      "The 28402 th iteration gives loss of 0.16348679614061928\n",
      "The 28403 th iteration gives loss of 0.1634845400046101\n",
      "The 28404 th iteration gives loss of 0.1634822840249382\n",
      "The 28405 th iteration gives loss of 0.1634800282015763\n",
      "The 28406 th iteration gives loss of 0.16347777253450205\n",
      "The 28407 th iteration gives loss of 0.1634755170236982\n",
      "The 28408 th iteration gives loss of 0.16347326166914566\n",
      "The 28409 th iteration gives loss of 0.16347100647079604\n",
      "The 28410 th iteration gives loss of 0.16346875142865036\n",
      "The 28411 th iteration gives loss of 0.16346649654268897\n",
      "The 28412 th iteration gives loss of 0.1634642418128661\n",
      "The 28413 th iteration gives loss of 0.1634619872391751\n",
      "The 28414 th iteration gives loss of 0.16345973282159282\n",
      "The 28415 th iteration gives loss of 0.16345747856008747\n",
      "The 28416 th iteration gives loss of 0.16345522445464428\n",
      "The 28417 th iteration gives loss of 0.16345297050522772\n",
      "The 28418 th iteration gives loss of 0.16345071671183084\n",
      "The 28419 th iteration gives loss of 0.16344846307442185\n",
      "The 28420 th iteration gives loss of 0.16344620959298392\n",
      "The 28421 th iteration gives loss of 0.1634439562674753\n",
      "The 28422 th iteration gives loss of 0.16344170309789646\n",
      "The 28423 th iteration gives loss of 0.16343945008421523\n",
      "The 28424 th iteration gives loss of 0.1634371972264002\n",
      "The 28425 th iteration gives loss of 0.16343494452443963\n",
      "The 28426 th iteration gives loss of 0.16343269197829674\n",
      "The 28427 th iteration gives loss of 0.16343043958797113\n",
      "The 28428 th iteration gives loss of 0.1634281873534224\n",
      "The 28429 th iteration gives loss of 0.1634259352746237\n",
      "The 28430 th iteration gives loss of 0.1634236833515716\n",
      "The 28431 th iteration gives loss of 0.16342143158422578\n",
      "The 28432 th iteration gives loss of 0.16341917997256503\n",
      "The 28433 th iteration gives loss of 0.16341692851657896\n",
      "The 28434 th iteration gives loss of 0.1634146772162264\n",
      "The 28435 th iteration gives loss of 0.1634124260714913\n",
      "The 28436 th iteration gives loss of 0.16341017508235997\n",
      "The 28437 th iteration gives loss of 0.1634079242487976\n",
      "The 28438 th iteration gives loss of 0.1634056735707838\n",
      "The 28439 th iteration gives loss of 0.16340342304830177\n",
      "The 28440 th iteration gives loss of 0.16340117268132637\n",
      "The 28441 th iteration gives loss of 0.1633989224698257\n",
      "The 28442 th iteration gives loss of 0.16339667241378217\n",
      "The 28443 th iteration gives loss of 0.16339442251317976\n",
      "The 28444 th iteration gives loss of 0.16339217276797993\n",
      "The 28445 th iteration gives loss of 0.1633899231781792\n",
      "The 28446 th iteration gives loss of 0.16338767374373792\n",
      "The 28447 th iteration gives loss of 0.16338542446463683\n",
      "The 28448 th iteration gives loss of 0.16338317534085914\n",
      "The 28449 th iteration gives loss of 0.16338092637237758\n",
      "The 28450 th iteration gives loss of 0.1633786775591731\n",
      "The 28451 th iteration gives loss of 0.16337642890121723\n",
      "The 28452 th iteration gives loss of 0.16337418039848473\n",
      "The 28453 th iteration gives loss of 0.16337193205095635\n",
      "The 28454 th iteration gives loss of 0.1633696838586212\n",
      "The 28455 th iteration gives loss of 0.1633674358214328\n",
      "The 28456 th iteration gives loss of 0.1633651879393808\n",
      "The 28457 th iteration gives loss of 0.16336294021244563\n",
      "The 28458 th iteration gives loss of 0.16336069264059908\n",
      "The 28459 th iteration gives loss of 0.1633584452238215\n",
      "The 28460 th iteration gives loss of 0.16335619796208906\n",
      "The 28461 th iteration gives loss of 0.1633539508553709\n",
      "The 28462 th iteration gives loss of 0.16335170390365378\n",
      "The 28463 th iteration gives loss of 0.16334945710690887\n",
      "The 28464 th iteration gives loss of 0.16334721046512235\n",
      "The 28465 th iteration gives loss of 0.16334496397826168\n",
      "The 28466 th iteration gives loss of 0.1633427176463019\n",
      "The 28467 th iteration gives loss of 0.16334047146922534\n",
      "The 28468 th iteration gives loss of 0.16333822544701346\n",
      "The 28469 th iteration gives loss of 0.16333597957963591\n",
      "The 28470 th iteration gives loss of 0.16333373386707223\n",
      "The 28471 th iteration gives loss of 0.1633314883093045\n",
      "The 28472 th iteration gives loss of 0.1633292429063023\n",
      "The 28473 th iteration gives loss of 0.1633269976580423\n",
      "The 28474 th iteration gives loss of 0.16332475256450962\n",
      "The 28475 th iteration gives loss of 0.16332250762566533\n",
      "The 28476 th iteration gives loss of 0.16332026284150733\n",
      "The 28477 th iteration gives loss of 0.16331801821200195\n",
      "The 28478 th iteration gives loss of 0.16331577373713077\n",
      "The 28479 th iteration gives loss of 0.16331352941685956\n",
      "The 28480 th iteration gives loss of 0.1633112852511749\n",
      "The 28481 th iteration gives loss of 0.16330904124004614\n",
      "The 28482 th iteration gives loss of 0.1633067973834658\n",
      "The 28483 th iteration gives loss of 0.16330455368140073\n",
      "The 28484 th iteration gives loss of 0.1633023101338238\n",
      "The 28485 th iteration gives loss of 0.1633000667407273\n",
      "The 28486 th iteration gives loss of 0.16329782350207384\n",
      "The 28487 th iteration gives loss of 0.1632955804178425\n",
      "The 28488 th iteration gives loss of 0.1632933374880167\n",
      "The 28489 th iteration gives loss of 0.16329109471256878\n",
      "The 28490 th iteration gives loss of 0.16328885209147345\n",
      "The 28491 th iteration gives loss of 0.16328660962471603\n",
      "The 28492 th iteration gives loss of 0.1632843673122628\n",
      "The 28493 th iteration gives loss of 0.1632821251540947\n",
      "The 28494 th iteration gives loss of 0.1632798831501967\n",
      "The 28495 th iteration gives loss of 0.16327764130053404\n",
      "The 28496 th iteration gives loss of 0.16327539960509935\n",
      "The 28497 th iteration gives loss of 0.16327315806385442\n",
      "The 28498 th iteration gives loss of 0.16327091667678706\n",
      "The 28499 th iteration gives loss of 0.1632686754438667\n",
      "The 28500 th iteration gives loss of 0.16326643436507263\n",
      "The 28501 th iteration gives loss of 0.1632641934403871\n",
      "The 28502 th iteration gives loss of 0.16326195266978388\n",
      "The 28503 th iteration gives loss of 0.16325971205323547\n",
      "The 28504 th iteration gives loss of 0.16325747159072418\n",
      "The 28505 th iteration gives loss of 0.16325523128222372\n",
      "The 28506 th iteration gives loss of 0.1632529911277181\n",
      "The 28507 th iteration gives loss of 0.16325075112718093\n",
      "The 28508 th iteration gives loss of 0.1632485112805861\n",
      "The 28509 th iteration gives loss of 0.16324627158791047\n",
      "The 28510 th iteration gives loss of 0.163244032049136\n",
      "The 28511 th iteration gives loss of 0.16324179266424146\n",
      "The 28512 th iteration gives loss of 0.16323955343319943\n",
      "The 28513 th iteration gives loss of 0.16323731435598648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 28514 th iteration gives loss of 0.1632350754325828\n",
      "The 28515 th iteration gives loss of 0.16323283666295774\n",
      "The 28516 th iteration gives loss of 0.16323059804710616\n",
      "The 28517 th iteration gives loss of 0.16322835958499385\n",
      "The 28518 th iteration gives loss of 0.16322612127659206\n",
      "The 28519 th iteration gives loss of 0.16322388312188318\n",
      "The 28520 th iteration gives loss of 0.16322164512085344\n",
      "The 28521 th iteration gives loss of 0.1632194072734666\n",
      "The 28522 th iteration gives loss of 0.16321716957970772\n",
      "The 28523 th iteration gives loss of 0.16321493203955342\n",
      "The 28524 th iteration gives loss of 0.16321269465297406\n",
      "The 28525 th iteration gives loss of 0.16321045741995724\n",
      "The 28526 th iteration gives loss of 0.16320822034047758\n",
      "The 28527 th iteration gives loss of 0.16320598341451054\n",
      "The 28528 th iteration gives loss of 0.16320374664203066\n",
      "The 28529 th iteration gives loss of 0.16320151002301633\n",
      "The 28530 th iteration gives loss of 0.163199273557451\n",
      "The 28531 th iteration gives loss of 0.16319703724529294\n",
      "The 28532 th iteration gives loss of 0.16319480108654308\n",
      "The 28533 th iteration gives loss of 0.16319256508117286\n",
      "The 28534 th iteration gives loss of 0.16319032922914933\n",
      "The 28535 th iteration gives loss of 0.16318809353045985\n",
      "The 28536 th iteration gives loss of 0.1631858579850735\n",
      "The 28537 th iteration gives loss of 0.1631836225929776\n",
      "The 28538 th iteration gives loss of 0.16318138735414436\n",
      "The 28539 th iteration gives loss of 0.1631791522685479\n",
      "The 28540 th iteration gives loss of 0.16317691733616585\n",
      "The 28541 th iteration gives loss of 0.16317468255697898\n",
      "The 28542 th iteration gives loss of 0.1631724479309662\n",
      "The 28543 th iteration gives loss of 0.16317021345809962\n",
      "The 28544 th iteration gives loss of 0.16316797913836695\n",
      "The 28545 th iteration gives loss of 0.1631657449717335\n",
      "The 28546 th iteration gives loss of 0.16316351095817852\n",
      "The 28547 th iteration gives loss of 0.16316127709768424\n",
      "The 28548 th iteration gives loss of 0.16315904339022394\n",
      "The 28549 th iteration gives loss of 0.1631568098357787\n",
      "The 28550 th iteration gives loss of 0.16315457643432174\n",
      "The 28551 th iteration gives loss of 0.16315234318583077\n",
      "The 28552 th iteration gives loss of 0.16315011009028432\n",
      "The 28553 th iteration gives loss of 0.16314787714766515\n",
      "The 28554 th iteration gives loss of 0.16314564435793713\n",
      "The 28555 th iteration gives loss of 0.16314341172110094\n",
      "The 28556 th iteration gives loss of 0.16314117923710963\n",
      "The 28557 th iteration gives loss of 0.1631389469059505\n",
      "The 28558 th iteration gives loss of 0.16313671472759825\n",
      "The 28559 th iteration gives loss of 0.16313448270203285\n",
      "The 28560 th iteration gives loss of 0.16313225082922914\n",
      "The 28561 th iteration gives loss of 0.16313001910917632\n",
      "The 28562 th iteration gives loss of 0.16312778754183174\n",
      "The 28563 th iteration gives loss of 0.16312555612719418\n",
      "The 28564 th iteration gives loss of 0.1631233248652243\n",
      "The 28565 th iteration gives loss of 0.16312109375590206\n",
      "The 28566 th iteration gives loss of 0.16311886279922172\n",
      "The 28567 th iteration gives loss of 0.16311663199513224\n",
      "The 28568 th iteration gives loss of 0.16311440134363\n",
      "The 28569 th iteration gives loss of 0.16311217084468085\n",
      "The 28570 th iteration gives loss of 0.1631099404982828\n",
      "The 28571 th iteration gives loss of 0.1631077103043923\n",
      "The 28572 th iteration gives loss of 0.1631054802629961\n",
      "The 28573 th iteration gives loss of 0.16310325037407053\n",
      "The 28574 th iteration gives loss of 0.16310102063758905\n",
      "The 28575 th iteration gives loss of 0.16309879105353964\n",
      "The 28576 th iteration gives loss of 0.16309656162189143\n",
      "The 28577 th iteration gives loss of 0.16309433234262408\n",
      "The 28578 th iteration gives loss of 0.16309210321570985\n",
      "The 28579 th iteration gives loss of 0.16308987424113286\n",
      "The 28580 th iteration gives loss of 0.16308764541886264\n",
      "The 28581 th iteration gives loss of 0.16308541674888793\n",
      "The 28582 th iteration gives loss of 0.1630831882311805\n",
      "The 28583 th iteration gives loss of 0.16308095986571516\n",
      "The 28584 th iteration gives loss of 0.16307873165247724\n",
      "The 28585 th iteration gives loss of 0.16307650359142706\n",
      "The 28586 th iteration gives loss of 0.1630742756825629\n",
      "The 28587 th iteration gives loss of 0.1630720479258536\n",
      "The 28588 th iteration gives loss of 0.16306982032127088\n",
      "The 28589 th iteration gives loss of 0.16306759286880418\n",
      "The 28590 th iteration gives loss of 0.1630653655684234\n",
      "The 28591 th iteration gives loss of 0.16306313842010461\n",
      "The 28592 th iteration gives loss of 0.16306091142382909\n",
      "The 28593 th iteration gives loss of 0.1630586845795722\n",
      "The 28594 th iteration gives loss of 0.16305645788730413\n",
      "The 28595 th iteration gives loss of 0.1630542313470248\n",
      "The 28596 th iteration gives loss of 0.16305200495869165\n",
      "The 28597 th iteration gives loss of 0.1630497787222886\n",
      "The 28598 th iteration gives loss of 0.16304755263779327\n",
      "The 28599 th iteration gives loss of 0.1630453267051809\n",
      "The 28600 th iteration gives loss of 0.16304310092442806\n",
      "The 28601 th iteration gives loss of 0.16304087529551356\n",
      "The 28602 th iteration gives loss of 0.16303864981842242\n",
      "The 28603 th iteration gives loss of 0.1630364244931335\n",
      "The 28604 th iteration gives loss of 0.1630341993196024\n",
      "The 28605 th iteration gives loss of 0.1630319742978237\n",
      "The 28606 th iteration gives loss of 0.16302974942777457\n",
      "The 28607 th iteration gives loss of 0.16302752470943188\n",
      "The 28608 th iteration gives loss of 0.16302530014277092\n",
      "The 28609 th iteration gives loss of 0.1630230757277665\n",
      "The 28610 th iteration gives loss of 0.16302085146439943\n",
      "The 28611 th iteration gives loss of 0.1630186273526503\n",
      "The 28612 th iteration gives loss of 0.16301640339249218\n",
      "The 28613 th iteration gives loss of 0.16301417958390116\n",
      "The 28614 th iteration gives loss of 0.16301195592685871\n",
      "The 28615 th iteration gives loss of 0.1630097324213485\n",
      "The 28616 th iteration gives loss of 0.1630075090673449\n",
      "The 28617 th iteration gives loss of 0.163005285864816\n",
      "The 28618 th iteration gives loss of 0.16300306281373905\n",
      "The 28619 th iteration gives loss of 0.1630008399141016\n",
      "The 28620 th iteration gives loss of 0.16299861716587563\n",
      "The 28621 th iteration gives loss of 0.16299639456904777\n",
      "The 28622 th iteration gives loss of 0.1629941721235815\n",
      "The 28623 th iteration gives loss of 0.1629919498294611\n",
      "The 28624 th iteration gives loss of 0.16298972768666647\n",
      "The 28625 th iteration gives loss of 0.16298750569517984\n",
      "The 28626 th iteration gives loss of 0.16298528385496489\n",
      "The 28627 th iteration gives loss of 0.1629830621660113\n",
      "The 28628 th iteration gives loss of 0.1629808406282918\n",
      "The 28629 th iteration gives loss of 0.1629786192417756\n",
      "The 28630 th iteration gives loss of 0.16297639800645314\n",
      "The 28631 th iteration gives loss of 0.16297417692229843\n",
      "The 28632 th iteration gives loss of 0.16297195598929087\n",
      "The 28633 th iteration gives loss of 0.16296973520740507\n",
      "The 28634 th iteration gives loss of 0.16296751457661754\n",
      "The 28635 th iteration gives loss of 0.16296529409691018\n",
      "The 28636 th iteration gives loss of 0.16296307376824923\n",
      "The 28637 th iteration gives loss of 0.16296085359062773\n",
      "The 28638 th iteration gives loss of 0.1629586335640253\n",
      "The 28639 th iteration gives loss of 0.16295641368839725\n",
      "The 28640 th iteration gives loss of 0.1629541939637361\n",
      "The 28641 th iteration gives loss of 0.16295197439002193\n",
      "The 28642 th iteration gives loss of 0.162949754967232\n",
      "The 28643 th iteration gives loss of 0.162947535695342\n",
      "The 28644 th iteration gives loss of 0.16294531657432787\n",
      "The 28645 th iteration gives loss of 0.16294309760416215\n",
      "The 28646 th iteration gives loss of 0.16294087878484076\n",
      "The 28647 th iteration gives loss of 0.16293866011631655\n",
      "The 28648 th iteration gives loss of 0.16293644159858212\n",
      "The 28649 th iteration gives loss of 0.16293422323161258\n",
      "The 28650 th iteration gives loss of 0.16293200501538874\n",
      "The 28651 th iteration gives loss of 0.16292978694989063\n",
      "The 28652 th iteration gives loss of 0.1629275690350796\n",
      "The 28653 th iteration gives loss of 0.162925351270945\n",
      "The 28654 th iteration gives loss of 0.16292313365747185\n",
      "The 28655 th iteration gives loss of 0.16292091619462556\n",
      "The 28656 th iteration gives loss of 0.16291869888238672\n",
      "The 28657 th iteration gives loss of 0.16291648172074602\n",
      "The 28658 th iteration gives loss of 0.1629142647096548\n",
      "The 28659 th iteration gives loss of 0.1629120478491172\n",
      "The 28660 th iteration gives loss of 0.16290983113908966\n",
      "The 28661 th iteration gives loss of 0.16290761457956973\n",
      "The 28662 th iteration gives loss of 0.1629053981705193\n",
      "The 28663 th iteration gives loss of 0.16290318191192063\n",
      "The 28664 th iteration gives loss of 0.16290096580376082\n",
      "The 28665 th iteration gives loss of 0.1628987498459998\n",
      "The 28666 th iteration gives loss of 0.1628965340386361\n",
      "The 28667 th iteration gives loss of 0.1628943183816262\n",
      "The 28668 th iteration gives loss of 0.16289210287497088\n",
      "The 28669 th iteration gives loss of 0.16288988751862798\n",
      "The 28670 th iteration gives loss of 0.16288767231257698\n",
      "The 28671 th iteration gives loss of 0.16288545725680612\n",
      "The 28672 th iteration gives loss of 0.16288324235129248\n",
      "The 28673 th iteration gives loss of 0.16288102759600645\n",
      "The 28674 th iteration gives loss of 0.1628788129909258\n",
      "The 28675 th iteration gives loss of 0.16287659853603714\n",
      "The 28676 th iteration gives loss of 0.16287438423130587\n",
      "The 28677 th iteration gives loss of 0.16287217007672658\n",
      "The 28678 th iteration gives loss of 0.1628699560722632\n",
      "The 28679 th iteration gives loss of 0.16286774221790087\n",
      "The 28680 th iteration gives loss of 0.16286552851360786\n",
      "The 28681 th iteration gives loss of 0.1628633149593641\n",
      "The 28682 th iteration gives loss of 0.1628611015551569\n",
      "The 28683 th iteration gives loss of 0.16285888830095913\n",
      "The 28684 th iteration gives loss of 0.16285667519675007\n",
      "The 28685 th iteration gives loss of 0.1628544622425042\n",
      "The 28686 th iteration gives loss of 0.1628522494381999\n",
      "The 28687 th iteration gives loss of 0.16285003678381124\n",
      "The 28688 th iteration gives loss of 0.16284782427932837\n",
      "The 28689 th iteration gives loss of 0.16284561192472097\n",
      "The 28690 th iteration gives loss of 0.1628433997199617\n",
      "The 28691 th iteration gives loss of 0.1628411876650328\n",
      "The 28692 th iteration gives loss of 0.1628389757599235\n",
      "The 28693 th iteration gives loss of 0.16283676400459307\n",
      "The 28694 th iteration gives loss of 0.16283455239902486\n",
      "The 28695 th iteration gives loss of 0.16283234094321\n",
      "The 28696 th iteration gives loss of 0.16283012963711307\n",
      "The 28697 th iteration gives loss of 0.1628279184807101\n",
      "The 28698 th iteration gives loss of 0.1628257074739838\n",
      "The 28699 th iteration gives loss of 0.1628234966169194\n",
      "The 28700 th iteration gives loss of 0.16282128590949013\n",
      "The 28701 th iteration gives loss of 0.16281907535165654\n",
      "The 28702 th iteration gives loss of 0.16281686494342934\n",
      "The 28703 th iteration gives loss of 0.16281465468474637\n",
      "The 28704 th iteration gives loss of 0.16281244457562025\n",
      "The 28705 th iteration gives loss of 0.1628102346160135\n",
      "The 28706 th iteration gives loss of 0.1628080248059075\n",
      "The 28707 th iteration gives loss of 0.1628058151452781\n",
      "The 28708 th iteration gives loss of 0.16280360563411028\n",
      "The 28709 th iteration gives loss of 0.162801396272368\n",
      "The 28710 th iteration gives loss of 0.1627991870600361\n",
      "The 28711 th iteration gives loss of 0.16279697799709708\n",
      "The 28712 th iteration gives loss of 0.16279476908352972\n",
      "The 28713 th iteration gives loss of 0.16279256031929634\n",
      "The 28714 th iteration gives loss of 0.16279035170439757\n",
      "The 28715 th iteration gives loss of 0.1627881432387924\n",
      "The 28716 th iteration gives loss of 0.16278593492246685\n",
      "The 28717 th iteration gives loss of 0.16278372675540115\n",
      "The 28718 th iteration gives loss of 0.16278151873757252\n",
      "The 28719 th iteration gives loss of 0.16277931086895445\n",
      "The 28720 th iteration gives loss of 0.16277710314952054\n",
      "The 28721 th iteration gives loss of 0.16277489557926222\n",
      "The 28722 th iteration gives loss of 0.16277268815815568\n",
      "The 28723 th iteration gives loss of 0.1627704808861647\n",
      "The 28724 th iteration gives loss of 0.16276827376328207\n",
      "The 28725 th iteration gives loss of 0.16276606678948105\n",
      "The 28726 th iteration gives loss of 0.16276385996472462\n",
      "The 28727 th iteration gives loss of 0.16276165328901762\n",
      "The 28728 th iteration gives loss of 0.16275944676232296\n",
      "The 28729 th iteration gives loss of 0.16275724038461653\n",
      "The 28730 th iteration gives loss of 0.16275503415588474\n",
      "The 28731 th iteration gives loss of 0.16275282807609792\n",
      "The 28732 th iteration gives loss of 0.16275062214523958\n",
      "The 28733 th iteration gives loss of 0.16274841636328097\n",
      "The 28734 th iteration gives loss of 0.16274621073021003\n",
      "The 28735 th iteration gives loss of 0.16274400524600094\n",
      "The 28736 th iteration gives loss of 0.16274179991062226\n",
      "The 28737 th iteration gives loss of 0.16273959472406727\n",
      "The 28738 th iteration gives loss of 0.16273738968630308\n",
      "The 28739 th iteration gives loss of 0.1627351847973159\n",
      "The 28740 th iteration gives loss of 0.1627329800570757\n",
      "The 28741 th iteration gives loss of 0.16273077546556836\n",
      "The 28742 th iteration gives loss of 0.16272857102276048\n",
      "The 28743 th iteration gives loss of 0.1627263667286416\n",
      "The 28744 th iteration gives loss of 0.16272416258317843\n",
      "The 28745 th iteration gives loss of 0.16272195858636002\n",
      "The 28746 th iteration gives loss of 0.1627197547381651\n",
      "The 28747 th iteration gives loss of 0.16271755103856114\n",
      "The 28748 th iteration gives loss of 0.16271534748753766\n",
      "The 28749 th iteration gives loss of 0.1627131440850608\n",
      "The 28750 th iteration gives loss of 0.1627109408311191\n",
      "The 28751 th iteration gives loss of 0.1627087377256805\n",
      "The 28752 th iteration gives loss of 0.16270653476873068\n",
      "The 28753 th iteration gives loss of 0.16270433196024792\n",
      "The 28754 th iteration gives loss of 0.1627021293002071\n",
      "The 28755 th iteration gives loss of 0.16269992678858478\n",
      "The 28756 th iteration gives loss of 0.16269772442536645\n",
      "The 28757 th iteration gives loss of 0.16269552221052566\n",
      "The 28758 th iteration gives loss of 0.16269332014403262\n",
      "The 28759 th iteration gives loss of 0.16269111822588064\n",
      "The 28760 th iteration gives loss of 0.16268891645603314\n",
      "The 28761 th iteration gives loss of 0.16268671483447988\n",
      "The 28762 th iteration gives loss of 0.16268451336119671\n",
      "The 28763 th iteration gives loss of 0.16268231203615796\n",
      "The 28764 th iteration gives loss of 0.16268011085934345\n",
      "The 28765 th iteration gives loss of 0.16267790983072367\n",
      "The 28766 th iteration gives loss of 0.16267570895029235\n",
      "The 28767 th iteration gives loss of 0.16267350821801554\n",
      "The 28768 th iteration gives loss of 0.1626713076338768\n",
      "The 28769 th iteration gives loss of 0.16266910719785163\n",
      "The 28770 th iteration gives loss of 0.16266690690992366\n",
      "The 28771 th iteration gives loss of 0.162664706770064\n",
      "The 28772 th iteration gives loss of 0.16266250677824465\n",
      "The 28773 th iteration gives loss of 0.162660306934464\n",
      "The 28774 th iteration gives loss of 0.1626581072386857\n",
      "The 28775 th iteration gives loss of 0.16265590769089244\n",
      "The 28776 th iteration gives loss of 0.16265370829105197\n",
      "The 28777 th iteration gives loss of 0.16265150903914433\n",
      "The 28778 th iteration gives loss of 0.16264930993516616\n",
      "The 28779 th iteration gives loss of 0.1626471109790867\n",
      "The 28780 th iteration gives loss of 0.16264491217088176\n",
      "The 28781 th iteration gives loss of 0.16264271351051862\n",
      "The 28782 th iteration gives loss of 0.16264051499799265\n",
      "The 28783 th iteration gives loss of 0.16263831663327813\n",
      "The 28784 th iteration gives loss of 0.16263611841634568\n",
      "The 28785 th iteration gives loss of 0.16263392034717641\n",
      "The 28786 th iteration gives loss of 0.1626317224257605\n",
      "The 28787 th iteration gives loss of 0.16262952465204936\n",
      "The 28788 th iteration gives loss of 0.16262732702604613\n",
      "The 28789 th iteration gives loss of 0.16262512954772299\n",
      "The 28790 th iteration gives loss of 0.16262293221705224\n",
      "The 28791 th iteration gives loss of 0.16262073503401275\n",
      "The 28792 th iteration gives loss of 0.1626185379985936\n",
      "The 28793 th iteration gives loss of 0.1626163411107522\n",
      "The 28794 th iteration gives loss of 0.16261414437049662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 28795 th iteration gives loss of 0.16261194777777638\n",
      "The 28796 th iteration gives loss of 0.16260975133258398\n",
      "The 28797 th iteration gives loss of 0.1626075550348896\n",
      "The 28798 th iteration gives loss of 0.1626053588846799\n",
      "The 28799 th iteration gives loss of 0.16260316288193677\n",
      "The 28800 th iteration gives loss of 0.16260096702662238\n",
      "The 28801 th iteration gives loss of 0.16259877131872918\n",
      "The 28802 th iteration gives loss of 0.1625965757582265\n",
      "The 28803 th iteration gives loss of 0.16259438034509993\n",
      "The 28804 th iteration gives loss of 0.16259218507932113\n",
      "The 28805 th iteration gives loss of 0.16258998996087243\n",
      "The 28806 th iteration gives loss of 0.16258779498973422\n",
      "The 28807 th iteration gives loss of 0.162585600165875\n",
      "The 28808 th iteration gives loss of 0.16258340548928235\n",
      "The 28809 th iteration gives loss of 0.16258121095993153\n",
      "The 28810 th iteration gives loss of 0.16257901657780205\n",
      "The 28811 th iteration gives loss of 0.1625768223428707\n",
      "The 28812 th iteration gives loss of 0.16257462825511582\n",
      "The 28813 th iteration gives loss of 0.16257243431452456\n",
      "The 28814 th iteration gives loss of 0.16257024052105887\n",
      "The 28815 th iteration gives loss of 0.16256804687470192\n",
      "The 28816 th iteration gives loss of 0.16256585337543858\n",
      "The 28817 th iteration gives loss of 0.1625636600232381\n",
      "The 28818 th iteration gives loss of 0.16256146681808897\n",
      "The 28819 th iteration gives loss of 0.16255927375996326\n",
      "The 28820 th iteration gives loss of 0.1625570808488377\n",
      "The 28821 th iteration gives loss of 0.1625548880846968\n",
      "The 28822 th iteration gives loss of 0.16255269546751586\n",
      "The 28823 th iteration gives loss of 0.16255050299727664\n",
      "The 28824 th iteration gives loss of 0.16254831067395323\n",
      "The 28825 th iteration gives loss of 0.16254611849752082\n",
      "The 28826 th iteration gives loss of 0.16254392646795512\n",
      "The 28827 th iteration gives loss of 0.16254173458525048\n",
      "The 28828 th iteration gives loss of 0.16253954284937275\n",
      "The 28829 th iteration gives loss of 0.1625373512603013\n",
      "The 28830 th iteration gives loss of 0.16253515981802372\n",
      "The 28831 th iteration gives loss of 0.16253296852250504\n",
      "The 28832 th iteration gives loss of 0.16253077737372537\n",
      "The 28833 th iteration gives loss of 0.1625285863716704\n",
      "The 28834 th iteration gives loss of 0.1625263955163189\n",
      "The 28835 th iteration gives loss of 0.16252420480764346\n",
      "The 28836 th iteration gives loss of 0.16252201424562304\n",
      "The 28837 th iteration gives loss of 0.16251982383023472\n",
      "The 28838 th iteration gives loss of 0.16251763356146026\n",
      "The 28839 th iteration gives loss of 0.1625154434392801\n",
      "The 28840 th iteration gives loss of 0.1625132534636637\n",
      "The 28841 th iteration gives loss of 0.16251106363460008\n",
      "The 28842 th iteration gives loss of 0.1625088739520581\n",
      "The 28843 th iteration gives loss of 0.1625066844160249\n",
      "The 28844 th iteration gives loss of 0.1625044950264733\n",
      "The 28845 th iteration gives loss of 0.1625023057833922\n",
      "The 28846 th iteration gives loss of 0.16250011668674047\n",
      "The 28847 th iteration gives loss of 0.1624979277365143\n",
      "The 28848 th iteration gives loss of 0.16249573893267646\n",
      "The 28849 th iteration gives loss of 0.1624935502752189\n",
      "The 28850 th iteration gives loss of 0.16249136176411846\n",
      "The 28851 th iteration gives loss of 0.162489173399342\n",
      "The 28852 th iteration gives loss of 0.1624869851808866\n",
      "The 28853 th iteration gives loss of 0.16248479710871533\n",
      "The 28854 th iteration gives loss of 0.1624826091828053\n",
      "The 28855 th iteration gives loss of 0.1624804214031442\n",
      "The 28856 th iteration gives loss of 0.16247823376970794\n",
      "The 28857 th iteration gives loss of 0.16247604628248127\n",
      "The 28858 th iteration gives loss of 0.1624738589414227\n",
      "The 28859 th iteration gives loss of 0.16247167174653201\n",
      "The 28860 th iteration gives loss of 0.16246948469777128\n",
      "The 28861 th iteration gives loss of 0.16246729779513142\n",
      "The 28862 th iteration gives loss of 0.16246511103858882\n",
      "The 28863 th iteration gives loss of 0.16246292442811136\n",
      "The 28864 th iteration gives loss of 0.16246073796368965\n",
      "The 28865 th iteration gives loss of 0.16245855164530004\n",
      "The 28866 th iteration gives loss of 0.1624563654729171\n",
      "The 28867 th iteration gives loss of 0.16245417944652296\n",
      "The 28868 th iteration gives loss of 0.16245199356609144\n",
      "The 28869 th iteration gives loss of 0.162449807831608\n",
      "The 28870 th iteration gives loss of 0.16244762224304685\n",
      "The 28871 th iteration gives loss of 0.16244543680038562\n",
      "The 28872 th iteration gives loss of 0.1624432515036052\n",
      "The 28873 th iteration gives loss of 0.16244106635267389\n",
      "The 28874 th iteration gives loss of 0.16243888134758425\n",
      "The 28875 th iteration gives loss of 0.16243669648829834\n",
      "The 28876 th iteration gives loss of 0.16243451177482565\n",
      "The 28877 th iteration gives loss of 0.16243232720711623\n",
      "The 28878 th iteration gives loss of 0.16243014278515483\n",
      "The 28879 th iteration gives loss of 0.1624279585089191\n",
      "The 28880 th iteration gives loss of 0.16242577437839884\n",
      "The 28881 th iteration gives loss of 0.1624235903935584\n",
      "The 28882 th iteration gives loss of 0.1624214065543832\n",
      "The 28883 th iteration gives loss of 0.1624192228608468\n",
      "The 28884 th iteration gives loss of 0.16241703931294044\n",
      "The 28885 th iteration gives loss of 0.16241485591062557\n",
      "The 28886 th iteration gives loss of 0.16241267265389003\n",
      "The 28887 th iteration gives loss of 0.1624104895427157\n",
      "The 28888 th iteration gives loss of 0.1624083065770724\n",
      "The 28889 th iteration gives loss of 0.16240612375694624\n",
      "The 28890 th iteration gives loss of 0.1624039410823103\n",
      "The 28891 th iteration gives loss of 0.16240175855314\n",
      "The 28892 th iteration gives loss of 0.16239957616943282\n",
      "The 28893 th iteration gives loss of 0.16239739393114352\n",
      "The 28894 th iteration gives loss of 0.16239521183826416\n",
      "The 28895 th iteration gives loss of 0.162393029890768\n",
      "The 28896 th iteration gives loss of 0.1623908480886355\n",
      "The 28897 th iteration gives loss of 0.16238866643184272\n",
      "The 28898 th iteration gives loss of 0.16238648492036897\n",
      "The 28899 th iteration gives loss of 0.1623843035542035\n",
      "The 28900 th iteration gives loss of 0.16238212233331123\n",
      "The 28901 th iteration gives loss of 0.16237994125767605\n",
      "The 28902 th iteration gives loss of 0.16237776032727727\n",
      "The 28903 th iteration gives loss of 0.1623755795420882\n",
      "The 28904 th iteration gives loss of 0.1623733989020911\n",
      "The 28905 th iteration gives loss of 0.1623712184072664\n",
      "The 28906 th iteration gives loss of 0.16236903805759587\n",
      "The 28907 th iteration gives loss of 0.16236685785304794\n",
      "The 28908 th iteration gives loss of 0.16236467779360797\n",
      "The 28909 th iteration gives loss of 0.16236249787924978\n",
      "The 28910 th iteration gives loss of 0.16236031810995363\n",
      "The 28911 th iteration gives loss of 0.16235813848570746\n",
      "The 28912 th iteration gives loss of 0.1623559590064788\n",
      "The 28913 th iteration gives loss of 0.16235377967225234\n",
      "The 28914 th iteration gives loss of 0.1623516004830036\n",
      "The 28915 th iteration gives loss of 0.16234942143871245\n",
      "The 28916 th iteration gives loss of 0.16234724253935068\n",
      "The 28917 th iteration gives loss of 0.16234506378491007\n",
      "The 28918 th iteration gives loss of 0.16234288517535778\n",
      "The 28919 th iteration gives loss of 0.16234070671066894\n",
      "The 28920 th iteration gives loss of 0.16233852839084378\n",
      "The 28921 th iteration gives loss of 0.16233635021583886\n",
      "The 28922 th iteration gives loss of 0.16233417218564772\n",
      "The 28923 th iteration gives loss of 0.16233199430024212\n",
      "The 28924 th iteration gives loss of 0.1623298165595974\n",
      "The 28925 th iteration gives loss of 0.1623276389636943\n",
      "The 28926 th iteration gives loss of 0.1623254615125141\n",
      "The 28927 th iteration gives loss of 0.1623232842060386\n",
      "The 28928 th iteration gives loss of 0.16232110704424557\n",
      "The 28929 th iteration gives loss of 0.1623189300270975\n",
      "The 28930 th iteration gives loss of 0.1623167531545935\n",
      "The 28931 th iteration gives loss of 0.1623145764267061\n",
      "The 28932 th iteration gives loss of 0.16231239984340567\n",
      "The 28933 th iteration gives loss of 0.16231022340468737\n",
      "The 28934 th iteration gives loss of 0.16230804711050786\n",
      "The 28935 th iteration gives loss of 0.16230587096087712\n",
      "The 28936 th iteration gives loss of 0.16230369495573627\n",
      "The 28937 th iteration gives loss of 0.16230151909508647\n",
      "The 28938 th iteration gives loss of 0.16229934337890756\n",
      "The 28939 th iteration gives loss of 0.16229716780718087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 28940 th iteration gives loss of 0.1622949923798698\n",
      "The 28941 th iteration gives loss of 0.16229281709695545\n",
      "The 28942 th iteration gives loss of 0.1622906419584275\n",
      "The 28943 th iteration gives loss of 0.16228846696425678\n",
      "The 28944 th iteration gives loss of 0.1622862921144312\n",
      "The 28945 th iteration gives loss of 0.16228411740891052\n",
      "The 28946 th iteration gives loss of 0.1622819428476947\n",
      "The 28947 th iteration gives loss of 0.16227976843074496\n",
      "The 28948 th iteration gives loss of 0.16227759415805043\n",
      "The 28949 th iteration gives loss of 0.16227542002960155\n",
      "The 28950 th iteration gives loss of 0.1622732460453503\n",
      "The 28951 th iteration gives loss of 0.16227107220529224\n",
      "The 28952 th iteration gives loss of 0.1622688985094003\n",
      "The 28953 th iteration gives loss of 0.1622667249576479\n",
      "The 28954 th iteration gives loss of 0.16226455155003158\n",
      "The 28955 th iteration gives loss of 0.16226237828652953\n",
      "The 28956 th iteration gives loss of 0.16226020516709524\n",
      "The 28957 th iteration gives loss of 0.16225803219172374\n",
      "The 28958 th iteration gives loss of 0.1622558593603976\n",
      "The 28959 th iteration gives loss of 0.16225368667308435\n",
      "The 28960 th iteration gives loss of 0.1622515141297755\n",
      "The 28961 th iteration gives loss of 0.16224934173044325\n",
      "The 28962 th iteration gives loss of 0.16224716947506368\n",
      "The 28963 th iteration gives loss of 0.1622449973636245\n",
      "The 28964 th iteration gives loss of 0.1622428253960868\n",
      "The 28965 th iteration gives loss of 0.16224065357245598\n",
      "The 28966 th iteration gives loss of 0.16223848189269002\n",
      "The 28967 th iteration gives loss of 0.16223631035676866\n",
      "The 28968 th iteration gives loss of 0.16223413896468747\n",
      "The 28969 th iteration gives loss of 0.16223196771639467\n",
      "The 28970 th iteration gives loss of 0.16222979661190162\n",
      "The 28971 th iteration gives loss of 0.1622276256511709\n",
      "The 28972 th iteration gives loss of 0.16222545483418982\n",
      "The 28973 th iteration gives loss of 0.16222328416092335\n",
      "The 28974 th iteration gives loss of 0.16222111363136255\n",
      "The 28975 th iteration gives loss of 0.16221894324548333\n",
      "The 28976 th iteration gives loss of 0.1622167730032579\n",
      "The 28977 th iteration gives loss of 0.16221460290467643\n",
      "The 28978 th iteration gives loss of 0.1622124329497102\n",
      "The 28979 th iteration gives loss of 0.1622102631383375\n",
      "The 28980 th iteration gives loss of 0.16220809347053547\n",
      "The 28981 th iteration gives loss of 0.16220592394628952\n",
      "The 28982 th iteration gives loss of 0.16220375456557573\n",
      "The 28983 th iteration gives loss of 0.1622015853283726\n",
      "The 28984 th iteration gives loss of 0.1621994162346607\n",
      "The 28985 th iteration gives loss of 0.1621972472844188\n",
      "The 28986 th iteration gives loss of 0.16219507847761486\n",
      "The 28987 th iteration gives loss of 0.162192909814247\n",
      "The 28988 th iteration gives loss of 0.16219074129427938\n",
      "The 28989 th iteration gives loss of 0.16218857291770947\n",
      "The 28990 th iteration gives loss of 0.16218640468449266\n",
      "The 28991 th iteration gives loss of 0.16218423659461093\n",
      "The 28992 th iteration gives loss of 0.16218206864805748\n",
      "The 28993 th iteration gives loss of 0.16217990084480274\n",
      "The 28994 th iteration gives loss of 0.16217773318482664\n",
      "The 28995 th iteration gives loss of 0.16217556566810834\n",
      "The 28996 th iteration gives loss of 0.16217339829462105\n",
      "The 28997 th iteration gives loss of 0.16217123106434894\n",
      "The 28998 th iteration gives loss of 0.16216906397727746\n",
      "The 28999 th iteration gives loss of 0.1621668970333859\n",
      "The 29000 th iteration gives loss of 0.16216473023263386\n",
      "The 29001 th iteration gives loss of 0.162162563575014\n",
      "The 29002 th iteration gives loss of 0.16216039706050592\n",
      "The 29003 th iteration gives loss of 0.1621582306890777\n",
      "The 29004 th iteration gives loss of 0.16215606446072808\n",
      "The 29005 th iteration gives loss of 0.16215389837542213\n",
      "The 29006 th iteration gives loss of 0.16215173243313974\n",
      "The 29007 th iteration gives loss of 0.16214956663387028\n",
      "The 29008 th iteration gives loss of 0.1621474009775802\n",
      "The 29009 th iteration gives loss of 0.16214523546424342\n",
      "The 29010 th iteration gives loss of 0.16214307009385295\n",
      "The 29011 th iteration gives loss of 0.16214090486638882\n",
      "The 29012 th iteration gives loss of 0.16213873978181215\n",
      "The 29013 th iteration gives loss of 0.1621365748401245\n",
      "The 29014 th iteration gives loss of 0.16213441004128323\n",
      "The 29015 th iteration gives loss of 0.16213224538528132\n",
      "The 29016 th iteration gives loss of 0.16213008087210357\n",
      "The 29017 th iteration gives loss of 0.16212791650171413\n",
      "The 29018 th iteration gives loss of 0.1621257522740906\n",
      "The 29019 th iteration gives loss of 0.16212358818922148\n",
      "The 29020 th iteration gives loss of 0.1621214242470863\n",
      "The 29021 th iteration gives loss of 0.16211926044766115\n",
      "The 29022 th iteration gives loss of 0.16211709679092445\n",
      "The 29023 th iteration gives loss of 0.16211493327684937\n",
      "The 29024 th iteration gives loss of 0.16211276990543194\n",
      "The 29025 th iteration gives loss of 0.16211060667663668\n",
      "The 29026 th iteration gives loss of 0.16210844359044396\n",
      "The 29027 th iteration gives loss of 0.1621062806468292\n",
      "The 29028 th iteration gives loss of 0.16210411784578507\n",
      "The 29029 th iteration gives loss of 0.1621019551872828\n",
      "The 29030 th iteration gives loss of 0.16209979267129868\n",
      "The 29031 th iteration gives loss of 0.16209763029781257\n",
      "The 29032 th iteration gives loss of 0.1620954680668025\n",
      "The 29033 th iteration gives loss of 0.16209330597825394\n",
      "The 29034 th iteration gives loss of 0.16209114403214586\n",
      "The 29035 th iteration gives loss of 0.1620889822284537\n",
      "The 29036 th iteration gives loss of 0.16208682056714807\n",
      "The 29037 th iteration gives loss of 0.16208465904822128\n",
      "The 29038 th iteration gives loss of 0.16208249767164484\n",
      "The 29039 th iteration gives loss of 0.16208033643740716\n",
      "The 29040 th iteration gives loss of 0.16207817534546942\n",
      "The 29041 th iteration gives loss of 0.16207601439582583\n",
      "The 29042 th iteration gives loss of 0.1620738535884503\n",
      "The 29043 th iteration gives loss of 0.16207169292333323\n",
      "The 29044 th iteration gives loss of 0.16206953240043112\n",
      "The 29045 th iteration gives loss of 0.16206737201974644\n",
      "The 29046 th iteration gives loss of 0.16206521178124003\n",
      "The 29047 th iteration gives loss of 0.16206305168489868\n",
      "The 29048 th iteration gives loss of 0.16206089173068966\n",
      "The 29049 th iteration gives loss of 0.16205873191861944\n",
      "The 29050 th iteration gives loss of 0.16205657224864226\n",
      "The 29051 th iteration gives loss of 0.1620544127207518\n",
      "The 29052 th iteration gives loss of 0.16205225333491405\n",
      "The 29053 th iteration gives loss of 0.1620500940911189\n",
      "The 29054 th iteration gives loss of 0.1620479349893462\n",
      "The 29055 th iteration gives loss of 0.16204577602956566\n",
      "The 29056 th iteration gives loss of 0.1620436172117596\n",
      "The 29057 th iteration gives loss of 0.1620414585359145\n",
      "The 29058 th iteration gives loss of 0.16203930000199535\n",
      "The 29059 th iteration gives loss of 0.1620371416099927\n",
      "The 29060 th iteration gives loss of 0.16203498335988256\n",
      "The 29061 th iteration gives loss of 0.16203282525164223\n",
      "The 29062 th iteration gives loss of 0.1620306672852598\n",
      "The 29063 th iteration gives loss of 0.16202850946070277\n",
      "The 29064 th iteration gives loss of 0.16202635177795333\n",
      "The 29065 th iteration gives loss of 0.16202419423698367\n",
      "The 29066 th iteration gives loss of 0.1620220368377981\n",
      "The 29067 th iteration gives loss of 0.16201987958034691\n",
      "The 29068 th iteration gives loss of 0.16201772246462456\n",
      "The 29069 th iteration gives loss of 0.16201556549060678\n",
      "The 29070 th iteration gives loss of 0.1620134086582716\n",
      "The 29071 th iteration gives loss of 0.16201125196759983\n",
      "The 29072 th iteration gives loss of 0.16200909541857658\n",
      "The 29073 th iteration gives loss of 0.1620069390111663\n",
      "The 29074 th iteration gives loss of 0.16200478274535976\n",
      "The 29075 th iteration gives loss of 0.1620026266211261\n",
      "The 29076 th iteration gives loss of 0.16200047063846285\n",
      "The 29077 th iteration gives loss of 0.16199831479733043\n",
      "The 29078 th iteration gives loss of 0.16199615909771417\n",
      "The 29079 th iteration gives loss of 0.16199400353959517\n",
      "The 29080 th iteration gives loss of 0.161991848122948\n",
      "The 29081 th iteration gives loss of 0.1619896928477623\n",
      "The 29082 th iteration gives loss of 0.1619875377140077\n",
      "The 29083 th iteration gives loss of 0.1619853827216604\n",
      "The 29084 th iteration gives loss of 0.16198322787071193\n",
      "The 29085 th iteration gives loss of 0.16198107316113303\n",
      "The 29086 th iteration gives loss of 0.1619789185929048\n",
      "The 29087 th iteration gives loss of 0.16197676416600398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 29088 th iteration gives loss of 0.1619746098804085\n",
      "The 29089 th iteration gives loss of 0.16197245573611144\n",
      "The 29090 th iteration gives loss of 0.16197030173307103\n",
      "The 29091 th iteration gives loss of 0.16196814787128003\n",
      "The 29092 th iteration gives loss of 0.1619659941507165\n",
      "The 29093 th iteration gives loss of 0.1619638405713609\n",
      "The 29094 th iteration gives loss of 0.1619616871331835\n",
      "The 29095 th iteration gives loss of 0.16195953383616793\n",
      "The 29096 th iteration gives loss of 0.16195738068029628\n",
      "The 29097 th iteration gives loss of 0.1619552276655562\n",
      "The 29098 th iteration gives loss of 0.16195307479190663\n",
      "The 29099 th iteration gives loss of 0.16195092205933834\n",
      "The 29100 th iteration gives loss of 0.16194876946783207\n",
      "The 29101 th iteration gives loss of 0.16194661701736024\n",
      "The 29102 th iteration gives loss of 0.16194446470790974\n",
      "The 29103 th iteration gives loss of 0.1619423125394556\n",
      "The 29104 th iteration gives loss of 0.1619401605119802\n",
      "The 29105 th iteration gives loss of 0.16193800862545832\n",
      "The 29106 th iteration gives loss of 0.16193585687986822\n",
      "The 29107 th iteration gives loss of 0.16193370527519763\n",
      "The 29108 th iteration gives loss of 0.16193155381141563\n",
      "The 29109 th iteration gives loss of 0.1619294024885139\n",
      "The 29110 th iteration gives loss of 0.1619272513064594\n",
      "The 29111 th iteration gives loss of 0.1619251002652341\n",
      "The 29112 th iteration gives loss of 0.16192294936482013\n",
      "The 29113 th iteration gives loss of 0.1619207986052007\n",
      "The 29114 th iteration gives loss of 0.1619186479863484\n",
      "The 29115 th iteration gives loss of 0.1619164975082449\n",
      "The 29116 th iteration gives loss of 0.16191434717086092\n",
      "The 29117 th iteration gives loss of 0.16191219697419207\n",
      "The 29118 th iteration gives loss of 0.16191004691820685\n",
      "The 29119 th iteration gives loss of 0.16190789700288977\n",
      "The 29120 th iteration gives loss of 0.16190574722821796\n",
      "The 29121 th iteration gives loss of 0.16190359759417294\n",
      "The 29122 th iteration gives loss of 0.16190144810071747\n",
      "The 29123 th iteration gives loss of 0.16189929874786244\n",
      "The 29124 th iteration gives loss of 0.16189714953556344\n",
      "The 29125 th iteration gives loss of 0.1618950004638032\n",
      "The 29126 th iteration gives loss of 0.1618928515325597\n",
      "The 29127 th iteration gives loss of 0.16189070274181777\n",
      "The 29128 th iteration gives loss of 0.1618885540915618\n",
      "The 29129 th iteration gives loss of 0.1618864055817639\n",
      "The 29130 th iteration gives loss of 0.16188425721239888\n",
      "The 29131 th iteration gives loss of 0.1618821089834598\n",
      "The 29132 th iteration gives loss of 0.16187996089490297\n",
      "The 29133 th iteration gives loss of 0.16187781294673065\n",
      "The 29134 th iteration gives loss of 0.16187566513891832\n",
      "The 29135 th iteration gives loss of 0.16187351747143153\n",
      "The 29136 th iteration gives loss of 0.16187136994427506\n",
      "The 29137 th iteration gives loss of 0.16186922255739375\n",
      "The 29138 th iteration gives loss of 0.16186707531079414\n",
      "The 29139 th iteration gives loss of 0.1618649282044401\n",
      "The 29140 th iteration gives loss of 0.16186278123833234\n",
      "The 29141 th iteration gives loss of 0.1618606344124242\n",
      "The 29142 th iteration gives loss of 0.1618584877267075\n",
      "The 29143 th iteration gives loss of 0.1618563411811523\n",
      "The 29144 th iteration gives loss of 0.1618541947757614\n",
      "The 29145 th iteration gives loss of 0.16185204851049567\n",
      "The 29146 th iteration gives loss of 0.16184990238533523\n",
      "The 29147 th iteration gives loss of 0.16184775640026225\n",
      "The 29148 th iteration gives loss of 0.1618456105552574\n",
      "The 29149 th iteration gives loss of 0.16184346485029558\n",
      "The 29150 th iteration gives loss of 0.1618413192853568\n",
      "The 29151 th iteration gives loss of 0.16183917386043764\n",
      "The 29152 th iteration gives loss of 0.16183702857548918\n",
      "The 29153 th iteration gives loss of 0.1618348834305029\n",
      "The 29154 th iteration gives loss of 0.16183273842546975\n",
      "The 29155 th iteration gives loss of 0.16183059356035404\n",
      "The 29156 th iteration gives loss of 0.16182844883514516\n",
      "The 29157 th iteration gives loss of 0.16182630424980707\n",
      "The 29158 th iteration gives loss of 0.1618241598043343\n",
      "The 29159 th iteration gives loss of 0.16182201549870878\n",
      "The 29160 th iteration gives loss of 0.16181987133289502\n",
      "The 29161 th iteration gives loss of 0.1618177273068877\n",
      "The 29162 th iteration gives loss of 0.16181558342064992\n",
      "The 29163 th iteration gives loss of 0.16181343967418196\n",
      "The 29164 th iteration gives loss of 0.16181129606744588\n",
      "The 29165 th iteration gives loss of 0.1618091526004227\n",
      "The 29166 th iteration gives loss of 0.1618070092730994\n",
      "The 29167 th iteration gives loss of 0.16180486608544975\n",
      "The 29168 th iteration gives loss of 0.16180272303745566\n",
      "The 29169 th iteration gives loss of 0.1618005801290981\n",
      "The 29170 th iteration gives loss of 0.1617984373603605\n",
      "The 29171 th iteration gives loss of 0.16179629473121102\n",
      "The 29172 th iteration gives loss of 0.16179415224163637\n",
      "The 29173 th iteration gives loss of 0.16179200989161394\n",
      "The 29174 th iteration gives loss of 0.16178986768112552\n",
      "The 29175 th iteration gives loss of 0.16178772561014845\n",
      "The 29176 th iteration gives loss of 0.16178558367865675\n",
      "The 29177 th iteration gives loss of 0.16178344188663465\n",
      "The 29178 th iteration gives loss of 0.16178130023406456\n",
      "The 29179 th iteration gives loss of 0.16177915872093213\n",
      "The 29180 th iteration gives loss of 0.1617770173472047\n",
      "The 29181 th iteration gives loss of 0.16177487611286287\n",
      "The 29182 th iteration gives loss of 0.16177273501788733\n",
      "The 29183 th iteration gives loss of 0.1617705940622678\n",
      "The 29184 th iteration gives loss of 0.16176845324597403\n",
      "The 29185 th iteration gives loss of 0.16176631256898275\n",
      "The 29186 th iteration gives loss of 0.161764172031279\n",
      "The 29187 th iteration gives loss of 0.1617620316328419\n",
      "The 29188 th iteration gives loss of 0.1617598913736458\n",
      "The 29189 th iteration gives loss of 0.16175775125368252\n",
      "The 29190 th iteration gives loss of 0.1617556112729273\n",
      "The 29191 th iteration gives loss of 0.16175347143134033\n",
      "The 29192 th iteration gives loss of 0.16175133172892608\n",
      "The 29193 th iteration gives loss of 0.16174919216565609\n",
      "The 29194 th iteration gives loss of 0.16174705274150558\n",
      "The 29195 th iteration gives loss of 0.1617449134564516\n",
      "The 29196 th iteration gives loss of 0.16174277431048453\n",
      "The 29197 th iteration gives loss of 0.16174063530358165\n",
      "The 29198 th iteration gives loss of 0.1617384964357212\n",
      "The 29199 th iteration gives loss of 0.16173635770687605\n",
      "The 29200 th iteration gives loss of 0.16173421911703492\n",
      "The 29201 th iteration gives loss of 0.16173208066616823\n",
      "The 29202 th iteration gives loss of 0.1617299423542629\n",
      "The 29203 th iteration gives loss of 0.16172780418129418\n",
      "The 29204 th iteration gives loss of 0.16172566614724126\n",
      "The 29205 th iteration gives loss of 0.1617235282520946\n",
      "The 29206 th iteration gives loss of 0.16172139049582074\n",
      "The 29207 th iteration gives loss of 0.16171925287840241\n",
      "The 29208 th iteration gives loss of 0.16171711539982195\n",
      "The 29209 th iteration gives loss of 0.16171497806005777\n",
      "The 29210 th iteration gives loss of 0.16171284085908844\n",
      "The 29211 th iteration gives loss of 0.1617107037968966\n",
      "The 29212 th iteration gives loss of 0.16170856687346027\n",
      "The 29213 th iteration gives loss of 0.16170643008875513\n",
      "The 29214 th iteration gives loss of 0.1617042934427776\n",
      "The 29215 th iteration gives loss of 0.16170215693547177\n",
      "The 29216 th iteration gives loss of 0.1617000205668487\n",
      "The 29217 th iteration gives loss of 0.16169788433688023\n",
      "The 29218 th iteration gives loss of 0.16169574824553884\n",
      "The 29219 th iteration gives loss of 0.1616936122928154\n",
      "The 29220 th iteration gives loss of 0.16169147647868284\n",
      "The 29221 th iteration gives loss of 0.16168934080312294\n",
      "The 29222 th iteration gives loss of 0.1616872052661138\n",
      "The 29223 th iteration gives loss of 0.16168506986763792\n",
      "The 29224 th iteration gives loss of 0.16168293460767139\n",
      "The 29225 th iteration gives loss of 0.161680799486192\n",
      "The 29226 th iteration gives loss of 0.16167866450318308\n",
      "The 29227 th iteration gives loss of 0.16167652965862953\n",
      "The 29228 th iteration gives loss of 0.1616743949524967\n",
      "The 29229 th iteration gives loss of 0.1616722603847753\n",
      "The 29230 th iteration gives loss of 0.16167012595544863\n",
      "The 29231 th iteration gives loss of 0.16166799166448\n",
      "The 29232 th iteration gives loss of 0.16166585751185988\n",
      "The 29233 th iteration gives loss of 0.16166372349757005\n",
      "The 29234 th iteration gives loss of 0.1616615896215861\n",
      "The 29235 th iteration gives loss of 0.16165945588389083\n",
      "The 29236 th iteration gives loss of 0.16165732228445723\n",
      "The 29237 th iteration gives loss of 0.16165518882327692\n",
      "The 29238 th iteration gives loss of 0.1616530555003139\n",
      "The 29239 th iteration gives loss of 0.1616509223155672\n",
      "The 29240 th iteration gives loss of 0.16164878926900558\n",
      "The 29241 th iteration gives loss of 0.16164665636060302\n",
      "The 29242 th iteration gives loss of 0.1616445235903425\n",
      "The 29243 th iteration gives loss of 0.16164239095821414\n",
      "The 29244 th iteration gives loss of 0.16164025846418678\n",
      "The 29245 th iteration gives loss of 0.1616381261082433\n",
      "The 29246 th iteration gives loss of 0.161635993890356\n",
      "The 29247 th iteration gives loss of 0.16163386181051684\n",
      "The 29248 th iteration gives loss of 0.16163172986870103\n",
      "The 29249 th iteration gives loss of 0.16162959806488697\n",
      "The 29250 th iteration gives loss of 0.1616274663990584\n",
      "The 29251 th iteration gives loss of 0.16162533487119013\n",
      "The 29252 th iteration gives loss of 0.16162320348126685\n",
      "The 29253 th iteration gives loss of 0.16162107222926575\n",
      "The 29254 th iteration gives loss of 0.16161894111515532\n",
      "The 29255 th iteration gives loss of 0.16161681013893348\n",
      "The 29256 th iteration gives loss of 0.16161467930057233\n",
      "The 29257 th iteration gives loss of 0.16161254860005708\n",
      "The 29258 th iteration gives loss of 0.16161041803735454\n",
      "The 29259 th iteration gives loss of 0.16160828761244647\n",
      "The 29260 th iteration gives loss of 0.1616061573253296\n",
      "The 29261 th iteration gives loss of 0.16160402717596317\n",
      "The 29262 th iteration gives loss of 0.16160189716434067\n",
      "The 29263 th iteration gives loss of 0.1615997672904375\n",
      "The 29264 th iteration gives loss of 0.16159763755423914\n",
      "The 29265 th iteration gives loss of 0.1615955079557141\n",
      "The 29266 th iteration gives loss of 0.16159337849484845\n",
      "The 29267 th iteration gives loss of 0.16159124917161166\n",
      "The 29268 th iteration gives loss of 0.16158911998601047\n",
      "The 29269 th iteration gives loss of 0.16158699093799267\n",
      "The 29270 th iteration gives loss of 0.16158486202755357\n",
      "The 29271 th iteration gives loss of 0.16158273325467434\n",
      "The 29272 th iteration gives loss of 0.1615806046193372\n",
      "The 29273 th iteration gives loss of 0.16157847612151316\n",
      "The 29274 th iteration gives loss of 0.16157634776117755\n",
      "The 29275 th iteration gives loss of 0.16157421953832854\n",
      "The 29276 th iteration gives loss of 0.16157209145293952\n",
      "The 29277 th iteration gives loss of 0.1615699635049817\n",
      "The 29278 th iteration gives loss of 0.1615678356944351\n",
      "The 29279 th iteration gives loss of 0.16156570802128392\n",
      "The 29280 th iteration gives loss of 0.1615635804855181\n",
      "The 29281 th iteration gives loss of 0.1615614530870999\n",
      "The 29282 th iteration gives loss of 0.16155932582602164\n",
      "The 29283 th iteration gives loss of 0.1615571987022563\n",
      "The 29284 th iteration gives loss of 0.16155507171578426\n",
      "The 29285 th iteration gives loss of 0.16155294486659522\n",
      "The 29286 th iteration gives loss of 0.1615508181546556\n",
      "The 29287 th iteration gives loss of 0.1615486915799463\n",
      "The 29288 th iteration gives loss of 0.1615465651424553\n",
      "The 29289 th iteration gives loss of 0.1615444388421636\n",
      "The 29290 th iteration gives loss of 0.16154231267904046\n",
      "The 29291 th iteration gives loss of 0.16154018665307282\n",
      "The 29292 th iteration gives loss of 0.1615380607642347\n",
      "The 29293 th iteration gives loss of 0.16153593501250824\n",
      "The 29294 th iteration gives loss of 0.16153380939788586\n",
      "The 29295 th iteration gives loss of 0.16153168392033568\n",
      "The 29296 th iteration gives loss of 0.16152955857982906\n",
      "The 29297 th iteration gives loss of 0.16152743337636621\n",
      "The 29298 th iteration gives loss of 0.16152530830990777\n",
      "The 29299 th iteration gives loss of 0.1615231833804473\n",
      "The 29300 th iteration gives loss of 0.1615210585879516\n",
      "The 29301 th iteration gives loss of 0.16151893393242311\n",
      "The 29302 th iteration gives loss of 0.1615168094138119\n",
      "The 29303 th iteration gives loss of 0.1615146850321238\n",
      "The 29304 th iteration gives loss of 0.16151256078732631\n",
      "The 29305 th iteration gives loss of 0.1615104366794018\n",
      "The 29306 th iteration gives loss of 0.16150831270832672\n",
      "The 29307 th iteration gives loss of 0.16150618887408213\n",
      "The 29308 th iteration gives loss of 0.16150406517665647\n",
      "The 29309 th iteration gives loss of 0.1615019416160135\n",
      "The 29310 th iteration gives loss of 0.1614998181921457\n",
      "The 29311 th iteration gives loss of 0.1614976949050343\n",
      "The 29312 th iteration gives loss of 0.1614955717546479\n",
      "The 29313 th iteration gives loss of 0.16149344874098656\n",
      "The 29314 th iteration gives loss of 0.16149132586400525\n",
      "The 29315 th iteration gives loss of 0.16148920312369328\n",
      "The 29316 th iteration gives loss of 0.16148708052003852\n",
      "The 29317 th iteration gives loss of 0.16148495805301025\n",
      "The 29318 th iteration gives loss of 0.16148283572260036\n",
      "The 29319 th iteration gives loss of 0.16148071352877758\n",
      "The 29320 th iteration gives loss of 0.16147859147152369\n",
      "The 29321 th iteration gives loss of 0.16147646955082845\n",
      "The 29322 th iteration gives loss of 0.16147434776665656\n",
      "The 29323 th iteration gives loss of 0.1614722261190064\n",
      "The 29324 th iteration gives loss of 0.1614701046078348\n",
      "The 29325 th iteration gives loss of 0.16146798323313644\n",
      "The 29326 th iteration gives loss of 0.1614658619948925\n",
      "The 29327 th iteration gives loss of 0.16146374089308227\n",
      "The 29328 th iteration gives loss of 0.16146161992767982\n",
      "The 29329 th iteration gives loss of 0.16145949909866433\n",
      "The 29330 th iteration gives loss of 0.16145737840602822\n",
      "The 29331 th iteration gives loss of 0.1614552578497383\n",
      "The 29332 th iteration gives loss of 0.16145313742978032\n",
      "The 29333 th iteration gives loss of 0.16145101714612778\n",
      "The 29334 th iteration gives loss of 0.1614488969987783\n",
      "The 29335 th iteration gives loss of 0.16144677698769186\n",
      "The 29336 th iteration gives loss of 0.1614446571128573\n",
      "The 29337 th iteration gives loss of 0.16144253737425257\n",
      "The 29338 th iteration gives loss of 0.16144041777186424\n",
      "The 29339 th iteration gives loss of 0.1614382983056605\n",
      "The 29340 th iteration gives loss of 0.16143617897563164\n",
      "The 29341 th iteration gives loss of 0.16143405978174794\n",
      "The 29342 th iteration gives loss of 0.16143194072400557\n",
      "The 29343 th iteration gives loss of 0.16142982180236845\n",
      "The 29344 th iteration gives loss of 0.161427703016819\n",
      "The 29345 th iteration gives loss of 0.16142558436733964\n",
      "The 29346 th iteration gives loss of 0.16142346585391443\n",
      "The 29347 th iteration gives loss of 0.16142134747652342\n",
      "The 29348 th iteration gives loss of 0.16141922923514299\n",
      "The 29349 th iteration gives loss of 0.16141711112975712\n",
      "The 29350 th iteration gives loss of 0.16141499316033622\n",
      "The 29351 th iteration gives loss of 0.16141287532686183\n",
      "The 29352 th iteration gives loss of 0.16141075762932774\n",
      "The 29353 th iteration gives loss of 0.16140864006770414\n",
      "The 29354 th iteration gives loss of 0.16140652264197702\n",
      "The 29355 th iteration gives loss of 0.16140440535211273\n",
      "The 29356 th iteration gives loss of 0.16140228819810126\n",
      "The 29357 th iteration gives loss of 0.1614001711799203\n",
      "The 29358 th iteration gives loss of 0.1613980542975556\n",
      "The 29359 th iteration gives loss of 0.16139593755098375\n",
      "The 29360 th iteration gives loss of 0.16139382094017582\n",
      "The 29361 th iteration gives loss of 0.16139170446512774\n",
      "The 29362 th iteration gives loss of 0.16138958812580362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 29363 th iteration gives loss of 0.1613874719221997\n",
      "The 29364 th iteration gives loss of 0.16138535585428868\n",
      "The 29365 th iteration gives loss of 0.16138323992204545\n",
      "The 29366 th iteration gives loss of 0.16138112412546016\n",
      "The 29367 th iteration gives loss of 0.16137900846449743\n",
      "The 29368 th iteration gives loss of 0.16137689293915278\n",
      "The 29369 th iteration gives loss of 0.16137477754940754\n",
      "The 29370 th iteration gives loss of 0.16137266229523062\n",
      "The 29371 th iteration gives loss of 0.16137054717660304\n",
      "The 29372 th iteration gives loss of 0.16136843219351082\n",
      "The 29373 th iteration gives loss of 0.16136631734593346\n",
      "The 29374 th iteration gives loss of 0.1613642026338451\n",
      "The 29375 th iteration gives loss of 0.16136208805723495\n",
      "The 29376 th iteration gives loss of 0.1613599736160712\n",
      "The 29377 th iteration gives loss of 0.1613578593103553\n",
      "The 29378 th iteration gives loss of 0.16135574514004225\n",
      "The 29379 th iteration gives loss of 0.16135363110512346\n",
      "The 29380 th iteration gives loss of 0.16135151720558089\n",
      "The 29381 th iteration gives loss of 0.1613494034413967\n",
      "The 29382 th iteration gives loss of 0.16134728981253946\n",
      "The 29383 th iteration gives loss of 0.16134517631900183\n",
      "The 29384 th iteration gives loss of 0.16134306296075823\n",
      "The 29385 th iteration gives loss of 0.16134094973779772\n",
      "The 29386 th iteration gives loss of 0.16133883665008003\n",
      "The 29387 th iteration gives loss of 0.16133672369760202\n",
      "The 29388 th iteration gives loss of 0.16133461088033513\n",
      "The 29389 th iteration gives loss of 0.16133249819826503\n",
      "The 29390 th iteration gives loss of 0.16133038565137434\n",
      "The 29391 th iteration gives loss of 0.1613282732396472\n",
      "The 29392 th iteration gives loss of 0.1613261609630502\n",
      "The 29393 th iteration gives loss of 0.16132404882156567\n",
      "The 29394 th iteration gives loss of 0.1613219368151717\n",
      "The 29395 th iteration gives loss of 0.1613198249438709\n",
      "The 29396 th iteration gives loss of 0.16131771320761315\n",
      "The 29397 th iteration gives loss of 0.16131560160640032\n",
      "The 29398 th iteration gives loss of 0.16131349014019997\n",
      "The 29399 th iteration gives loss of 0.1613113788089992\n",
      "The 29400 th iteration gives loss of 0.16130926761277425\n",
      "The 29401 th iteration gives loss of 0.1613071565515096\n",
      "The 29402 th iteration gives loss of 0.1613050456251794\n",
      "The 29403 th iteration gives loss of 0.16130293483377545\n",
      "The 29404 th iteration gives loss of 0.16130082417726446\n",
      "The 29405 th iteration gives loss of 0.1612987136556378\n",
      "The 29406 th iteration gives loss of 0.1612966032688607\n",
      "The 29407 th iteration gives loss of 0.16129449301692544\n",
      "The 29408 th iteration gives loss of 0.16129238289980968\n",
      "The 29409 th iteration gives loss of 0.1612902729174965\n",
      "The 29410 th iteration gives loss of 0.16128816306996493\n",
      "The 29411 th iteration gives loss of 0.16128605335719656\n",
      "The 29412 th iteration gives loss of 0.16128394377916186\n",
      "The 29413 th iteration gives loss of 0.16128183433584542\n",
      "The 29414 th iteration gives loss of 0.16127972502723037\n",
      "The 29415 th iteration gives loss of 0.16127761585330377\n",
      "The 29416 th iteration gives loss of 0.16127550681403394\n",
      "The 29417 th iteration gives loss of 0.16127339790940734\n",
      "The 29418 th iteration gives loss of 0.16127128913940791\n",
      "The 29419 th iteration gives loss of 0.16126918050399774\n",
      "The 29420 th iteration gives loss of 0.1612670720031744\n",
      "The 29421 th iteration gives loss of 0.16126496363691528\n",
      "The 29422 th iteration gives loss of 0.16126285540520102\n",
      "The 29423 th iteration gives loss of 0.1612607473080134\n",
      "The 29424 th iteration gives loss of 0.16125863934532145\n",
      "The 29425 th iteration gives loss of 0.16125653151712283\n",
      "The 29426 th iteration gives loss of 0.16125442382337712\n",
      "The 29427 th iteration gives loss of 0.16125231626407974\n",
      "The 29428 th iteration gives loss of 0.16125020883920346\n",
      "The 29429 th iteration gives loss of 0.16124810154874356\n",
      "The 29430 th iteration gives loss of 0.16124599439266102\n",
      "The 29431 th iteration gives loss of 0.1612438873709391\n",
      "The 29432 th iteration gives loss of 0.16124178048357712\n",
      "The 29433 th iteration gives loss of 0.16123967373053066\n",
      "The 29434 th iteration gives loss of 0.16123756711180176\n",
      "The 29435 th iteration gives loss of 0.16123546062735328\n",
      "The 29436 th iteration gives loss of 0.16123335427716565\n",
      "The 29437 th iteration gives loss of 0.16123124806123504\n",
      "The 29438 th iteration gives loss of 0.16122914197952987\n",
      "The 29439 th iteration gives loss of 0.16122703603202448\n",
      "The 29440 th iteration gives loss of 0.16122493021871792\n",
      "The 29441 th iteration gives loss of 0.16122282453957512\n",
      "The 29442 th iteration gives loss of 0.16122071899458162\n",
      "The 29443 th iteration gives loss of 0.16121861358371847\n",
      "The 29444 th iteration gives loss of 0.16121650830696233\n",
      "The 29445 th iteration gives loss of 0.16121440316429952\n",
      "The 29446 th iteration gives loss of 0.16121229815570692\n",
      "The 29447 th iteration gives loss of 0.161210193281167\n",
      "The 29448 th iteration gives loss of 0.16120808854066257\n",
      "The 29449 th iteration gives loss of 0.16120598393416213\n",
      "The 29450 th iteration gives loss of 0.16120387946164652\n",
      "The 29451 th iteration gives loss of 0.1612017751231175\n",
      "The 29452 th iteration gives loss of 0.16119967091854037\n",
      "The 29453 th iteration gives loss of 0.16119756684788655\n",
      "The 29454 th iteration gives loss of 0.16119546291114675\n",
      "The 29455 th iteration gives loss of 0.16119335910831242\n",
      "The 29456 th iteration gives loss of 0.16119125543934903\n",
      "The 29457 th iteration gives loss of 0.16118915190423677\n",
      "The 29458 th iteration gives loss of 0.1611870485029595\n",
      "The 29459 th iteration gives loss of 0.16118494523549387\n",
      "The 29460 th iteration gives loss of 0.1611828421018296\n",
      "The 29461 th iteration gives loss of 0.16118073910194236\n",
      "The 29462 th iteration gives loss of 0.16117863623581125\n",
      "The 29463 th iteration gives loss of 0.16117653350341637\n",
      "The 29464 th iteration gives loss of 0.1611744309047346\n",
      "The 29465 th iteration gives loss of 0.1611723284397527\n",
      "The 29466 th iteration gives loss of 0.16117022610844464\n",
      "The 29467 th iteration gives loss of 0.16116812391079988\n",
      "The 29468 th iteration gives loss of 0.16116602184679385\n",
      "The 29469 th iteration gives loss of 0.16116391991641096\n",
      "The 29470 th iteration gives loss of 0.16116181811961774\n",
      "The 29471 th iteration gives loss of 0.16115971645641478\n",
      "The 29472 th iteration gives loss of 0.161157614926769\n",
      "The 29473 th iteration gives loss of 0.1611555135306653\n",
      "The 29474 th iteration gives loss of 0.16115341226807894\n",
      "The 29475 th iteration gives loss of 0.1611513111390113\n",
      "The 29476 th iteration gives loss of 0.161149210143412\n",
      "The 29477 th iteration gives loss of 0.16114710928127784\n",
      "The 29478 th iteration gives loss of 0.16114500855258165\n",
      "The 29479 th iteration gives loss of 0.16114290795730718\n",
      "The 29480 th iteration gives loss of 0.16114080749545154\n",
      "The 29481 th iteration gives loss of 0.16113870716697307\n",
      "The 29482 th iteration gives loss of 0.16113660697185697\n",
      "The 29483 th iteration gives loss of 0.16113450691008585\n",
      "The 29484 th iteration gives loss of 0.16113240698163694\n",
      "The 29485 th iteration gives loss of 0.16113030718651433\n",
      "The 29486 th iteration gives loss of 0.1611282075246619\n",
      "The 29487 th iteration gives loss of 0.16112610799608237\n",
      "The 29488 th iteration gives loss of 0.16112400860074313\n",
      "The 29489 th iteration gives loss of 0.16112190933864015\n",
      "The 29490 th iteration gives loss of 0.161119810209747\n",
      "The 29491 th iteration gives loss of 0.16111771121404003\n",
      "The 29492 th iteration gives loss of 0.16111561235151017\n",
      "The 29493 th iteration gives loss of 0.16111351362211793\n",
      "The 29494 th iteration gives loss of 0.16111141502586598\n",
      "The 29495 th iteration gives loss of 0.16110931656271527\n",
      "The 29496 th iteration gives loss of 0.16110721823266788\n",
      "The 29497 th iteration gives loss of 0.161105120035699\n",
      "The 29498 th iteration gives loss of 0.1611030219717716\n",
      "The 29499 th iteration gives loss of 0.16110092404088144\n",
      "The 29500 th iteration gives loss of 0.1610988262430043\n",
      "The 29501 th iteration gives loss of 0.1610967285781168\n",
      "The 29502 th iteration gives loss of 0.16109463104620667\n",
      "The 29503 th iteration gives loss of 0.16109253364725878\n",
      "The 29504 th iteration gives loss of 0.16109043638123946\n",
      "The 29505 th iteration gives loss of 0.16108833924814192\n",
      "The 29506 th iteration gives loss of 0.1610862422479422\n",
      "The 29507 th iteration gives loss of 0.16108414538061486\n",
      "The 29508 th iteration gives loss of 0.16108204864614714\n",
      "The 29509 th iteration gives loss of 0.16107995204452785\n",
      "The 29510 th iteration gives loss of 0.1610778555757159\n",
      "The 29511 th iteration gives loss of 0.16107575923970693\n",
      "The 29512 th iteration gives loss of 0.16107366303647247\n",
      "The 29513 th iteration gives loss of 0.16107156696601022\n",
      "The 29514 th iteration gives loss of 0.16106947102828287\n",
      "The 29515 th iteration gives loss of 0.1610673752232825\n",
      "The 29516 th iteration gives loss of 0.16106527955097952\n",
      "The 29517 th iteration gives loss of 0.1610631840113627\n",
      "The 29518 th iteration gives loss of 0.16106108860441362\n",
      "The 29519 th iteration gives loss of 0.16105899333010007\n",
      "The 29520 th iteration gives loss of 0.16105689818841848\n",
      "The 29521 th iteration gives loss of 0.16105480317933676\n",
      "The 29522 th iteration gives loss of 0.16105270830285154\n",
      "The 29523 th iteration gives loss of 0.16105061355891978\n",
      "The 29524 th iteration gives loss of 0.16104851894754485\n",
      "The 29525 th iteration gives loss of 0.1610464244686957\n",
      "The 29526 th iteration gives loss of 0.16104433012235542\n",
      "The 29527 th iteration gives loss of 0.1610422359084965\n",
      "The 29528 th iteration gives loss of 0.16104014182711782\n",
      "The 29529 th iteration gives loss of 0.1610380478781891\n",
      "The 29530 th iteration gives loss of 0.1610359540616837\n",
      "The 29531 th iteration gives loss of 0.16103386037759662\n",
      "The 29532 th iteration gives loss of 0.16103176682590242\n",
      "The 29533 th iteration gives loss of 0.16102967340657523\n",
      "The 29534 th iteration gives loss of 0.16102758011960636\n",
      "The 29535 th iteration gives loss of 0.16102548696496344\n",
      "The 29536 th iteration gives loss of 0.16102339394264523\n",
      "The 29537 th iteration gives loss of 0.16102130105261964\n",
      "The 29538 th iteration gives loss of 0.16101920829487265\n",
      "The 29539 th iteration gives loss of 0.16101711566937726\n",
      "The 29540 th iteration gives loss of 0.16101502317611516\n",
      "The 29541 th iteration gives loss of 0.16101293081507853\n",
      "The 29542 th iteration gives loss of 0.16101083858623275\n",
      "The 29543 th iteration gives loss of 0.16100874648957014\n",
      "The 29544 th iteration gives loss of 0.1610066545250725\n",
      "The 29545 th iteration gives loss of 0.16100456269271074\n",
      "The 29546 th iteration gives loss of 0.1610024709924706\n",
      "The 29547 th iteration gives loss of 0.16100037942433285\n",
      "The 29548 th iteration gives loss of 0.1609982879882785\n",
      "The 29549 th iteration gives loss of 0.1609961966842859\n",
      "The 29550 th iteration gives loss of 0.16099410551233992\n",
      "The 29551 th iteration gives loss of 0.16099201447240832\n",
      "The 29552 th iteration gives loss of 0.16098992356449227\n",
      "The 29553 th iteration gives loss of 0.16098783278856105\n",
      "The 29554 th iteration gives loss of 0.16098574214459682\n",
      "The 29555 th iteration gives loss of 0.16098365163257336\n",
      "The 29556 th iteration gives loss of 0.1609815612524796\n",
      "The 29557 th iteration gives loss of 0.16097947100429966\n",
      "The 29558 th iteration gives loss of 0.1609773808880028\n",
      "The 29559 th iteration gives loss of 0.16097529090357807\n",
      "The 29560 th iteration gives loss of 0.16097320105101093\n",
      "The 29561 th iteration gives loss of 0.1609711113302735\n",
      "The 29562 th iteration gives loss of 0.16096902174134206\n",
      "The 29563 th iteration gives loss of 0.16096693228420128\n",
      "The 29564 th iteration gives loss of 0.16096484295883745\n",
      "The 29565 th iteration gives loss of 0.16096275376523012\n",
      "The 29566 th iteration gives loss of 0.16096066470335818\n",
      "The 29567 th iteration gives loss of 0.16095857577319786\n",
      "The 29568 th iteration gives loss of 0.16095648697473744\n",
      "The 29569 th iteration gives loss of 0.1609543983079568\n",
      "The 29570 th iteration gives loss of 0.1609523097728245\n",
      "The 29571 th iteration gives loss of 0.16095022136933865\n",
      "The 29572 th iteration gives loss of 0.16094813309747344\n",
      "The 29573 th iteration gives loss of 0.1609460449572045\n",
      "The 29574 th iteration gives loss of 0.16094395694851818\n",
      "The 29575 th iteration gives loss of 0.16094186907138944\n",
      "The 29576 th iteration gives loss of 0.16093978132580575\n",
      "The 29577 th iteration gives loss of 0.1609376937117448\n",
      "The 29578 th iteration gives loss of 0.16093560622918757\n",
      "The 29579 th iteration gives loss of 0.16093351887811705\n",
      "The 29580 th iteration gives loss of 0.16093143165851012\n",
      "The 29581 th iteration gives loss of 0.16092934457035538\n",
      "The 29582 th iteration gives loss of 0.16092725761361587\n",
      "The 29583 th iteration gives loss of 0.16092517078828875\n",
      "The 29584 th iteration gives loss of 0.16092308409435196\n",
      "The 29585 th iteration gives loss of 0.16092099753178324\n",
      "The 29586 th iteration gives loss of 0.16091891110056472\n",
      "The 29587 th iteration gives loss of 0.16091682480067368\n",
      "The 29588 th iteration gives loss of 0.16091473863210587\n",
      "The 29589 th iteration gives loss of 0.16091265259482007\n",
      "The 29590 th iteration gives loss of 0.16091056668881126\n",
      "The 29591 th iteration gives loss of 0.16090848091405632\n",
      "The 29592 th iteration gives loss of 0.16090639527053405\n",
      "The 29593 th iteration gives loss of 0.1609043097582236\n",
      "The 29594 th iteration gives loss of 0.16090222437711857\n",
      "The 29595 th iteration gives loss of 0.1609001391271847\n",
      "The 29596 th iteration gives loss of 0.16089805400841467\n",
      "The 29597 th iteration gives loss of 0.1608959690207801\n",
      "The 29598 th iteration gives loss of 0.16089388416425923\n",
      "The 29599 th iteration gives loss of 0.16089179943884208\n",
      "The 29600 th iteration gives loss of 0.160889714844508\n",
      "The 29601 th iteration gives loss of 0.16088763038124063\n",
      "The 29602 th iteration gives loss of 0.16088554604900734\n",
      "The 29603 th iteration gives loss of 0.16088346184780664\n",
      "The 29604 th iteration gives loss of 0.16088137777760697\n",
      "The 29605 th iteration gives loss of 0.16087929383839344\n",
      "The 29606 th iteration gives loss of 0.16087721003013836\n",
      "The 29607 th iteration gives loss of 0.16087512635284307\n",
      "The 29608 th iteration gives loss of 0.16087304280646883\n",
      "The 29609 th iteration gives loss of 0.16087095939100407\n",
      "The 29610 th iteration gives loss of 0.1608688761064317\n",
      "The 29611 th iteration gives loss of 0.160866792952722\n",
      "The 29612 th iteration gives loss of 0.16086470992986826\n",
      "The 29613 th iteration gives loss of 0.16086262703784987\n",
      "The 29614 th iteration gives loss of 0.16086054427664656\n",
      "The 29615 th iteration gives loss of 0.16085846164622625\n",
      "The 29616 th iteration gives loss of 0.16085637914658893\n",
      "The 29617 th iteration gives loss of 0.16085429677771046\n",
      "The 29618 th iteration gives loss of 0.16085221453955925\n",
      "The 29619 th iteration gives loss of 0.16085013243213625\n",
      "The 29620 th iteration gives loss of 0.16084805045540423\n",
      "The 29621 th iteration gives loss of 0.16084596860935582\n",
      "The 29622 th iteration gives loss of 0.1608438868939614\n",
      "The 29623 th iteration gives loss of 0.16084180530920922\n",
      "The 29624 th iteration gives loss of 0.16083972385508083\n",
      "The 29625 th iteration gives loss of 0.16083764253155344\n",
      "The 29626 th iteration gives loss of 0.16083556133861068\n",
      "The 29627 th iteration gives loss of 0.1608334802762381\n",
      "The 29628 th iteration gives loss of 0.1608313993444078\n",
      "The 29629 th iteration gives loss of 0.16082931854310667\n",
      "The 29630 th iteration gives loss of 0.16082723787230555\n",
      "The 29631 th iteration gives loss of 0.16082515733199926\n",
      "The 29632 th iteration gives loss of 0.16082307692216266\n",
      "The 29633 th iteration gives loss of 0.16082099664277286\n",
      "The 29634 th iteration gives loss of 0.16081891649381447\n",
      "The 29635 th iteration gives loss of 0.1608168364752658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 29636 th iteration gives loss of 0.1608147565871146\n",
      "The 29637 th iteration gives loss of 0.1608126768293363\n",
      "The 29638 th iteration gives loss of 0.1608105972019069\n",
      "The 29639 th iteration gives loss of 0.16080851770482274\n",
      "The 29640 th iteration gives loss of 0.16080643833805106\n",
      "The 29641 th iteration gives loss of 0.16080435910157842\n",
      "The 29642 th iteration gives loss of 0.16080227999538438\n",
      "The 29643 th iteration gives loss of 0.16080020101944378\n",
      "The 29644 th iteration gives loss of 0.16079812217375689\n",
      "The 29645 th iteration gives loss of 0.16079604345828097\n",
      "The 29646 th iteration gives loss of 0.16079396487301154\n",
      "The 29647 th iteration gives loss of 0.16079188641792916\n",
      "The 29648 th iteration gives loss of 0.1607898080929963\n",
      "The 29649 th iteration gives loss of 0.16078772989821988\n",
      "The 29650 th iteration gives loss of 0.16078565183357046\n",
      "The 29651 th iteration gives loss of 0.16078357389902595\n",
      "The 29652 th iteration gives loss of 0.16078149609456532\n",
      "The 29653 th iteration gives loss of 0.16077941842018972\n",
      "The 29654 th iteration gives loss of 0.16077734087584614\n",
      "The 29655 th iteration gives loss of 0.16077526346154156\n",
      "The 29656 th iteration gives loss of 0.1607731861772465\n",
      "The 29657 th iteration gives loss of 0.16077110902294492\n",
      "The 29658 th iteration gives loss of 0.16076903199862297\n",
      "The 29659 th iteration gives loss of 0.16076695510425473\n",
      "The 29660 th iteration gives loss of 0.160764878339823\n",
      "The 29661 th iteration gives loss of 0.1607628017053091\n",
      "The 29662 th iteration gives loss of 0.16076072520069076\n",
      "The 29663 th iteration gives loss of 0.1607586488259479\n",
      "The 29664 th iteration gives loss of 0.16075657258107073\n",
      "The 29665 th iteration gives loss of 0.16075449646603862\n",
      "The 29666 th iteration gives loss of 0.16075242048082125\n",
      "The 29667 th iteration gives loss of 0.16075034462541152\n",
      "The 29668 th iteration gives loss of 0.16074826889978613\n",
      "The 29669 th iteration gives loss of 0.1607461933039206\n",
      "The 29670 th iteration gives loss of 0.160744117837808\n",
      "The 29671 th iteration gives loss of 0.16074204250142377\n",
      "The 29672 th iteration gives loss of 0.16073996729474743\n",
      "The 29673 th iteration gives loss of 0.16073789221776327\n",
      "The 29674 th iteration gives loss of 0.16073581727044306\n",
      "The 29675 th iteration gives loss of 0.16073374245277194\n",
      "The 29676 th iteration gives loss of 0.16073166776473707\n",
      "The 29677 th iteration gives loss of 0.16072959320632454\n",
      "The 29678 th iteration gives loss of 0.16072751877750024\n",
      "The 29679 th iteration gives loss of 0.16072544447824294\n",
      "The 29680 th iteration gives loss of 0.16072337030855513\n",
      "The 29681 th iteration gives loss of 0.16072129626840204\n",
      "The 29682 th iteration gives loss of 0.16071922235777317\n",
      "The 29683 th iteration gives loss of 0.16071714857663838\n",
      "The 29684 th iteration gives loss of 0.16071507492498624\n",
      "The 29685 th iteration gives loss of 0.16071300140280106\n",
      "The 29686 th iteration gives loss of 0.160710928010049\n",
      "The 29687 th iteration gives loss of 0.1607088547467289\n",
      "The 29688 th iteration gives loss of 0.16070678161280566\n",
      "The 29689 th iteration gives loss of 0.16070470860827768\n",
      "The 29690 th iteration gives loss of 0.1607026357331183\n",
      "The 29691 th iteration gives loss of 0.16070056298730917\n",
      "The 29692 th iteration gives loss of 0.1606984903708262\n",
      "The 29693 th iteration gives loss of 0.1606964178836604\n",
      "The 29694 th iteration gives loss of 0.16069434552577563\n",
      "The 29695 th iteration gives loss of 0.16069227329717184\n",
      "The 29696 th iteration gives loss of 0.16069020119782204\n",
      "The 29697 th iteration gives loss of 0.16068812922770484\n",
      "The 29698 th iteration gives loss of 0.16068605738680689\n",
      "The 29699 th iteration gives loss of 0.16068398567510586\n",
      "The 29700 th iteration gives loss of 0.16068191409258603\n",
      "The 29701 th iteration gives loss of 0.16067984263922047\n",
      "The 29702 th iteration gives loss of 0.16067777131500002\n",
      "The 29703 th iteration gives loss of 0.16067570011990676\n",
      "The 29704 th iteration gives loss of 0.16067362905391339\n",
      "The 29705 th iteration gives loss of 0.16067155811700481\n",
      "The 29706 th iteration gives loss of 0.16066948730916553\n",
      "The 29707 th iteration gives loss of 0.16066741663036707\n",
      "The 29708 th iteration gives loss of 0.16066534608059158\n",
      "The 29709 th iteration gives loss of 0.1606632756598357\n",
      "The 29710 th iteration gives loss of 0.1606612053680671\n",
      "The 29711 th iteration gives loss of 0.1606591352052725\n",
      "The 29712 th iteration gives loss of 0.1606570651714229\n",
      "The 29713 th iteration gives loss of 0.16065499526652244\n",
      "The 29714 th iteration gives loss of 0.16065292549051877\n",
      "The 29715 th iteration gives loss of 0.16065085584341549\n",
      "The 29716 th iteration gives loss of 0.16064878632520307\n",
      "The 29717 th iteration gives loss of 0.16064671693583318\n",
      "The 29718 th iteration gives loss of 0.16064464767531683\n",
      "The 29719 th iteration gives loss of 0.1606425785436087\n",
      "The 29720 th iteration gives loss of 0.16064050954071044\n",
      "The 29721 th iteration gives loss of 0.1606384406665963\n",
      "The 29722 th iteration gives loss of 0.16063637192124575\n",
      "The 29723 th iteration gives loss of 0.16063430330463901\n",
      "The 29724 th iteration gives loss of 0.16063223481675484\n",
      "The 29725 th iteration gives loss of 0.16063016645759096\n",
      "The 29726 th iteration gives loss of 0.16062809822710578\n",
      "The 29727 th iteration gives loss of 0.16062603012529517\n",
      "The 29728 th iteration gives loss of 0.16062396215213312\n",
      "The 29729 th iteration gives loss of 0.16062189430760584\n",
      "The 29730 th iteration gives loss of 0.16061982659169596\n",
      "The 29731 th iteration gives loss of 0.16061775900437625\n",
      "The 29732 th iteration gives loss of 0.16061569154563332\n",
      "The 29733 th iteration gives loss of 0.16061362421545708\n",
      "The 29734 th iteration gives loss of 0.16061155701380894\n",
      "The 29735 th iteration gives loss of 0.16060948994068477\n",
      "The 29736 th iteration gives loss of 0.16060742299606437\n",
      "The 29737 th iteration gives loss of 0.16060535617992625\n",
      "The 29738 th iteration gives loss of 0.16060328949224673\n",
      "The 29739 th iteration gives loss of 0.16060122293301862\n",
      "The 29740 th iteration gives loss of 0.16059915650221723\n",
      "The 29741 th iteration gives loss of 0.1605970901998183\n",
      "The 29742 th iteration gives loss of 0.16059502402580986\n",
      "The 29743 th iteration gives loss of 0.16059295798018372\n",
      "The 29744 th iteration gives loss of 0.1605908920629003\n",
      "The 29745 th iteration gives loss of 0.16058882627394988\n",
      "The 29746 th iteration gives loss of 0.16058676061330163\n",
      "The 29747 th iteration gives loss of 0.16058469508096537\n",
      "The 29748 th iteration gives loss of 0.16058262967690115\n",
      "The 29749 th iteration gives loss of 0.1605805644010909\n",
      "The 29750 th iteration gives loss of 0.1605784992535248\n",
      "The 29751 th iteration gives loss of 0.16057643423417522\n",
      "The 29752 th iteration gives loss of 0.16057436934302616\n",
      "The 29753 th iteration gives loss of 0.1605723045800652\n",
      "The 29754 th iteration gives loss of 0.16057023994526523\n",
      "The 29755 th iteration gives loss of 0.16056817543861665\n",
      "The 29756 th iteration gives loss of 0.1605661110600913\n",
      "The 29757 th iteration gives loss of 0.16056404680966826\n",
      "The 29758 th iteration gives loss of 0.16056198268733654\n",
      "The 29759 th iteration gives loss of 0.16055991869307928\n",
      "The 29760 th iteration gives loss of 0.1605578548268734\n",
      "The 29761 th iteration gives loss of 0.16055579108870327\n",
      "The 29762 th iteration gives loss of 0.16055372747854618\n",
      "The 29763 th iteration gives loss of 0.16055166399638418\n",
      "The 29764 th iteration gives loss of 0.1605496006421983\n",
      "The 29765 th iteration gives loss of 0.16054753741597397\n",
      "The 29766 th iteration gives loss of 0.1605454743176867\n",
      "The 29767 th iteration gives loss of 0.1605434113473217\n",
      "The 29768 th iteration gives loss of 0.16054134850485782\n",
      "The 29769 th iteration gives loss of 0.16053928579028018\n",
      "The 29770 th iteration gives loss of 0.16053722320356603\n",
      "The 29771 th iteration gives loss of 0.16053516074469867\n",
      "The 29772 th iteration gives loss of 0.16053309841365948\n",
      "The 29773 th iteration gives loss of 0.16053103621043593\n",
      "The 29774 th iteration gives loss of 0.16052897413499895\n",
      "The 29775 th iteration gives loss of 0.16052691218732604\n",
      "The 29776 th iteration gives loss of 0.16052485036741712\n",
      "The 29777 th iteration gives loss of 0.16052278867524164\n",
      "The 29778 th iteration gives loss of 0.160520727110775\n",
      "The 29779 th iteration gives loss of 0.16051866567400846\n",
      "The 29780 th iteration gives loss of 0.16051660436492143\n",
      "The 29781 th iteration gives loss of 0.16051454318350059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 29782 th iteration gives loss of 0.16051248212970828\n",
      "The 29783 th iteration gives loss of 0.16051042120354697\n",
      "The 29784 th iteration gives loss of 0.16050836040499475\n",
      "The 29785 th iteration gives loss of 0.16050629973402372\n",
      "The 29786 th iteration gives loss of 0.16050423919061704\n",
      "The 29787 th iteration gives loss of 0.16050217877476036\n",
      "The 29788 th iteration gives loss of 0.1605001184864338\n",
      "The 29789 th iteration gives loss of 0.16049805832562308\n",
      "The 29790 th iteration gives loss of 0.16049599829230152\n",
      "The 29791 th iteration gives loss of 0.16049393838645298\n",
      "The 29792 th iteration gives loss of 0.16049187860805905\n",
      "The 29793 th iteration gives loss of 0.16048981895710582\n",
      "The 29794 th iteration gives loss of 0.1604877594335621\n",
      "The 29795 th iteration gives loss of 0.1604857000374282\n",
      "The 29796 th iteration gives loss of 0.16048364076866628\n",
      "The 29797 th iteration gives loss of 0.16048158162727164\n",
      "The 29798 th iteration gives loss of 0.16047952261322385\n",
      "The 29799 th iteration gives loss of 0.16047746372649588\n",
      "The 29800 th iteration gives loss of 0.16047540496708565\n",
      "The 29801 th iteration gives loss of 0.1604733463349515\n",
      "The 29802 th iteration gives loss of 0.16047128783008546\n",
      "The 29803 th iteration gives loss of 0.16046922945247616\n",
      "The 29804 th iteration gives loss of 0.16046717120209777\n",
      "The 29805 th iteration gives loss of 0.160465113078935\n",
      "The 29806 th iteration gives loss of 0.16046305508296552\n",
      "The 29807 th iteration gives loss of 0.1604609972141809\n",
      "The 29808 th iteration gives loss of 0.16045893947254306\n",
      "The 29809 th iteration gives loss of 0.1604568818580487\n",
      "The 29810 th iteration gives loss of 0.1604548243706762\n",
      "The 29811 th iteration gives loss of 0.16045276701040656\n",
      "The 29812 th iteration gives loss of 0.1604507097772192\n",
      "The 29813 th iteration gives loss of 0.1604486526710949\n",
      "The 29814 th iteration gives loss of 0.16044659569202963\n",
      "The 29815 th iteration gives loss of 0.16044453883998058\n",
      "The 29816 th iteration gives loss of 0.16044248211494108\n",
      "The 29817 th iteration gives loss of 0.1604404255168982\n",
      "The 29818 th iteration gives loss of 0.16043836904582645\n",
      "The 29819 th iteration gives loss of 0.16043631270170947\n",
      "The 29820 th iteration gives loss of 0.16043425648453483\n",
      "The 29821 th iteration gives loss of 0.1604322003942687\n",
      "The 29822 th iteration gives loss of 0.1604301444309073\n",
      "The 29823 th iteration gives loss of 0.16042808859442065\n",
      "The 29824 th iteration gives loss of 0.16042603288479337\n",
      "The 29825 th iteration gives loss of 0.16042397730201516\n",
      "The 29826 th iteration gives loss of 0.16042192184606505\n",
      "The 29827 th iteration gives loss of 0.1604198665169164\n",
      "The 29828 th iteration gives loss of 0.16041781131455832\n",
      "The 29829 th iteration gives loss of 0.16041575623896168\n",
      "The 29830 th iteration gives loss of 0.16041370129011914\n",
      "The 29831 th iteration gives loss of 0.16041164646801456\n",
      "The 29832 th iteration gives loss of 0.16040959177261455\n",
      "The 29833 th iteration gives loss of 0.16040753720392403\n",
      "The 29834 th iteration gives loss of 0.16040548276189906\n",
      "The 29835 th iteration gives loss of 0.16040342844653352\n",
      "The 29836 th iteration gives loss of 0.160401374257809\n",
      "The 29837 th iteration gives loss of 0.1603993201957097\n",
      "The 29838 th iteration gives loss of 0.160397266260204\n",
      "The 29839 th iteration gives loss of 0.16039521245129545\n",
      "The 29840 th iteration gives loss of 0.1603931587689482\n",
      "The 29841 th iteration gives loss of 0.16039110521314373\n",
      "The 29842 th iteration gives loss of 0.16038905178387344\n",
      "The 29843 th iteration gives loss of 0.16038699848111299\n",
      "The 29844 th iteration gives loss of 0.16038494530484407\n",
      "The 29845 th iteration gives loss of 0.16038289225504002\n",
      "The 29846 th iteration gives loss of 0.16038083933170438\n",
      "The 29847 th iteration gives loss of 0.1603787865348039\n",
      "The 29848 th iteration gives loss of 0.16037673386431575\n",
      "The 29849 th iteration gives loss of 0.16037468132023744\n",
      "The 29850 th iteration gives loss of 0.16037262890254397\n",
      "The 29851 th iteration gives loss of 0.16037057661119747\n",
      "The 29852 th iteration gives loss of 0.1603685244462082\n",
      "The 29853 th iteration gives loss of 0.16036647240753596\n",
      "The 29854 th iteration gives loss of 0.1603644204951769\n",
      "The 29855 th iteration gives loss of 0.16036236870911064\n",
      "The 29856 th iteration gives loss of 0.16036031704930867\n",
      "The 29857 th iteration gives loss of 0.16035826551575616\n",
      "The 29858 th iteration gives loss of 0.16035621410844483\n",
      "The 29859 th iteration gives loss of 0.16035416282734782\n",
      "The 29860 th iteration gives loss of 0.1603521116724522\n",
      "The 29861 th iteration gives loss of 0.16035006064373233\n",
      "The 29862 th iteration gives loss of 0.16034800974117383\n",
      "The 29863 th iteration gives loss of 0.16034595896475579\n",
      "The 29864 th iteration gives loss of 0.1603439083144646\n",
      "The 29865 th iteration gives loss of 0.16034185779027949\n",
      "The 29866 th iteration gives loss of 0.16033980739217923\n",
      "The 29867 th iteration gives loss of 0.16033775712015647\n",
      "The 29868 th iteration gives loss of 0.16033570697416522\n",
      "The 29869 th iteration gives loss of 0.16033365695422006\n",
      "The 29870 th iteration gives loss of 0.1603316070602848\n",
      "The 29871 th iteration gives loss of 0.16032955729234813\n",
      "The 29872 th iteration gives loss of 0.16032750765038364\n",
      "The 29873 th iteration gives loss of 0.16032545813438415\n",
      "The 29874 th iteration gives loss of 0.16032340874432435\n",
      "The 29875 th iteration gives loss of 0.16032135948017914\n",
      "The 29876 th iteration gives loss of 0.16031931034193928\n",
      "The 29877 th iteration gives loss of 0.16031726132958557\n",
      "The 29878 th iteration gives loss of 0.1603152124431058\n",
      "The 29879 th iteration gives loss of 0.16031316368246615\n",
      "The 29880 th iteration gives loss of 0.16031111504766266\n",
      "The 29881 th iteration gives loss of 0.16030906653866223\n",
      "The 29882 th iteration gives loss of 0.16030701815545853\n",
      "The 29883 th iteration gives loss of 0.1603049698980298\n",
      "The 29884 th iteration gives loss of 0.16030292176636202\n",
      "The 29885 th iteration gives loss of 0.1603008737604306\n",
      "The 29886 th iteration gives loss of 0.16029882588021768\n",
      "The 29887 th iteration gives loss of 0.16029677812570428\n",
      "The 29888 th iteration gives loss of 0.16029473049688167\n",
      "The 29889 th iteration gives loss of 0.16029268299371793\n",
      "The 29890 th iteration gives loss of 0.1602906356162034\n",
      "The 29891 th iteration gives loss of 0.16028858836431853\n",
      "The 29892 th iteration gives loss of 0.1602865412380395\n",
      "The 29893 th iteration gives loss of 0.16028449423736232\n",
      "The 29894 th iteration gives loss of 0.16028244736225147\n",
      "The 29895 th iteration gives loss of 0.16028040061269955\n",
      "The 29896 th iteration gives loss of 0.16027835398868034\n",
      "The 29897 th iteration gives loss of 0.16027630749018124\n",
      "The 29898 th iteration gives loss of 0.1602742611171838\n",
      "The 29899 th iteration gives loss of 0.16027221486967352\n",
      "The 29900 th iteration gives loss of 0.16027016874762534\n",
      "The 29901 th iteration gives loss of 0.16026812275101393\n",
      "The 29902 th iteration gives loss of 0.16026607687983455\n",
      "The 29903 th iteration gives loss of 0.1602640311340683\n",
      "The 29904 th iteration gives loss of 0.1602619855136888\n",
      "The 29905 th iteration gives loss of 0.16025994001867924\n",
      "The 29906 th iteration gives loss of 0.16025789464903398\n",
      "The 29907 th iteration gives loss of 0.1602558494047211\n",
      "The 29908 th iteration gives loss of 0.16025380428571592\n",
      "The 29909 th iteration gives loss of 0.16025175929202046\n",
      "The 29910 th iteration gives loss of 0.16024971442360386\n",
      "The 29911 th iteration gives loss of 0.16024766968045404\n",
      "The 29912 th iteration gives loss of 0.16024562506254295\n",
      "The 29913 th iteration gives loss of 0.16024358056985993\n",
      "The 29914 th iteration gives loss of 0.1602415362023822\n",
      "The 29915 th iteration gives loss of 0.16023949196009843\n",
      "The 29916 th iteration gives loss of 0.16023744784298677\n",
      "The 29917 th iteration gives loss of 0.16023540385102533\n",
      "The 29918 th iteration gives loss of 0.1602333599842035\n",
      "The 29919 th iteration gives loss of 0.16023131624249634\n",
      "The 29920 th iteration gives loss of 0.16022927262588912\n",
      "The 29921 th iteration gives loss of 0.160227229134363\n",
      "The 29922 th iteration gives loss of 0.16022518576789913\n",
      "The 29923 th iteration gives loss of 0.16022314252647785\n",
      "The 29924 th iteration gives loss of 0.16022109941008691\n",
      "The 29925 th iteration gives loss of 0.16021905641870104\n",
      "The 29926 th iteration gives loss of 0.16021701355230383\n",
      "The 29927 th iteration gives loss of 0.1602149708108734\n",
      "The 29928 th iteration gives loss of 0.16021292819440294\n",
      "The 29929 th iteration gives loss of 0.1602108857028693\n",
      "The 29930 th iteration gives loss of 0.16020884333624782\n",
      "The 29931 th iteration gives loss of 0.1602068010945243\n",
      "The 29932 th iteration gives loss of 0.1602047589776798\n",
      "The 29933 th iteration gives loss of 0.1602027169857005\n",
      "The 29934 th iteration gives loss of 0.1602006751185655\n",
      "The 29935 th iteration gives loss of 0.1601986333762602\n",
      "The 29936 th iteration gives loss of 0.16019659175875492\n",
      "The 29937 th iteration gives loss of 0.1601945502660426\n",
      "The 29938 th iteration gives loss of 0.16019250889810538\n",
      "The 29939 th iteration gives loss of 0.1601904676549177\n",
      "The 29940 th iteration gives loss of 0.1601884265364622\n",
      "The 29941 th iteration gives loss of 0.16018638554272818\n",
      "The 29942 th iteration gives loss of 0.16018434467368703\n",
      "The 29943 th iteration gives loss of 0.16018230392932953\n",
      "The 29944 th iteration gives loss of 0.16018026330963742\n",
      "The 29945 th iteration gives loss of 0.1601782228145826\n",
      "The 29946 th iteration gives loss of 0.16017618244416384\n",
      "The 29947 th iteration gives loss of 0.16017414219834525\n",
      "The 29948 th iteration gives loss of 0.16017210207712096\n",
      "The 29949 th iteration gives loss of 0.1601700620804612\n",
      "The 29950 th iteration gives loss of 0.1601680222083664\n",
      "The 29951 th iteration gives loss of 0.16016598246080027\n",
      "The 29952 th iteration gives loss of 0.16016394283774535\n",
      "The 29953 th iteration gives loss of 0.16016190333919866\n",
      "The 29954 th iteration gives loss of 0.16015986396512769\n",
      "The 29955 th iteration gives loss of 0.1601578247155264\n",
      "The 29956 th iteration gives loss of 0.16015578559037008\n",
      "The 29957 th iteration gives loss of 0.16015374658962828\n",
      "The 29958 th iteration gives loss of 0.16015170771330742\n",
      "The 29959 th iteration gives loss of 0.16014966896136815\n",
      "The 29960 th iteration gives loss of 0.16014763033380466\n",
      "The 29961 th iteration gives loss of 0.16014559183059346\n",
      "The 29962 th iteration gives loss of 0.16014355345171996\n",
      "The 29963 th iteration gives loss of 0.16014151519716197\n",
      "The 29964 th iteration gives loss of 0.16013947706690543\n",
      "The 29965 th iteration gives loss of 0.16013743906093064\n",
      "The 29966 th iteration gives loss of 0.1601354011792229\n",
      "The 29967 th iteration gives loss of 0.16013336342175988\n",
      "The 29968 th iteration gives loss of 0.1601313257885231\n",
      "The 29969 th iteration gives loss of 0.1601292882794933\n",
      "The 29970 th iteration gives loss of 0.1601272508946536\n",
      "The 29971 th iteration gives loss of 0.1601252136339918\n",
      "The 29972 th iteration gives loss of 0.16012317649748742\n",
      "The 29973 th iteration gives loss of 0.16012113948511958\n",
      "The 29974 th iteration gives loss of 0.16011910259686135\n",
      "The 29975 th iteration gives loss of 0.16011706583270915\n",
      "The 29976 th iteration gives loss of 0.16011502919263979\n",
      "The 29977 th iteration gives loss of 0.1601129926766311\n",
      "The 29978 th iteration gives loss of 0.16011095628467537\n",
      "The 29979 th iteration gives loss of 0.1601089200167432\n",
      "The 29980 th iteration gives loss of 0.1601068838728315\n",
      "The 29981 th iteration gives loss of 0.16010484785290358\n",
      "The 29982 th iteration gives loss of 0.160102811956957\n",
      "The 29983 th iteration gives loss of 0.16010077618496527\n",
      "The 29984 th iteration gives loss of 0.16009874053691514\n",
      "The 29985 th iteration gives loss of 0.160096705012773\n",
      "The 29986 th iteration gives loss of 0.16009466961253757\n",
      "The 29987 th iteration gives loss of 0.16009263433619558\n",
      "The 29988 th iteration gives loss of 0.16009059918370988\n",
      "The 29989 th iteration gives loss of 0.16008856415507386\n",
      "The 29990 th iteration gives loss of 0.16008652925026767\n",
      "The 29991 th iteration gives loss of 0.1600844944692804\n",
      "The 29992 th iteration gives loss of 0.1600824598120811\n",
      "The 29993 th iteration gives loss of 0.1600804252786625\n",
      "The 29994 th iteration gives loss of 0.16007839086900444\n",
      "The 29995 th iteration gives loss of 0.1600763565830802\n",
      "The 29996 th iteration gives loss of 0.16007432242087963\n",
      "The 29997 th iteration gives loss of 0.16007228838238263\n",
      "The 29998 th iteration gives loss of 0.16007025446757145\n",
      "The 29999 th iteration gives loss of 0.16006822067642776\n",
      "The 30000 th iteration gives loss of 0.16006618700894235\n",
      "The 30001 th iteration gives loss of 0.1600641534650794\n",
      "The 30002 th iteration gives loss of 0.160062120044837\n",
      "The 30003 th iteration gives loss of 0.16006008674818684\n",
      "The 30004 th iteration gives loss of 0.1600580535751154\n",
      "The 30005 th iteration gives loss of 0.16005602052560894\n",
      "The 30006 th iteration gives loss of 0.1600539875996381\n",
      "The 30007 th iteration gives loss of 0.1600519547971908\n",
      "The 30008 th iteration gives loss of 0.16004992211825608\n",
      "The 30009 th iteration gives loss of 0.16004788956281163\n",
      "The 30010 th iteration gives loss of 0.16004585713083197\n",
      "The 30011 th iteration gives loss of 0.16004382482231377\n",
      "The 30012 th iteration gives loss of 0.16004179263721555\n",
      "The 30013 th iteration gives loss of 0.16003976057554253\n",
      "The 30014 th iteration gives loss of 0.16003772863726382\n",
      "The 30015 th iteration gives loss of 0.1600356968223681\n",
      "The 30016 th iteration gives loss of 0.16003366513083958\n",
      "The 30017 th iteration gives loss of 0.16003163356264605\n",
      "The 30018 th iteration gives loss of 0.16002960211778405\n",
      "The 30019 th iteration gives loss of 0.16002757079623095\n",
      "The 30020 th iteration gives loss of 0.16002553959796625\n",
      "The 30021 th iteration gives loss of 0.16002350852297956\n",
      "The 30022 th iteration gives loss of 0.16002147757124913\n",
      "The 30023 th iteration gives loss of 0.16001944674274918\n",
      "The 30024 th iteration gives loss of 0.16001741603746794\n",
      "The 30025 th iteration gives loss of 0.16001538545539348\n",
      "The 30026 th iteration gives loss of 0.16001335499650188\n",
      "The 30027 th iteration gives loss of 0.16001132466077495\n",
      "The 30028 th iteration gives loss of 0.1600092944481934\n",
      "The 30029 th iteration gives loss of 0.1600072643587474\n",
      "The 30030 th iteration gives loss of 0.160005234392407\n",
      "The 30031 th iteration gives loss of 0.1600032045491682\n",
      "The 30032 th iteration gives loss of 0.16000117482899506\n",
      "The 30033 th iteration gives loss of 0.15999914523188583\n",
      "The 30034 th iteration gives loss of 0.1599971157578105\n",
      "The 30035 th iteration gives loss of 0.15999508640676216\n",
      "The 30036 th iteration gives loss of 0.1599930571787193\n",
      "The 30037 th iteration gives loss of 0.15999102807365845\n",
      "The 30038 th iteration gives loss of 0.15998899909156758\n",
      "The 30039 th iteration gives loss of 0.1599869702324278\n",
      "The 30040 th iteration gives loss of 0.15998494149622514\n",
      "The 30041 th iteration gives loss of 0.159982912882931\n",
      "The 30042 th iteration gives loss of 0.15998088439254055\n",
      "The 30043 th iteration gives loss of 0.15997885602502063\n",
      "The 30044 th iteration gives loss of 0.15997682778036776\n",
      "The 30045 th iteration gives loss of 0.15997479965855785\n",
      "The 30046 th iteration gives loss of 0.15997277165957538\n",
      "The 30047 th iteration gives loss of 0.1599707437833967\n",
      "The 30048 th iteration gives loss of 0.159968716030005\n",
      "The 30049 th iteration gives loss of 0.15996668839939088\n",
      "The 30050 th iteration gives loss of 0.1599646608915327\n",
      "The 30051 th iteration gives loss of 0.1599626335064023\n",
      "The 30052 th iteration gives loss of 0.15996060624399638\n",
      "The 30053 th iteration gives loss of 0.1599585791042878\n",
      "The 30054 th iteration gives loss of 0.159956552087263\n",
      "The 30055 th iteration gives loss of 0.15995452519290387\n",
      "The 30056 th iteration gives loss of 0.1599524984211962\n",
      "The 30057 th iteration gives loss of 0.15995047177211416\n",
      "The 30058 th iteration gives loss of 0.15994844524564072\n",
      "The 30059 th iteration gives loss of 0.15994641884176916\n",
      "The 30060 th iteration gives loss of 0.15994439256046308\n",
      "The 30061 th iteration gives loss of 0.15994236640172188\n",
      "The 30062 th iteration gives loss of 0.1599403403655175\n",
      "The 30063 th iteration gives loss of 0.15993831445183082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 30064 th iteration gives loss of 0.15993628866065793\n",
      "The 30065 th iteration gives loss of 0.15993426299196514\n",
      "The 30066 th iteration gives loss of 0.15993223744573642\n",
      "The 30067 th iteration gives loss of 0.1599302120219708\n",
      "The 30068 th iteration gives loss of 0.15992818672063783\n",
      "The 30069 th iteration gives loss of 0.15992616154171665\n",
      "The 30070 th iteration gives loss of 0.15992413648519244\n",
      "The 30071 th iteration gives loss of 0.1599221115510448\n",
      "The 30072 th iteration gives loss of 0.15992008673926023\n",
      "The 30073 th iteration gives loss of 0.1599180620498278\n",
      "The 30074 th iteration gives loss of 0.15991603748271155\n",
      "The 30075 th iteration gives loss of 0.15991401303790606\n",
      "The 30076 th iteration gives loss of 0.15991198871539758\n",
      "The 30077 th iteration gives loss of 0.1599099645151561\n",
      "The 30078 th iteration gives loss of 0.15990794043717355\n",
      "The 30079 th iteration gives loss of 0.1599059164814302\n",
      "The 30080 th iteration gives loss of 0.15990389264790028\n",
      "The 30081 th iteration gives loss of 0.1599018689365679\n",
      "The 30082 th iteration gives loss of 0.15989984534742782\n",
      "The 30083 th iteration gives loss of 0.15989782188046145\n",
      "The 30084 th iteration gives loss of 0.15989579853563765\n",
      "The 30085 th iteration gives loss of 0.15989377531294865\n",
      "The 30086 th iteration gives loss of 0.15989175221236127\n",
      "The 30087 th iteration gives loss of 0.15988972923387884\n",
      "The 30088 th iteration gives loss of 0.15988770637747202\n",
      "The 30089 th iteration gives loss of 0.1598856836431195\n",
      "The 30090 th iteration gives loss of 0.15988366103082025\n",
      "The 30091 th iteration gives loss of 0.15988163854053777\n",
      "The 30092 th iteration gives loss of 0.15987961617226776\n",
      "The 30093 th iteration gives loss of 0.15987759392598153\n",
      "The 30094 th iteration gives loss of 0.15987557180166492\n",
      "The 30095 th iteration gives loss of 0.15987354979930468\n",
      "The 30096 th iteration gives loss of 0.15987152791888848\n",
      "The 30097 th iteration gives loss of 0.15986950616038056\n",
      "The 30098 th iteration gives loss of 0.1598674845237745\n",
      "The 30099 th iteration gives loss of 0.1598654630090538\n",
      "The 30100 th iteration gives loss of 0.15986344161620025\n",
      "The 30101 th iteration gives loss of 0.15986142034518855\n",
      "The 30102 th iteration gives loss of 0.15985939919601003\n",
      "The 30103 th iteration gives loss of 0.15985737816864304\n",
      "The 30104 th iteration gives loss of 0.1598553572630665\n",
      "The 30105 th iteration gives loss of 0.15985333647926728\n",
      "The 30106 th iteration gives loss of 0.15985131581723697\n",
      "The 30107 th iteration gives loss of 0.15984929527693986\n",
      "The 30108 th iteration gives loss of 0.15984727485836123\n",
      "The 30109 th iteration gives loss of 0.15984525456149223\n",
      "The 30110 th iteration gives loss of 0.15984323438631615\n",
      "The 30111 th iteration gives loss of 0.15984121433280413\n",
      "The 30112 th iteration gives loss of 0.15983919440094682\n",
      "The 30113 th iteration gives loss of 0.15983717459073296\n",
      "The 30114 th iteration gives loss of 0.1598351549021232\n",
      "The 30115 th iteration gives loss of 0.15983313533511817\n",
      "The 30116 th iteration gives loss of 0.15983111588969479\n",
      "The 30117 th iteration gives loss of 0.15982909656583616\n",
      "The 30118 th iteration gives loss of 0.15982707736352453\n",
      "The 30119 th iteration gives loss of 0.15982505828274574\n",
      "The 30120 th iteration gives loss of 0.15982303932347\n",
      "The 30121 th iteration gives loss of 0.15982102048569363\n",
      "The 30122 th iteration gives loss of 0.15981900176939565\n",
      "The 30123 th iteration gives loss of 0.15981698317455312\n",
      "The 30124 th iteration gives loss of 0.15981496470115594\n",
      "The 30125 th iteration gives loss of 0.15981294634917403\n",
      "The 30126 th iteration gives loss of 0.15981092811859543\n",
      "The 30127 th iteration gives loss of 0.15980891000941094\n",
      "The 30128 th iteration gives loss of 0.1598068920215983\n",
      "The 30129 th iteration gives loss of 0.15980487415514283\n",
      "The 30130 th iteration gives loss of 0.1598028564100173\n",
      "The 30131 th iteration gives loss of 0.15980083878620316\n",
      "The 30132 th iteration gives loss of 0.15979882128369335\n",
      "The 30133 th iteration gives loss of 0.15979680390246856\n",
      "The 30134 th iteration gives loss of 0.15979478664251143\n",
      "The 30135 th iteration gives loss of 0.15979276950379293\n",
      "The 30136 th iteration gives loss of 0.15979075248630395\n",
      "The 30137 th iteration gives loss of 0.15978873559002643\n",
      "The 30138 th iteration gives loss of 0.15978671881495235\n",
      "The 30139 th iteration gives loss of 0.15978470216105078\n",
      "The 30140 th iteration gives loss of 0.15978268562830206\n",
      "The 30141 th iteration gives loss of 0.15978066921669284\n",
      "The 30142 th iteration gives loss of 0.15977865292621318\n",
      "The 30143 th iteration gives loss of 0.1597766367568377\n",
      "The 30144 th iteration gives loss of 0.15977462070854828\n",
      "The 30145 th iteration gives loss of 0.15977260478134023\n",
      "The 30146 th iteration gives loss of 0.15977058897517254\n",
      "The 30147 th iteration gives loss of 0.15976857329004723\n",
      "The 30148 th iteration gives loss of 0.15976655772595064\n",
      "The 30149 th iteration gives loss of 0.1597645422828328\n",
      "The 30150 th iteration gives loss of 0.15976252696071583\n",
      "The 30151 th iteration gives loss of 0.1597605117595542\n",
      "The 30152 th iteration gives loss of 0.15975849667934505\n",
      "The 30153 th iteration gives loss of 0.15975648172006757\n",
      "The 30154 th iteration gives loss of 0.15975446688169825\n",
      "The 30155 th iteration gives loss of 0.15975245216423084\n",
      "The 30156 th iteration gives loss of 0.1597504375676326\n",
      "The 30157 th iteration gives loss of 0.15974842309189827\n",
      "The 30158 th iteration gives loss of 0.15974640873700657\n",
      "The 30159 th iteration gives loss of 0.15974439450293831\n",
      "The 30160 th iteration gives loss of 0.15974238038967878\n",
      "The 30161 th iteration gives loss of 0.15974036639720918\n",
      "The 30162 th iteration gives loss of 0.1597383525255145\n",
      "The 30163 th iteration gives loss of 0.1597363387745798\n",
      "The 30164 th iteration gives loss of 0.15973432514437222\n",
      "The 30165 th iteration gives loss of 0.15973231163488663\n",
      "The 30166 th iteration gives loss of 0.15973029824609836\n",
      "The 30167 th iteration gives loss of 0.15972828497800956\n",
      "The 30168 th iteration gives loss of 0.15972627183057903\n",
      "The 30169 th iteration gives loss of 0.1597242588037974\n",
      "The 30170 th iteration gives loss of 0.1597222458976548\n",
      "The 30171 th iteration gives loss of 0.1597202331121227\n",
      "The 30172 th iteration gives loss of 0.15971822044718378\n",
      "The 30173 th iteration gives loss of 0.15971620790283003\n",
      "The 30174 th iteration gives loss of 0.15971419547903565\n",
      "The 30175 th iteration gives loss of 0.1597121831757875\n",
      "The 30176 th iteration gives loss of 0.15971017099306783\n",
      "The 30177 th iteration gives loss of 0.15970815893085244\n",
      "The 30178 th iteration gives loss of 0.15970614698913377\n",
      "The 30179 th iteration gives loss of 0.15970413516788554\n",
      "The 30180 th iteration gives loss of 0.1597021234670991\n",
      "The 30181 th iteration gives loss of 0.15970011188674665\n",
      "The 30182 th iteration gives loss of 0.15969810042682378\n",
      "The 30183 th iteration gives loss of 0.15969608908729965\n",
      "The 30184 th iteration gives loss of 0.1596940778681662\n",
      "The 30185 th iteration gives loss of 0.15969206676940262\n",
      "The 30186 th iteration gives loss of 0.1596900557909915\n",
      "The 30187 th iteration gives loss of 0.15968804493291175\n",
      "The 30188 th iteration gives loss of 0.15968603419515195\n",
      "The 30189 th iteration gives loss of 0.15968402357769335\n",
      "The 30190 th iteration gives loss of 0.15968201308051877\n",
      "The 30191 th iteration gives loss of 0.15968000270360555\n",
      "The 30192 th iteration gives loss of 0.1596779924469411\n",
      "The 30193 th iteration gives loss of 0.1596759823105051\n",
      "The 30194 th iteration gives loss of 0.15967397229428826\n",
      "The 30195 th iteration gives loss of 0.15967196239825934\n",
      "The 30196 th iteration gives loss of 0.15966995262241782\n",
      "The 30197 th iteration gives loss of 0.15966794296672152\n",
      "The 30198 th iteration gives loss of 0.15966593343117444\n",
      "The 30199 th iteration gives loss of 0.1596639240157554\n",
      "The 30200 th iteration gives loss of 0.1596619147204466\n",
      "The 30201 th iteration gives loss of 0.15965990554522938\n",
      "The 30202 th iteration gives loss of 0.15965789649007545\n",
      "The 30203 th iteration gives loss of 0.1596558875549808\n",
      "The 30204 th iteration gives loss of 0.1596538787399362\n",
      "The 30205 th iteration gives loss of 0.15965187004490083\n",
      "The 30206 th iteration gives loss of 0.15964986146987392\n",
      "The 30207 th iteration gives loss of 0.15964785301482598\n",
      "The 30208 th iteration gives loss of 0.15964584467975415\n",
      "The 30209 th iteration gives loss of 0.15964383646462513\n",
      "The 30210 th iteration gives loss of 0.15964182836944094\n",
      "The 30211 th iteration gives loss of 0.15963982039417296\n",
      "The 30212 th iteration gives loss of 0.1596378125388018\n",
      "The 30213 th iteration gives loss of 0.15963580480331283\n",
      "The 30214 th iteration gives loss of 0.15963379718768092\n",
      "The 30215 th iteration gives loss of 0.1596317896918964\n",
      "The 30216 th iteration gives loss of 0.15962978231594646\n",
      "The 30217 th iteration gives loss of 0.1596277750598102\n",
      "The 30218 th iteration gives loss of 0.15962576792346694\n",
      "The 30219 th iteration gives loss of 0.15962376090689984\n",
      "The 30220 th iteration gives loss of 0.15962175401009626\n",
      "The 30221 th iteration gives loss of 0.15961974723303096\n",
      "The 30222 th iteration gives loss of 0.15961774057569286\n",
      "The 30223 th iteration gives loss of 0.15961573403806084\n",
      "The 30224 th iteration gives loss of 0.15961372762011533\n",
      "The 30225 th iteration gives loss of 0.1596117213218523\n",
      "The 30226 th iteration gives loss of 0.15960971514324584\n",
      "The 30227 th iteration gives loss of 0.1596077090842652\n",
      "The 30228 th iteration gives loss of 0.15960570314491485\n",
      "The 30229 th iteration gives loss of 0.15960369732517524\n",
      "The 30230 th iteration gives loss of 0.15960169162501134\n",
      "The 30231 th iteration gives loss of 0.15959968604441202\n",
      "The 30232 th iteration gives loss of 0.15959768058337517\n",
      "The 30233 th iteration gives loss of 0.15959567524186416\n",
      "The 30234 th iteration gives loss of 0.15959367001987637\n",
      "The 30235 th iteration gives loss of 0.15959166491738316\n",
      "The 30236 th iteration gives loss of 0.15958965993437918\n",
      "The 30237 th iteration gives loss of 0.1595876550708323\n",
      "The 30238 th iteration gives loss of 0.15958565032674027\n",
      "The 30239 th iteration gives loss of 0.15958364570207975\n",
      "The 30240 th iteration gives loss of 0.15958164119682638\n",
      "The 30241 th iteration gives loss of 0.1595796368109748\n",
      "The 30242 th iteration gives loss of 0.15957763254449953\n",
      "The 30243 th iteration gives loss of 0.1595756283973802\n",
      "The 30244 th iteration gives loss of 0.15957362436960615\n",
      "The 30245 th iteration gives loss of 0.15957162046115647\n",
      "The 30246 th iteration gives loss of 0.1595696166720265\n",
      "The 30247 th iteration gives loss of 0.15956761300218003\n",
      "The 30248 th iteration gives loss of 0.1595656094516108\n",
      "The 30249 th iteration gives loss of 0.1595636060202984\n",
      "The 30250 th iteration gives loss of 0.15956160270822356\n",
      "The 30251 th iteration gives loss of 0.15955959951537335\n",
      "The 30252 th iteration gives loss of 0.15955759644173254\n",
      "The 30253 th iteration gives loss of 0.15955559348726783\n",
      "The 30254 th iteration gives loss of 0.15955359065198416\n",
      "The 30255 th iteration gives loss of 0.15955158793585103\n",
      "The 30256 th iteration gives loss of 0.15954958533885144\n",
      "The 30257 th iteration gives loss of 0.15954758286097978\n",
      "The 30258 th iteration gives loss of 0.15954558050220646\n",
      "The 30259 th iteration gives loss of 0.15954357826251095\n",
      "The 30260 th iteration gives loss of 0.15954157614188796\n",
      "The 30261 th iteration gives loss of 0.15953957414031808\n",
      "The 30262 th iteration gives loss of 0.15953757225777393\n",
      "The 30263 th iteration gives loss of 0.1595355704942507\n",
      "The 30264 th iteration gives loss of 0.1595335688497294\n",
      "The 30265 th iteration gives loss of 0.15953156732417506\n",
      "The 30266 th iteration gives loss of 0.1595295659175929\n",
      "The 30267 th iteration gives loss of 0.15952756462995255\n",
      "The 30268 th iteration gives loss of 0.15952556346124921\n",
      "The 30269 th iteration gives loss of 0.15952356241145255\n",
      "The 30270 th iteration gives loss of 0.15952156148055738\n",
      "The 30271 th iteration gives loss of 0.1595195606685298\n",
      "The 30272 th iteration gives loss of 0.15951755997536787\n",
      "The 30273 th iteration gives loss of 0.15951555940104903\n",
      "The 30274 th iteration gives loss of 0.1595135589455481\n",
      "The 30275 th iteration gives loss of 0.15951155860886346\n",
      "The 30276 th iteration gives loss of 0.15950955839097167\n",
      "The 30277 th iteration gives loss of 0.15950755829185193\n",
      "The 30278 th iteration gives loss of 0.15950555831148677\n",
      "The 30279 th iteration gives loss of 0.15950355844985786\n",
      "The 30280 th iteration gives loss of 0.15950155870696406\n",
      "The 30281 th iteration gives loss of 0.15949955908276311\n",
      "The 30282 th iteration gives loss of 0.15949755957725623\n",
      "The 30283 th iteration gives loss of 0.15949556019042052\n",
      "The 30284 th iteration gives loss of 0.15949356092224176\n",
      "The 30285 th iteration gives loss of 0.15949156177268445\n",
      "The 30286 th iteration gives loss of 0.1594895627417636\n",
      "The 30287 th iteration gives loss of 0.15948756382943596\n",
      "The 30288 th iteration gives loss of 0.15948556503568936\n",
      "The 30289 th iteration gives loss of 0.15948356636052133\n",
      "The 30290 th iteration gives loss of 0.15948156780389292\n",
      "The 30291 th iteration gives loss of 0.15947956936580554\n",
      "The 30292 th iteration gives loss of 0.15947757104623336\n",
      "The 30293 th iteration gives loss of 0.15947557284515823\n",
      "The 30294 th iteration gives loss of 0.15947357476256288\n",
      "The 30295 th iteration gives loss of 0.1594715767984336\n",
      "The 30296 th iteration gives loss of 0.1594695789527506\n",
      "The 30297 th iteration gives loss of 0.15946758122550758\n",
      "The 30298 th iteration gives loss of 0.15946558361666482\n",
      "The 30299 th iteration gives loss of 0.1594635861262263\n",
      "The 30300 th iteration gives loss of 0.15946158875416763\n",
      "The 30301 th iteration gives loss of 0.15945959150045727\n",
      "The 30302 th iteration gives loss of 0.15945759436510315\n",
      "The 30303 th iteration gives loss of 0.1594555973480736\n",
      "The 30304 th iteration gives loss of 0.15945360044935084\n",
      "The 30305 th iteration gives loss of 0.15945160366892436\n",
      "The 30306 th iteration gives loss of 0.1594496070067695\n",
      "The 30307 th iteration gives loss of 0.15944761046288639\n",
      "The 30308 th iteration gives loss of 0.1594456140372343\n",
      "The 30309 th iteration gives loss of 0.15944361772980611\n",
      "The 30310 th iteration gives loss of 0.15944162154058716\n",
      "The 30311 th iteration gives loss of 0.15943962546955995\n",
      "The 30312 th iteration gives loss of 0.15943762951671095\n",
      "The 30313 th iteration gives loss of 0.15943563368200586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 30314 th iteration gives loss of 0.15943363796544394\n",
      "The 30315 th iteration gives loss of 0.15943164236700808\n",
      "The 30316 th iteration gives loss of 0.15942964688668024\n",
      "The 30317 th iteration gives loss of 0.15942765152443342\n",
      "The 30318 th iteration gives loss of 0.1594256562802565\n",
      "The 30319 th iteration gives loss of 0.15942366115413603\n",
      "The 30320 th iteration gives loss of 0.15942166614604175\n",
      "The 30321 th iteration gives loss of 0.15941967125598622\n",
      "The 30322 th iteration gives loss of 0.15941767648391225\n",
      "The 30323 th iteration gives loss of 0.15941568182982838\n",
      "The 30324 th iteration gives loss of 0.1594136872937247\n",
      "The 30325 th iteration gives loss of 0.15941169287555934\n",
      "The 30326 th iteration gives loss of 0.1594096985753289\n",
      "The 30327 th iteration gives loss of 0.15940770439301205\n",
      "The 30328 th iteration gives loss of 0.15940571032860595\n",
      "The 30329 th iteration gives loss of 0.15940371638207085\n",
      "The 30330 th iteration gives loss of 0.15940172255340215\n",
      "The 30331 th iteration gives loss of 0.1593997288425901\n",
      "The 30332 th iteration gives loss of 0.15939773524961093\n",
      "The 30333 th iteration gives loss of 0.15939574177443974\n",
      "The 30334 th iteration gives loss of 0.1593937484170663\n",
      "The 30335 th iteration gives loss of 0.15939175517746965\n",
      "The 30336 th iteration gives loss of 0.1593897620556381\n",
      "The 30337 th iteration gives loss of 0.1593877690515571\n",
      "The 30338 th iteration gives loss of 0.15938577616519944\n",
      "The 30339 th iteration gives loss of 0.15938378339655268\n",
      "The 30340 th iteration gives loss of 0.15938179074559705\n",
      "The 30341 th iteration gives loss of 0.15937979821233017\n",
      "The 30342 th iteration gives loss of 0.15937780579671426\n",
      "The 30343 th iteration gives loss of 0.15937581349874722\n",
      "The 30344 th iteration gives loss of 0.15937382131841316\n",
      "The 30345 th iteration gives loss of 0.1593718292556755\n",
      "The 30346 th iteration gives loss of 0.15936983731053855\n",
      "The 30347 th iteration gives loss of 0.15936784548297203\n",
      "The 30348 th iteration gives loss of 0.15936585377296858\n",
      "The 30349 th iteration gives loss of 0.15936386218050216\n",
      "The 30350 th iteration gives loss of 0.15936187070556251\n",
      "The 30351 th iteration gives loss of 0.15935987934812976\n",
      "The 30352 th iteration gives loss of 0.1593578881081937\n",
      "The 30353 th iteration gives loss of 0.1593558969857283\n",
      "The 30354 th iteration gives loss of 0.15935390598071184\n",
      "The 30355 th iteration gives loss of 0.1593519150931344\n",
      "The 30356 th iteration gives loss of 0.15934992432298606\n",
      "The 30357 th iteration gives loss of 0.1593479336702346\n",
      "The 30358 th iteration gives loss of 0.15934594313487918\n",
      "The 30359 th iteration gives loss of 0.15934395271689453\n",
      "The 30360 th iteration gives loss of 0.1593419624162601\n",
      "The 30361 th iteration gives loss of 0.1593399722329732\n",
      "The 30362 th iteration gives loss of 0.15933798216699868\n",
      "The 30363 th iteration gives loss of 0.15933599221832392\n",
      "The 30364 th iteration gives loss of 0.15933400238694007\n",
      "The 30365 th iteration gives loss of 0.15933201267282462\n",
      "The 30366 th iteration gives loss of 0.15933002307596258\n",
      "The 30367 th iteration gives loss of 0.15932803359633518\n",
      "The 30368 th iteration gives loss of 0.15932604423392444\n",
      "The 30369 th iteration gives loss of 0.1593240549887243\n",
      "The 30370 th iteration gives loss of 0.15932206586069522\n",
      "The 30371 th iteration gives loss of 0.1593200768498404\n",
      "The 30372 th iteration gives loss of 0.15931808795613872\n",
      "The 30373 th iteration gives loss of 0.15931609917956835\n",
      "The 30374 th iteration gives loss of 0.15931411052011082\n",
      "The 30375 th iteration gives loss of 0.1593121219777561\n",
      "The 30376 th iteration gives loss of 0.15931013355249196\n",
      "The 30377 th iteration gives loss of 0.15930814524429002\n",
      "The 30378 th iteration gives loss of 0.1593061570531274\n",
      "The 30379 th iteration gives loss of 0.15930416897899882\n",
      "The 30380 th iteration gives loss of 0.15930218102189092\n",
      "The 30381 th iteration gives loss of 0.1593001931817812\n",
      "The 30382 th iteration gives loss of 0.15929820545864515\n",
      "The 30383 th iteration gives loss of 0.15929621785248463\n",
      "The 30384 th iteration gives loss of 0.15929423036326157\n",
      "The 30385 th iteration gives loss of 0.15929224299097333\n",
      "The 30386 th iteration gives loss of 0.15929025573559571\n",
      "The 30387 th iteration gives loss of 0.15928826859711515\n",
      "The 30388 th iteration gives loss of 0.15928628157551386\n",
      "The 30389 th iteration gives loss of 0.15928429467077887\n",
      "The 30390 th iteration gives loss of 0.1592823078828952\n",
      "The 30391 th iteration gives loss of 0.15928032121183594\n",
      "The 30392 th iteration gives loss of 0.15927833465757246\n",
      "The 30393 th iteration gives loss of 0.15927634822011844\n",
      "The 30394 th iteration gives loss of 0.1592743618994409\n",
      "The 30395 th iteration gives loss of 0.15927237569553115\n",
      "The 30396 th iteration gives loss of 0.1592703896083522\n",
      "The 30397 th iteration gives loss of 0.15926840363790923\n",
      "The 30398 th iteration gives loss of 0.15926641778417722\n",
      "The 30399 th iteration gives loss of 0.15926443204713805\n",
      "The 30400 th iteration gives loss of 0.15926244642677276\n",
      "The 30401 th iteration gives loss of 0.15926046092306925\n",
      "The 30402 th iteration gives loss of 0.1592584755360126\n",
      "The 30403 th iteration gives loss of 0.15925649026557434\n",
      "The 30404 th iteration gives loss of 0.1592545051117513\n",
      "The 30405 th iteration gives loss of 0.15925252007451093\n",
      "The 30406 th iteration gives loss of 0.15925053515386012\n",
      "The 30407 th iteration gives loss of 0.15924855034975768\n",
      "The 30408 th iteration gives loss of 0.1592465656622066\n",
      "The 30409 th iteration gives loss of 0.15924458109117184\n",
      "The 30410 th iteration gives loss of 0.1592425966366458\n",
      "The 30411 th iteration gives loss of 0.15924061229860964\n",
      "The 30412 th iteration gives loss of 0.15923862807704808\n",
      "The 30413 th iteration gives loss of 0.15923664397195314\n",
      "The 30414 th iteration gives loss of 0.1592346599832875\n",
      "The 30415 th iteration gives loss of 0.1592326761110545\n",
      "The 30416 th iteration gives loss of 0.15923069235521953\n",
      "The 30417 th iteration gives loss of 0.15922870871578412\n",
      "The 30418 th iteration gives loss of 0.1592267251927187\n",
      "The 30419 th iteration gives loss of 0.15922474178600443\n",
      "The 30420 th iteration gives loss of 0.15922275849563072\n",
      "The 30421 th iteration gives loss of 0.1592207753215836\n",
      "The 30422 th iteration gives loss of 0.1592187922638486\n",
      "The 30423 th iteration gives loss of 0.15921680932239377\n",
      "The 30424 th iteration gives loss of 0.1592148264972106\n",
      "The 30425 th iteration gives loss of 0.15921284378828304\n",
      "The 30426 th iteration gives loss of 0.15921086119560302\n",
      "The 30427 th iteration gives loss of 0.1592088787191337\n",
      "The 30428 th iteration gives loss of 0.15920689635887342\n",
      "The 30429 th iteration gives loss of 0.1592049141148072\n",
      "The 30430 th iteration gives loss of 0.15920293198690535\n",
      "The 30431 th iteration gives loss of 0.15920094997515735\n",
      "The 30432 th iteration gives loss of 0.15919896807954476\n",
      "The 30433 th iteration gives loss of 0.15919698630005832\n",
      "The 30434 th iteration gives loss of 0.1591950046366825\n",
      "The 30435 th iteration gives loss of 0.1591930230893806\n",
      "The 30436 th iteration gives loss of 0.15919104165815703\n",
      "The 30437 th iteration gives loss of 0.1591890603429864\n",
      "The 30438 th iteration gives loss of 0.15918707914385266\n",
      "The 30439 th iteration gives loss of 0.1591850980607393\n",
      "The 30440 th iteration gives loss of 0.15918311709363206\n",
      "The 30441 th iteration gives loss of 0.15918113624250657\n",
      "The 30442 th iteration gives loss of 0.15917915550735875\n",
      "The 30443 th iteration gives loss of 0.15917717488815997\n",
      "The 30444 th iteration gives loss of 0.15917519438489428\n",
      "The 30445 th iteration gives loss of 0.15917321399754614\n",
      "The 30446 th iteration gives loss of 0.1591712337261073\n",
      "The 30447 th iteration gives loss of 0.15916925357054987\n",
      "The 30448 th iteration gives loss of 0.159167273530869\n",
      "The 30449 th iteration gives loss of 0.15916529360702666\n",
      "The 30450 th iteration gives loss of 0.1591633137990323\n",
      "The 30451 th iteration gives loss of 0.15916133410685301\n",
      "The 30452 th iteration gives loss of 0.15915935453047325\n",
      "The 30453 th iteration gives loss of 0.15915737506988656\n",
      "The 30454 th iteration gives loss of 0.15915539572506504\n",
      "The 30455 th iteration gives loss of 0.15915341649599202\n",
      "The 30456 th iteration gives loss of 0.15915143738266052\n",
      "The 30457 th iteration gives loss of 0.1591494583850407\n",
      "The 30458 th iteration gives loss of 0.15914747950313282\n",
      "The 30459 th iteration gives loss of 0.15914550073689926\n",
      "The 30460 th iteration gives loss of 0.15914352208634533\n",
      "The 30461 th iteration gives loss of 0.15914154355143603\n",
      "The 30462 th iteration gives loss of 0.1591395651321649\n",
      "The 30463 th iteration gives loss of 0.15913758682850662\n",
      "The 30464 th iteration gives loss of 0.1591356086404497\n",
      "The 30465 th iteration gives loss of 0.15913363056798027\n",
      "The 30466 th iteration gives loss of 0.15913165261107867\n",
      "The 30467 th iteration gives loss of 0.1591296747697314\n",
      "The 30468 th iteration gives loss of 0.15912769704391883\n",
      "The 30469 th iteration gives loss of 0.15912571943362297\n",
      "The 30470 th iteration gives loss of 0.15912374193882356\n",
      "The 30471 th iteration gives loss of 0.15912176455950802\n",
      "The 30472 th iteration gives loss of 0.159119787295671\n",
      "The 30473 th iteration gives loss of 0.15911781014728651\n",
      "The 30474 th iteration gives loss of 0.159115833114332\n",
      "The 30475 th iteration gives loss of 0.15911385619678925\n",
      "The 30476 th iteration gives loss of 0.1591118793946473\n",
      "The 30477 th iteration gives loss of 0.15910990270789185\n",
      "The 30478 th iteration gives loss of 0.15910792613650884\n",
      "The 30479 th iteration gives loss of 0.15910594968047212\n",
      "The 30480 th iteration gives loss of 0.15910397333977686\n",
      "The 30481 th iteration gives loss of 0.15910199711439635\n",
      "The 30482 th iteration gives loss of 0.15910002100431667\n",
      "The 30483 th iteration gives loss of 0.15909804500951774\n",
      "The 30484 th iteration gives loss of 0.1590960691299877\n",
      "The 30485 th iteration gives loss of 0.15909409336570957\n",
      "The 30486 th iteration gives loss of 0.15909211771667064\n",
      "The 30487 th iteration gives loss of 0.15909014218284018\n",
      "The 30488 th iteration gives loss of 0.15908816676421683\n",
      "The 30489 th iteration gives loss of 0.15908619146077888\n",
      "The 30490 th iteration gives loss of 0.15908421627250738\n",
      "The 30491 th iteration gives loss of 0.1590822411993872\n",
      "The 30492 th iteration gives loss of 0.15908026624139754\n",
      "The 30493 th iteration gives loss of 0.15907829139852997\n",
      "The 30494 th iteration gives loss of 0.15907631667076408\n",
      "The 30495 th iteration gives loss of 0.15907434205808482\n",
      "The 30496 th iteration gives loss of 0.15907236756047052\n",
      "The 30497 th iteration gives loss of 0.1590703931779056\n",
      "The 30498 th iteration gives loss of 0.15906841891037704\n",
      "The 30499 th iteration gives loss of 0.15906644475786405\n",
      "The 30500 th iteration gives loss of 0.15906447072036525\n",
      "The 30501 th iteration gives loss of 0.15906249679783904\n",
      "The 30502 th iteration gives loss of 0.15906052299027992\n",
      "The 30503 th iteration gives loss of 0.15905854929768515\n",
      "The 30504 th iteration gives loss of 0.15905657572000906\n",
      "The 30505 th iteration gives loss of 0.15905460225726764\n",
      "The 30506 th iteration gives loss of 0.15905262890941815\n",
      "The 30507 th iteration gives loss of 0.15905065567645688\n",
      "The 30508 th iteration gives loss of 0.1590486825583625\n",
      "The 30509 th iteration gives loss of 0.15904670955512132\n",
      "The 30510 th iteration gives loss of 0.15904473666671462\n",
      "The 30511 th iteration gives loss of 0.1590427638931295\n",
      "The 30512 th iteration gives loss of 0.1590407912343416\n",
      "The 30513 th iteration gives loss of 0.15903881869034023\n",
      "The 30514 th iteration gives loss of 0.15903684626111134\n",
      "The 30515 th iteration gives loss of 0.15903487394663632\n",
      "The 30516 th iteration gives loss of 0.159032901746893\n",
      "The 30517 th iteration gives loss of 0.15903092966187626\n",
      "The 30518 th iteration gives loss of 0.15902895769155195\n",
      "The 30519 th iteration gives loss of 0.1590269858359273\n",
      "The 30520 th iteration gives loss of 0.15902501409496264\n",
      "The 30521 th iteration gives loss of 0.15902304246864782\n",
      "The 30522 th iteration gives loss of 0.1590210709569797\n",
      "The 30523 th iteration gives loss of 0.15901909955992632\n",
      "The 30524 th iteration gives loss of 0.15901712827747266\n",
      "The 30525 th iteration gives loss of 0.1590151571096118\n",
      "The 30526 th iteration gives loss of 0.15901318605631704\n",
      "The 30527 th iteration gives loss of 0.15901121511757946\n",
      "The 30528 th iteration gives loss of 0.15900924429337945\n",
      "The 30529 th iteration gives loss of 0.15900727358370126\n",
      "The 30530 th iteration gives loss of 0.15900530298852109\n",
      "The 30531 th iteration gives loss of 0.15900333250783513\n",
      "The 30532 th iteration gives loss of 0.15900136214161487\n",
      "The 30533 th iteration gives loss of 0.15899939188984893\n",
      "The 30534 th iteration gives loss of 0.15899742175252304\n",
      "The 30535 th iteration gives loss of 0.15899545172962318\n",
      "The 30536 th iteration gives loss of 0.15899348182112305\n",
      "The 30537 th iteration gives loss of 0.15899151202701262\n",
      "The 30538 th iteration gives loss of 0.1589895423472724\n",
      "The 30539 th iteration gives loss of 0.15898757278188774\n",
      "The 30540 th iteration gives loss of 0.1589856033308492\n",
      "The 30541 th iteration gives loss of 0.15898363399412185\n",
      "The 30542 th iteration gives loss of 0.1589816647717074\n",
      "The 30543 th iteration gives loss of 0.15897969566357706\n",
      "The 30544 th iteration gives loss of 0.15897772666973162\n",
      "The 30545 th iteration gives loss of 0.15897575779014056\n",
      "The 30546 th iteration gives loss of 0.15897378902478035\n",
      "The 30547 th iteration gives loss of 0.15897182037364252\n",
      "The 30548 th iteration gives loss of 0.15896985183671497\n",
      "The 30549 th iteration gives loss of 0.15896788341398074\n",
      "The 30550 th iteration gives loss of 0.1589659151054148\n",
      "The 30551 th iteration gives loss of 0.1589639469110112\n",
      "The 30552 th iteration gives loss of 0.1589619788307491\n",
      "The 30553 th iteration gives loss of 0.15896001086461128\n",
      "The 30554 th iteration gives loss of 0.15895804301257593\n",
      "The 30555 th iteration gives loss of 0.15895607527464048\n",
      "The 30556 th iteration gives loss of 0.15895410765077422\n",
      "The 30557 th iteration gives loss of 0.15895214014096612\n",
      "The 30558 th iteration gives loss of 0.15895017274519752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 30559 th iteration gives loss of 0.1589482054634587\n",
      "The 30560 th iteration gives loss of 0.1589462382957313\n",
      "The 30561 th iteration gives loss of 0.15894427124198968\n",
      "The 30562 th iteration gives loss of 0.15894230430222844\n",
      "The 30563 th iteration gives loss of 0.15894033747642586\n",
      "The 30564 th iteration gives loss of 0.15893837076457\n",
      "The 30565 th iteration gives loss of 0.15893640416664692\n",
      "The 30566 th iteration gives loss of 0.15893443768261803\n",
      "The 30567 th iteration gives loss of 0.15893247131249438\n",
      "The 30568 th iteration gives loss of 0.1589305050562383\n",
      "The 30569 th iteration gives loss of 0.15892853891385475\n",
      "The 30570 th iteration gives loss of 0.15892657288530954\n",
      "The 30571 th iteration gives loss of 0.158924606970595\n",
      "The 30572 th iteration gives loss of 0.15892264116969412\n",
      "The 30573 th iteration gives loss of 0.15892067548258718\n",
      "The 30574 th iteration gives loss of 0.15891870990925636\n",
      "The 30575 th iteration gives loss of 0.15891674444969323\n",
      "The 30576 th iteration gives loss of 0.15891477910386992\n",
      "The 30577 th iteration gives loss of 0.15891281387178063\n",
      "The 30578 th iteration gives loss of 0.15891084875340744\n",
      "The 30579 th iteration gives loss of 0.15890888374872408\n",
      "The 30580 th iteration gives loss of 0.15890691885772049\n",
      "The 30581 th iteration gives loss of 0.15890495408038358\n",
      "The 30582 th iteration gives loss of 0.15890298941669662\n",
      "The 30583 th iteration gives loss of 0.15890102486663804\n",
      "The 30584 th iteration gives loss of 0.15889906043019728\n",
      "The 30585 th iteration gives loss of 0.15889709610735253\n",
      "The 30586 th iteration gives loss of 0.15889513189808088\n",
      "The 30587 th iteration gives loss of 0.1588931678023874\n",
      "The 30588 th iteration gives loss of 0.15889120382024516\n",
      "The 30589 th iteration gives loss of 0.15888923995163273\n",
      "The 30590 th iteration gives loss of 0.15888727619653265\n",
      "The 30591 th iteration gives loss of 0.15888531255493366\n",
      "The 30592 th iteration gives loss of 0.15888334902682177\n",
      "The 30593 th iteration gives loss of 0.15888138561217724\n",
      "The 30594 th iteration gives loss of 0.15887942231098245\n",
      "The 30595 th iteration gives loss of 0.15887745912321866\n",
      "The 30596 th iteration gives loss of 0.1588754960488758\n",
      "The 30597 th iteration gives loss of 0.15887353308793783\n",
      "The 30598 th iteration gives loss of 0.15887157024038517\n",
      "The 30599 th iteration gives loss of 0.15886960750619852\n",
      "The 30600 th iteration gives loss of 0.15886764488535896\n",
      "The 30601 th iteration gives loss of 0.158865682377868\n",
      "The 30602 th iteration gives loss of 0.15886371998369211\n",
      "The 30603 th iteration gives loss of 0.1588617577028173\n",
      "The 30604 th iteration gives loss of 0.15885979553523372\n",
      "The 30605 th iteration gives loss of 0.15885783348091756\n",
      "The 30606 th iteration gives loss of 0.15885587153985806\n",
      "The 30607 th iteration gives loss of 0.1588539097120392\n",
      "The 30608 th iteration gives loss of 0.1588519479974364\n",
      "The 30609 th iteration gives loss of 0.15884998639604025\n",
      "The 30610 th iteration gives loss of 0.15884802490783806\n",
      "The 30611 th iteration gives loss of 0.1588460635328071\n",
      "The 30612 th iteration gives loss of 0.15884410227093096\n",
      "The 30613 th iteration gives loss of 0.15884214112219358\n",
      "The 30614 th iteration gives loss of 0.15884018008658513\n",
      "The 30615 th iteration gives loss of 0.15883821916407959\n",
      "The 30616 th iteration gives loss of 0.15883625835467033\n",
      "The 30617 th iteration gives loss of 0.15883429765833393\n",
      "The 30618 th iteration gives loss of 0.15883233707505637\n",
      "The 30619 th iteration gives loss of 0.15883037660481936\n",
      "The 30620 th iteration gives loss of 0.15882841624760763\n",
      "The 30621 th iteration gives loss of 0.15882645600340345\n",
      "The 30622 th iteration gives loss of 0.1588244958722101\n",
      "The 30623 th iteration gives loss of 0.15882253585398057\n",
      "The 30624 th iteration gives loss of 0.15882057594871238\n",
      "The 30625 th iteration gives loss of 0.15881861615638557\n",
      "The 30626 th iteration gives loss of 0.15881665647698476\n",
      "The 30627 th iteration gives loss of 0.15881469691051084\n",
      "The 30628 th iteration gives loss of 0.15881273745692265\n",
      "The 30629 th iteration gives loss of 0.1588107781162191\n",
      "The 30630 th iteration gives loss of 0.158808818888376\n",
      "The 30631 th iteration gives loss of 0.15880685977337752\n",
      "The 30632 th iteration gives loss of 0.15880490077121526\n",
      "The 30633 th iteration gives loss of 0.158802941881863\n",
      "The 30634 th iteration gives loss of 0.15880098310530674\n",
      "The 30635 th iteration gives loss of 0.15879902444153976\n",
      "The 30636 th iteration gives loss of 0.15879706589052378\n",
      "The 30637 th iteration gives loss of 0.15879510745226985\n",
      "The 30638 th iteration gives loss of 0.15879314912675152\n",
      "The 30639 th iteration gives loss of 0.15879119091394509\n",
      "The 30640 th iteration gives loss of 0.1587892328138377\n",
      "The 30641 th iteration gives loss of 0.15878727482641178\n",
      "The 30642 th iteration gives loss of 0.15878531695166248\n",
      "The 30643 th iteration gives loss of 0.15878335918955885\n",
      "The 30644 th iteration gives loss of 0.15878140154009265\n",
      "The 30645 th iteration gives loss of 0.1587794440032483\n",
      "The 30646 th iteration gives loss of 0.15877748657900573\n",
      "The 30647 th iteration gives loss of 0.1587755292673512\n",
      "The 30648 th iteration gives loss of 0.15877357206826254\n",
      "The 30649 th iteration gives loss of 0.1587716149817327\n",
      "The 30650 th iteration gives loss of 0.1587696580077331\n",
      "The 30651 th iteration gives loss of 0.15876770114626304\n",
      "The 30652 th iteration gives loss of 0.15876574439729668\n",
      "The 30653 th iteration gives loss of 0.15876378776082298\n",
      "The 30654 th iteration gives loss of 0.15876183123681653\n",
      "The 30655 th iteration gives loss of 0.15875987482526635\n",
      "The 30656 th iteration gives loss of 0.1587579185261691\n",
      "The 30657 th iteration gives loss of 0.158755962339488\n",
      "The 30658 th iteration gives loss of 0.15875400626521882\n",
      "The 30659 th iteration gives loss of 0.1587520503033391\n",
      "The 30660 th iteration gives loss of 0.15875009445383925\n",
      "The 30661 th iteration gives loss of 0.15874813871669682\n",
      "The 30662 th iteration gives loss of 0.15874618309189428\n",
      "The 30663 th iteration gives loss of 0.15874422757941462\n",
      "The 30664 th iteration gives loss of 0.15874227217925724\n",
      "The 30665 th iteration gives loss of 0.15874031689138693\n",
      "The 30666 th iteration gives loss of 0.15873836171580386\n",
      "The 30667 th iteration gives loss of 0.1587364066524789\n",
      "The 30668 th iteration gives loss of 0.15873445170139822\n",
      "The 30669 th iteration gives loss of 0.15873249686254748\n",
      "The 30670 th iteration gives loss of 0.15873054213591362\n",
      "The 30671 th iteration gives loss of 0.15872858752147864\n",
      "The 30672 th iteration gives loss of 0.15872663301922746\n",
      "The 30673 th iteration gives loss of 0.15872467862913428\n",
      "The 30674 th iteration gives loss of 0.15872272435119955\n",
      "The 30675 th iteration gives loss of 0.15872077018538697\n",
      "The 30676 th iteration gives loss of 0.15871881613170194\n",
      "The 30677 th iteration gives loss of 0.1587168621901141\n",
      "The 30678 th iteration gives loss of 0.15871490836061034\n",
      "The 30679 th iteration gives loss of 0.1587129546431718\n",
      "The 30680 th iteration gives loss of 0.15871100103779076\n",
      "The 30681 th iteration gives loss of 0.15870904754444132\n",
      "The 30682 th iteration gives loss of 0.15870709416311662\n",
      "The 30683 th iteration gives loss of 0.15870514089379784\n",
      "The 30684 th iteration gives loss of 0.15870318773646944\n",
      "The 30685 th iteration gives loss of 0.15870123469110495\n",
      "The 30686 th iteration gives loss of 0.15869928175769876\n",
      "The 30687 th iteration gives loss of 0.1586973289362294\n",
      "The 30688 th iteration gives loss of 0.15869537622669103\n",
      "The 30689 th iteration gives loss of 0.15869342362905778\n",
      "The 30690 th iteration gives loss of 0.15869147114331242\n",
      "The 30691 th iteration gives loss of 0.15868951876944418\n",
      "The 30692 th iteration gives loss of 0.15868756650743357\n",
      "The 30693 th iteration gives loss of 0.15868561435726566\n",
      "The 30694 th iteration gives loss of 0.15868366231891778\n",
      "The 30695 th iteration gives loss of 0.15868171039239057\n",
      "The 30696 th iteration gives loss of 0.1586797585776594\n",
      "The 30697 th iteration gives loss of 0.1586778068746968\n",
      "The 30698 th iteration gives loss of 0.15867585528350303\n",
      "The 30699 th iteration gives loss of 0.15867390380405086\n",
      "The 30700 th iteration gives loss of 0.15867195243632878\n",
      "The 30701 th iteration gives loss of 0.15867000118032332\n",
      "The 30702 th iteration gives loss of 0.15866805003601878\n",
      "The 30703 th iteration gives loss of 0.15866609900338977\n",
      "The 30704 th iteration gives loss of 0.15866414808242782\n",
      "The 30705 th iteration gives loss of 0.15866219727312034\n",
      "The 30706 th iteration gives loss of 0.15866024657544636\n",
      "The 30707 th iteration gives loss of 0.15865829598937958\n",
      "The 30708 th iteration gives loss of 0.15865634551492847\n",
      "The 30709 th iteration gives loss of 0.15865439515204888\n",
      "The 30710 th iteration gives loss of 0.15865244490075128\n",
      "The 30711 th iteration gives loss of 0.15865049476099805\n",
      "The 30712 th iteration gives loss of 0.15864854473278056\n",
      "The 30713 th iteration gives loss of 0.15864659481608817\n",
      "The 30714 th iteration gives loss of 0.1586446450109007\n",
      "The 30715 th iteration gives loss of 0.15864269531720096\n",
      "The 30716 th iteration gives loss of 0.15864074573497533\n",
      "The 30717 th iteration gives loss of 0.15863879626420693\n",
      "The 30718 th iteration gives loss of 0.15863684690487695\n",
      "The 30719 th iteration gives loss of 0.15863489765696884\n",
      "The 30720 th iteration gives loss of 0.1586329485204736\n",
      "The 30721 th iteration gives loss of 0.1586309994953765\n",
      "The 30722 th iteration gives loss of 0.15862905058164273\n",
      "The 30723 th iteration gives loss of 0.1586271017792771\n",
      "The 30724 th iteration gives loss of 0.15862515308825176\n",
      "The 30725 th iteration gives loss of 0.15862320450856138\n",
      "The 30726 th iteration gives loss of 0.1586212560401719\n",
      "The 30727 th iteration gives loss of 0.1586193076830927\n",
      "The 30728 th iteration gives loss of 0.15861735943728342\n",
      "The 30729 th iteration gives loss of 0.15861541130274495\n",
      "The 30730 th iteration gives loss of 0.1586134632794509\n",
      "The 30731 th iteration gives loss of 0.15861151536739315\n",
      "The 30732 th iteration gives loss of 0.15860956756654834\n",
      "The 30733 th iteration gives loss of 0.1586076198769045\n",
      "The 30734 th iteration gives loss of 0.15860567229844166\n",
      "The 30735 th iteration gives loss of 0.15860372483115065\n",
      "The 30736 th iteration gives loss of 0.15860177747500714\n",
      "The 30737 th iteration gives loss of 0.15859983023000065\n",
      "The 30738 th iteration gives loss of 0.15859788309611994\n",
      "The 30739 th iteration gives loss of 0.15859593607333908\n",
      "The 30740 th iteration gives loss of 0.15859398916164755\n",
      "The 30741 th iteration gives loss of 0.15859204236102822\n",
      "The 30742 th iteration gives loss of 0.15859009567146434\n",
      "The 30743 th iteration gives loss of 0.158588149092947\n",
      "The 30744 th iteration gives loss of 0.1585862026254491\n",
      "The 30745 th iteration gives loss of 0.1585842562689507\n",
      "The 30746 th iteration gives loss of 0.1585823100234576\n",
      "The 30747 th iteration gives loss of 0.15858036388893323\n",
      "The 30748 th iteration gives loss of 0.158578417865373\n",
      "The 30749 th iteration gives loss of 0.15857647195274688\n",
      "The 30750 th iteration gives loss of 0.15857452615105683\n",
      "The 30751 th iteration gives loss of 0.15857258046027797\n",
      "The 30752 th iteration gives loss of 0.15857063488040143\n",
      "The 30753 th iteration gives loss of 0.15856868941140206\n",
      "The 30754 th iteration gives loss of 0.15856674405326876\n",
      "The 30755 th iteration gives loss of 0.15856479880597218\n",
      "The 30756 th iteration gives loss of 0.15856285366952375\n",
      "The 30757 th iteration gives loss of 0.15856090864388284\n",
      "The 30758 th iteration gives loss of 0.1585589637290445\n",
      "The 30759 th iteration gives loss of 0.1585570189249839\n",
      "The 30760 th iteration gives loss of 0.15855507423170723\n",
      "The 30761 th iteration gives loss of 0.15855312964917542\n",
      "The 30762 th iteration gives loss of 0.15855118517737798\n",
      "The 30763 th iteration gives loss of 0.15854924081629695\n",
      "The 30764 th iteration gives loss of 0.15854729656592795\n",
      "The 30765 th iteration gives loss of 0.15854535242624876\n",
      "The 30766 th iteration gives loss of 0.15854340839724274\n",
      "The 30767 th iteration gives loss of 0.1585414644788942\n",
      "The 30768 th iteration gives loss of 0.1585395206711821\n",
      "The 30769 th iteration gives loss of 0.15853757697409504\n",
      "The 30770 th iteration gives loss of 0.15853563338761825\n",
      "The 30771 th iteration gives loss of 0.158533689911734\n",
      "The 30772 th iteration gives loss of 0.15853174654642918\n",
      "The 30773 th iteration gives loss of 0.15852980329168834\n",
      "The 30774 th iteration gives loss of 0.1585278601474914\n",
      "The 30775 th iteration gives loss of 0.1585259171138261\n",
      "The 30776 th iteration gives loss of 0.15852397419067435\n",
      "The 30777 th iteration gives loss of 0.1585220313780135\n",
      "The 30778 th iteration gives loss of 0.1585200886758386\n",
      "The 30779 th iteration gives loss of 0.1585181460841293\n",
      "The 30780 th iteration gives loss of 0.15851620360287824\n",
      "The 30781 th iteration gives loss of 0.1585142612320579\n",
      "The 30782 th iteration gives loss of 0.1585123189716505\n",
      "The 30783 th iteration gives loss of 0.15851037682164892\n",
      "The 30784 th iteration gives loss of 0.15850843478203408\n",
      "The 30785 th iteration gives loss of 0.15850649285279\n",
      "The 30786 th iteration gives loss of 0.15850455103389818\n",
      "The 30787 th iteration gives loss of 0.15850260932534896\n",
      "The 30788 th iteration gives loss of 0.15850066772712312\n",
      "The 30789 th iteration gives loss of 0.1584987262392079\n",
      "The 30790 th iteration gives loss of 0.15849678486157906\n",
      "The 30791 th iteration gives loss of 0.15849484359422297\n",
      "The 30792 th iteration gives loss of 0.15849290243712874\n",
      "The 30793 th iteration gives loss of 0.15849096139027694\n",
      "The 30794 th iteration gives loss of 0.15848902045365387\n",
      "The 30795 th iteration gives loss of 0.15848707962724026\n",
      "The 30796 th iteration gives loss of 0.15848513891103147\n",
      "The 30797 th iteration gives loss of 0.15848319830500054\n",
      "The 30798 th iteration gives loss of 0.15848125780913008\n",
      "The 30799 th iteration gives loss of 0.15847931742340882\n",
      "The 30800 th iteration gives loss of 0.15847737714783125\n",
      "The 30801 th iteration gives loss of 0.15847543698236613\n",
      "The 30802 th iteration gives loss of 0.1584734969269887\n",
      "The 30803 th iteration gives loss of 0.15847155698170656\n",
      "The 30804 th iteration gives loss of 0.1584696171464979\n",
      "The 30805 th iteration gives loss of 0.15846767742132917\n",
      "The 30806 th iteration gives loss of 0.15846573780620943\n",
      "The 30807 th iteration gives loss of 0.15846379830111001\n",
      "The 30808 th iteration gives loss of 0.15846185890601497\n",
      "The 30809 th iteration gives loss of 0.15845991962091113\n",
      "The 30810 th iteration gives loss of 0.15845798044578802\n",
      "The 30811 th iteration gives loss of 0.15845604138061262\n",
      "The 30812 th iteration gives loss of 0.15845410242539018\n",
      "The 30813 th iteration gives loss of 0.15845216358008782\n",
      "The 30814 th iteration gives loss of 0.1584502248446955\n",
      "The 30815 th iteration gives loss of 0.15844828621920506\n",
      "The 30816 th iteration gives loss of 0.15844634770359092\n",
      "The 30817 th iteration gives loss of 0.1584444092978393\n",
      "The 30818 th iteration gives loss of 0.15844247100193012\n",
      "The 30819 th iteration gives loss of 0.1584405328158633\n",
      "The 30820 th iteration gives loss of 0.1584385947396132\n",
      "The 30821 th iteration gives loss of 0.15843665677315205\n",
      "The 30822 th iteration gives loss of 0.15843471891648755\n",
      "The 30823 th iteration gives loss of 0.15843278116958825\n",
      "The 30824 th iteration gives loss of 0.15843084353244413\n",
      "The 30825 th iteration gives loss of 0.15842890600502832\n",
      "The 30826 th iteration gives loss of 0.15842696858734714\n",
      "The 30827 th iteration gives loss of 0.15842503127936766\n",
      "The 30828 th iteration gives loss of 0.15842309408107802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 30829 th iteration gives loss of 0.15842115699245624\n",
      "The 30830 th iteration gives loss of 0.15841922001350162\n",
      "The 30831 th iteration gives loss of 0.15841728314418704\n",
      "The 30832 th iteration gives loss of 0.1584153463844946\n",
      "The 30833 th iteration gives loss of 0.15841340973442225\n",
      "The 30834 th iteration gives loss of 0.15841147319394291\n",
      "The 30835 th iteration gives loss of 0.1584095367630382\n",
      "The 30836 th iteration gives loss of 0.15840760044170962\n",
      "The 30837 th iteration gives loss of 0.1584056642299201\n",
      "The 30838 th iteration gives loss of 0.1584037281276655\n",
      "The 30839 th iteration gives loss of 0.15840179213491865\n",
      "The 30840 th iteration gives loss of 0.1583998562516829\n",
      "The 30841 th iteration gives loss of 0.1583979204779319\n",
      "The 30842 th iteration gives loss of 0.15839598481364617\n",
      "The 30843 th iteration gives loss of 0.15839404925881795\n",
      "The 30844 th iteration gives loss of 0.15839211381342622\n",
      "The 30845 th iteration gives loss of 0.15839017847745657\n",
      "The 30846 th iteration gives loss of 0.1583882432508965\n",
      "The 30847 th iteration gives loss of 0.1583863081337228\n",
      "The 30848 th iteration gives loss of 0.15838437312592438\n",
      "The 30849 th iteration gives loss of 0.1583824382274911\n",
      "The 30850 th iteration gives loss of 0.15838050343840238\n",
      "The 30851 th iteration gives loss of 0.15837856875863335\n",
      "The 30852 th iteration gives loss of 0.15837663418818346\n",
      "The 30853 th iteration gives loss of 0.15837469972702842\n",
      "The 30854 th iteration gives loss of 0.15837276537515038\n",
      "The 30855 th iteration gives loss of 0.15837083113253997\n",
      "The 30856 th iteration gives loss of 0.15836889699918047\n",
      "The 30857 th iteration gives loss of 0.15836696297505154\n",
      "The 30858 th iteration gives loss of 0.15836502906014094\n",
      "The 30859 th iteration gives loss of 0.1583630952544315\n",
      "The 30860 th iteration gives loss of 0.1583611615579127\n",
      "The 30861 th iteration gives loss of 0.15835922797056717\n",
      "The 30862 th iteration gives loss of 0.15835729449237387\n",
      "The 30863 th iteration gives loss of 0.15835536112332543\n",
      "The 30864 th iteration gives loss of 0.15835342786339107\n",
      "The 30865 th iteration gives loss of 0.15835149471257295\n",
      "The 30866 th iteration gives loss of 0.1583495616708394\n",
      "The 30867 th iteration gives loss of 0.15834762873818597\n",
      "The 30868 th iteration gives loss of 0.1583456959145934\n",
      "The 30869 th iteration gives loss of 0.15834376320004986\n",
      "The 30870 th iteration gives loss of 0.15834183059452764\n",
      "The 30871 th iteration gives loss of 0.15833989809802287\n",
      "The 30872 th iteration gives loss of 0.15833796571052425\n",
      "The 30873 th iteration gives loss of 0.15833603343200098\n",
      "The 30874 th iteration gives loss of 0.15833410126245254\n",
      "The 30875 th iteration gives loss of 0.15833216920185364\n",
      "The 30876 th iteration gives loss of 0.15833023725018477\n",
      "The 30877 th iteration gives loss of 0.15832830540744017\n",
      "The 30878 th iteration gives loss of 0.15832637367359878\n",
      "The 30879 th iteration gives loss of 0.15832444204864324\n",
      "The 30880 th iteration gives loss of 0.1583225105325626\n",
      "The 30881 th iteration gives loss of 0.15832057912534858\n",
      "The 30882 th iteration gives loss of 0.15831864782696342\n",
      "The 30883 th iteration gives loss of 0.15831671663740893\n",
      "The 30884 th iteration gives loss of 0.1583147855566657\n",
      "The 30885 th iteration gives loss of 0.15831285458471991\n",
      "The 30886 th iteration gives loss of 0.15831092372155203\n",
      "The 30887 th iteration gives loss of 0.158308992967146\n",
      "The 30888 th iteration gives loss of 0.15830706232149463\n",
      "The 30889 th iteration gives loss of 0.15830513178456546\n",
      "The 30890 th iteration gives loss of 0.15830320135635909\n",
      "The 30891 th iteration gives loss of 0.15830127103685368\n",
      "The 30892 th iteration gives loss of 0.15829934082603286\n",
      "The 30893 th iteration gives loss of 0.15829741072388478\n",
      "The 30894 th iteration gives loss of 0.15829548073039026\n",
      "The 30895 th iteration gives loss of 0.15829355084553878\n",
      "The 30896 th iteration gives loss of 0.15829162106930525\n",
      "The 30897 th iteration gives loss of 0.15828969140167756\n",
      "The 30898 th iteration gives loss of 0.158287761842646\n",
      "The 30899 th iteration gives loss of 0.15828583239218538\n",
      "The 30900 th iteration gives loss of 0.15828390305029308\n",
      "The 30901 th iteration gives loss of 0.158281973816942\n",
      "The 30902 th iteration gives loss of 0.15828004469211965\n",
      "The 30903 th iteration gives loss of 0.15827811567581165\n",
      "The 30904 th iteration gives loss of 0.15827618676800642\n",
      "The 30905 th iteration gives loss of 0.15827425796867226\n",
      "The 30906 th iteration gives loss of 0.15827232927781626\n",
      "The 30907 th iteration gives loss of 0.15827040069541265\n",
      "The 30908 th iteration gives loss of 0.158268472221449\n",
      "The 30909 th iteration gives loss of 0.15826654385589428\n",
      "The 30910 th iteration gives loss of 0.15826461559874944\n",
      "The 30911 th iteration gives loss of 0.1582626874500011\n",
      "The 30912 th iteration gives loss of 0.1582607594096189\n",
      "The 30913 th iteration gives loss of 0.15825883147759257\n",
      "The 30914 th iteration gives loss of 0.15825690365391057\n",
      "The 30915 th iteration gives loss of 0.1582549759385573\n",
      "The 30916 th iteration gives loss of 0.15825304833151715\n",
      "The 30917 th iteration gives loss of 0.15825112083276896\n",
      "The 30918 th iteration gives loss of 0.15824919344231117\n",
      "The 30919 th iteration gives loss of 0.15824726616011223\n",
      "The 30920 th iteration gives loss of 0.1582453389861668\n",
      "The 30921 th iteration gives loss of 0.15824341192044447\n",
      "The 30922 th iteration gives loss of 0.15824148496294674\n",
      "The 30923 th iteration gives loss of 0.15823955811364704\n",
      "The 30924 th iteration gives loss of 0.15823763137253957\n",
      "The 30925 th iteration gives loss of 0.1582357047396059\n",
      "The 30926 th iteration gives loss of 0.15823377821482604\n",
      "The 30927 th iteration gives loss of 0.15823185179818416\n",
      "The 30928 th iteration gives loss of 0.15822992548967033\n",
      "The 30929 th iteration gives loss of 0.1582279992892631\n",
      "The 30930 th iteration gives loss of 0.15822607319695764\n",
      "The 30931 th iteration gives loss of 0.15822414721272604\n",
      "The 30932 th iteration gives loss of 0.1582222213365622\n",
      "The 30933 th iteration gives loss of 0.15822029556844142\n",
      "The 30934 th iteration gives loss of 0.1582183699083541\n",
      "The 30935 th iteration gives loss of 0.15821644435627832\n",
      "The 30936 th iteration gives loss of 0.1582145189122101\n",
      "The 30937 th iteration gives loss of 0.15821259357612266\n",
      "The 30938 th iteration gives loss of 0.15821066834800326\n",
      "The 30939 th iteration gives loss of 0.1582087432278454\n",
      "The 30940 th iteration gives loss of 0.15820681821561922\n",
      "The 30941 th iteration gives loss of 0.158204893311326\n",
      "The 30942 th iteration gives loss of 0.15820296851493626\n",
      "The 30943 th iteration gives loss of 0.15820104382643582\n",
      "The 30944 th iteration gives loss of 0.158199119245809\n",
      "The 30945 th iteration gives loss of 0.1581971947730485\n",
      "The 30946 th iteration gives loss of 0.15819527040814274\n",
      "The 30947 th iteration gives loss of 0.15819334615105912\n",
      "The 30948 th iteration gives loss of 0.1581914220017915\n",
      "The 30949 th iteration gives loss of 0.1581894979603231\n",
      "The 30950 th iteration gives loss of 0.15818757402663852\n",
      "The 30951 th iteration gives loss of 0.15818565020072273\n",
      "The 30952 th iteration gives loss of 0.15818372648256618\n",
      "The 30953 th iteration gives loss of 0.15818180287213446\n",
      "The 30954 th iteration gives loss of 0.15817987936943606\n",
      "The 30955 th iteration gives loss of 0.15817795597443743\n",
      "The 30956 th iteration gives loss of 0.1581760326871313\n",
      "The 30957 th iteration gives loss of 0.1581741095075096\n",
      "The 30958 th iteration gives loss of 0.1581721864355406\n",
      "The 30959 th iteration gives loss of 0.158170263471224\n",
      "The 30960 th iteration gives loss of 0.15816834061453272\n",
      "The 30961 th iteration gives loss of 0.15816641786545216\n",
      "The 30962 th iteration gives loss of 0.1581644952239708\n",
      "The 30963 th iteration gives loss of 0.15816257269007222\n",
      "The 30964 th iteration gives loss of 0.15816065026374404\n",
      "The 30965 th iteration gives loss of 0.1581587279449691\n",
      "The 30966 th iteration gives loss of 0.15815680573372604\n",
      "The 30967 th iteration gives loss of 0.15815488363001487\n",
      "The 30968 th iteration gives loss of 0.1581529616338028\n",
      "The 30969 th iteration gives loss of 0.15815103974508052\n",
      "The 30970 th iteration gives loss of 0.15814911796383627\n",
      "The 30971 th iteration gives loss of 0.15814719629005256\n",
      "The 30972 th iteration gives loss of 0.15814527472371243\n",
      "The 30973 th iteration gives loss of 0.15814335326479845\n",
      "The 30974 th iteration gives loss of 0.15814143191329674\n",
      "The 30975 th iteration gives loss of 0.15813951066919651\n",
      "The 30976 th iteration gives loss of 0.15813758953248205\n",
      "The 30977 th iteration gives loss of 0.158135668503133\n",
      "The 30978 th iteration gives loss of 0.158133747581127\n",
      "The 30979 th iteration gives loss of 0.15813182676646662\n",
      "The 30980 th iteration gives loss of 0.1581299060591282\n",
      "The 30981 th iteration gives loss of 0.15812798545909393\n",
      "The 30982 th iteration gives loss of 0.1581260649663489\n",
      "The 30983 th iteration gives loss of 0.15812414458087903\n",
      "The 30984 th iteration gives loss of 0.15812222430266948\n",
      "The 30985 th iteration gives loss of 0.15812030413170095\n",
      "The 30986 th iteration gives loss of 0.15811838406796103\n",
      "The 30987 th iteration gives loss of 0.15811646411144265\n",
      "The 30988 th iteration gives loss of 0.1581145442621195\n",
      "The 30989 th iteration gives loss of 0.15811262451997413\n",
      "The 30990 th iteration gives loss of 0.1581107048849931\n",
      "The 30991 th iteration gives loss of 0.1581087853571698\n",
      "The 30992 th iteration gives loss of 0.15810686593647882\n",
      "The 30993 th iteration gives loss of 0.15810494662291608\n",
      "The 30994 th iteration gives loss of 0.1581030274164541\n",
      "The 30995 th iteration gives loss of 0.1581011083170837\n",
      "The 30996 th iteration gives loss of 0.15809918932478903\n",
      "The 30997 th iteration gives loss of 0.15809727043955446\n",
      "The 30998 th iteration gives loss of 0.15809535166136027\n",
      "The 30999 th iteration gives loss of 0.1580934329901997\n",
      "The 31000 th iteration gives loss of 0.15809151442605596\n",
      "The 31001 th iteration gives loss of 0.1580895959689061\n",
      "The 31002 th iteration gives loss of 0.1580876776187433\n",
      "The 31003 th iteration gives loss of 0.15808575937554084\n",
      "The 31004 th iteration gives loss of 0.15808384123929475\n",
      "The 31005 th iteration gives loss of 0.15808192320998987\n",
      "The 31006 th iteration gives loss of 0.15808000528760005\n",
      "The 31007 th iteration gives loss of 0.15807808747211707\n",
      "The 31008 th iteration gives loss of 0.15807616976352373\n",
      "The 31009 th iteration gives loss of 0.15807425216181226\n",
      "The 31010 th iteration gives loss of 0.1580723346669526\n",
      "The 31011 th iteration gives loss of 0.1580704172789498\n",
      "The 31012 th iteration gives loss of 0.15806849999776698\n",
      "The 31013 th iteration gives loss of 0.15806658282339636\n",
      "The 31014 th iteration gives loss of 0.15806466575582698\n",
      "The 31015 th iteration gives loss of 0.1580627487950461\n",
      "The 31016 th iteration gives loss of 0.1580608319410262\n",
      "The 31017 th iteration gives loss of 0.15805891519376564\n",
      "The 31018 th iteration gives loss of 0.1580569985532469\n",
      "The 31019 th iteration gives loss of 0.15805508201944907\n",
      "The 31020 th iteration gives loss of 0.15805316559234464\n",
      "The 31021 th iteration gives loss of 0.15805124927194467\n",
      "The 31022 th iteration gives loss of 0.15804933305821833\n",
      "The 31023 th iteration gives loss of 0.1580474169511517\n",
      "The 31024 th iteration gives loss of 0.15804550095073702\n",
      "The 31025 th iteration gives loss of 0.15804358505694976\n",
      "The 31026 th iteration gives loss of 0.15804166926977575\n",
      "The 31027 th iteration gives loss of 0.15803975358920436\n",
      "The 31028 th iteration gives loss of 0.15803783801521867\n",
      "The 31029 th iteration gives loss of 0.1580359225477962\n",
      "The 31030 th iteration gives loss of 0.15803400718693073\n",
      "The 31031 th iteration gives loss of 0.15803209193260823\n",
      "The 31032 th iteration gives loss of 0.1580301767848072\n",
      "The 31033 th iteration gives loss of 0.15802826174351178\n",
      "The 31034 th iteration gives loss of 0.15802634680870775\n",
      "The 31035 th iteration gives loss of 0.1580244319803926\n",
      "The 31036 th iteration gives loss of 0.15802251725854008\n",
      "The 31037 th iteration gives loss of 0.15802060264313098\n",
      "The 31038 th iteration gives loss of 0.15801868813414566\n",
      "The 31039 th iteration gives loss of 0.15801677373157821\n",
      "The 31040 th iteration gives loss of 0.1580148594354217\n",
      "The 31041 th iteration gives loss of 0.1580129452456463\n",
      "The 31042 th iteration gives loss of 0.15801103116224394\n",
      "The 31043 th iteration gives loss of 0.15800911718519922\n",
      "The 31044 th iteration gives loss of 0.1580072033144899\n",
      "The 31045 th iteration gives loss of 0.15800528955011287\n",
      "The 31046 th iteration gives loss of 0.15800337589204122\n",
      "The 31047 th iteration gives loss of 0.1580014623402647\n",
      "The 31048 th iteration gives loss of 0.15799954889476994\n",
      "The 31049 th iteration gives loss of 0.15799763555553378\n",
      "The 31050 th iteration gives loss of 0.15799572232254766\n",
      "The 31051 th iteration gives loss of 0.1579938091957981\n",
      "The 31052 th iteration gives loss of 0.1579918961752712\n",
      "The 31053 th iteration gives loss of 0.1579899832609452\n",
      "The 31054 th iteration gives loss of 0.15798807045281266\n",
      "The 31055 th iteration gives loss of 0.15798615775084873\n",
      "The 31056 th iteration gives loss of 0.15798424515504572\n",
      "The 31057 th iteration gives loss of 0.15798233266538173\n",
      "The 31058 th iteration gives loss of 0.15798042028184023\n",
      "The 31059 th iteration gives loss of 0.15797850800441093\n",
      "The 31060 th iteration gives loss of 0.15797659583308654\n",
      "The 31061 th iteration gives loss of 0.15797468376784085\n",
      "The 31062 th iteration gives loss of 0.1579727718086592\n",
      "The 31063 th iteration gives loss of 0.1579708599555347\n",
      "The 31064 th iteration gives loss of 0.15796894820844162\n",
      "The 31065 th iteration gives loss of 0.15796703656737132\n",
      "The 31066 th iteration gives loss of 0.15796512503231186\n",
      "The 31067 th iteration gives loss of 0.1579632136032392\n",
      "The 31068 th iteration gives loss of 0.15796130228013613\n",
      "The 31069 th iteration gives loss of 0.15795939106299955\n",
      "The 31070 th iteration gives loss of 0.15795747995180662\n",
      "The 31071 th iteration gives loss of 0.15795556894654203\n",
      "The 31072 th iteration gives loss of 0.15795365804719672\n",
      "The 31073 th iteration gives loss of 0.15795174725375513\n",
      "The 31074 th iteration gives loss of 0.15794983656618874\n",
      "The 31075 th iteration gives loss of 0.15794792598449692\n",
      "The 31076 th iteration gives loss of 0.15794601550865736\n",
      "The 31077 th iteration gives loss of 0.15794410513866003\n",
      "The 31078 th iteration gives loss of 0.15794219487448105\n",
      "The 31079 th iteration gives loss of 0.15794028471611518\n",
      "The 31080 th iteration gives loss of 0.15793837466353713\n",
      "The 31081 th iteration gives loss of 0.15793646471673695\n",
      "The 31082 th iteration gives loss of 0.15793455487571076\n",
      "The 31083 th iteration gives loss of 0.1579326451404243\n",
      "The 31084 th iteration gives loss of 0.15793073551087553\n",
      "The 31085 th iteration gives loss of 0.1579288259870343\n",
      "The 31086 th iteration gives loss of 0.15792691656890548\n",
      "The 31087 th iteration gives loss of 0.15792500725646352\n",
      "The 31088 th iteration gives loss of 0.1579230980496899\n",
      "The 31089 th iteration gives loss of 0.15792118894857793\n",
      "The 31090 th iteration gives loss of 0.15791927995310692\n",
      "The 31091 th iteration gives loss of 0.1579173710632578\n",
      "The 31092 th iteration gives loss of 0.1579154622790246\n",
      "The 31093 th iteration gives loss of 0.1579135536003886\n",
      "The 31094 th iteration gives loss of 0.15791164502733257\n",
      "The 31095 th iteration gives loss of 0.15790973655984195\n",
      "The 31096 th iteration gives loss of 0.15790782819790486\n",
      "The 31097 th iteration gives loss of 0.15790591994150976\n",
      "The 31098 th iteration gives loss of 0.15790401179062866\n",
      "The 31099 th iteration gives loss of 0.15790210374525196\n",
      "The 31100 th iteration gives loss of 0.1579001958053639\n",
      "The 31101 th iteration gives loss of 0.1578982879709528\n",
      "The 31102 th iteration gives loss of 0.1578963802420099\n",
      "The 31103 th iteration gives loss of 0.15789447261851286\n",
      "The 31104 th iteration gives loss of 0.15789256510043373\n",
      "The 31105 th iteration gives loss of 0.15789065768778227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 31106 th iteration gives loss of 0.1578887503805317\n",
      "The 31107 th iteration gives loss of 0.157886843178659\n",
      "The 31108 th iteration gives loss of 0.15788493608215962\n",
      "The 31109 th iteration gives loss of 0.15788302909101015\n",
      "The 31110 th iteration gives loss of 0.15788112220520423\n",
      "The 31111 th iteration gives loss of 0.15787921542472932\n",
      "The 31112 th iteration gives loss of 0.15787730874955994\n",
      "The 31113 th iteration gives loss of 0.15787540217968019\n",
      "The 31114 th iteration gives loss of 0.1578734957150823\n",
      "The 31115 th iteration gives loss of 0.15787158935575007\n",
      "The 31116 th iteration gives loss of 0.1578696831016714\n",
      "The 31117 th iteration gives loss of 0.15786777695282111\n",
      "The 31118 th iteration gives loss of 0.1578658709091951\n",
      "The 31119 th iteration gives loss of 0.15786396497077132\n",
      "The 31120 th iteration gives loss of 0.1578620591375341\n",
      "The 31121 th iteration gives loss of 0.15786015340947918\n",
      "The 31122 th iteration gives loss of 0.15785824778657828\n",
      "The 31123 th iteration gives loss of 0.1578563422688138\n",
      "The 31124 th iteration gives loss of 0.1578544368561863\n",
      "The 31125 th iteration gives loss of 0.1578525315486745\n",
      "The 31126 th iteration gives loss of 0.15785062634625122\n",
      "The 31127 th iteration gives loss of 0.15784872124892538\n",
      "The 31128 th iteration gives loss of 0.1578468162566624\n",
      "The 31129 th iteration gives loss of 0.15784491136945436\n",
      "The 31130 th iteration gives loss of 0.1578430065872853\n",
      "The 31131 th iteration gives loss of 0.15784110191013656\n",
      "The 31132 th iteration gives loss of 0.15783919733799828\n",
      "The 31133 th iteration gives loss of 0.15783729287085763\n",
      "The 31134 th iteration gives loss of 0.15783538850868456\n",
      "The 31135 th iteration gives loss of 0.1578334842514849\n",
      "The 31136 th iteration gives loss of 0.15783158009923232\n",
      "The 31137 th iteration gives loss of 0.15782967605191578\n",
      "The 31138 th iteration gives loss of 0.15782777210951318\n",
      "The 31139 th iteration gives loss of 0.15782586827202022\n",
      "The 31140 th iteration gives loss of 0.1578239645394093\n",
      "The 31141 th iteration gives loss of 0.15782206091167755\n",
      "The 31142 th iteration gives loss of 0.15782015738879718\n",
      "The 31143 th iteration gives loss of 0.1578182539707605\n",
      "The 31144 th iteration gives loss of 0.15781635065754965\n",
      "The 31145 th iteration gives loss of 0.15781444744915438\n",
      "The 31146 th iteration gives loss of 0.15781254434556793\n",
      "The 31147 th iteration gives loss of 0.1578106413467542\n",
      "The 31148 th iteration gives loss of 0.1578087384527109\n",
      "The 31149 th iteration gives loss of 0.1578068356634259\n",
      "The 31150 th iteration gives loss of 0.15780493297888007\n",
      "The 31151 th iteration gives loss of 0.1578030303990474\n",
      "The 31152 th iteration gives loss of 0.15780112792393675\n",
      "The 31153 th iteration gives loss of 0.1577992255535123\n",
      "The 31154 th iteration gives loss of 0.1577973232877745\n",
      "The 31155 th iteration gives loss of 0.15779542112669281\n",
      "The 31156 th iteration gives loss of 0.15779351907025307\n",
      "The 31157 th iteration gives loss of 0.15779161711845244\n",
      "The 31158 th iteration gives loss of 0.1577897152712749\n",
      "The 31159 th iteration gives loss of 0.15778781352869287\n",
      "The 31160 th iteration gives loss of 0.1577859118907079\n",
      "The 31161 th iteration gives loss of 0.15778401035729667\n",
      "The 31162 th iteration gives loss of 0.15778210892843667\n",
      "The 31163 th iteration gives loss of 0.1577802076041327\n",
      "The 31164 th iteration gives loss of 0.15777830638434606\n",
      "The 31165 th iteration gives loss of 0.1577764052690802\n",
      "The 31166 th iteration gives loss of 0.15777450425831013\n",
      "The 31167 th iteration gives loss of 0.1577726033520239\n",
      "The 31168 th iteration gives loss of 0.1577707025502064\n",
      "The 31169 th iteration gives loss of 0.1577688018528447\n",
      "The 31170 th iteration gives loss of 0.15776690125992293\n",
      "The 31171 th iteration gives loss of 0.15776500077142983\n",
      "The 31172 th iteration gives loss of 0.15776310038733946\n",
      "The 31173 th iteration gives loss of 0.15776120010763872\n",
      "The 31174 th iteration gives loss of 0.15775929993232515\n",
      "The 31175 th iteration gives loss of 0.15775739986136858\n",
      "The 31176 th iteration gives loss of 0.1577554998947734\n",
      "The 31177 th iteration gives loss of 0.15775360003250793\n",
      "The 31178 th iteration gives loss of 0.15775170027456423\n",
      "The 31179 th iteration gives loss of 0.1577498006209209\n",
      "The 31180 th iteration gives loss of 0.1577479010715713\n",
      "The 31181 th iteration gives loss of 0.15774600162649252\n",
      "The 31182 th iteration gives loss of 0.15774410228567687\n",
      "The 31183 th iteration gives loss of 0.15774220304910253\n",
      "The 31184 th iteration gives loss of 0.15774030391676433\n",
      "The 31185 th iteration gives loss of 0.15773840488864274\n",
      "The 31186 th iteration gives loss of 0.15773650596471414\n",
      "The 31187 th iteration gives loss of 0.15773460714497628\n",
      "The 31188 th iteration gives loss of 0.15773270842941303\n",
      "The 31189 th iteration gives loss of 0.15773080981799462\n",
      "The 31190 th iteration gives loss of 0.15772891131072814\n",
      "The 31191 th iteration gives loss of 0.15772701290757876\n",
      "The 31192 th iteration gives loss of 0.15772511460854235\n",
      "The 31193 th iteration gives loss of 0.15772321641360454\n",
      "The 31194 th iteration gives loss of 0.1577213183227488\n",
      "The 31195 th iteration gives loss of 0.15771942033596378\n",
      "The 31196 th iteration gives loss of 0.1577175224532248\n",
      "The 31197 th iteration gives loss of 0.15771562467452\n",
      "The 31198 th iteration gives loss of 0.15771372699983582\n",
      "The 31199 th iteration gives loss of 0.1577118294291678\n",
      "The 31200 th iteration gives loss of 0.15770993196248645\n",
      "The 31201 th iteration gives loss of 0.15770803459978583\n",
      "The 31202 th iteration gives loss of 0.15770613734104233\n",
      "The 31203 th iteration gives loss of 0.1577042401862467\n",
      "The 31204 th iteration gives loss of 0.15770234313538584\n",
      "The 31205 th iteration gives loss of 0.1577004461884444\n",
      "The 31206 th iteration gives loss of 0.15769854934539995\n",
      "The 31207 th iteration gives loss of 0.1576966526062476\n",
      "The 31208 th iteration gives loss of 0.15769475597097263\n",
      "The 31209 th iteration gives loss of 0.1576928594395519\n",
      "The 31210 th iteration gives loss of 0.1576909630119791\n",
      "The 31211 th iteration gives loss of 0.1576890666882254\n",
      "The 31212 th iteration gives loss of 0.1576871704682878\n",
      "The 31213 th iteration gives loss of 0.15768527435215707\n",
      "The 31214 th iteration gives loss of 0.157683378339805\n",
      "The 31215 th iteration gives loss of 0.15768148243122607\n",
      "The 31216 th iteration gives loss of 0.15767958662639364\n",
      "The 31217 th iteration gives loss of 0.15767769092530234\n",
      "The 31218 th iteration gives loss of 0.15767579532793854\n",
      "The 31219 th iteration gives loss of 0.15767389983429295\n",
      "The 31220 th iteration gives loss of 0.15767200444433482\n",
      "The 31221 th iteration gives loss of 0.157670109158051\n",
      "The 31222 th iteration gives loss of 0.1576682139754415\n",
      "The 31223 th iteration gives loss of 0.15766631889648264\n",
      "The 31224 th iteration gives loss of 0.1576644239211578\n",
      "The 31225 th iteration gives loss of 0.15766252904945768\n",
      "The 31226 th iteration gives loss of 0.15766063428136284\n",
      "The 31227 th iteration gives loss of 0.15765873961685384\n",
      "The 31228 th iteration gives loss of 0.1576568450559262\n",
      "The 31229 th iteration gives loss of 0.15765495059855766\n",
      "The 31230 th iteration gives loss of 0.15765305624474266\n",
      "The 31231 th iteration gives loss of 0.15765116199444984\n",
      "The 31232 th iteration gives loss of 0.15764926784767935\n",
      "The 31233 th iteration gives loss of 0.15764737380441146\n",
      "The 31234 th iteration gives loss of 0.15764547986463057\n",
      "The 31235 th iteration gives loss of 0.15764358602832507\n",
      "The 31236 th iteration gives loss of 0.1576416922954789\n",
      "The 31237 th iteration gives loss of 0.15763979866607095\n",
      "The 31238 th iteration gives loss of 0.1576379051400938\n",
      "The 31239 th iteration gives loss of 0.15763601171753147\n",
      "The 31240 th iteration gives loss of 0.15763411839837904\n",
      "The 31241 th iteration gives loss of 0.15763222518259806\n",
      "The 31242 th iteration gives loss of 0.15763033207018762\n",
      "The 31243 th iteration gives loss of 0.1576284390611335\n",
      "The 31244 th iteration gives loss of 0.15762654615542046\n",
      "The 31245 th iteration gives loss of 0.15762465335303016\n",
      "The 31246 th iteration gives loss of 0.1576227606539574\n",
      "The 31247 th iteration gives loss of 0.15762086805817835\n",
      "The 31248 th iteration gives loss of 0.15761897556568016\n",
      "The 31249 th iteration gives loss of 0.1576170831764443\n",
      "The 31250 th iteration gives loss of 0.15761519089045892\n",
      "The 31251 th iteration gives loss of 0.1576132987077136\n",
      "The 31252 th iteration gives loss of 0.15761140662819312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 31253 th iteration gives loss of 0.15760951465187636\n",
      "The 31254 th iteration gives loss of 0.1576076227787579\n",
      "The 31255 th iteration gives loss of 0.15760573100880906\n",
      "The 31256 th iteration gives loss of 0.15760383934203165\n",
      "The 31257 th iteration gives loss of 0.1576019477783984\n",
      "The 31258 th iteration gives loss of 0.15760005631790205\n",
      "The 31259 th iteration gives loss of 0.1575981649605151\n",
      "The 31260 th iteration gives loss of 0.1575962737062452\n",
      "The 31261 th iteration gives loss of 0.15759438255505104\n",
      "The 31262 th iteration gives loss of 0.15759249150694218\n",
      "The 31263 th iteration gives loss of 0.15759060056188484\n",
      "The 31264 th iteration gives loss of 0.15758870971988667\n",
      "The 31265 th iteration gives loss of 0.15758681898090438\n",
      "The 31266 th iteration gives loss of 0.157584928344944\n",
      "The 31267 th iteration gives loss of 0.1575830378119846\n",
      "The 31268 th iteration gives loss of 0.1575811473820108\n",
      "The 31269 th iteration gives loss of 0.1575792570550106\n",
      "The 31270 th iteration gives loss of 0.15757736683097426\n",
      "The 31271 th iteration gives loss of 0.157575476709869\n",
      "The 31272 th iteration gives loss of 0.15757358669169705\n",
      "The 31273 th iteration gives loss of 0.1575716967764358\n",
      "The 31274 th iteration gives loss of 0.1575698069640799\n",
      "The 31275 th iteration gives loss of 0.15756791725459673\n",
      "The 31276 th iteration gives loss of 0.15756602764799113\n",
      "The 31277 th iteration gives loss of 0.1575641381442316\n",
      "The 31278 th iteration gives loss of 0.15756224874331262\n",
      "The 31279 th iteration gives loss of 0.1575603594452274\n",
      "The 31280 th iteration gives loss of 0.15755847024994338\n",
      "The 31281 th iteration gives loss of 0.15755658115745874\n",
      "The 31282 th iteration gives loss of 0.15755469216775203\n",
      "The 31283 th iteration gives loss of 0.15755280328081764\n",
      "The 31284 th iteration gives loss of 0.157550914496634\n",
      "The 31285 th iteration gives loss of 0.15754902581517988\n",
      "The 31286 th iteration gives loss of 0.1575471372364557\n",
      "The 31287 th iteration gives loss of 0.15754524876043366\n",
      "The 31288 th iteration gives loss of 0.15754336038710215\n",
      "The 31289 th iteration gives loss of 0.15754147211645717\n",
      "The 31290 th iteration gives loss of 0.15753958394847004\n",
      "The 31291 th iteration gives loss of 0.15753769588313385\n",
      "The 31292 th iteration gives loss of 0.15753580792042982\n",
      "The 31293 th iteration gives loss of 0.15753392006034583\n",
      "The 31294 th iteration gives loss of 0.15753203230286564\n",
      "The 31295 th iteration gives loss of 0.15753014464797352\n",
      "The 31296 th iteration gives loss of 0.15752825709566723\n",
      "The 31297 th iteration gives loss of 0.15752636964590894\n",
      "The 31298 th iteration gives loss of 0.15752448229870383\n",
      "The 31299 th iteration gives loss of 0.15752259505403363\n",
      "The 31300 th iteration gives loss of 0.15752070791187506\n",
      "The 31301 th iteration gives loss of 0.15751882087221158\n",
      "The 31302 th iteration gives loss of 0.15751693393504776\n",
      "The 31303 th iteration gives loss of 0.15751504710035708\n",
      "The 31304 th iteration gives loss of 0.15751316036812604\n",
      "The 31305 th iteration gives loss of 0.15751127373833265\n",
      "The 31306 th iteration gives loss of 0.1575093872109639\n",
      "The 31307 th iteration gives loss of 0.15750750078601702\n",
      "The 31308 th iteration gives loss of 0.157505614463469\n",
      "The 31309 th iteration gives loss of 0.15750372824330633\n",
      "The 31310 th iteration gives loss of 0.15750184212551452\n",
      "The 31311 th iteration gives loss of 0.15749995611008083\n",
      "The 31312 th iteration gives loss of 0.15749807019698617\n",
      "The 31313 th iteration gives loss of 0.15749618438622207\n",
      "The 31314 th iteration gives loss of 0.15749429867776632\n",
      "The 31315 th iteration gives loss of 0.15749241307160425\n",
      "The 31316 th iteration gives loss of 0.15749052756774282\n",
      "The 31317 th iteration gives loss of 0.15748864216613737\n",
      "The 31318 th iteration gives loss of 0.1574867568667856\n",
      "The 31319 th iteration gives loss of 0.15748487166966726\n",
      "The 31320 th iteration gives loss of 0.15748298657478527\n",
      "The 31321 th iteration gives loss of 0.15748110158210982\n",
      "The 31322 th iteration gives loss of 0.1574792166916276\n",
      "The 31323 th iteration gives loss of 0.1574773319033224\n",
      "The 31324 th iteration gives loss of 0.1574754472171887\n",
      "The 31325 th iteration gives loss of 0.15747356263320705\n",
      "The 31326 th iteration gives loss of 0.15747167815135915\n",
      "The 31327 th iteration gives loss of 0.15746979377163411\n",
      "The 31328 th iteration gives loss of 0.1574679094940227\n",
      "The 31329 th iteration gives loss of 0.15746602531850218\n",
      "The 31330 th iteration gives loss of 0.15746414124506364\n",
      "The 31331 th iteration gives loss of 0.15746225727368696\n",
      "The 31332 th iteration gives loss of 0.157460373404358\n",
      "The 31333 th iteration gives loss of 0.1574584896370686\n",
      "The 31334 th iteration gives loss of 0.15745660597179628\n",
      "The 31335 th iteration gives loss of 0.15745472240853856\n",
      "The 31336 th iteration gives loss of 0.15745283894725848\n",
      "The 31337 th iteration gives loss of 0.1574509555879646\n",
      "The 31338 th iteration gives loss of 0.15744907233063835\n",
      "The 31339 th iteration gives loss of 0.15744718917525719\n",
      "The 31340 th iteration gives loss of 0.1574453061217995\n",
      "The 31341 th iteration gives loss of 0.15744342317026513\n",
      "The 31342 th iteration gives loss of 0.15744154032063581\n",
      "The 31343 th iteration gives loss of 0.15743965757289632\n",
      "The 31344 th iteration gives loss of 0.15743777492702837\n",
      "The 31345 th iteration gives loss of 0.15743589238302916\n",
      "The 31346 th iteration gives loss of 0.15743400994087992\n",
      "The 31347 th iteration gives loss of 0.15743212760055506\n",
      "The 31348 th iteration gives loss of 0.15743024536205003\n",
      "The 31349 th iteration gives loss of 0.15742836322534326\n",
      "The 31350 th iteration gives loss of 0.1574264811904282\n",
      "The 31351 th iteration gives loss of 0.15742459925729088\n",
      "The 31352 th iteration gives loss of 0.15742271742590233\n",
      "The 31353 th iteration gives loss of 0.15742083569626464\n",
      "The 31354 th iteration gives loss of 0.15741895406835646\n",
      "The 31355 th iteration gives loss of 0.1574170725421681\n",
      "The 31356 th iteration gives loss of 0.15741519111766827\n",
      "The 31357 th iteration gives loss of 0.15741330979486856\n",
      "The 31358 th iteration gives loss of 0.157411428573734\n",
      "The 31359 th iteration gives loss of 0.1574095474542567\n",
      "The 31360 th iteration gives loss of 0.15740766643642942\n",
      "The 31361 th iteration gives loss of 0.15740578552022136\n",
      "The 31362 th iteration gives loss of 0.15740390470563806\n",
      "The 31363 th iteration gives loss of 0.15740202399264475\n",
      "The 31364 th iteration gives loss of 0.15740014338124103\n",
      "The 31365 th iteration gives loss of 0.1573982628714144\n",
      "The 31366 th iteration gives loss of 0.1573963824631336\n",
      "The 31367 th iteration gives loss of 0.15739450215639814\n",
      "The 31368 th iteration gives loss of 0.15739262195118306\n",
      "The 31369 th iteration gives loss of 0.157390741847495\n",
      "The 31370 th iteration gives loss of 0.15738886184530027\n",
      "The 31371 th iteration gives loss of 0.1573869819445812\n",
      "The 31372 th iteration gives loss of 0.15738510214533777\n",
      "The 31373 th iteration gives loss of 0.15738322244754688\n",
      "The 31374 th iteration gives loss of 0.15738134285120195\n",
      "The 31375 th iteration gives loss of 0.15737946335627578\n",
      "The 31376 th iteration gives loss of 0.15737758396276397\n",
      "The 31377 th iteration gives loss of 0.15737570467064896\n",
      "The 31378 th iteration gives loss of 0.15737382547992068\n",
      "The 31379 th iteration gives loss of 0.1573719463905595\n",
      "The 31380 th iteration gives loss of 0.1573700674025484\n",
      "The 31381 th iteration gives loss of 0.15736818851587106\n",
      "The 31382 th iteration gives loss of 0.15736630973052998\n",
      "The 31383 th iteration gives loss of 0.15736443104649478\n",
      "The 31384 th iteration gives loss of 0.15736255246375788\n",
      "The 31385 th iteration gives loss of 0.15736067398229575\n",
      "The 31386 th iteration gives loss of 0.15735879560210073\n",
      "The 31387 th iteration gives loss of 0.15735691732316223\n",
      "The 31388 th iteration gives loss of 0.1573550391454645\n",
      "The 31389 th iteration gives loss of 0.157353161068986\n",
      "The 31390 th iteration gives loss of 0.15735128309371782\n",
      "The 31391 th iteration gives loss of 0.15734940521964602\n",
      "The 31392 th iteration gives loss of 0.15734752744675642\n",
      "The 31393 th iteration gives loss of 0.15734564977502838\n",
      "The 31394 th iteration gives loss of 0.15734377220446408\n",
      "The 31395 th iteration gives loss of 0.15734189473502333\n",
      "The 31396 th iteration gives loss of 0.15734001736671116\n",
      "The 31397 th iteration gives loss of 0.15733814009950886\n",
      "The 31398 th iteration gives loss of 0.15733626293339104\n",
      "The 31399 th iteration gives loss of 0.1573343858683583\n",
      "The 31400 th iteration gives loss of 0.15733250890439235\n",
      "The 31401 th iteration gives loss of 0.1573306320414733\n",
      "The 31402 th iteration gives loss of 0.15732875527958776\n",
      "The 31403 th iteration gives loss of 0.15732687861873804\n",
      "The 31404 th iteration gives loss of 0.1573250020588798\n",
      "The 31405 th iteration gives loss of 0.15732312560002126\n",
      "The 31406 th iteration gives loss of 0.15732124924214988\n",
      "The 31407 th iteration gives loss of 0.15731937298523102\n",
      "The 31408 th iteration gives loss of 0.15731749682926774\n",
      "The 31409 th iteration gives loss of 0.15731562077423578\n",
      "The 31410 th iteration gives loss of 0.15731374482013247\n",
      "The 31411 th iteration gives loss of 0.15731186896692728\n",
      "The 31412 th iteration gives loss of 0.15730999321462721\n",
      "The 31413 th iteration gives loss of 0.1573081175631935\n",
      "The 31414 th iteration gives loss of 0.15730624201262813\n",
      "The 31415 th iteration gives loss of 0.15730436656290867\n",
      "The 31416 th iteration gives loss of 0.1573024912140289\n",
      "The 31417 th iteration gives loss of 0.15730061596596884\n",
      "The 31418 th iteration gives loss of 0.1572987408187204\n",
      "The 31419 th iteration gives loss of 0.15729686577225133\n",
      "The 31420 th iteration gives loss of 0.15729499082656448\n",
      "The 31421 th iteration gives loss of 0.15729311598163992\n",
      "The 31422 th iteration gives loss of 0.1572912412374684\n",
      "The 31423 th iteration gives loss of 0.15728936659402803\n",
      "The 31424 th iteration gives loss of 0.157287492051314\n",
      "The 31425 th iteration gives loss of 0.15728561760930582\n",
      "The 31426 th iteration gives loss of 0.15728374326798353\n",
      "The 31427 th iteration gives loss of 0.15728186902733407\n",
      "The 31428 th iteration gives loss of 0.157279994887355\n",
      "The 31429 th iteration gives loss of 0.1572781208480255\n",
      "The 31430 th iteration gives loss of 0.1572762469093229\n",
      "The 31431 th iteration gives loss of 0.15727437307124656\n",
      "The 31432 th iteration gives loss of 0.15727249933377066\n",
      "The 31433 th iteration gives loss of 0.1572706256968854\n",
      "The 31434 th iteration gives loss of 0.1572687521605775\n",
      "The 31435 th iteration gives loss of 0.15726687872483353\n",
      "The 31436 th iteration gives loss of 0.15726500538963606\n",
      "The 31437 th iteration gives loss of 0.15726313215498292\n",
      "The 31438 th iteration gives loss of 0.157261259020839\n",
      "The 31439 th iteration gives loss of 0.15725938598719755\n",
      "The 31440 th iteration gives loss of 0.1572575130540549\n",
      "The 31441 th iteration gives loss of 0.15725564022137872\n",
      "The 31442 th iteration gives loss of 0.1572537674891781\n",
      "The 31443 th iteration gives loss of 0.15725189485741245\n",
      "The 31444 th iteration gives loss of 0.15725002232609517\n",
      "The 31445 th iteration gives loss of 0.15724814989518274\n",
      "The 31446 th iteration gives loss of 0.15724627756468787\n",
      "The 31447 th iteration gives loss of 0.15724440533457348\n",
      "The 31448 th iteration gives loss of 0.15724253320483522\n",
      "The 31449 th iteration gives loss of 0.1572406611754666\n",
      "The 31450 th iteration gives loss of 0.15723878924643597\n",
      "The 31451 th iteration gives loss of 0.15723691741774437\n",
      "The 31452 th iteration gives loss of 0.15723504568937843\n",
      "The 31453 th iteration gives loss of 0.15723317406131326\n",
      "The 31454 th iteration gives loss of 0.15723130253354023\n",
      "The 31455 th iteration gives loss of 0.1572294311060435\n",
      "The 31456 th iteration gives loss of 0.15722755977879946\n",
      "The 31457 th iteration gives loss of 0.1572256885518084\n",
      "The 31458 th iteration gives loss of 0.1572238174250565\n",
      "The 31459 th iteration gives loss of 0.1572219463985192\n",
      "The 31460 th iteration gives loss of 0.1572200754721867\n",
      "The 31461 th iteration gives loss of 0.15721820464604877\n",
      "The 31462 th iteration gives loss of 0.15721633392008588\n",
      "The 31463 th iteration gives loss of 0.15721446329428307\n",
      "The 31464 th iteration gives loss of 0.15721259276862976\n",
      "The 31465 th iteration gives loss of 0.1572107223431056\n",
      "The 31466 th iteration gives loss of 0.15720885201771004\n",
      "The 31467 th iteration gives loss of 0.1572069817924189\n",
      "The 31468 th iteration gives loss of 0.1572051116672056\n",
      "The 31469 th iteration gives loss of 0.15720324164207963\n",
      "The 31470 th iteration gives loss of 0.15720137171701398\n",
      "The 31471 th iteration gives loss of 0.15719950189200216\n",
      "The 31472 th iteration gives loss of 0.15719763216701785\n",
      "The 31473 th iteration gives loss of 0.15719576254205894\n",
      "The 31474 th iteration gives loss of 0.15719389301710057\n",
      "The 31475 th iteration gives loss of 0.15719202359212936\n",
      "The 31476 th iteration gives loss of 0.15719015426713867\n",
      "The 31477 th iteration gives loss of 0.15718828504211602\n",
      "The 31478 th iteration gives loss of 0.15718641591703594\n",
      "The 31479 th iteration gives loss of 0.15718454689189323\n",
      "The 31480 th iteration gives loss of 0.1571826779666761\n",
      "The 31481 th iteration gives loss of 0.15718080914135543\n",
      "The 31482 th iteration gives loss of 0.15717894041593042\n",
      "The 31483 th iteration gives loss of 0.15717707179038543\n",
      "The 31484 th iteration gives loss of 0.15717520326470444\n",
      "The 31485 th iteration gives loss of 0.15717333483886659\n",
      "The 31486 th iteration gives loss of 0.157171466512871\n",
      "The 31487 th iteration gives loss of 0.15716959828669333\n",
      "The 31488 th iteration gives loss of 0.15716773016031588\n",
      "The 31489 th iteration gives loss of 0.1571658621337353\n",
      "The 31490 th iteration gives loss of 0.1571639942069355\n",
      "The 31491 th iteration gives loss of 0.15716212637989932\n",
      "The 31492 th iteration gives loss of 0.15716025865260302\n",
      "The 31493 th iteration gives loss of 0.15715839102505724\n",
      "The 31494 th iteration gives loss of 0.15715652349721862\n",
      "The 31495 th iteration gives loss of 0.15715465606909465\n",
      "The 31496 th iteration gives loss of 0.15715278874066393\n",
      "The 31497 th iteration gives loss of 0.15715092151190638\n",
      "The 31498 th iteration gives loss of 0.15714905438282287\n",
      "The 31499 th iteration gives loss of 0.1571471873533814\n",
      "The 31500 th iteration gives loss of 0.15714532042358162\n",
      "The 31501 th iteration gives loss of 0.15714345359340198\n",
      "The 31502 th iteration gives loss of 0.15714158686282886\n",
      "The 31503 th iteration gives loss of 0.15713972023185396\n",
      "The 31504 th iteration gives loss of 0.15713785370045225\n",
      "The 31505 th iteration gives loss of 0.1571359872686214\n",
      "The 31506 th iteration gives loss of 0.1571341209363351\n",
      "The 31507 th iteration gives loss of 0.15713225470359515\n",
      "The 31508 th iteration gives loss of 0.15713038857037687\n",
      "The 31509 th iteration gives loss of 0.15712852253665574\n",
      "The 31510 th iteration gives loss of 0.15712665660243774\n",
      "The 31511 th iteration gives loss of 0.15712479076770172\n",
      "The 31512 th iteration gives loss of 0.15712292503242725\n",
      "The 31513 th iteration gives loss of 0.157121059396611\n",
      "The 31514 th iteration gives loss of 0.15711919386023007\n",
      "The 31515 th iteration gives loss of 0.15711732842327322\n",
      "The 31516 th iteration gives loss of 0.15711546308571428\n",
      "The 31517 th iteration gives loss of 0.15711359784756707\n",
      "The 31518 th iteration gives loss of 0.15711173270879758\n",
      "The 31519 th iteration gives loss of 0.15710986766939336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 31520 th iteration gives loss of 0.15710800272934247\n",
      "The 31521 th iteration gives loss of 0.1571061378886311\n",
      "The 31522 th iteration gives loss of 0.1571042731472442\n",
      "The 31523 th iteration gives loss of 0.15710240850516363\n",
      "The 31524 th iteration gives loss of 0.1571005439623853\n",
      "The 31525 th iteration gives loss of 0.15709867951888964\n",
      "The 31526 th iteration gives loss of 0.15709681517465834\n",
      "The 31527 th iteration gives loss of 0.15709495092968473\n",
      "The 31528 th iteration gives loss of 0.15709308678395123\n",
      "The 31529 th iteration gives loss of 0.1570912227374475\n",
      "The 31530 th iteration gives loss of 0.15708935879014246\n",
      "The 31531 th iteration gives loss of 0.1570874949420444\n",
      "The 31532 th iteration gives loss of 0.1570856311931281\n",
      "The 31533 th iteration gives loss of 0.15708376754338374\n",
      "The 31534 th iteration gives loss of 0.1570819039927978\n",
      "The 31535 th iteration gives loss of 0.15708004054134797\n",
      "The 31536 th iteration gives loss of 0.1570781771890336\n",
      "The 31537 th iteration gives loss of 0.15707631393582386\n",
      "The 31538 th iteration gives loss of 0.15707445078170823\n",
      "The 31539 th iteration gives loss of 0.15707258772668367\n",
      "The 31540 th iteration gives loss of 0.1570707247707315\n",
      "The 31541 th iteration gives loss of 0.1570688619138423\n",
      "The 31542 th iteration gives loss of 0.15706699915598774\n",
      "The 31543 th iteration gives loss of 0.15706513649715925\n",
      "The 31544 th iteration gives loss of 0.15706327393735553\n",
      "The 31545 th iteration gives loss of 0.15706141147654457\n",
      "The 31546 th iteration gives loss of 0.1570595491147253\n",
      "The 31547 th iteration gives loss of 0.15705768685187235\n",
      "The 31548 th iteration gives loss of 0.15705582468798454\n",
      "The 31549 th iteration gives loss of 0.15705396262303395\n",
      "The 31550 th iteration gives loss of 0.15705210065702144\n",
      "The 31551 th iteration gives loss of 0.1570502387899204\n",
      "The 31552 th iteration gives loss of 0.1570483770217143\n",
      "The 31553 th iteration gives loss of 0.1570465153524077\n",
      "The 31554 th iteration gives loss of 0.15704465378197083\n",
      "The 31555 th iteration gives loss of 0.1570427923103942\n",
      "The 31556 th iteration gives loss of 0.1570409309376618\n",
      "The 31557 th iteration gives loss of 0.157039069663761\n",
      "The 31558 th iteration gives loss of 0.15703720848867603\n",
      "The 31559 th iteration gives loss of 0.15703534741240688\n",
      "The 31560 th iteration gives loss of 0.15703348643492063\n",
      "The 31561 th iteration gives loss of 0.15703162555620628\n",
      "The 31562 th iteration gives loss of 0.15702976477625424\n",
      "The 31563 th iteration gives loss of 0.15702790409505274\n",
      "The 31564 th iteration gives loss of 0.15702604351258143\n",
      "The 31565 th iteration gives loss of 0.1570241830288366\n",
      "The 31566 th iteration gives loss of 0.15702232264379498\n",
      "The 31567 th iteration gives loss of 0.15702046235743652\n",
      "The 31568 th iteration gives loss of 0.15701860216976138\n",
      "The 31569 th iteration gives loss of 0.15701674208075264\n",
      "The 31570 th iteration gives loss of 0.15701488209038908\n",
      "The 31571 th iteration gives loss of 0.15701302219866203\n",
      "The 31572 th iteration gives loss of 0.15701116240555502\n",
      "The 31573 th iteration gives loss of 0.15700930271105162\n",
      "The 31574 th iteration gives loss of 0.15700744311515052\n",
      "The 31575 th iteration gives loss of 0.1570055836178235\n",
      "The 31576 th iteration gives loss of 0.15700372421906442\n",
      "The 31577 th iteration gives loss of 0.15700186491885462\n",
      "The 31578 th iteration gives loss of 0.15700000571718867\n",
      "The 31579 th iteration gives loss of 0.15699814661403408\n",
      "The 31580 th iteration gives loss of 0.15699628760939677\n",
      "The 31581 th iteration gives loss of 0.15699442870325817\n",
      "The 31582 th iteration gives loss of 0.15699256989559177\n",
      "The 31583 th iteration gives loss of 0.15699071118639463\n",
      "The 31584 th iteration gives loss of 0.156988852575656\n",
      "The 31585 th iteration gives loss of 0.1569869940633512\n",
      "The 31586 th iteration gives loss of 0.15698513564947217\n",
      "The 31587 th iteration gives loss of 0.15698327733400563\n",
      "The 31588 th iteration gives loss of 0.1569814191169333\n",
      "The 31589 th iteration gives loss of 0.15697956099825336\n",
      "The 31590 th iteration gives loss of 0.15697770297793526\n",
      "The 31591 th iteration gives loss of 0.15697584505597748\n",
      "The 31592 th iteration gives loss of 0.15697398723235786\n",
      "The 31593 th iteration gives loss of 0.1569721295070666\n",
      "The 31594 th iteration gives loss of 0.15697027188009235\n",
      "The 31595 th iteration gives loss of 0.15696841435142242\n",
      "The 31596 th iteration gives loss of 0.15696655692102565\n",
      "The 31597 th iteration gives loss of 0.1569646995889002\n",
      "The 31598 th iteration gives loss of 0.15696284235504457\n",
      "The 31599 th iteration gives loss of 0.15696098521941998\n",
      "The 31600 th iteration gives loss of 0.1569591281820322\n",
      "The 31601 th iteration gives loss of 0.15695727124285588\n",
      "The 31602 th iteration gives loss of 0.1569554144018888\n",
      "The 31603 th iteration gives loss of 0.1569535576591064\n",
      "The 31604 th iteration gives loss of 0.15695170101449557\n",
      "The 31605 th iteration gives loss of 0.15694984446804444\n",
      "The 31606 th iteration gives loss of 0.15694798801974308\n",
      "The 31607 th iteration gives loss of 0.15694613166957253\n",
      "The 31608 th iteration gives loss of 0.15694427541751851\n",
      "The 31609 th iteration gives loss of 0.15694241926357214\n",
      "The 31610 th iteration gives loss of 0.15694056320771507\n",
      "The 31611 th iteration gives loss of 0.1569387072499393\n",
      "The 31612 th iteration gives loss of 0.15693685139022162\n",
      "The 31613 th iteration gives loss of 0.15693499562854837\n",
      "The 31614 th iteration gives loss of 0.15693313996491073\n",
      "The 31615 th iteration gives loss of 0.15693128439929507\n",
      "The 31616 th iteration gives loss of 0.15692942893168713\n",
      "The 31617 th iteration gives loss of 0.1569275735620759\n",
      "The 31618 th iteration gives loss of 0.1569257182904427\n",
      "The 31619 th iteration gives loss of 0.15692386311677398\n",
      "The 31620 th iteration gives loss of 0.15692200804105016\n",
      "The 31621 th iteration gives loss of 0.15692015306327445\n",
      "The 31622 th iteration gives loss of 0.15691829818340858\n",
      "The 31623 th iteration gives loss of 0.15691644340146052\n",
      "The 31624 th iteration gives loss of 0.15691458871740602\n",
      "The 31625 th iteration gives loss of 0.1569127341312373\n",
      "The 31626 th iteration gives loss of 0.15691087964293796\n",
      "The 31627 th iteration gives loss of 0.1569090252524862\n",
      "The 31628 th iteration gives loss of 0.1569071709598734\n",
      "The 31629 th iteration gives loss of 0.15690531676509087\n",
      "The 31630 th iteration gives loss of 0.15690346266812205\n",
      "The 31631 th iteration gives loss of 0.15690160866895064\n",
      "The 31632 th iteration gives loss of 0.15689975476756135\n",
      "The 31633 th iteration gives loss of 0.1568979009639413\n",
      "The 31634 th iteration gives loss of 0.15689604725808057\n",
      "The 31635 th iteration gives loss of 0.1568941936499596\n",
      "The 31636 th iteration gives loss of 0.15689234013956319\n",
      "The 31637 th iteration gives loss of 0.15689048672689238\n",
      "The 31638 th iteration gives loss of 0.15688863341191822\n",
      "The 31639 th iteration gives loss of 0.1568867801946327\n",
      "The 31640 th iteration gives loss of 0.15688492707502213\n",
      "The 31641 th iteration gives loss of 0.15688307405307142\n",
      "The 31642 th iteration gives loss of 0.15688122112876687\n",
      "The 31643 th iteration gives loss of 0.15687936830209173\n",
      "The 31644 th iteration gives loss of 0.15687751557303986\n",
      "The 31645 th iteration gives loss of 0.15687566294158325\n",
      "The 31646 th iteration gives loss of 0.15687381040771886\n",
      "The 31647 th iteration gives loss of 0.15687195797143044\n",
      "The 31648 th iteration gives loss of 0.15687010563271006\n",
      "The 31649 th iteration gives loss of 0.15686825339154034\n",
      "The 31650 th iteration gives loss of 0.1568664012478972\n",
      "The 31651 th iteration gives loss of 0.15686454920177859\n",
      "The 31652 th iteration gives loss of 0.15686269725316582\n",
      "The 31653 th iteration gives loss of 0.1568608454020549\n",
      "The 31654 th iteration gives loss of 0.15685899364841444\n",
      "The 31655 th iteration gives loss of 0.15685714199224077\n",
      "The 31656 th iteration gives loss of 0.15685529043351723\n",
      "The 31657 th iteration gives loss of 0.15685343897223933\n",
      "The 31658 th iteration gives loss of 0.15685158760837847\n",
      "The 31659 th iteration gives loss of 0.15684973634193564\n",
      "The 31660 th iteration gives loss of 0.15684788517289522\n",
      "The 31661 th iteration gives loss of 0.15684603410122339\n",
      "The 31662 th iteration gives loss of 0.15684418312692588\n",
      "The 31663 th iteration gives loss of 0.156842332249978\n",
      "The 31664 th iteration gives loss of 0.15684048147037372\n",
      "The 31665 th iteration gives loss of 0.15683863078810129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 31666 th iteration gives loss of 0.15683678020314412\n",
      "The 31667 th iteration gives loss of 0.1568349297154836\n",
      "The 31668 th iteration gives loss of 0.15683307932510385\n",
      "The 31669 th iteration gives loss of 0.15683122903200067\n",
      "The 31670 th iteration gives loss of 0.15682937883616335\n",
      "The 31671 th iteration gives loss of 0.15682752873755937\n",
      "The 31672 th iteration gives loss of 0.15682567873619754\n",
      "The 31673 th iteration gives loss of 0.1568238288320427\n",
      "The 31674 th iteration gives loss of 0.15682197902509698\n",
      "The 31675 th iteration gives loss of 0.15682012931533917\n",
      "The 31676 th iteration gives loss of 0.15681827970275888\n",
      "The 31677 th iteration gives loss of 0.15681643018733918\n",
      "The 31678 th iteration gives loss of 0.15681458076907084\n",
      "The 31679 th iteration gives loss of 0.15681273144793312\n",
      "The 31680 th iteration gives loss of 0.15681088222391892\n",
      "The 31681 th iteration gives loss of 0.15680903309700592\n",
      "The 31682 th iteration gives loss of 0.1568071840671911\n",
      "The 31683 th iteration gives loss of 0.15680533513445494\n",
      "The 31684 th iteration gives loss of 0.15680348629878302\n",
      "The 31685 th iteration gives loss of 0.15680163756015925\n",
      "The 31686 th iteration gives loss of 0.15679978891858334\n",
      "The 31687 th iteration gives loss of 0.1567979403740271\n",
      "The 31688 th iteration gives loss of 0.1567960919264867\n",
      "The 31689 th iteration gives loss of 0.15679424357593097\n",
      "The 31690 th iteration gives loss of 0.15679239532236786\n",
      "The 31691 th iteration gives loss of 0.15679054716576976\n",
      "The 31692 th iteration gives loss of 0.15678869910612467\n",
      "The 31693 th iteration gives loss of 0.15678685114342697\n",
      "The 31694 th iteration gives loss of 0.156785003277657\n",
      "The 31695 th iteration gives loss of 0.15678315550880115\n",
      "The 31696 th iteration gives loss of 0.15678130783683883\n",
      "The 31697 th iteration gives loss of 0.15677946026178005\n",
      "The 31698 th iteration gives loss of 0.15677761278358007\n",
      "The 31699 th iteration gives loss of 0.15677576540224142\n",
      "The 31700 th iteration gives loss of 0.15677391811775385\n",
      "The 31701 th iteration gives loss of 0.15677207093009154\n",
      "The 31702 th iteration gives loss of 0.15677022383924485\n",
      "The 31703 th iteration gives loss of 0.1567683768452052\n",
      "The 31704 th iteration gives loss of 0.15676652994796447\n",
      "The 31705 th iteration gives loss of 0.15676468314750117\n",
      "The 31706 th iteration gives loss of 0.15676283644379388\n",
      "The 31707 th iteration gives loss of 0.1567609898368396\n",
      "The 31708 th iteration gives loss of 0.15675914332661733\n",
      "The 31709 th iteration gives loss of 0.15675729691312118\n",
      "The 31710 th iteration gives loss of 0.1567554505963252\n",
      "The 31711 th iteration gives loss of 0.15675360437623287\n",
      "The 31712 th iteration gives loss of 0.15675175825281729\n",
      "The 31713 th iteration gives loss of 0.15674991222606866\n",
      "The 31714 th iteration gives loss of 0.1567480662959705\n",
      "The 31715 th iteration gives loss of 0.1567462204625236\n",
      "The 31716 th iteration gives loss of 0.15674437472569097\n",
      "The 31717 th iteration gives loss of 0.15674252908547814\n",
      "The 31718 th iteration gives loss of 0.15674068354185786\n",
      "The 31719 th iteration gives loss of 0.15673883809483347\n",
      "The 31720 th iteration gives loss of 0.15673699274436853\n",
      "The 31721 th iteration gives loss of 0.1567351474904744\n",
      "The 31722 th iteration gives loss of 0.1567333023331103\n",
      "The 31723 th iteration gives loss of 0.15673145727228627\n",
      "The 31724 th iteration gives loss of 0.1567296123079722\n",
      "The 31725 th iteration gives loss of 0.15672776744016226\n",
      "The 31726 th iteration gives loss of 0.1567259226688432\n",
      "The 31727 th iteration gives loss of 0.15672407799400295\n",
      "The 31728 th iteration gives loss of 0.15672223341562386\n",
      "The 31729 th iteration gives loss of 0.15672038893368562\n",
      "The 31730 th iteration gives loss of 0.15671854454819314\n",
      "The 31731 th iteration gives loss of 0.15671670025911327\n",
      "The 31732 th iteration gives loss of 0.15671485606644736\n",
      "The 31733 th iteration gives loss of 0.156713011970172\n",
      "The 31734 th iteration gives loss of 0.15671116797027332\n",
      "The 31735 th iteration gives loss of 0.1567093240667432\n",
      "The 31736 th iteration gives loss of 0.15670748025956327\n",
      "The 31737 th iteration gives loss of 0.15670563654872696\n",
      "The 31738 th iteration gives loss of 0.15670379293421505\n",
      "The 31739 th iteration gives loss of 0.15670194941601884\n",
      "The 31740 th iteration gives loss of 0.1567001059941085\n",
      "The 31741 th iteration gives loss of 0.1566982626684914\n",
      "The 31742 th iteration gives loss of 0.15669641943914966\n",
      "The 31743 th iteration gives loss of 0.1566945763060629\n",
      "The 31744 th iteration gives loss of 0.1566927332692124\n",
      "The 31745 th iteration gives loss of 0.15669089032859088\n",
      "The 31746 th iteration gives loss of 0.1566890474841915\n",
      "The 31747 th iteration gives loss of 0.15668720473599249\n",
      "The 31748 th iteration gives loss of 0.15668536208397746\n",
      "The 31749 th iteration gives loss of 0.15668351952815107\n",
      "The 31750 th iteration gives loss of 0.156681677068475\n",
      "The 31751 th iteration gives loss of 0.15667983470496027\n",
      "The 31752 th iteration gives loss of 0.15667799243756372\n",
      "The 31753 th iteration gives loss of 0.15667615026629964\n",
      "The 31754 th iteration gives loss of 0.156674308191131\n",
      "The 31755 th iteration gives loss of 0.15667246621206485\n",
      "The 31756 th iteration gives loss of 0.15667062432907652\n",
      "The 31757 th iteration gives loss of 0.15666878254216327\n",
      "The 31758 th iteration gives loss of 0.15666694085129163\n",
      "The 31759 th iteration gives loss of 0.15666509925646804\n",
      "The 31760 th iteration gives loss of 0.15666325775765952\n",
      "The 31761 th iteration gives loss of 0.1566614163548701\n",
      "The 31762 th iteration gives loss of 0.15665957504807332\n",
      "The 31763 th iteration gives loss of 0.1566577338372655\n",
      "The 31764 th iteration gives loss of 0.1566558927224316\n",
      "The 31765 th iteration gives loss of 0.15665405170355126\n",
      "The 31766 th iteration gives loss of 0.1566522107806204\n",
      "The 31767 th iteration gives loss of 0.15665036995360887\n",
      "The 31768 th iteration gives loss of 0.15664852922251557\n",
      "The 31769 th iteration gives loss of 0.1566466885873316\n",
      "The 31770 th iteration gives loss of 0.1566448480480356\n",
      "The 31771 th iteration gives loss of 0.15664300760462055\n",
      "The 31772 th iteration gives loss of 0.15664116725705812\n",
      "The 31773 th iteration gives loss of 0.1566393270053592\n",
      "The 31774 th iteration gives loss of 0.15663748684948434\n",
      "The 31775 th iteration gives loss of 0.15663564678943873\n",
      "The 31776 th iteration gives loss of 0.1566338068251947\n",
      "The 31777 th iteration gives loss of 0.15663196695674647\n",
      "The 31778 th iteration gives loss of 0.15663012718408253\n",
      "The 31779 th iteration gives loss of 0.15662828750718372\n",
      "The 31780 th iteration gives loss of 0.15662644792603617\n",
      "The 31781 th iteration gives loss of 0.15662460844063458\n",
      "The 31782 th iteration gives loss of 0.15662276905095243\n",
      "The 31783 th iteration gives loss of 0.1566209297569953\n",
      "The 31784 th iteration gives loss of 0.15661909055872844\n",
      "The 31785 th iteration gives loss of 0.15661725145615016\n",
      "The 31786 th iteration gives loss of 0.15661541244924437\n",
      "The 31787 th iteration gives loss of 0.15661357353799665\n",
      "The 31788 th iteration gives loss of 0.15661173472240467\n",
      "The 31789 th iteration gives loss of 0.15660989600243191\n",
      "The 31790 th iteration gives loss of 0.15660805737808176\n",
      "The 31791 th iteration gives loss of 0.1566062188493317\n",
      "The 31792 th iteration gives loss of 0.15660438041617575\n",
      "The 31793 th iteration gives loss of 0.156602542078602\n",
      "The 31794 th iteration gives loss of 0.15660070383659433\n",
      "The 31795 th iteration gives loss of 0.1565988656901378\n",
      "The 31796 th iteration gives loss of 0.15659702763920874\n",
      "The 31797 th iteration gives loss of 0.15659518968381025\n",
      "The 31798 th iteration gives loss of 0.15659335182392137\n",
      "The 31799 th iteration gives loss of 0.15659151405953572\n",
      "The 31800 th iteration gives loss of 0.15658967639062693\n",
      "The 31801 th iteration gives loss of 0.15658783881718552\n",
      "The 31802 th iteration gives loss of 0.15658600133920422\n",
      "The 31803 th iteration gives loss of 0.15658416395665786\n",
      "The 31804 th iteration gives loss of 0.15658232666955266\n",
      "The 31805 th iteration gives loss of 0.15658048947785783\n",
      "The 31806 th iteration gives loss of 0.15657865238156712\n",
      "The 31807 th iteration gives loss of 0.15657681538065643\n",
      "The 31808 th iteration gives loss of 0.15657497847513127\n",
      "The 31809 th iteration gives loss of 0.156573141664962\n",
      "The 31810 th iteration gives loss of 0.15657130495014276\n",
      "The 31811 th iteration gives loss of 0.15656946833066032\n",
      "The 31812 th iteration gives loss of 0.15656763180650096\n",
      "The 31813 th iteration gives loss of 0.15656579537764426\n",
      "The 31814 th iteration gives loss of 0.15656395904408188\n",
      "The 31815 th iteration gives loss of 0.15656212280579643\n",
      "The 31816 th iteration gives loss of 0.15656028666278557\n",
      "The 31817 th iteration gives loss of 0.15655845061502627\n",
      "The 31818 th iteration gives loss of 0.1565566146625156\n",
      "The 31819 th iteration gives loss of 0.1565547788052199\n",
      "The 31820 th iteration gives loss of 0.15655294304314113\n",
      "The 31821 th iteration gives loss of 0.15655110737625993\n",
      "The 31822 th iteration gives loss of 0.15654927180457215\n",
      "The 31823 th iteration gives loss of 0.15654743632805565\n",
      "The 31824 th iteration gives loss of 0.15654560094668954\n",
      "The 31825 th iteration gives loss of 0.15654376566047848\n",
      "The 31826 th iteration gives loss of 0.15654193046939185\n",
      "The 31827 th iteration gives loss of 0.156540095373436\n",
      "The 31828 th iteration gives loss of 0.15653826037257818\n",
      "The 31829 th iteration gives loss of 0.15653642546681737\n",
      "The 31830 th iteration gives loss of 0.15653459065613262\n",
      "The 31831 th iteration gives loss of 0.15653275594051294\n",
      "The 31832 th iteration gives loss of 0.15653092131994092\n",
      "The 31833 th iteration gives loss of 0.15652908679441527\n",
      "The 31834 th iteration gives loss of 0.15652725236390685\n",
      "The 31835 th iteration gives loss of 0.15652541802840844\n",
      "The 31836 th iteration gives loss of 0.1565235837879171\n",
      "The 31837 th iteration gives loss of 0.15652174964240748\n",
      "The 31838 th iteration gives loss of 0.15651991559186448\n",
      "The 31839 th iteration gives loss of 0.1565180816362883\n",
      "The 31840 th iteration gives loss of 0.15651624777565507\n",
      "The 31841 th iteration gives loss of 0.15651441400994268\n",
      "The 31842 th iteration gives loss of 0.15651258033915166\n",
      "The 31843 th iteration gives loss of 0.15651074676327065\n",
      "The 31844 th iteration gives loss of 0.1565089132822756\n",
      "The 31845 th iteration gives loss of 0.1565070798961652\n",
      "The 31846 th iteration gives loss of 0.1565052466049129\n",
      "The 31847 th iteration gives loss of 0.15650341340850848\n",
      "The 31848 th iteration gives loss of 0.1565015803069437\n",
      "The 31849 th iteration gives loss of 0.15649974730020771\n",
      "The 31850 th iteration gives loss of 0.1564979143882685\n",
      "The 31851 th iteration gives loss of 0.15649608157113917\n",
      "The 31852 th iteration gives loss of 0.1564942488487802\n",
      "The 31853 th iteration gives loss of 0.15649241622120239\n",
      "The 31854 th iteration gives loss of 0.15649058368838314\n",
      "The 31855 th iteration gives loss of 0.15648875125030007\n",
      "The 31856 th iteration gives loss of 0.15648691890695243\n",
      "The 31857 th iteration gives loss of 0.15648508665831104\n",
      "The 31858 th iteration gives loss of 0.15648325450437495\n",
      "The 31859 th iteration gives loss of 0.15648142244513247\n",
      "The 31860 th iteration gives loss of 0.15647959048056706\n",
      "The 31861 th iteration gives loss of 0.15647775861066715\n",
      "The 31862 th iteration gives loss of 0.1564759268354058\n",
      "The 31863 th iteration gives loss of 0.15647409515479158\n",
      "The 31864 th iteration gives loss of 0.15647226356879648\n",
      "The 31865 th iteration gives loss of 0.15647043207740252\n",
      "The 31866 th iteration gives loss of 0.15646860068061438\n",
      "The 31867 th iteration gives loss of 0.15646676937841048\n",
      "The 31868 th iteration gives loss of 0.15646493817076748\n",
      "The 31869 th iteration gives loss of 0.15646310705768895\n",
      "The 31870 th iteration gives loss of 0.1564612760391437\n",
      "The 31871 th iteration gives loss of 0.15645944511513246\n",
      "The 31872 th iteration gives loss of 0.15645761428563837\n",
      "The 31873 th iteration gives loss of 0.15645578355064024\n",
      "The 31874 th iteration gives loss of 0.15645395291013775\n",
      "The 31875 th iteration gives loss of 0.15645212236411646\n",
      "The 31876 th iteration gives loss of 0.15645029191254375\n",
      "The 31877 th iteration gives loss of 0.15644846155542463\n",
      "The 31878 th iteration gives loss of 0.15644663129273992\n",
      "The 31879 th iteration gives loss of 0.15644480112448375\n",
      "The 31880 th iteration gives loss of 0.15644297105063304\n",
      "The 31881 th iteration gives loss of 0.15644114107117565\n",
      "The 31882 th iteration gives loss of 0.15643931118610363\n",
      "The 31883 th iteration gives loss of 0.15643748139539757\n",
      "The 31884 th iteration gives loss of 0.1564356516990407\n",
      "The 31885 th iteration gives loss of 0.15643382209703718\n",
      "The 31886 th iteration gives loss of 0.15643199258936388\n",
      "The 31887 th iteration gives loss of 0.15643016317600242\n",
      "The 31888 th iteration gives loss of 0.15642833385693594\n",
      "The 31889 th iteration gives loss of 0.15642650463216703\n",
      "The 31890 th iteration gives loss of 0.1564246755016759\n",
      "The 31891 th iteration gives loss of 0.1564228464654319\n",
      "The 31892 th iteration gives loss of 0.15642101752345514\n",
      "The 31893 th iteration gives loss of 0.15641918867570329\n",
      "The 31894 th iteration gives loss of 0.1564173599221694\n",
      "The 31895 th iteration gives loss of 0.15641553126285718\n",
      "The 31896 th iteration gives loss of 0.15641370269773294\n",
      "The 31897 th iteration gives loss of 0.1564118742267952\n",
      "The 31898 th iteration gives loss of 0.15641004585002155\n",
      "The 31899 th iteration gives loss of 0.1564082175674043\n",
      "The 31900 th iteration gives loss of 0.1564063893789298\n",
      "The 31901 th iteration gives loss of 0.15640456128458735\n",
      "The 31902 th iteration gives loss of 0.1564027332843664\n",
      "The 31903 th iteration gives loss of 0.15640090537824164\n",
      "The 31904 th iteration gives loss of 0.15639907756619392\n",
      "The 31905 th iteration gives loss of 0.15639724984823275\n",
      "The 31906 th iteration gives loss of 0.1563954222243392\n",
      "The 31907 th iteration gives loss of 0.1563935946944902\n",
      "The 31908 th iteration gives loss of 0.15639176725868256\n",
      "The 31909 th iteration gives loss of 0.1563899399168942\n",
      "The 31910 th iteration gives loss of 0.1563881126691125\n",
      "The 31911 th iteration gives loss of 0.15638628551532888\n",
      "The 31912 th iteration gives loss of 0.15638445845552412\n",
      "The 31913 th iteration gives loss of 0.15638263148968756\n",
      "The 31914 th iteration gives loss of 0.15638080461781856\n",
      "The 31915 th iteration gives loss of 0.1563789778398792\n",
      "The 31916 th iteration gives loss of 0.1563771511558834\n",
      "The 31917 th iteration gives loss of 0.15637532456579573\n",
      "The 31918 th iteration gives loss of 0.15637349806961684\n",
      "The 31919 th iteration gives loss of 0.15637167166731983\n",
      "The 31920 th iteration gives loss of 0.15636984535890783\n",
      "The 31921 th iteration gives loss of 0.15636801914435183\n",
      "The 31922 th iteration gives loss of 0.1563661930236501\n",
      "The 31923 th iteration gives loss of 0.15636436699679\n",
      "The 31924 th iteration gives loss of 0.15636254106374503\n",
      "The 31925 th iteration gives loss of 0.15636071522451228\n",
      "The 31926 th iteration gives loss of 0.15635888947907634\n",
      "The 31927 th iteration gives loss of 0.15635706382743894\n",
      "The 31928 th iteration gives loss of 0.15635523826955597\n",
      "The 31929 th iteration gives loss of 0.15635341280543788\n",
      "The 31930 th iteration gives loss of 0.1563515874350596\n",
      "The 31931 th iteration gives loss of 0.15634976215841928\n",
      "The 31932 th iteration gives loss of 0.15634793697548513\n",
      "The 31933 th iteration gives loss of 0.15634611188625971\n",
      "The 31934 th iteration gives loss of 0.15634428689073215\n",
      "The 31935 th iteration gives loss of 0.15634246198887455\n",
      "The 31936 th iteration gives loss of 0.15634063718068572\n",
      "The 31937 th iteration gives loss of 0.1563388124661466\n",
      "The 31938 th iteration gives loss of 0.15633698784524833\n",
      "The 31939 th iteration gives loss of 0.15633516331797415\n",
      "The 31940 th iteration gives loss of 0.15633333888431808\n",
      "The 31941 th iteration gives loss of 0.1563315145442539\n",
      "The 31942 th iteration gives loss of 0.15632969029777824\n",
      "The 31943 th iteration gives loss of 0.1563278661448731\n",
      "The 31944 th iteration gives loss of 0.15632604208552844\n",
      "The 31945 th iteration gives loss of 0.15632421811972838\n",
      "The 31946 th iteration gives loss of 0.15632239424746175\n",
      "The 31947 th iteration gives loss of 0.15632057046871042\n",
      "The 31948 th iteration gives loss of 0.15631874678347918\n",
      "The 31949 th iteration gives loss of 0.15631692319172455\n",
      "The 31950 th iteration gives loss of 0.15631509969345764\n",
      "The 31951 th iteration gives loss of 0.15631327628864816\n",
      "The 31952 th iteration gives loss of 0.15631145297730514\n",
      "The 31953 th iteration gives loss of 0.15630962975939874\n",
      "The 31954 th iteration gives loss of 0.1563078066349213\n",
      "The 31955 th iteration gives loss of 0.15630598360385872\n",
      "The 31956 th iteration gives loss of 0.15630416066619554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 31957 th iteration gives loss of 0.1563023378219177\n",
      "The 31958 th iteration gives loss of 0.15630051507100926\n",
      "The 31959 th iteration gives loss of 0.1562986924134657\n",
      "The 31960 th iteration gives loss of 0.15629686984927005\n",
      "The 31961 th iteration gives loss of 0.1562950473784083\n",
      "The 31962 th iteration gives loss of 0.15629322500087392\n",
      "The 31963 th iteration gives loss of 0.15629140271664868\n",
      "The 31964 th iteration gives loss of 0.1562895805257148\n",
      "The 31965 th iteration gives loss of 0.15628775842805756\n",
      "The 31966 th iteration gives loss of 0.15628593642367908\n",
      "The 31967 th iteration gives loss of 0.15628411451254967\n",
      "The 31968 th iteration gives loss of 0.15628229269466326\n",
      "The 31969 th iteration gives loss of 0.15628047097001638\n",
      "The 31970 th iteration gives loss of 0.15627864933857957\n",
      "The 31971 th iteration gives loss of 0.15627682780034458\n",
      "The 31972 th iteration gives loss of 0.15627500635529928\n",
      "The 31973 th iteration gives loss of 0.15627318500342893\n",
      "The 31974 th iteration gives loss of 0.15627136374472778\n",
      "The 31975 th iteration gives loss of 0.1562695425791771\n",
      "The 31976 th iteration gives loss of 0.15626772150676324\n",
      "The 31977 th iteration gives loss of 0.15626590052746653\n",
      "The 31978 th iteration gives loss of 0.15626407964129002\n",
      "The 31979 th iteration gives loss of 0.15626225884820982\n",
      "The 31980 th iteration gives loss of 0.15626043814820983\n",
      "The 31981 th iteration gives loss of 0.15625861754128412\n",
      "The 31982 th iteration gives loss of 0.1562567970274159\n",
      "The 31983 th iteration gives loss of 0.15625497660659726\n",
      "The 31984 th iteration gives loss of 0.1562531562788115\n",
      "The 31985 th iteration gives loss of 0.15625133604404107\n",
      "The 31986 th iteration gives loss of 0.15624951590228317\n",
      "The 31987 th iteration gives loss of 0.15624769585351084\n",
      "The 31988 th iteration gives loss of 0.15624587589772293\n",
      "The 31989 th iteration gives loss of 0.1562440560348997\n",
      "The 31990 th iteration gives loss of 0.15624223626502437\n",
      "The 31991 th iteration gives loss of 0.156240416588096\n",
      "The 31992 th iteration gives loss of 0.15623859700410073\n",
      "The 31993 th iteration gives loss of 0.15623677751301276\n",
      "The 31994 th iteration gives loss of 0.15623495811482777\n",
      "The 31995 th iteration gives loss of 0.15623313880952458\n",
      "The 31996 th iteration gives loss of 0.15623131959710446\n",
      "The 31997 th iteration gives loss of 0.15622950047754744\n",
      "The 31998 th iteration gives loss of 0.1562276814508343\n",
      "The 31999 th iteration gives loss of 0.15622586251696433\n",
      "The 32000 th iteration gives loss of 0.15622404367591686\n",
      "The 32001 th iteration gives loss of 0.15622222492766819\n",
      "The 32002 th iteration gives loss of 0.15622040627221967\n",
      "The 32003 th iteration gives loss of 0.1562185877095532\n",
      "The 32004 th iteration gives loss of 0.15621676923966044\n",
      "The 32005 th iteration gives loss of 0.15621495086252904\n",
      "The 32006 th iteration gives loss of 0.1562131325781411\n",
      "The 32007 th iteration gives loss of 0.15621131438647792\n",
      "The 32008 th iteration gives loss of 0.15620949628753775\n",
      "The 32009 th iteration gives loss of 0.15620767828129525\n",
      "The 32010 th iteration gives loss of 0.15620586036775488\n",
      "The 32011 th iteration gives loss of 0.15620404254688658\n",
      "The 32012 th iteration gives loss of 0.15620222481868595\n",
      "The 32013 th iteration gives loss of 0.1562004071831354\n",
      "The 32014 th iteration gives loss of 0.15619858964023023\n",
      "The 32015 th iteration gives loss of 0.15619677218994565\n",
      "The 32016 th iteration gives loss of 0.15619495483227608\n",
      "The 32017 th iteration gives loss of 0.15619313756720862\n",
      "The 32018 th iteration gives loss of 0.1561913203947332\n",
      "The 32019 th iteration gives loss of 0.15618950331482417\n",
      "The 32020 th iteration gives loss of 0.1561876863274855\n",
      "The 32021 th iteration gives loss of 0.1561858694326879\n",
      "The 32022 th iteration gives loss of 0.15618405263042576\n",
      "The 32023 th iteration gives loss of 0.15618223592068475\n",
      "The 32024 th iteration gives loss of 0.15618041930344875\n",
      "The 32025 th iteration gives loss of 0.15617860277871953\n",
      "The 32026 th iteration gives loss of 0.1561767863464681\n",
      "The 32027 th iteration gives loss of 0.15617497000668396\n",
      "The 32028 th iteration gives loss of 0.1561731537593617\n",
      "The 32029 th iteration gives loss of 0.15617133760447582\n",
      "The 32030 th iteration gives loss of 0.1561695215420285\n",
      "The 32031 th iteration gives loss of 0.15616770557200105\n",
      "The 32032 th iteration gives loss of 0.15616588969437328\n",
      "The 32033 th iteration gives loss of 0.15616407390914067\n",
      "The 32034 th iteration gives loss of 0.15616225821628293\n",
      "The 32035 th iteration gives loss of 0.15616044261578976\n",
      "The 32036 th iteration gives loss of 0.15615862710764744\n",
      "The 32037 th iteration gives loss of 0.156156811691853\n",
      "The 32038 th iteration gives loss of 0.1561549963683809\n",
      "The 32039 th iteration gives loss of 0.15615318113722462\n",
      "The 32040 th iteration gives loss of 0.1561513659983727\n",
      "The 32041 th iteration gives loss of 0.15614955095180147\n",
      "The 32042 th iteration gives loss of 0.15614773599750437\n",
      "The 32043 th iteration gives loss of 0.1561459211354675\n",
      "The 32044 th iteration gives loss of 0.1561441063656919\n",
      "The 32045 th iteration gives loss of 0.1561422916881407\n",
      "The 32046 th iteration gives loss of 0.1561404771028163\n",
      "The 32047 th iteration gives loss of 0.1561386626096971\n",
      "The 32048 th iteration gives loss of 0.1561368482087747\n",
      "The 32049 th iteration gives loss of 0.15613503390004124\n",
      "The 32050 th iteration gives loss of 0.15613321968347693\n",
      "The 32051 th iteration gives loss of 0.15613140555906765\n",
      "The 32052 th iteration gives loss of 0.1561295915268007\n",
      "The 32053 th iteration gives loss of 0.15612777758667362\n",
      "The 32054 th iteration gives loss of 0.1561259637386593\n",
      "The 32055 th iteration gives loss of 0.15612414998275545\n",
      "The 32056 th iteration gives loss of 0.1561223363189386\n",
      "The 32057 th iteration gives loss of 0.15612052274720692\n",
      "The 32058 th iteration gives loss of 0.15611870926754143\n",
      "The 32059 th iteration gives loss of 0.15611689587992433\n",
      "The 32060 th iteration gives loss of 0.15611508258435322\n",
      "The 32061 th iteration gives loss of 0.15611326938080833\n",
      "The 32062 th iteration gives loss of 0.15611145626927495\n",
      "The 32063 th iteration gives loss of 0.1561096432497503\n",
      "The 32064 th iteration gives loss of 0.15610783032220815\n",
      "The 32065 th iteration gives loss of 0.1561060174866525\n",
      "The 32066 th iteration gives loss of 0.1561042047430472\n",
      "The 32067 th iteration gives loss of 0.15610239209139434\n",
      "The 32068 th iteration gives loss of 0.15610057953169007\n",
      "The 32069 th iteration gives loss of 0.15609876706389922\n",
      "The 32070 th iteration gives loss of 0.15609695468802234\n",
      "The 32071 th iteration gives loss of 0.1560951424040438\n",
      "The 32072 th iteration gives loss of 0.15609333021195151\n",
      "The 32073 th iteration gives loss of 0.1560915181117274\n",
      "The 32074 th iteration gives loss of 0.15608970610336684\n",
      "The 32075 th iteration gives loss of 0.1560878941868535\n",
      "The 32076 th iteration gives loss of 0.15608608236217364\n",
      "The 32077 th iteration gives loss of 0.15608427062932131\n",
      "The 32078 th iteration gives loss of 0.1560824589882671\n",
      "The 32079 th iteration gives loss of 0.1560806474390093\n",
      "The 32080 th iteration gives loss of 0.15607883598153463\n",
      "The 32081 th iteration gives loss of 0.15607702461582706\n",
      "The 32082 th iteration gives loss of 0.15607521334187538\n",
      "The 32083 th iteration gives loss of 0.15607340215967266\n",
      "The 32084 th iteration gives loss of 0.15607159106919608\n",
      "The 32085 th iteration gives loss of 0.1560697800704322\n",
      "The 32086 th iteration gives loss of 0.1560679691633837\n",
      "The 32087 th iteration gives loss of 0.15606615834801915\n",
      "The 32088 th iteration gives loss of 0.1560643476243288\n",
      "The 32089 th iteration gives loss of 0.15606253699231198\n",
      "The 32090 th iteration gives loss of 0.15606072645195118\n",
      "The 32091 th iteration gives loss of 0.15605891600322272\n",
      "The 32092 th iteration gives loss of 0.1560571056461222\n",
      "The 32093 th iteration gives loss of 0.15605529538063287\n",
      "The 32094 th iteration gives loss of 0.1560534852067519\n",
      "The 32095 th iteration gives loss of 0.15605167512445556\n",
      "The 32096 th iteration gives loss of 0.15604986513373367\n",
      "The 32097 th iteration gives loss of 0.15604805523458234\n",
      "The 32098 th iteration gives loss of 0.15604624542696816\n",
      "The 32099 th iteration gives loss of 0.156044435710898\n",
      "The 32100 th iteration gives loss of 0.15604262608635253\n",
      "The 32101 th iteration gives loss of 0.15604081655331556\n",
      "The 32102 th iteration gives loss of 0.1560390071117819\n",
      "The 32103 th iteration gives loss of 0.1560371977617294\n",
      "The 32104 th iteration gives loss of 0.1560353885031442\n",
      "The 32105 th iteration gives loss of 0.1560335793360258\n",
      "The 32106 th iteration gives loss of 0.15603177026035064\n",
      "The 32107 th iteration gives loss of 0.1560299612761032\n",
      "The 32108 th iteration gives loss of 0.15602815238328363\n",
      "The 32109 th iteration gives loss of 0.15602634358187514\n",
      "The 32110 th iteration gives loss of 0.15602453487185627\n",
      "The 32111 th iteration gives loss of 0.15602272625322253\n",
      "The 32112 th iteration gives loss of 0.1560209177259564\n",
      "The 32113 th iteration gives loss of 0.15601910929004312\n",
      "The 32114 th iteration gives loss of 0.1560173009454776\n",
      "The 32115 th iteration gives loss of 0.15601549269223786\n",
      "The 32116 th iteration gives loss of 0.15601368453032627\n",
      "The 32117 th iteration gives loss of 0.15601187645971407\n",
      "The 32118 th iteration gives loss of 0.15601006848039675\n",
      "The 32119 th iteration gives loss of 0.15600826059235684\n",
      "The 32120 th iteration gives loss of 0.15600645279557993\n",
      "The 32121 th iteration gives loss of 0.15600464509005801\n",
      "The 32122 th iteration gives loss of 0.15600283747578275\n",
      "The 32123 th iteration gives loss of 0.1560010299527248\n",
      "The 32124 th iteration gives loss of 0.155999222520887\n",
      "The 32125 th iteration gives loss of 0.15599741518025906\n",
      "The 32126 th iteration gives loss of 0.15599560793080985\n",
      "The 32127 th iteration gives loss of 0.1559938007725402\n",
      "The 32128 th iteration gives loss of 0.15599199370543568\n",
      "The 32129 th iteration gives loss of 0.15599018672948756\n",
      "The 32130 th iteration gives loss of 0.15598837984467234\n",
      "The 32131 th iteration gives loss of 0.1559865730509817\n",
      "The 32132 th iteration gives loss of 0.15598476634840305\n",
      "The 32133 th iteration gives loss of 0.15598295973692583\n",
      "The 32134 th iteration gives loss of 0.15598115321653575\n",
      "The 32135 th iteration gives loss of 0.15597934678721753\n",
      "The 32136 th iteration gives loss of 0.15597754044896267\n",
      "The 32137 th iteration gives loss of 0.15597573420176136\n",
      "The 32138 th iteration gives loss of 0.155973928045588\n",
      "The 32139 th iteration gives loss of 0.1559721219804373\n",
      "The 32140 th iteration gives loss of 0.15597031600630043\n",
      "The 32141 th iteration gives loss of 0.15596851012315804\n",
      "The 32142 th iteration gives loss of 0.15596670433100365\n",
      "The 32143 th iteration gives loss of 0.15596489862982385\n",
      "The 32144 th iteration gives loss of 0.155963093019588\n",
      "The 32145 th iteration gives loss of 0.15596128750031557\n",
      "The 32146 th iteration gives loss of 0.15595948207196858\n",
      "The 32147 th iteration gives loss of 0.15595767673454206\n",
      "The 32148 th iteration gives loss of 0.1559558714880234\n",
      "The 32149 th iteration gives loss of 0.15595406633239428\n",
      "The 32150 th iteration gives loss of 0.1559522612676588\n",
      "The 32151 th iteration gives loss of 0.1559504562937845\n",
      "The 32152 th iteration gives loss of 0.15594865141076625\n",
      "The 32153 th iteration gives loss of 0.1559468466186006\n",
      "The 32154 th iteration gives loss of 0.1559450419172553\n",
      "The 32155 th iteration gives loss of 0.1559432373067338\n",
      "The 32156 th iteration gives loss of 0.15594143278701242\n",
      "The 32157 th iteration gives loss of 0.15593962835808736\n",
      "The 32158 th iteration gives loss of 0.15593782401994438\n",
      "The 32159 th iteration gives loss of 0.1559360197725662\n",
      "The 32160 th iteration gives loss of 0.1559342156159449\n",
      "The 32161 th iteration gives loss of 0.1559324115500653\n",
      "The 32162 th iteration gives loss of 0.15593060757491112\n",
      "The 32163 th iteration gives loss of 0.15592880369047402\n",
      "The 32164 th iteration gives loss of 0.1559269998967401\n",
      "The 32165 th iteration gives loss of 0.1559251961937029\n",
      "The 32166 th iteration gives loss of 0.15592339258133364\n",
      "The 32167 th iteration gives loss of 0.155921589059637\n",
      "The 32168 th iteration gives loss of 0.15591978562858924\n",
      "The 32169 th iteration gives loss of 0.15591798228818132\n",
      "The 32170 th iteration gives loss of 0.1559161790383974\n",
      "The 32171 th iteration gives loss of 0.15591437587923013\n",
      "The 32172 th iteration gives loss of 0.15591257281065726\n",
      "The 32173 th iteration gives loss of 0.1559107698326813\n",
      "The 32174 th iteration gives loss of 0.1559089669452829\n",
      "The 32175 th iteration gives loss of 0.15590716414844724\n",
      "The 32176 th iteration gives loss of 0.15590536144215963\n",
      "The 32177 th iteration gives loss of 0.15590355882641135\n",
      "The 32178 th iteration gives loss of 0.15590175630119255\n",
      "The 32179 th iteration gives loss of 0.15589995386647673\n",
      "The 32180 th iteration gives loss of 0.155898151522264\n",
      "The 32181 th iteration gives loss of 0.15589634926853535\n",
      "The 32182 th iteration gives loss of 0.15589454710528078\n",
      "The 32183 th iteration gives loss of 0.1558927450324882\n",
      "The 32184 th iteration gives loss of 0.15589094305014708\n",
      "The 32185 th iteration gives loss of 0.1558891411582444\n",
      "The 32186 th iteration gives loss of 0.15588733935675522\n",
      "The 32187 th iteration gives loss of 0.1558855376456716\n",
      "The 32188 th iteration gives loss of 0.15588373602500225\n",
      "The 32189 th iteration gives loss of 0.15588193449471704\n",
      "The 32190 th iteration gives loss of 0.1558801330547999\n",
      "The 32191 th iteration gives loss of 0.1558783317052408\n",
      "The 32192 th iteration gives loss of 0.1558765304460317\n",
      "The 32193 th iteration gives loss of 0.15587472927715873\n",
      "The 32194 th iteration gives loss of 0.15587292819859705\n",
      "The 32195 th iteration gives loss of 0.1558711272103485\n",
      "The 32196 th iteration gives loss of 0.15586932631240133\n",
      "The 32197 th iteration gives loss of 0.15586752550473262\n",
      "The 32198 th iteration gives loss of 0.15586572478733687\n",
      "The 32199 th iteration gives loss of 0.15586392416020212\n",
      "The 32200 th iteration gives loss of 0.15586212362330626\n",
      "The 32201 th iteration gives loss of 0.15586032317665113\n",
      "The 32202 th iteration gives loss of 0.15585852282021045\n",
      "The 32203 th iteration gives loss of 0.1558567225539745\n",
      "The 32204 th iteration gives loss of 0.15585492237794152\n",
      "The 32205 th iteration gives loss of 0.1558531222920846\n",
      "The 32206 th iteration gives loss of 0.15585132229639753\n",
      "The 32207 th iteration gives loss of 0.15584952239086425\n",
      "The 32208 th iteration gives loss of 0.15584772257548582\n",
      "The 32209 th iteration gives loss of 0.1558459228502331\n",
      "The 32210 th iteration gives loss of 0.1558441232150971\n",
      "The 32211 th iteration gives loss of 0.15584232367007197\n",
      "The 32212 th iteration gives loss of 0.15584052421512953\n",
      "The 32213 th iteration gives loss of 0.1558387248502797\n",
      "The 32214 th iteration gives loss of 0.15583692557549178\n",
      "The 32215 th iteration gives loss of 0.15583512639076344\n",
      "The 32216 th iteration gives loss of 0.15583332729606839\n",
      "The 32217 th iteration gives loss of 0.15583152829141506\n",
      "The 32218 th iteration gives loss of 0.15582972937677222\n",
      "The 32219 th iteration gives loss of 0.15582793055213512\n",
      "The 32220 th iteration gives loss of 0.15582613181749116\n",
      "The 32221 th iteration gives loss of 0.1558243331728258\n",
      "The 32222 th iteration gives loss of 0.155822534618135\n",
      "The 32223 th iteration gives loss of 0.15582073615339315\n",
      "The 32224 th iteration gives loss of 0.1558189377785858\n",
      "The 32225 th iteration gives loss of 0.15581713949371198\n",
      "The 32226 th iteration gives loss of 0.15581534129875937\n",
      "The 32227 th iteration gives loss of 0.155813543193713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 32228 th iteration gives loss of 0.15581174517854757\n",
      "The 32229 th iteration gives loss of 0.15580994725326577\n",
      "The 32230 th iteration gives loss of 0.1558081494178517\n",
      "The 32231 th iteration gives loss of 0.15580635167228463\n",
      "The 32232 th iteration gives loss of 0.155804554016565\n",
      "The 32233 th iteration gives loss of 0.15580275645066868\n",
      "The 32234 th iteration gives loss of 0.15580095897459864\n",
      "The 32235 th iteration gives loss of 0.15579916158831555\n",
      "The 32236 th iteration gives loss of 0.15579736429182844\n",
      "The 32237 th iteration gives loss of 0.1557955670851227\n",
      "The 32238 th iteration gives loss of 0.15579376996817626\n",
      "The 32239 th iteration gives loss of 0.15579197294098765\n",
      "The 32240 th iteration gives loss of 0.15579017600353234\n",
      "The 32241 th iteration gives loss of 0.15578837915581345\n",
      "The 32242 th iteration gives loss of 0.15578658239780377\n",
      "The 32243 th iteration gives loss of 0.1557847857294947\n",
      "The 32244 th iteration gives loss of 0.15578298915087796\n",
      "The 32245 th iteration gives loss of 0.15578119266193097\n",
      "The 32246 th iteration gives loss of 0.15577939626265716\n",
      "The 32247 th iteration gives loss of 0.155777599953031\n",
      "The 32248 th iteration gives loss of 0.15577580373304217\n",
      "The 32249 th iteration gives loss of 0.15577400760268273\n",
      "The 32250 th iteration gives loss of 0.1557722115619385\n",
      "The 32251 th iteration gives loss of 0.15577041561079444\n",
      "The 32252 th iteration gives loss of 0.15576861974923237\n",
      "The 32253 th iteration gives loss of 0.1557668239772544\n",
      "The 32254 th iteration gives loss of 0.15576502829483962\n",
      "The 32255 th iteration gives loss of 0.15576323270197973\n",
      "The 32256 th iteration gives loss of 0.15576143719864907\n",
      "The 32257 th iteration gives loss of 0.15575964178483592\n",
      "The 32258 th iteration gives loss of 0.15575784646054924\n",
      "The 32259 th iteration gives loss of 0.15575605122576433\n",
      "The 32260 th iteration gives loss of 0.15575425608046659\n",
      "The 32261 th iteration gives loss of 0.1557524610246368\n",
      "The 32262 th iteration gives loss of 0.15575066605828403\n",
      "The 32263 th iteration gives loss of 0.15574887118136913\n",
      "The 32264 th iteration gives loss of 0.15574707639389607\n",
      "The 32265 th iteration gives loss of 0.1557452816958556\n",
      "The 32266 th iteration gives loss of 0.15574348708721608\n",
      "The 32267 th iteration gives loss of 0.1557416925679867\n",
      "The 32268 th iteration gives loss of 0.15573989813813668\n",
      "The 32269 th iteration gives loss of 0.15573810379766492\n",
      "The 32270 th iteration gives loss of 0.1557363095465583\n",
      "The 32271 th iteration gives loss of 0.15573451538479685\n",
      "The 32272 th iteration gives loss of 0.15573272131237445\n",
      "The 32273 th iteration gives loss of 0.15573092732927987\n",
      "The 32274 th iteration gives loss of 0.15572913343549563\n",
      "The 32275 th iteration gives loss of 0.15572733963101204\n",
      "The 32276 th iteration gives loss of 0.15572554591581197\n",
      "The 32277 th iteration gives loss of 0.15572375228989832\n",
      "The 32278 th iteration gives loss of 0.15572195875323985\n",
      "The 32279 th iteration gives loss of 0.15572016530582813\n",
      "The 32280 th iteration gives loss of 0.15571837194765653\n",
      "The 32281 th iteration gives loss of 0.155716578678703\n",
      "The 32282 th iteration gives loss of 0.15571478549897128\n",
      "The 32283 th iteration gives loss of 0.15571299240843597\n",
      "The 32284 th iteration gives loss of 0.15571119940709358\n",
      "The 32285 th iteration gives loss of 0.15570940649491435\n",
      "The 32286 th iteration gives loss of 0.1557076136719045\n",
      "The 32287 th iteration gives loss of 0.1557058209380451\n",
      "The 32288 th iteration gives loss of 0.15570402829331723\n",
      "The 32289 th iteration gives loss of 0.15570223573771796\n",
      "The 32290 th iteration gives loss of 0.15570044327123606\n",
      "The 32291 th iteration gives loss of 0.15569865089384832\n",
      "The 32292 th iteration gives loss of 0.15569685860554477\n",
      "The 32293 th iteration gives loss of 0.15569506640631395\n",
      "The 32294 th iteration gives loss of 0.1556932742961579\n",
      "The 32295 th iteration gives loss of 0.15569148227504162\n",
      "The 32296 th iteration gives loss of 0.15568969034296468\n",
      "The 32297 th iteration gives loss of 0.15568789849991238\n",
      "The 32298 th iteration gives loss of 0.15568610674587277\n",
      "The 32299 th iteration gives loss of 0.15568431508083594\n",
      "The 32300 th iteration gives loss of 0.15568252350478415\n",
      "The 32301 th iteration gives loss of 0.15568073201770044\n",
      "The 32302 th iteration gives loss of 0.15567894061958276\n",
      "The 32303 th iteration gives loss of 0.15567714931042168\n",
      "The 32304 th iteration gives loss of 0.15567535809019048\n",
      "The 32305 th iteration gives loss of 0.15567356695888765\n",
      "The 32306 th iteration gives loss of 0.15567177591649745\n",
      "The 32307 th iteration gives loss of 0.15566998496300624\n",
      "The 32308 th iteration gives loss of 0.15566819409840135\n",
      "The 32309 th iteration gives loss of 0.15566640332267231\n",
      "The 32310 th iteration gives loss of 0.15566461263580758\n",
      "The 32311 th iteration gives loss of 0.15566282203778878\n",
      "The 32312 th iteration gives loss of 0.15566103152860997\n",
      "The 32313 th iteration gives loss of 0.15565924110825657\n",
      "The 32314 th iteration gives loss of 0.15565745077671206\n",
      "The 32315 th iteration gives loss of 0.15565566053397195\n",
      "The 32316 th iteration gives loss of 0.15565387038002118\n",
      "The 32317 th iteration gives loss of 0.1556520803148418\n",
      "The 32318 th iteration gives loss of 0.15565029033842545\n",
      "The 32319 th iteration gives loss of 0.15564850045076012\n",
      "The 32320 th iteration gives loss of 0.15564671065183755\n",
      "The 32321 th iteration gives loss of 0.15564492094163396\n",
      "The 32322 th iteration gives loss of 0.15564313132014587\n",
      "The 32323 th iteration gives loss of 0.15564134178736513\n",
      "The 32324 th iteration gives loss of 0.15563955234327156\n",
      "The 32325 th iteration gives loss of 0.15563776298784146\n",
      "The 32326 th iteration gives loss of 0.15563597372108257\n",
      "The 32327 th iteration gives loss of 0.15563418454297406\n",
      "The 32328 th iteration gives loss of 0.15563239545350438\n",
      "The 32329 th iteration gives loss of 0.15563060645265986\n",
      "The 32330 th iteration gives loss of 0.1556288175404325\n",
      "The 32331 th iteration gives loss of 0.15562702871680345\n",
      "The 32332 th iteration gives loss of 0.155625239981759\n",
      "The 32333 th iteration gives loss of 0.15562345133530298\n",
      "The 32334 th iteration gives loss of 0.15562166277739914\n",
      "The 32335 th iteration gives loss of 0.15561987430804936\n",
      "The 32336 th iteration gives loss of 0.15561808592724247\n",
      "The 32337 th iteration gives loss of 0.15561629763495455\n",
      "The 32338 th iteration gives loss of 0.1556145094311877\n",
      "The 32339 th iteration gives loss of 0.15561272131592357\n",
      "The 32340 th iteration gives loss of 0.1556109332891461\n",
      "The 32341 th iteration gives loss of 0.15560914535085077\n",
      "The 32342 th iteration gives loss of 0.15560735750101196\n",
      "The 32343 th iteration gives loss of 0.15560556973963163\n",
      "The 32344 th iteration gives loss of 0.1556037820666928\n",
      "The 32345 th iteration gives loss of 0.1556019944821744\n",
      "The 32346 th iteration gives loss of 0.15560020698608465\n",
      "The 32347 th iteration gives loss of 0.1555984195783843\n",
      "The 32348 th iteration gives loss of 0.1555966322590795\n",
      "The 32349 th iteration gives loss of 0.15559484502815216\n",
      "The 32350 th iteration gives loss of 0.15559305788558275\n",
      "The 32351 th iteration gives loss of 0.1555912708313761\n",
      "The 32352 th iteration gives loss of 0.15558948386550228\n",
      "The 32353 th iteration gives loss of 0.1555876969879659\n",
      "The 32354 th iteration gives loss of 0.15558591019874232\n",
      "The 32355 th iteration gives loss of 0.1555841234978216\n",
      "The 32356 th iteration gives loss of 0.15558233688519935\n",
      "The 32357 th iteration gives loss of 0.1555805503608511\n",
      "The 32358 th iteration gives loss of 0.1555787639247668\n",
      "The 32359 th iteration gives loss of 0.15557697757693792\n",
      "The 32360 th iteration gives loss of 0.15557519131734826\n",
      "The 32361 th iteration gives loss of 0.1555734051459896\n",
      "The 32362 th iteration gives loss of 0.15557161906284767\n",
      "The 32363 th iteration gives loss of 0.15556983306791233\n",
      "The 32364 th iteration gives loss of 0.15556804716116654\n",
      "The 32365 th iteration gives loss of 0.15556626134261115\n",
      "The 32366 th iteration gives loss of 0.15556447561221487\n",
      "The 32367 th iteration gives loss of 0.15556268996997505\n",
      "The 32368 th iteration gives loss of 0.1555609044158726\n",
      "The 32369 th iteration gives loss of 0.15555911894991126\n",
      "The 32370 th iteration gives loss of 0.1555573335720656\n",
      "The 32371 th iteration gives loss of 0.15555554828232743\n",
      "The 32372 th iteration gives loss of 0.15555376308067542\n",
      "The 32373 th iteration gives loss of 0.15555197796711076\n",
      "The 32374 th iteration gives loss of 0.15555019294161368\n",
      "The 32375 th iteration gives loss of 0.155548408004172\n",
      "The 32376 th iteration gives loss of 0.15554662315477302\n",
      "The 32377 th iteration gives loss of 0.15554483839340735\n",
      "The 32378 th iteration gives loss of 0.15554305372005905\n",
      "The 32379 th iteration gives loss of 0.15554126913471988\n",
      "The 32380 th iteration gives loss of 0.1555394846373759\n",
      "The 32381 th iteration gives loss of 0.15553770022801913\n",
      "The 32382 th iteration gives loss of 0.155535915906632\n",
      "The 32383 th iteration gives loss of 0.1555341316731974\n",
      "The 32384 th iteration gives loss of 0.1555323475277142\n",
      "The 32385 th iteration gives loss of 0.15553056347016503\n",
      "The 32386 th iteration gives loss of 0.15552877950053368\n",
      "The 32387 th iteration gives loss of 0.15552699561880326\n",
      "The 32388 th iteration gives loss of 0.1555252118249701\n",
      "The 32389 th iteration gives loss of 0.1555234281190316\n",
      "The 32390 th iteration gives loss of 0.15552164450095868\n",
      "The 32391 th iteration gives loss of 0.15551986097075676\n",
      "The 32392 th iteration gives loss of 0.15551807752838911\n",
      "The 32393 th iteration gives loss of 0.15551629417386242\n",
      "The 32394 th iteration gives loss of 0.15551451090716095\n",
      "The 32395 th iteration gives loss of 0.1555127277282593\n",
      "The 32396 th iteration gives loss of 0.15551094463716814\n",
      "The 32397 th iteration gives loss of 0.15550916163384934\n",
      "The 32398 th iteration gives loss of 0.15550737871830808\n",
      "The 32399 th iteration gives loss of 0.1555055958905346\n",
      "The 32400 th iteration gives loss of 0.15550381315051084\n",
      "The 32401 th iteration gives loss of 0.15550203049821654\n",
      "The 32402 th iteration gives loss of 0.1555002479336492\n",
      "The 32403 th iteration gives loss of 0.15549846545678853\n",
      "The 32404 th iteration gives loss of 0.1554966830676328\n",
      "The 32405 th iteration gives loss of 0.15549490076616815\n",
      "The 32406 th iteration gives loss of 0.15549311855237866\n",
      "The 32407 th iteration gives loss of 0.15549133642625107\n",
      "The 32408 th iteration gives loss of 0.1554895543877719\n",
      "The 32409 th iteration gives loss of 0.15548777243693018\n",
      "The 32410 th iteration gives loss of 0.15548599057371285\n",
      "The 32411 th iteration gives loss of 0.15548420879811908\n",
      "The 32412 th iteration gives loss of 0.15548242711011226\n",
      "The 32413 th iteration gives loss of 0.15548064550970708\n",
      "The 32414 th iteration gives loss of 0.1554788639968754\n",
      "The 32415 th iteration gives loss of 0.15547708257160447\n",
      "The 32416 th iteration gives loss of 0.1554753012338906\n",
      "The 32417 th iteration gives loss of 0.15547351998371123\n",
      "The 32418 th iteration gives loss of 0.15547173882106818\n",
      "The 32419 th iteration gives loss of 0.15546995774594086\n",
      "The 32420 th iteration gives loss of 0.15546817675831173\n",
      "The 32421 th iteration gives loss of 0.15546639585817662\n",
      "The 32422 th iteration gives loss of 0.15546461504551928\n",
      "The 32423 th iteration gives loss of 0.15546283432032754\n",
      "The 32424 th iteration gives loss of 0.155461053682595\n",
      "The 32425 th iteration gives loss of 0.15545927313230612\n",
      "The 32426 th iteration gives loss of 0.1554574926694445\n",
      "The 32427 th iteration gives loss of 0.1554557122939938\n",
      "The 32428 th iteration gives loss of 0.15545393200595398\n",
      "The 32429 th iteration gives loss of 0.1554521518053073\n",
      "The 32430 th iteration gives loss of 0.1554503716920451\n",
      "The 32431 th iteration gives loss of 0.15544859166615427\n",
      "The 32432 th iteration gives loss of 0.1554468117276157\n",
      "The 32433 th iteration gives loss of 0.155445031876415\n",
      "The 32434 th iteration gives loss of 0.1554432521125605\n",
      "The 32435 th iteration gives loss of 0.1554414724360146\n",
      "The 32436 th iteration gives loss of 0.15543969284678205\n",
      "The 32437 th iteration gives loss of 0.15543791334484947\n",
      "The 32438 th iteration gives loss of 0.15543613393019295\n",
      "The 32439 th iteration gives loss of 0.15543435460281133\n",
      "The 32440 th iteration gives loss of 0.15543257536268865\n",
      "The 32441 th iteration gives loss of 0.15543079620981057\n",
      "The 32442 th iteration gives loss of 0.1554290171441679\n",
      "The 32443 th iteration gives loss of 0.1554272381657438\n",
      "The 32444 th iteration gives loss of 0.155425459274535\n",
      "The 32445 th iteration gives loss of 0.15542368047052219\n",
      "The 32446 th iteration gives loss of 0.15542190175369788\n",
      "The 32447 th iteration gives loss of 0.15542012312404718\n",
      "The 32448 th iteration gives loss of 0.15541834458155432\n",
      "The 32449 th iteration gives loss of 0.1554165661262135\n",
      "The 32450 th iteration gives loss of 0.15541478775801573\n",
      "The 32451 th iteration gives loss of 0.15541300947693057\n",
      "The 32452 th iteration gives loss of 0.15541123128296397\n",
      "The 32453 th iteration gives loss of 0.1554094531760968\n",
      "The 32454 th iteration gives loss of 0.15540767515632428\n",
      "The 32455 th iteration gives loss of 0.1554058972236276\n",
      "The 32456 th iteration gives loss of 0.15540411937798634\n",
      "The 32457 th iteration gives loss of 0.1554023416194015\n",
      "The 32458 th iteration gives loss of 0.15540056394785165\n",
      "The 32459 th iteration gives loss of 0.15539878636333618\n",
      "The 32460 th iteration gives loss of 0.15539700886583313\n",
      "The 32461 th iteration gives loss of 0.15539523145533135\n",
      "The 32462 th iteration gives loss of 0.15539345413181577\n",
      "The 32463 th iteration gives loss of 0.15539167689529193\n",
      "The 32464 th iteration gives loss of 0.15538989974573067\n",
      "The 32465 th iteration gives loss of 0.1553881226831181\n",
      "The 32466 th iteration gives loss of 0.15538634570744753\n",
      "The 32467 th iteration gives loss of 0.15538456881871412\n",
      "The 32468 th iteration gives loss of 0.15538279201689226\n",
      "The 32469 th iteration gives loss of 0.15538101530198148\n",
      "The 32470 th iteration gives loss of 0.15537923867396344\n",
      "The 32471 th iteration gives loss of 0.1553774621328278\n",
      "The 32472 th iteration gives loss of 0.15537568567856472\n",
      "The 32473 th iteration gives loss of 0.15537390931114864\n",
      "The 32474 th iteration gives loss of 0.1553721330305872\n",
      "The 32475 th iteration gives loss of 0.15537035683685155\n",
      "The 32476 th iteration gives loss of 0.15536858072994442\n",
      "The 32477 th iteration gives loss of 0.15536680470984007\n",
      "The 32478 th iteration gives loss of 0.15536502877652844\n",
      "The 32479 th iteration gives loss of 0.15536325293000705\n",
      "The 32480 th iteration gives loss of 0.15536147717025595\n",
      "The 32481 th iteration gives loss of 0.15535970149726636\n",
      "The 32482 th iteration gives loss of 0.15535792591102834\n",
      "The 32483 th iteration gives loss of 0.1553561504115234\n",
      "The 32484 th iteration gives loss of 0.15535437499874494\n",
      "The 32485 th iteration gives loss of 0.1553525996726731\n",
      "The 32486 th iteration gives loss of 0.15535082443330353\n",
      "The 32487 th iteration gives loss of 0.15534904928061874\n",
      "The 32488 th iteration gives loss of 0.1553472742146099\n",
      "The 32489 th iteration gives loss of 0.15534549923526758\n",
      "The 32490 th iteration gives loss of 0.15534372434256882\n",
      "The 32491 th iteration gives loss of 0.15534194953651642\n",
      "The 32492 th iteration gives loss of 0.15534017481709178\n",
      "The 32493 th iteration gives loss of 0.15533840018428197\n",
      "The 32494 th iteration gives loss of 0.155336625638073\n",
      "The 32495 th iteration gives loss of 0.15533485117845872\n",
      "The 32496 th iteration gives loss of 0.1553330768054145\n",
      "The 32497 th iteration gives loss of 0.15533130251893856\n",
      "The 32498 th iteration gives loss of 0.15532952831901545\n",
      "The 32499 th iteration gives loss of 0.15532775420563621\n",
      "The 32500 th iteration gives loss of 0.1553259801787911\n",
      "The 32501 th iteration gives loss of 0.1553242062384614\n",
      "The 32502 th iteration gives loss of 0.1553224323846392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 32503 th iteration gives loss of 0.1553206586173094\n",
      "The 32504 th iteration gives loss of 0.15531888493645699\n",
      "The 32505 th iteration gives loss of 0.15531711134208012\n",
      "The 32506 th iteration gives loss of 0.15531533783416074\n",
      "The 32507 th iteration gives loss of 0.1553135644126829\n",
      "The 32508 th iteration gives loss of 0.15531179107764392\n",
      "The 32509 th iteration gives loss of 0.15531001782902465\n",
      "The 32510 th iteration gives loss of 0.15530824466681264\n",
      "The 32511 th iteration gives loss of 0.15530647159099575\n",
      "The 32512 th iteration gives loss of 0.1553046986015708\n",
      "The 32513 th iteration gives loss of 0.15530292569851317\n",
      "The 32514 th iteration gives loss of 0.15530115288181578\n",
      "The 32515 th iteration gives loss of 0.15529938015147268\n",
      "The 32516 th iteration gives loss of 0.15529760750746557\n",
      "The 32517 th iteration gives loss of 0.1552958349497796\n",
      "The 32518 th iteration gives loss of 0.15529406247840646\n",
      "The 32519 th iteration gives loss of 0.155292290093339\n",
      "The 32520 th iteration gives loss of 0.1552905177945603\n",
      "The 32521 th iteration gives loss of 0.15528874558205716\n",
      "The 32522 th iteration gives loss of 0.15528697345581388\n",
      "The 32523 th iteration gives loss of 0.1552852014158257\n",
      "The 32524 th iteration gives loss of 0.1552834294620723\n",
      "The 32525 th iteration gives loss of 0.15528165759455714\n",
      "The 32526 th iteration gives loss of 0.1552798858132569\n",
      "The 32527 th iteration gives loss of 0.15527811411815223\n",
      "The 32528 th iteration gives loss of 0.1552763425092535\n",
      "The 32529 th iteration gives loss of 0.1552745709865243\n",
      "The 32530 th iteration gives loss of 0.15527279954996243\n",
      "The 32531 th iteration gives loss of 0.15527102819956762\n",
      "The 32532 th iteration gives loss of 0.15526925693530683\n",
      "The 32533 th iteration gives loss of 0.15526748575718588\n",
      "The 32534 th iteration gives loss of 0.1552657146651776\n",
      "The 32535 th iteration gives loss of 0.15526394365928514\n",
      "The 32536 th iteration gives loss of 0.1552621727394833\n",
      "The 32537 th iteration gives loss of 0.15526040190576235\n",
      "The 32538 th iteration gives loss of 0.15525863115812022\n",
      "The 32539 th iteration gives loss of 0.1552568604965343\n",
      "The 32540 th iteration gives loss of 0.15525508992100176\n",
      "The 32541 th iteration gives loss of 0.15525331943150072\n",
      "The 32542 th iteration gives loss of 0.1552515490280227\n",
      "The 32543 th iteration gives loss of 0.1552497787105563\n",
      "The 32544 th iteration gives loss of 0.15524800847909215\n",
      "The 32545 th iteration gives loss of 0.15524623833361187\n",
      "The 32546 th iteration gives loss of 0.1552444682741181\n",
      "The 32547 th iteration gives loss of 0.15524269830057885\n",
      "The 32548 th iteration gives loss of 0.15524092841298737\n",
      "The 32549 th iteration gives loss of 0.15523915861134976\n",
      "The 32550 th iteration gives loss of 0.15523738889563035\n",
      "The 32551 th iteration gives loss of 0.15523561926583243\n",
      "The 32552 th iteration gives loss of 0.1552338497219306\n",
      "The 32553 th iteration gives loss of 0.15523208026392687\n",
      "The 32554 th iteration gives loss of 0.1552303108917968\n",
      "The 32555 th iteration gives loss of 0.15522854160554528\n",
      "The 32556 th iteration gives loss of 0.15522677240514293\n",
      "The 32557 th iteration gives loss of 0.15522500329058844\n",
      "The 32558 th iteration gives loss of 0.1552232342618632\n",
      "The 32559 th iteration gives loss of 0.1552214653189628\n",
      "The 32560 th iteration gives loss of 0.15521969646186226\n",
      "The 32561 th iteration gives loss of 0.15521792769056506\n",
      "The 32562 th iteration gives loss of 0.15521615900505167\n",
      "The 32563 th iteration gives loss of 0.15521439040530752\n",
      "The 32564 th iteration gives loss of 0.1552126218913197\n",
      "The 32565 th iteration gives loss of 0.1552108534630758\n",
      "The 32566 th iteration gives loss of 0.15520908512057666\n",
      "The 32567 th iteration gives loss of 0.1552073168638026\n",
      "The 32568 th iteration gives loss of 0.15520554869273806\n",
      "The 32569 th iteration gives loss of 0.15520378060737658\n",
      "The 32570 th iteration gives loss of 0.15520201260770577\n",
      "The 32571 th iteration gives loss of 0.1552002446937082\n",
      "The 32572 th iteration gives loss of 0.1551984768653701\n",
      "The 32573 th iteration gives loss of 0.15519670912269265\n",
      "The 32574 th iteration gives loss of 0.15519494146565044\n",
      "The 32575 th iteration gives loss of 0.15519317389424722\n",
      "The 32576 th iteration gives loss of 0.1551914064084491\n",
      "The 32577 th iteration gives loss of 0.15518963900825997\n",
      "The 32578 th iteration gives loss of 0.1551878716936612\n",
      "The 32579 th iteration gives loss of 0.15518610446464484\n",
      "The 32580 th iteration gives loss of 0.15518433732119996\n",
      "The 32581 th iteration gives loss of 0.15518257026330914\n",
      "The 32582 th iteration gives loss of 0.15518080329096834\n",
      "The 32583 th iteration gives loss of 0.15517903640415265\n",
      "The 32584 th iteration gives loss of 0.15517726960285225\n",
      "The 32585 th iteration gives loss of 0.15517550288707738\n",
      "The 32586 th iteration gives loss of 0.15517373625678796\n",
      "The 32587 th iteration gives loss of 0.155171969711997\n",
      "The 32588 th iteration gives loss of 0.1551702032526656\n",
      "The 32589 th iteration gives loss of 0.1551684368788021\n",
      "The 32590 th iteration gives loss of 0.1551666705903893\n",
      "The 32591 th iteration gives loss of 0.1551649043874176\n",
      "The 32592 th iteration gives loss of 0.15516313826986577\n",
      "The 32593 th iteration gives loss of 0.15516137223773108\n",
      "The 32594 th iteration gives loss of 0.1551596062909939\n",
      "The 32595 th iteration gives loss of 0.15515784042964634\n",
      "The 32596 th iteration gives loss of 0.1551560746536826\n",
      "The 32597 th iteration gives loss of 0.15515430896308086\n",
      "The 32598 th iteration gives loss of 0.15515254335783826\n",
      "The 32599 th iteration gives loss of 0.15515077783793355\n",
      "The 32600 th iteration gives loss of 0.1551490124033636\n",
      "The 32601 th iteration gives loss of 0.1551472470541102\n",
      "The 32602 th iteration gives loss of 0.1551454817901636\n",
      "The 32603 th iteration gives loss of 0.15514371661151305\n",
      "The 32604 th iteration gives loss of 0.15514195151814464\n",
      "The 32605 th iteration gives loss of 0.15514018651004227\n",
      "The 32606 th iteration gives loss of 0.15513842158720473\n",
      "The 32607 th iteration gives loss of 0.15513665674961263\n",
      "The 32608 th iteration gives loss of 0.15513489199725233\n",
      "The 32609 th iteration gives loss of 0.1551331273301209\n",
      "The 32610 th iteration gives loss of 0.1551313627482008\n",
      "The 32611 th iteration gives loss of 0.15512959825147632\n",
      "The 32612 th iteration gives loss of 0.15512783383994813\n",
      "The 32613 th iteration gives loss of 0.15512606951358063\n",
      "The 32614 th iteration gives loss of 0.1551243052723875\n",
      "The 32615 th iteration gives loss of 0.15512254111634746\n",
      "The 32616 th iteration gives loss of 0.15512077704545224\n",
      "The 32617 th iteration gives loss of 0.1551190130596815\n",
      "The 32618 th iteration gives loss of 0.15511724915902447\n",
      "The 32619 th iteration gives loss of 0.15511548534347055\n",
      "The 32620 th iteration gives loss of 0.15511372161300854\n",
      "The 32621 th iteration gives loss of 0.15511195796763297\n",
      "The 32622 th iteration gives loss of 0.15511019440732632\n",
      "The 32623 th iteration gives loss of 0.15510843093207888\n",
      "The 32624 th iteration gives loss of 0.15510666754187039\n",
      "The 32625 th iteration gives loss of 0.15510490423669845\n",
      "The 32626 th iteration gives loss of 0.15510314101655276\n",
      "The 32627 th iteration gives loss of 0.15510137788141587\n",
      "The 32628 th iteration gives loss of 0.155099614831267\n",
      "The 32629 th iteration gives loss of 0.15509785186611302\n",
      "The 32630 th iteration gives loss of 0.15509608898593547\n",
      "The 32631 th iteration gives loss of 0.15509432619071623\n",
      "The 32632 th iteration gives loss of 0.15509256348045108\n",
      "The 32633 th iteration gives loss of 0.15509080085512944\n",
      "The 32634 th iteration gives loss of 0.15508903831472823\n",
      "The 32635 th iteration gives loss of 0.15508727585924015\n",
      "The 32636 th iteration gives loss of 0.15508551348864974\n",
      "The 32637 th iteration gives loss of 0.15508375120296247\n",
      "The 32638 th iteration gives loss of 0.15508198900215456\n",
      "The 32639 th iteration gives loss of 0.1550802268862094\n",
      "The 32640 th iteration gives loss of 0.15507846485511992\n",
      "The 32641 th iteration gives loss of 0.1550767029088774\n",
      "The 32642 th iteration gives loss of 0.15507494104747052\n",
      "The 32643 th iteration gives loss of 0.15507317927087114\n",
      "The 32644 th iteration gives loss of 0.15507141757909362\n",
      "The 32645 th iteration gives loss of 0.15506965597210884\n",
      "The 32646 th iteration gives loss of 0.15506789444991226\n",
      "The 32647 th iteration gives loss of 0.15506613301248218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 32648 th iteration gives loss of 0.15506437165981676\n",
      "The 32649 th iteration gives loss of 0.1550626103919001\n",
      "The 32650 th iteration gives loss of 0.15506084920872568\n",
      "The 32651 th iteration gives loss of 0.15505908811027308\n",
      "The 32652 th iteration gives loss of 0.1550573270965421\n",
      "The 32653 th iteration gives loss of 0.15505556616750454\n",
      "The 32654 th iteration gives loss of 0.15505380532316168\n",
      "The 32655 th iteration gives loss of 0.15505204456349614\n",
      "The 32656 th iteration gives loss of 0.1550502838884945\n",
      "The 32657 th iteration gives loss of 0.1550485232981511\n",
      "The 32658 th iteration gives loss of 0.15504676279245852\n",
      "The 32659 th iteration gives loss of 0.15504500237138863\n",
      "The 32660 th iteration gives loss of 0.15504324203494246\n",
      "The 32661 th iteration gives loss of 0.15504148178310923\n",
      "The 32662 th iteration gives loss of 0.15503972161586382\n",
      "The 32663 th iteration gives loss of 0.15503796153320715\n",
      "The 32664 th iteration gives loss of 0.1550362015351158\n",
      "The 32665 th iteration gives loss of 0.15503444162159466\n",
      "The 32666 th iteration gives loss of 0.15503268179262264\n",
      "The 32667 th iteration gives loss of 0.1550309220481821\n",
      "The 32668 th iteration gives loss of 0.15502916238826817\n",
      "The 32669 th iteration gives loss of 0.1550274028128702\n",
      "The 32670 th iteration gives loss of 0.15502564332197205\n",
      "The 32671 th iteration gives loss of 0.15502388391556865\n",
      "The 32672 th iteration gives loss of 0.15502212459364453\n",
      "The 32673 th iteration gives loss of 0.15502036535618033\n",
      "The 32674 th iteration gives loss of 0.15501860620317898\n",
      "The 32675 th iteration gives loss of 0.15501684713462055\n",
      "The 32676 th iteration gives loss of 0.1550150881504838\n",
      "The 32677 th iteration gives loss of 0.1550133292507753\n",
      "The 32678 th iteration gives loss of 0.15501157043547598\n",
      "The 32679 th iteration gives loss of 0.155009811704567\n",
      "The 32680 th iteration gives loss of 0.15500805305804857\n",
      "The 32681 th iteration gives loss of 0.15500629449590048\n",
      "The 32682 th iteration gives loss of 0.15500453601811387\n",
      "The 32683 th iteration gives loss of 0.1550027776246761\n",
      "The 32684 th iteration gives loss of 0.15500101931557644\n",
      "The 32685 th iteration gives loss of 0.1549992610908003\n",
      "The 32686 th iteration gives loss of 0.15499750295034642\n",
      "The 32687 th iteration gives loss of 0.15499574489418388\n",
      "The 32688 th iteration gives loss of 0.15499398692231414\n",
      "The 32689 th iteration gives loss of 0.1549922290347289\n",
      "The 32690 th iteration gives loss of 0.1549904712314081\n",
      "The 32691 th iteration gives loss of 0.15498871351233762\n",
      "The 32692 th iteration gives loss of 0.15498695587751618\n",
      "The 32693 th iteration gives loss of 0.15498519832692498\n",
      "The 32694 th iteration gives loss of 0.15498344086055454\n",
      "The 32695 th iteration gives loss of 0.15498168347839372\n",
      "The 32696 th iteration gives loss of 0.1549799261804289\n",
      "The 32697 th iteration gives loss of 0.154978168966653\n",
      "The 32698 th iteration gives loss of 0.15497641183704713\n",
      "The 32699 th iteration gives loss of 0.15497465479160155\n",
      "The 32700 th iteration gives loss of 0.15497289783030888\n",
      "The 32701 th iteration gives loss of 0.15497114095314993\n",
      "The 32702 th iteration gives loss of 0.1549693841601176\n",
      "The 32703 th iteration gives loss of 0.15496762745120282\n",
      "The 32704 th iteration gives loss of 0.15496587082638977\n",
      "The 32705 th iteration gives loss of 0.154964114285674\n",
      "The 32706 th iteration gives loss of 0.1549623578290335\n",
      "The 32707 th iteration gives loss of 0.1549606014564577\n",
      "The 32708 th iteration gives loss of 0.15495884516793484\n",
      "The 32709 th iteration gives loss of 0.15495708896346835\n",
      "The 32710 th iteration gives loss of 0.15495533284302795\n",
      "The 32711 th iteration gives loss of 0.15495357680661495\n",
      "The 32712 th iteration gives loss of 0.154951820854211\n",
      "The 32713 th iteration gives loss of 0.15495006498579678\n",
      "The 32714 th iteration gives loss of 0.15494830920137193\n",
      "The 32715 th iteration gives loss of 0.1549465535009247\n",
      "The 32716 th iteration gives loss of 0.1549447978844311\n",
      "The 32717 th iteration gives loss of 0.15494304235189452\n",
      "The 32718 th iteration gives loss of 0.15494128690330108\n",
      "The 32719 th iteration gives loss of 0.15493953153863066\n",
      "The 32720 th iteration gives loss of 0.15493777625788066\n",
      "The 32721 th iteration gives loss of 0.1549360210610312\n",
      "The 32722 th iteration gives loss of 0.15493426594807522\n",
      "The 32723 th iteration gives loss of 0.1549325109189978\n",
      "The 32724 th iteration gives loss of 0.1549307559737941\n",
      "The 32725 th iteration gives loss of 0.15492900111244678\n",
      "The 32726 th iteration gives loss of 0.15492724633494281\n",
      "The 32727 th iteration gives loss of 0.15492549164127722\n",
      "The 32728 th iteration gives loss of 0.15492373703143\n",
      "The 32729 th iteration gives loss of 0.1549219825054027\n",
      "The 32730 th iteration gives loss of 0.15492022806316025\n",
      "The 32731 th iteration gives loss of 0.15491847370471923\n",
      "The 32732 th iteration gives loss of 0.15491671943004454\n",
      "The 32733 th iteration gives loss of 0.1549149652391445\n",
      "The 32734 th iteration gives loss of 0.15491321113198853\n",
      "The 32735 th iteration gives loss of 0.15491145710857832\n",
      "The 32736 th iteration gives loss of 0.15490970316889832\n",
      "The 32737 th iteration gives loss of 0.15490794931293708\n",
      "The 32738 th iteration gives loss of 0.15490619554067306\n",
      "The 32739 th iteration gives loss of 0.15490444185211533\n",
      "The 32740 th iteration gives loss of 0.1549026882472303\n",
      "The 32741 th iteration gives loss of 0.15490093472602035\n",
      "The 32742 th iteration gives loss of 0.15489918128846833\n",
      "The 32743 th iteration gives loss of 0.15489742793456607\n",
      "The 32744 th iteration gives loss of 0.15489567466430065\n",
      "The 32745 th iteration gives loss of 0.15489392147766506\n",
      "The 32746 th iteration gives loss of 0.15489216837463882\n",
      "The 32747 th iteration gives loss of 0.15489041535520714\n",
      "The 32748 th iteration gives loss of 0.15488866241937205\n",
      "The 32749 th iteration gives loss of 0.15488690956711568\n",
      "The 32750 th iteration gives loss of 0.1548851567984198\n",
      "The 32751 th iteration gives loss of 0.1548834041132866\n",
      "The 32752 th iteration gives loss of 0.15488165151168898\n",
      "The 32753 th iteration gives loss of 0.1548798989936313\n",
      "The 32754 th iteration gives loss of 0.15487814655909443\n",
      "The 32755 th iteration gives loss of 0.15487639420806182\n",
      "The 32756 th iteration gives loss of 0.1548746419405216\n",
      "The 32757 th iteration gives loss of 0.1548728897564674\n",
      "The 32758 th iteration gives loss of 0.15487113765589408\n",
      "The 32759 th iteration gives loss of 0.15486938563878444\n",
      "The 32760 th iteration gives loss of 0.15486763370511833\n",
      "The 32761 th iteration gives loss of 0.1548658818548936\n",
      "The 32762 th iteration gives loss of 0.1548641300880971\n",
      "The 32763 th iteration gives loss of 0.15486237840471584\n",
      "The 32764 th iteration gives loss of 0.15486062680473092\n",
      "The 32765 th iteration gives loss of 0.154858875288152\n",
      "The 32766 th iteration gives loss of 0.1548571238549535\n",
      "The 32767 th iteration gives loss of 0.15485537250511303\n",
      "The 32768 th iteration gives loss of 0.15485362123863655\n",
      "The 32769 th iteration gives loss of 0.15485187005550324\n",
      "The 32770 th iteration gives loss of 0.15485011895570847\n",
      "The 32771 th iteration gives loss of 0.1548483679392316\n",
      "The 32772 th iteration gives loss of 0.15484661700607405\n",
      "The 32773 th iteration gives loss of 0.15484486615621154\n",
      "The 32774 th iteration gives loss of 0.1548431153896362\n",
      "The 32775 th iteration gives loss of 0.15484136470633775\n",
      "The 32776 th iteration gives loss of 0.1548396141063138\n",
      "The 32777 th iteration gives loss of 0.1548378635895336\n",
      "The 32778 th iteration gives loss of 0.1548361131559961\n",
      "The 32779 th iteration gives loss of 0.15483436280568943\n",
      "The 32780 th iteration gives loss of 0.154832612538603\n",
      "The 32781 th iteration gives loss of 0.15483086235472188\n",
      "The 32782 th iteration gives loss of 0.15482911225404383\n",
      "The 32783 th iteration gives loss of 0.15482736223653973\n",
      "The 32784 th iteration gives loss of 0.1548256123022133\n",
      "The 32785 th iteration gives loss of 0.15482386245104765\n",
      "The 32786 th iteration gives loss of 0.15482211268303686\n",
      "The 32787 th iteration gives loss of 0.15482036299815652\n",
      "The 32788 th iteration gives loss of 0.15481861339640215\n",
      "The 32789 th iteration gives loss of 0.15481686387777102\n",
      "The 32790 th iteration gives loss of 0.1548151144422331\n",
      "The 32791 th iteration gives loss of 0.15481336508978644\n",
      "The 32792 th iteration gives loss of 0.15481161582042213\n",
      "The 32793 th iteration gives loss of 0.15480986663413943\n",
      "The 32794 th iteration gives loss of 0.1548081175308999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 32795 th iteration gives loss of 0.1548063685107034\n",
      "The 32796 th iteration gives loss of 0.1548046195735488\n",
      "The 32797 th iteration gives loss of 0.15480287071941992\n",
      "The 32798 th iteration gives loss of 0.1548011219482933\n",
      "The 32799 th iteration gives loss of 0.154799373260176\n",
      "The 32800 th iteration gives loss of 0.15479762465504143\n",
      "The 32801 th iteration gives loss of 0.1547958761328807\n",
      "The 32802 th iteration gives loss of 0.1547941276936807\n",
      "The 32803 th iteration gives loss of 0.15479237933744178\n",
      "The 32804 th iteration gives loss of 0.15479063106414293\n",
      "The 32805 th iteration gives loss of 0.15478888287377493\n",
      "The 32806 th iteration gives loss of 0.15478713476633163\n",
      "The 32807 th iteration gives loss of 0.154785386741785\n",
      "The 32808 th iteration gives loss of 0.15478363880014137\n",
      "The 32809 th iteration gives loss of 0.15478189094137607\n",
      "The 32810 th iteration gives loss of 0.15478014316548278\n",
      "The 32811 th iteration gives loss of 0.15477839547246192\n",
      "The 32812 th iteration gives loss of 0.15477664786227902\n",
      "The 32813 th iteration gives loss of 0.15477490033494073\n",
      "The 32814 th iteration gives loss of 0.15477315289042637\n",
      "The 32815 th iteration gives loss of 0.15477140552873173\n",
      "The 32816 th iteration gives loss of 0.1547696582498385\n",
      "The 32817 th iteration gives loss of 0.15476791105373536\n",
      "The 32818 th iteration gives loss of 0.15476616394041684\n",
      "The 32819 th iteration gives loss of 0.1547644169098651\n",
      "The 32820 th iteration gives loss of 0.15476266996207272\n",
      "The 32821 th iteration gives loss of 0.1547609230970167\n",
      "The 32822 th iteration gives loss of 0.1547591763147039\n",
      "The 32823 th iteration gives loss of 0.15475742961511488\n",
      "The 32824 th iteration gives loss of 0.15475568299823655\n",
      "The 32825 th iteration gives loss of 0.15475393646405974\n",
      "The 32826 th iteration gives loss of 0.154752190012574\n",
      "The 32827 th iteration gives loss of 0.15475044364376397\n",
      "The 32828 th iteration gives loss of 0.15474869735761412\n",
      "The 32829 th iteration gives loss of 0.154746951154115\n",
      "The 32830 th iteration gives loss of 0.15474520503327524\n",
      "The 32831 th iteration gives loss of 0.1547434589950578\n",
      "The 32832 th iteration gives loss of 0.15474171303945897\n",
      "The 32833 th iteration gives loss of 0.1547399671664736\n",
      "The 32834 th iteration gives loss of 0.15473822137608628\n",
      "The 32835 th iteration gives loss of 0.15473647566827353\n",
      "The 32836 th iteration gives loss of 0.15473473004304691\n",
      "The 32837 th iteration gives loss of 0.1547329845003784\n",
      "The 32838 th iteration gives loss of 0.15473123904025468\n",
      "The 32839 th iteration gives loss of 0.15472949366267919\n",
      "The 32840 th iteration gives loss of 0.15472774836762984\n",
      "The 32841 th iteration gives loss of 0.15472600315509988\n",
      "The 32842 th iteration gives loss of 0.15472425802506395\n",
      "The 32843 th iteration gives loss of 0.15472251297753356\n",
      "The 32844 th iteration gives loss of 0.1547207680124863\n",
      "The 32845 th iteration gives loss of 0.15471902312990993\n",
      "The 32846 th iteration gives loss of 0.15471727832978385\n",
      "The 32847 th iteration gives loss of 0.15471553361211562\n",
      "The 32848 th iteration gives loss of 0.1547137889768853\n",
      "The 32849 th iteration gives loss of 0.15471204442407668\n",
      "The 32850 th iteration gives loss of 0.1547102999536788\n",
      "The 32851 th iteration gives loss of 0.1547085555656845\n",
      "The 32852 th iteration gives loss of 0.1547068112600769\n",
      "The 32853 th iteration gives loss of 0.15470506703685855\n",
      "The 32854 th iteration gives loss of 0.1547033228960035\n",
      "The 32855 th iteration gives loss of 0.15470157883750538\n",
      "The 32856 th iteration gives loss of 0.15469983486135105\n",
      "The 32857 th iteration gives loss of 0.1546980909675311\n",
      "The 32858 th iteration gives loss of 0.15469634715603875\n",
      "The 32859 th iteration gives loss of 0.15469460342685135\n",
      "The 32860 th iteration gives loss of 0.15469285977996228\n",
      "The 32861 th iteration gives loss of 0.15469111621536333\n",
      "The 32862 th iteration gives loss of 0.15468937273304317\n",
      "The 32863 th iteration gives loss of 0.15468762933299446\n",
      "The 32864 th iteration gives loss of 0.1546858860151886\n",
      "The 32865 th iteration gives loss of 0.15468414277962536\n",
      "The 32866 th iteration gives loss of 0.15468239962630356\n",
      "The 32867 th iteration gives loss of 0.154680656555192\n",
      "The 32868 th iteration gives loss of 0.15467891356629473\n",
      "The 32869 th iteration gives loss of 0.1546771706595912\n",
      "The 32870 th iteration gives loss of 0.1546754278350722\n",
      "The 32871 th iteration gives loss of 0.1546736850927277\n",
      "The 32872 th iteration gives loss of 0.15467194243254967\n",
      "The 32873 th iteration gives loss of 0.15467019985451677\n",
      "The 32874 th iteration gives loss of 0.15466845735862603\n",
      "The 32875 th iteration gives loss of 0.15466671494486608\n",
      "The 32876 th iteration gives loss of 0.1546649726132174\n",
      "The 32877 th iteration gives loss of 0.1546632303636761\n",
      "The 32878 th iteration gives loss of 0.15466148819623748\n",
      "The 32879 th iteration gives loss of 0.15465974611088504\n",
      "The 32880 th iteration gives loss of 0.15465800410758956\n",
      "The 32881 th iteration gives loss of 0.15465626218635795\n",
      "The 32882 th iteration gives loss of 0.15465452034718216\n",
      "The 32883 th iteration gives loss of 0.1546527785900342\n",
      "The 32884 th iteration gives loss of 0.15465103691491644\n",
      "The 32885 th iteration gives loss of 0.15464929532181831\n",
      "The 32886 th iteration gives loss of 0.1546475538107195\n",
      "The 32887 th iteration gives loss of 0.1546458123816065\n",
      "The 32888 th iteration gives loss of 0.1546440710344799\n",
      "The 32889 th iteration gives loss of 0.15464232976932776\n",
      "The 32890 th iteration gives loss of 0.15464058858612828\n",
      "The 32891 th iteration gives loss of 0.15463884748487547\n",
      "The 32892 th iteration gives loss of 0.15463710646555884\n",
      "The 32893 th iteration gives loss of 0.15463536552816243\n",
      "The 32894 th iteration gives loss of 0.154633624672676\n",
      "The 32895 th iteration gives loss of 0.15463188389909668\n",
      "The 32896 th iteration gives loss of 0.15463014320740687\n",
      "The 32897 th iteration gives loss of 0.1546284025976033\n",
      "The 32898 th iteration gives loss of 0.15462666206965997\n",
      "The 32899 th iteration gives loss of 0.15462492162357036\n",
      "The 32900 th iteration gives loss of 0.1546231812593196\n",
      "The 32901 th iteration gives loss of 0.15462144097690877\n",
      "The 32902 th iteration gives loss of 0.15461970077631654\n",
      "The 32903 th iteration gives loss of 0.15461796065753342\n",
      "The 32904 th iteration gives loss of 0.15461622062055771\n",
      "The 32905 th iteration gives loss of 0.1546144806653615\n",
      "The 32906 th iteration gives loss of 0.1546127407919453\n",
      "The 32907 th iteration gives loss of 0.15461100100029088\n",
      "The 32908 th iteration gives loss of 0.15460926129039476\n",
      "The 32909 th iteration gives loss of 0.15460752166223693\n",
      "The 32910 th iteration gives loss of 0.15460578211580792\n",
      "The 32911 th iteration gives loss of 0.15460404265110203\n",
      "The 32912 th iteration gives loss of 0.15460230326810306\n",
      "The 32913 th iteration gives loss of 0.15460056396680252\n",
      "The 32914 th iteration gives loss of 0.15459882474718642\n",
      "The 32915 th iteration gives loss of 0.15459708560924995\n",
      "The 32916 th iteration gives loss of 0.1545953465529624\n",
      "The 32917 th iteration gives loss of 0.15459360757833873\n",
      "The 32918 th iteration gives loss of 0.1545918686853554\n",
      "The 32919 th iteration gives loss of 0.15459012987399903\n",
      "The 32920 th iteration gives loss of 0.15458839114425788\n",
      "The 32921 th iteration gives loss of 0.15458665249612494\n",
      "The 32922 th iteration gives loss of 0.15458491392959495\n",
      "The 32923 th iteration gives loss of 0.15458317544463762\n",
      "The 32924 th iteration gives loss of 0.15458143704124705\n",
      "The 32925 th iteration gives loss of 0.15457969871943186\n",
      "The 32926 th iteration gives loss of 0.15457796047916156\n",
      "The 32927 th iteration gives loss of 0.15457622232043078\n",
      "The 32928 th iteration gives loss of 0.15457448424322723\n",
      "The 32929 th iteration gives loss of 0.15457274624753167\n",
      "The 32930 th iteration gives loss of 0.15457100833335954\n",
      "The 32931 th iteration gives loss of 0.1545692705006654\n",
      "The 32932 th iteration gives loss of 0.15456753274946233\n",
      "The 32933 th iteration gives loss of 0.15456579507972046\n",
      "The 32934 th iteration gives loss of 0.15456405749144367\n",
      "The 32935 th iteration gives loss of 0.15456231998461792\n",
      "The 32936 th iteration gives loss of 0.1545605825592302\n",
      "The 32937 th iteration gives loss of 0.15455884521526395\n",
      "The 32938 th iteration gives loss of 0.15455710795271219\n",
      "The 32939 th iteration gives loss of 0.15455537077156106\n",
      "The 32940 th iteration gives loss of 0.1545536336717982\n",
      "The 32941 th iteration gives loss of 0.15455189665342375\n",
      "The 32942 th iteration gives loss of 0.1545501597164223\n",
      "The 32943 th iteration gives loss of 0.15454842286077902\n",
      "The 32944 th iteration gives loss of 0.15454668608647187\n",
      "The 32945 th iteration gives loss of 0.15454494939350666\n",
      "The 32946 th iteration gives loss of 0.15454321278186542\n",
      "The 32947 th iteration gives loss of 0.15454147625153683\n",
      "The 32948 th iteration gives loss of 0.15453973980250682\n",
      "The 32949 th iteration gives loss of 0.15453800343477764\n",
      "The 32950 th iteration gives loss of 0.15453626714831636\n",
      "The 32951 th iteration gives loss of 0.15453453094312458\n",
      "The 32952 th iteration gives loss of 0.1545327948191956\n",
      "The 32953 th iteration gives loss of 0.15453105877651307\n",
      "The 32954 th iteration gives loss of 0.15452932281506146\n",
      "The 32955 th iteration gives loss of 0.15452758693483346\n",
      "The 32956 th iteration gives loss of 0.1545258511358146\n",
      "The 32957 th iteration gives loss of 0.15452411541799857\n",
      "The 32958 th iteration gives loss of 0.15452237978136976\n",
      "The 32959 th iteration gives loss of 0.15452064422591572\n",
      "The 32960 th iteration gives loss of 0.1545189087516315\n",
      "The 32961 th iteration gives loss of 0.15451717335851103\n",
      "The 32962 th iteration gives loss of 0.15451543804652582\n",
      "The 32963 th iteration gives loss of 0.15451370281567642\n",
      "The 32964 th iteration gives loss of 0.15451196766594605\n",
      "The 32965 th iteration gives loss of 0.15451023259732993\n",
      "The 32966 th iteration gives loss of 0.15450849760980828\n",
      "The 32967 th iteration gives loss of 0.15450676270338007\n",
      "The 32968 th iteration gives loss of 0.15450502787802806\n",
      "The 32969 th iteration gives loss of 0.1545032931337463\n",
      "The 32970 th iteration gives loss of 0.15450155847051222\n",
      "The 32971 th iteration gives loss of 0.1544998238883196\n",
      "The 32972 th iteration gives loss of 0.15449808938716408\n",
      "The 32973 th iteration gives loss of 0.15449635496702976\n",
      "The 32974 th iteration gives loss of 0.15449462062789898\n",
      "The 32975 th iteration gives loss of 0.15449288636977093\n",
      "The 32976 th iteration gives loss of 0.15449115219263138\n",
      "The 32977 th iteration gives loss of 0.1544894180964673\n",
      "The 32978 th iteration gives loss of 0.1544876840812664\n",
      "The 32979 th iteration gives loss of 0.15448595014702457\n",
      "The 32980 th iteration gives loss of 0.15448421629371778\n",
      "The 32981 th iteration gives loss of 0.1544824825213487\n",
      "The 32982 th iteration gives loss of 0.15448074882988908\n",
      "The 32983 th iteration gives loss of 0.15447901521934784\n",
      "The 32984 th iteration gives loss of 0.154477281689697\n",
      "The 32985 th iteration gives loss of 0.15447554824094184\n",
      "The 32986 th iteration gives loss of 0.15447381487305606\n",
      "The 32987 th iteration gives loss of 0.1544720815860331\n",
      "The 32988 th iteration gives loss of 0.15447034837986395\n",
      "The 32989 th iteration gives loss of 0.15446861525454067\n",
      "The 32990 th iteration gives loss of 0.1544668822100467\n",
      "The 32991 th iteration gives loss of 0.1544651492463723\n",
      "The 32992 th iteration gives loss of 0.1544634163635025\n",
      "The 32993 th iteration gives loss of 0.15446168356143497\n",
      "The 32994 th iteration gives loss of 0.154459950840148\n",
      "The 32995 th iteration gives loss of 0.15445821819963748\n",
      "The 32996 th iteration gives loss of 0.1544564856398917\n",
      "The 32997 th iteration gives loss of 0.1544547531608955\n",
      "The 32998 th iteration gives loss of 0.15445302076264023\n",
      "The 32999 th iteration gives loss of 0.15445128844511938\n",
      "The 33000 th iteration gives loss of 0.1544495562083117\n",
      "The 33001 th iteration gives loss of 0.1544478240522086\n",
      "The 33002 th iteration gives loss of 0.15444609197681225\n",
      "The 33003 th iteration gives loss of 0.15444435998209613\n",
      "The 33004 th iteration gives loss of 0.15444262806805698\n",
      "The 33005 th iteration gives loss of 0.15444089623467436\n",
      "The 33006 th iteration gives loss of 0.15443916448194286\n",
      "The 33007 th iteration gives loss of 0.154437432809855\n",
      "The 33008 th iteration gives loss of 0.15443570121840036\n",
      "The 33009 th iteration gives loss of 0.1544339697075587\n",
      "The 33010 th iteration gives loss of 0.15443223827732538\n",
      "The 33011 th iteration gives loss of 0.15443050692769383\n",
      "The 33012 th iteration gives loss of 0.15442877565864396\n",
      "The 33013 th iteration gives loss of 0.15442704447016614\n",
      "The 33014 th iteration gives loss of 0.1544253133622493\n",
      "The 33015 th iteration gives loss of 0.15442358233488607\n",
      "The 33016 th iteration gives loss of 0.15442185138805836\n",
      "The 33017 th iteration gives loss of 0.15442012052176846\n",
      "The 33018 th iteration gives loss of 0.15441838973599076\n",
      "The 33019 th iteration gives loss of 0.1544166590307236\n",
      "The 33020 th iteration gives loss of 0.15441492840594578\n",
      "The 33021 th iteration gives loss of 0.15441319786165142\n",
      "The 33022 th iteration gives loss of 0.15441146739784606\n",
      "The 33023 th iteration gives loss of 0.15440973701449015\n",
      "The 33024 th iteration gives loss of 0.15440800671159055\n",
      "The 33025 th iteration gives loss of 0.15440627648912775\n",
      "The 33026 th iteration gives loss of 0.1544045463470939\n",
      "The 33027 th iteration gives loss of 0.15440281628547717\n",
      "The 33028 th iteration gives loss of 0.1544010863042728\n",
      "The 33029 th iteration gives loss of 0.15439935640345792\n",
      "The 33030 th iteration gives loss of 0.15439762658303297\n",
      "The 33031 th iteration gives loss of 0.15439589684298055\n",
      "The 33032 th iteration gives loss of 0.15439416718329343\n",
      "The 33033 th iteration gives loss of 0.1543924376039499\n",
      "The 33034 th iteration gives loss of 0.1543907081049442\n",
      "The 33035 th iteration gives loss of 0.15438897868627344\n",
      "The 33036 th iteration gives loss of 0.15438724934792022\n",
      "The 33037 th iteration gives loss of 0.15438552008987225\n",
      "The 33038 th iteration gives loss of 0.15438379091211812\n",
      "The 33039 th iteration gives loss of 0.15438206181464495\n",
      "The 33040 th iteration gives loss of 0.15438033279745497\n",
      "The 33041 th iteration gives loss of 0.15437860386052885\n",
      "The 33042 th iteration gives loss of 0.15437687500384562\n",
      "The 33043 th iteration gives loss of 0.15437514622740467\n",
      "The 33044 th iteration gives loss of 0.1543734175311956\n",
      "The 33045 th iteration gives loss of 0.15437168891520311\n",
      "The 33046 th iteration gives loss of 0.1543699603794162\n",
      "The 33047 th iteration gives loss of 0.15436823192383228\n",
      "The 33048 th iteration gives loss of 0.15436650354841958\n",
      "The 33049 th iteration gives loss of 0.15436477525318273\n",
      "The 33050 th iteration gives loss of 0.15436304703811954\n",
      "The 33051 th iteration gives loss of 0.15436131890320948\n",
      "The 33052 th iteration gives loss of 0.15435959084842887\n",
      "The 33053 th iteration gives loss of 0.15435786287378447\n",
      "The 33054 th iteration gives loss of 0.1543561349792562\n",
      "The 33055 th iteration gives loss of 0.15435440716483356\n",
      "The 33056 th iteration gives loss of 0.15435267943050757\n",
      "The 33057 th iteration gives loss of 0.15435095177626573\n",
      "The 33058 th iteration gives loss of 0.15434922420210348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 33059 th iteration gives loss of 0.1543474967079972\n",
      "The 33060 th iteration gives loss of 0.15434576929394875\n",
      "The 33061 th iteration gives loss of 0.15434404195994095\n",
      "The 33062 th iteration gives loss of 0.15434231470594767\n",
      "The 33063 th iteration gives loss of 0.15434058753198476\n",
      "The 33064 th iteration gives loss of 0.15433886043803607\n",
      "The 33065 th iteration gives loss of 0.1543371334240715\n",
      "The 33066 th iteration gives loss of 0.15433540649010366\n",
      "The 33067 th iteration gives loss of 0.15433367963610686\n",
      "The 33068 th iteration gives loss of 0.154331952862075\n",
      "The 33069 th iteration gives loss of 0.1543302261679902\n",
      "The 33070 th iteration gives loss of 0.15432849955384567\n",
      "The 33071 th iteration gives loss of 0.15432677301964268\n",
      "The 33072 th iteration gives loss of 0.15432504656535187\n",
      "The 33073 th iteration gives loss of 0.15432332019097345\n",
      "The 33074 th iteration gives loss of 0.15432159389648298\n",
      "The 33075 th iteration gives loss of 0.15431986768188813\n",
      "The 33076 th iteration gives loss of 0.1543181415471632\n",
      "The 33077 th iteration gives loss of 0.15431641549230213\n",
      "The 33078 th iteration gives loss of 0.15431468951729824\n",
      "The 33079 th iteration gives loss of 0.15431296362213104\n",
      "The 33080 th iteration gives loss of 0.15431123780679565\n",
      "The 33081 th iteration gives loss of 0.1543095120712852\n",
      "The 33082 th iteration gives loss of 0.1543077864155821\n",
      "The 33083 th iteration gives loss of 0.1543060608396733\n",
      "The 33084 th iteration gives loss of 0.1543043353435544\n",
      "The 33085 th iteration gives loss of 0.15430260992721243\n",
      "The 33086 th iteration gives loss of 0.1543008845906325\n",
      "The 33087 th iteration gives loss of 0.15429915933380625\n",
      "The 33088 th iteration gives loss of 0.15429743415672953\n",
      "The 33089 th iteration gives loss of 0.15429570905937867\n",
      "The 33090 th iteration gives loss of 0.15429398404175093\n",
      "The 33091 th iteration gives loss of 0.15429225910383018\n",
      "The 33092 th iteration gives loss of 0.15429053424561165\n",
      "The 33093 th iteration gives loss of 0.15428880946707624\n",
      "The 33094 th iteration gives loss of 0.15428708476822173\n",
      "The 33095 th iteration gives loss of 0.15428536014903638\n",
      "The 33096 th iteration gives loss of 0.15428363560949857\n",
      "The 33097 th iteration gives loss of 0.15428191114960618\n",
      "The 33098 th iteration gives loss of 0.1542801867693457\n",
      "The 33099 th iteration gives loss of 0.15427846246870902\n",
      "The 33100 th iteration gives loss of 0.15427673824768914\n",
      "The 33101 th iteration gives loss of 0.15427501410626568\n",
      "The 33102 th iteration gives loss of 0.15427329004442678\n",
      "The 33103 th iteration gives loss of 0.15427156606216827\n",
      "The 33104 th iteration gives loss of 0.15426984215948097\n",
      "The 33105 th iteration gives loss of 0.1542681183363442\n",
      "The 33106 th iteration gives loss of 0.15426639459275904\n",
      "The 33107 th iteration gives loss of 0.15426467092869922\n",
      "The 33108 th iteration gives loss of 0.15426294734416587\n",
      "The 33109 th iteration gives loss of 0.1542612238391467\n",
      "The 33110 th iteration gives loss of 0.1542595004136259\n",
      "The 33111 th iteration gives loss of 0.154257777067599\n",
      "The 33112 th iteration gives loss of 0.15425605380104154\n",
      "The 33113 th iteration gives loss of 0.15425433061396682\n",
      "The 33114 th iteration gives loss of 0.15425260750633984\n",
      "The 33115 th iteration gives loss of 0.15425088447815788\n",
      "The 33116 th iteration gives loss of 0.15424916152941726\n",
      "The 33117 th iteration gives loss of 0.15424743866009452\n",
      "The 33118 th iteration gives loss of 0.15424571587018854\n",
      "The 33119 th iteration gives loss of 0.1542439931596831\n",
      "The 33120 th iteration gives loss of 0.15424227052857853\n",
      "The 33121 th iteration gives loss of 0.154240547976846\n",
      "The 33122 th iteration gives loss of 0.15423882550448187\n",
      "The 33123 th iteration gives loss of 0.15423710311148126\n",
      "The 33124 th iteration gives loss of 0.15423538079782353\n",
      "The 33125 th iteration gives loss of 0.15423365856350246\n",
      "The 33126 th iteration gives loss of 0.15423193640850685\n",
      "The 33127 th iteration gives loss of 0.15423021433282896\n",
      "The 33128 th iteration gives loss of 0.15422849233645633\n",
      "The 33129 th iteration gives loss of 0.1542267704193769\n",
      "The 33130 th iteration gives loss of 0.15422504858157401\n",
      "The 33131 th iteration gives loss of 0.15422332682303944\n",
      "The 33132 th iteration gives loss of 0.15422160514377434\n",
      "The 33133 th iteration gives loss of 0.1542198835437556\n",
      "The 33134 th iteration gives loss of 0.1542181620229735\n",
      "The 33135 th iteration gives loss of 0.15421644058142364\n",
      "The 33136 th iteration gives loss of 0.1542147192190837\n",
      "The 33137 th iteration gives loss of 0.15421299793594997\n",
      "The 33138 th iteration gives loss of 0.15421127673201485\n",
      "The 33139 th iteration gives loss of 0.15420955560726327\n",
      "The 33140 th iteration gives loss of 0.1542078345616835\n",
      "The 33141 th iteration gives loss of 0.1542061135952604\n",
      "The 33142 th iteration gives loss of 0.15420439270799305\n",
      "The 33143 th iteration gives loss of 0.15420267189986905\n",
      "The 33144 th iteration gives loss of 0.15420095117086596\n",
      "The 33145 th iteration gives loss of 0.15419923052098633\n",
      "The 33146 th iteration gives loss of 0.1541975099502066\n",
      "The 33147 th iteration gives loss of 0.15419578945852255\n",
      "The 33148 th iteration gives loss of 0.15419406904594019\n",
      "The 33149 th iteration gives loss of 0.15419234871242182\n",
      "The 33150 th iteration gives loss of 0.15419062845796755\n",
      "The 33151 th iteration gives loss of 0.15418890828256404\n",
      "The 33152 th iteration gives loss of 0.15418718818620283\n",
      "The 33153 th iteration gives loss of 0.15418546816887993\n",
      "The 33154 th iteration gives loss of 0.15418374823056713\n",
      "The 33155 th iteration gives loss of 0.15418202837126777\n",
      "The 33156 th iteration gives loss of 0.15418030859095966\n",
      "The 33157 th iteration gives loss of 0.15417858888965014\n",
      "The 33158 th iteration gives loss of 0.15417686926730775\n",
      "The 33159 th iteration gives loss of 0.1541751497239389\n",
      "The 33160 th iteration gives loss of 0.15417343025952363\n",
      "The 33161 th iteration gives loss of 0.15417171087404005\n",
      "The 33162 th iteration gives loss of 0.15416999156750574\n",
      "The 33163 th iteration gives loss of 0.15416827233988953\n",
      "The 33164 th iteration gives loss of 0.15416655319118178\n",
      "The 33165 th iteration gives loss of 0.15416483412136978\n",
      "The 33166 th iteration gives loss of 0.15416311513044728\n",
      "The 33167 th iteration gives loss of 0.1541613962184067\n",
      "The 33168 th iteration gives loss of 0.1541596773852402\n",
      "The 33169 th iteration gives loss of 0.1541579586309204\n",
      "The 33170 th iteration gives loss of 0.15415623995545177\n",
      "The 33171 th iteration gives loss of 0.15415452135881763\n",
      "The 33172 th iteration gives loss of 0.15415280284100397\n",
      "The 33173 th iteration gives loss of 0.15415108440200484\n",
      "The 33174 th iteration gives loss of 0.15414936604180957\n",
      "The 33175 th iteration gives loss of 0.15414764776040818\n",
      "The 33176 th iteration gives loss of 0.15414592955779022\n",
      "The 33177 th iteration gives loss of 0.15414421143393403\n",
      "The 33178 th iteration gives loss of 0.1541424933888384\n",
      "The 33179 th iteration gives loss of 0.15414077542249008\n",
      "The 33180 th iteration gives loss of 0.15413905753488574\n",
      "The 33181 th iteration gives loss of 0.1541373397260047\n",
      "The 33182 th iteration gives loss of 0.15413562199583386\n",
      "The 33183 th iteration gives loss of 0.15413390434437013\n",
      "The 33184 th iteration gives loss of 0.15413218677160093\n",
      "The 33185 th iteration gives loss of 0.1541304692775144\n",
      "The 33186 th iteration gives loss of 0.15412875186210212\n",
      "The 33187 th iteration gives loss of 0.1541270345253483\n",
      "The 33188 th iteration gives loss of 0.15412531726725054\n",
      "The 33189 th iteration gives loss of 0.15412360008779097\n",
      "The 33190 th iteration gives loss of 0.15412188298695462\n",
      "The 33191 th iteration gives loss of 0.1541201659647384\n",
      "The 33192 th iteration gives loss of 0.15411844902113303\n",
      "The 33193 th iteration gives loss of 0.1541167321561201\n",
      "The 33194 th iteration gives loss of 0.15411501536969027\n",
      "The 33195 th iteration gives loss of 0.15411329866183954\n",
      "The 33196 th iteration gives loss of 0.15411158203255274\n",
      "The 33197 th iteration gives loss of 0.15410986548182048\n",
      "The 33198 th iteration gives loss of 0.1541081490096291\n",
      "The 33199 th iteration gives loss of 0.1541064326159668\n",
      "The 33200 th iteration gives loss of 0.15410471630082523\n",
      "The 33201 th iteration gives loss of 0.15410300006419111\n",
      "The 33202 th iteration gives loss of 0.1541012839060647\n",
      "The 33203 th iteration gives loss of 0.154099567826424\n",
      "The 33204 th iteration gives loss of 0.1540978518252548\n",
      "The 33205 th iteration gives loss of 0.15409613590255875\n",
      "The 33206 th iteration gives loss of 0.1540944200583108\n",
      "The 33207 th iteration gives loss of 0.154092704292517\n",
      "The 33208 th iteration gives loss of 0.1540909886051552\n",
      "The 33209 th iteration gives loss of 0.1540892729962104\n",
      "The 33210 th iteration gives loss of 0.15408755746568226\n",
      "The 33211 th iteration gives loss of 0.1540858420135515\n",
      "The 33212 th iteration gives loss of 0.15408412663981486\n",
      "The 33213 th iteration gives loss of 0.15408241134445846\n",
      "The 33214 th iteration gives loss of 0.1540806961274676\n",
      "The 33215 th iteration gives loss of 0.1540789809888433\n",
      "The 33216 th iteration gives loss of 0.15407726592855941\n",
      "The 33217 th iteration gives loss of 0.15407555094661401\n",
      "The 33218 th iteration gives loss of 0.15407383604300232\n",
      "The 33219 th iteration gives loss of 0.15407212121769745\n",
      "The 33220 th iteration gives loss of 0.1540704064707063\n",
      "The 33221 th iteration gives loss of 0.15406869180199922\n",
      "The 33222 th iteration gives loss of 0.15406697721157597\n",
      "The 33223 th iteration gives loss of 0.1540652626994277\n",
      "The 33224 th iteration gives loss of 0.15406354826554347\n",
      "The 33225 th iteration gives loss of 0.1540618339099048\n",
      "The 33226 th iteration gives loss of 0.15406011963250607\n",
      "The 33227 th iteration gives loss of 0.15405840543334434\n",
      "The 33228 th iteration gives loss of 0.15405669131239358\n",
      "The 33229 th iteration gives loss of 0.15405497726965503\n",
      "The 33230 th iteration gives loss of 0.15405326330511349\n",
      "The 33231 th iteration gives loss of 0.15405154941875254\n",
      "The 33232 th iteration gives loss of 0.15404983561057103\n",
      "The 33233 th iteration gives loss of 0.1540481218805563\n",
      "The 33234 th iteration gives loss of 0.15404640822868487\n",
      "The 33235 th iteration gives loss of 0.15404469465495776\n",
      "The 33236 th iteration gives loss of 0.15404298115937282\n",
      "The 33237 th iteration gives loss of 0.1540412677419129\n",
      "The 33238 th iteration gives loss of 0.1540395544025535\n",
      "The 33239 th iteration gives loss of 0.1540378411412958\n",
      "The 33240 th iteration gives loss of 0.1540361279581256\n",
      "The 33241 th iteration gives loss of 0.15403441485303904\n",
      "The 33242 th iteration gives loss of 0.15403270182601106\n",
      "The 33243 th iteration gives loss of 0.154030988877045\n",
      "The 33244 th iteration gives loss of 0.15402927600612318\n",
      "The 33245 th iteration gives loss of 0.15402756321324343\n",
      "The 33246 th iteration gives loss of 0.15402585049839027\n",
      "The 33247 th iteration gives loss of 0.15402413786154767\n",
      "The 33248 th iteration gives loss of 0.15402242530270818\n",
      "The 33249 th iteration gives loss of 0.15402071282186158\n",
      "The 33250 th iteration gives loss of 0.1540190004189923\n",
      "The 33251 th iteration gives loss of 0.15401728809409973\n",
      "The 33252 th iteration gives loss of 0.15401557584716302\n",
      "The 33253 th iteration gives loss of 0.15401386367818018\n",
      "The 33254 th iteration gives loss of 0.15401215158713502\n",
      "The 33255 th iteration gives loss of 0.1540104395740198\n",
      "The 33256 th iteration gives loss of 0.15400872763882906\n",
      "The 33257 th iteration gives loss of 0.1540070157815309\n",
      "The 33258 th iteration gives loss of 0.15400530400213372\n",
      "The 33259 th iteration gives loss of 0.15400359230062388\n",
      "The 33260 th iteration gives loss of 0.1540018806769859\n",
      "The 33261 th iteration gives loss of 0.15400016913121245\n",
      "The 33262 th iteration gives loss of 0.15399845766329193\n",
      "The 33263 th iteration gives loss of 0.15399674627321408\n",
      "The 33264 th iteration gives loss of 0.15399503496097094\n",
      "The 33265 th iteration gives loss of 0.1539933237265474\n",
      "The 33266 th iteration gives loss of 0.1539916125699314\n",
      "The 33267 th iteration gives loss of 0.15398990149111796\n",
      "The 33268 th iteration gives loss of 0.15398819049009313\n",
      "The 33269 th iteration gives loss of 0.15398647956684694\n",
      "The 33270 th iteration gives loss of 0.15398476872136882\n",
      "The 33271 th iteration gives loss of 0.1539830579536464\n",
      "The 33272 th iteration gives loss of 0.15398134726366827\n",
      "The 33273 th iteration gives loss of 0.1539796366514276\n",
      "The 33274 th iteration gives loss of 0.15397792611691719\n",
      "The 33275 th iteration gives loss of 0.15397621566011818\n",
      "The 33276 th iteration gives loss of 0.15397450528101184\n",
      "The 33277 th iteration gives loss of 0.15397279497961108\n",
      "The 33278 th iteration gives loss of 0.15397108475589322\n",
      "The 33279 th iteration gives loss of 0.15396937460983598\n",
      "The 33280 th iteration gives loss of 0.15396766454144603\n",
      "The 33281 th iteration gives loss of 0.15396595455070583\n",
      "The 33282 th iteration gives loss of 0.15396424463760364\n",
      "The 33283 th iteration gives loss of 0.15396253480213035\n",
      "The 33284 th iteration gives loss of 0.15396082504428263\n",
      "The 33285 th iteration gives loss of 0.1539591153640314\n",
      "The 33286 th iteration gives loss of 0.1539574057613813\n",
      "The 33287 th iteration gives loss of 0.1539556962363134\n",
      "The 33288 th iteration gives loss of 0.15395398678882632\n",
      "The 33289 th iteration gives loss of 0.15395227741890094\n",
      "The 33290 th iteration gives loss of 0.1539505681265342\n",
      "The 33291 th iteration gives loss of 0.15394885891170565\n",
      "The 33292 th iteration gives loss of 0.15394714977441082\n",
      "The 33293 th iteration gives loss of 0.1539454407146397\n",
      "The 33294 th iteration gives loss of 0.15394373173238174\n",
      "The 33295 th iteration gives loss of 0.15394202282761787\n",
      "The 33296 th iteration gives loss of 0.15394031400034686\n",
      "The 33297 th iteration gives loss of 0.1539386052505562\n",
      "The 33298 th iteration gives loss of 0.15393689657823043\n",
      "The 33299 th iteration gives loss of 0.15393518798337014\n",
      "The 33300 th iteration gives loss of 0.1539334794659531\n",
      "The 33301 th iteration gives loss of 0.15393177102597083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 33302 th iteration gives loss of 0.15393006266342008\n",
      "The 33303 th iteration gives loss of 0.1539283543782832\n",
      "The 33304 th iteration gives loss of 0.15392664617054855\n",
      "The 33305 th iteration gives loss of 0.1539249380402112\n",
      "The 33306 th iteration gives loss of 0.15392322998725413\n",
      "The 33307 th iteration gives loss of 0.15392152201167328\n",
      "The 33308 th iteration gives loss of 0.15391981411345565\n",
      "The 33309 th iteration gives loss of 0.153918106292583\n",
      "The 33310 th iteration gives loss of 0.15391639854905273\n",
      "The 33311 th iteration gives loss of 0.15391469088285664\n",
      "The 33312 th iteration gives loss of 0.15391298329397998\n",
      "The 33313 th iteration gives loss of 0.153911275782415\n",
      "The 33314 th iteration gives loss of 0.1539095683481471\n",
      "The 33315 th iteration gives loss of 0.15390786099116563\n",
      "The 33316 th iteration gives loss of 0.15390615371146424\n",
      "The 33317 th iteration gives loss of 0.15390444650902058\n",
      "The 33318 th iteration gives loss of 0.15390273938384627\n",
      "The 33319 th iteration gives loss of 0.153901032335911\n",
      "The 33320 th iteration gives loss of 0.15389932536520556\n",
      "The 33321 th iteration gives loss of 0.15389761847173297\n",
      "The 33322 th iteration gives loss of 0.1538959116554729\n",
      "The 33323 th iteration gives loss of 0.1538942049164139\n",
      "The 33324 th iteration gives loss of 0.1538924982545447\n",
      "The 33325 th iteration gives loss of 0.15389079166985908\n",
      "The 33326 th iteration gives loss of 0.15388908516233948\n",
      "The 33327 th iteration gives loss of 0.1538873787319896\n",
      "The 33328 th iteration gives loss of 0.15388567237878376\n",
      "The 33329 th iteration gives loss of 0.15388396610272112\n",
      "The 33330 th iteration gives loss of 0.15388225990378598\n",
      "The 33331 th iteration gives loss of 0.1538805537819727\n",
      "The 33332 th iteration gives loss of 0.1538788477372605\n",
      "The 33333 th iteration gives loss of 0.15387714176965006\n",
      "The 33334 th iteration gives loss of 0.15387543587912136\n",
      "The 33335 th iteration gives loss of 0.15387373006567964\n",
      "The 33336 th iteration gives loss of 0.15387202432929187\n",
      "The 33337 th iteration gives loss of 0.1538703186699623\n",
      "The 33338 th iteration gives loss of 0.15386861308767877\n",
      "The 33339 th iteration gives loss of 0.15386690758242605\n",
      "The 33340 th iteration gives loss of 0.1538652021541992\n",
      "The 33341 th iteration gives loss of 0.1538634968029859\n",
      "The 33342 th iteration gives loss of 0.15386179152876936\n",
      "The 33343 th iteration gives loss of 0.1538600863315431\n",
      "The 33344 th iteration gives loss of 0.15385838121130038\n",
      "The 33345 th iteration gives loss of 0.15385667616802692\n",
      "The 33346 th iteration gives loss of 0.15385497120171385\n",
      "The 33347 th iteration gives loss of 0.1538532663123529\n",
      "The 33348 th iteration gives loss of 0.1538515614999311\n",
      "The 33349 th iteration gives loss of 0.15384985676442525\n",
      "The 33350 th iteration gives loss of 0.1538481521058458\n",
      "The 33351 th iteration gives loss of 0.15384644752417348\n",
      "The 33352 th iteration gives loss of 0.15384474301939022\n",
      "The 33353 th iteration gives loss of 0.15384303859150283\n",
      "The 33354 th iteration gives loss of 0.15384133424048718\n",
      "The 33355 th iteration gives loss of 0.15383962996632725\n",
      "The 33356 th iteration gives loss of 0.1538379257690342\n",
      "The 33357 th iteration gives loss of 0.1538362216485803\n",
      "The 33358 th iteration gives loss of 0.15383451760495584\n",
      "The 33359 th iteration gives loss of 0.15383281363815796\n",
      "The 33360 th iteration gives loss of 0.1538311097481675\n",
      "The 33361 th iteration gives loss of 0.15382940593498518\n",
      "The 33362 th iteration gives loss of 0.15382770219858524\n",
      "The 33363 th iteration gives loss of 0.1538259985389684\n",
      "The 33364 th iteration gives loss of 0.15382429495612\n",
      "The 33365 th iteration gives loss of 0.1538225914500384\n",
      "The 33366 th iteration gives loss of 0.15382088802069266\n",
      "The 33367 th iteration gives loss of 0.15381918466809147\n",
      "The 33368 th iteration gives loss of 0.15381748139221837\n",
      "The 33369 th iteration gives loss of 0.15381577819306153\n",
      "The 33370 th iteration gives loss of 0.15381407507061365\n",
      "The 33371 th iteration gives loss of 0.1538123720248621\n",
      "The 33372 th iteration gives loss of 0.15381066905579202\n",
      "The 33373 th iteration gives loss of 0.15380896616339496\n",
      "The 33374 th iteration gives loss of 0.15380726334766506\n",
      "The 33375 th iteration gives loss of 0.1538055606085963\n",
      "The 33376 th iteration gives loss of 0.15380385794616075\n",
      "The 33377 th iteration gives loss of 0.15380215536035488\n",
      "The 33378 th iteration gives loss of 0.15380045285118066\n",
      "The 33379 th iteration gives loss of 0.1537987504186128\n",
      "The 33380 th iteration gives loss of 0.15379704806264846\n",
      "The 33381 th iteration gives loss of 0.1537953457832696\n",
      "The 33382 th iteration gives loss of 0.15379364358047642\n",
      "The 33383 th iteration gives loss of 0.15379194145425104\n",
      "The 33384 th iteration gives loss of 0.15379023940458683\n",
      "The 33385 th iteration gives loss of 0.15378853743147455\n",
      "The 33386 th iteration gives loss of 0.15378683553489764\n",
      "The 33387 th iteration gives loss of 0.15378513371484306\n",
      "The 33388 th iteration gives loss of 0.1537834319713091\n",
      "The 33389 th iteration gives loss of 0.1537817303042771\n",
      "The 33390 th iteration gives loss of 0.15378002871375032\n",
      "The 33391 th iteration gives loss of 0.15377832719970458\n",
      "The 33392 th iteration gives loss of 0.15377662576213558\n",
      "The 33393 th iteration gives loss of 0.1537749244010284\n",
      "The 33394 th iteration gives loss of 0.15377322311637498\n",
      "The 33395 th iteration gives loss of 0.15377152190817067\n",
      "The 33396 th iteration gives loss of 0.1537698207763904\n",
      "The 33397 th iteration gives loss of 0.1537681197210349\n",
      "The 33398 th iteration gives loss of 0.1537664187421018\n",
      "The 33399 th iteration gives loss of 0.1537647178395614\n",
      "The 33400 th iteration gives loss of 0.1537630170134196\n",
      "The 33401 th iteration gives loss of 0.1537613162636501\n",
      "The 33402 th iteration gives loss of 0.1537596155902531\n",
      "The 33403 th iteration gives loss of 0.1537579149932222\n",
      "The 33404 th iteration gives loss of 0.1537562144725322\n",
      "The 33405 th iteration gives loss of 0.15375451402818954\n",
      "The 33406 th iteration gives loss of 0.15375281366017254\n",
      "The 33407 th iteration gives loss of 0.15375111336847075\n",
      "The 33408 th iteration gives loss of 0.15374941315307342\n",
      "The 33409 th iteration gives loss of 0.15374771301397971\n",
      "The 33410 th iteration gives loss of 0.15374601295117402\n",
      "The 33411 th iteration gives loss of 0.15374431296463614\n",
      "The 33412 th iteration gives loss of 0.15374261305437165\n",
      "The 33413 th iteration gives loss of 0.15374091322035496\n",
      "The 33414 th iteration gives loss of 0.15373921346258834\n",
      "The 33415 th iteration gives loss of 0.15373751378105638\n",
      "The 33416 th iteration gives loss of 0.15373581417574705\n",
      "The 33417 th iteration gives loss of 0.15373411464665318\n",
      "The 33418 th iteration gives loss of 0.153732415193755\n",
      "The 33419 th iteration gives loss of 0.1537307158170551\n",
      "The 33420 th iteration gives loss of 0.15372901651653167\n",
      "The 33421 th iteration gives loss of 0.15372731729218994\n",
      "The 33422 th iteration gives loss of 0.15372561814400482\n",
      "The 33423 th iteration gives loss of 0.15372391907197094\n",
      "The 33424 th iteration gives loss of 0.1537222200760779\n",
      "The 33425 th iteration gives loss of 0.15372052115631002\n",
      "The 33426 th iteration gives loss of 0.15371882231266348\n",
      "The 33427 th iteration gives loss of 0.1537171235451284\n",
      "The 33428 th iteration gives loss of 0.1537154248536959\n",
      "The 33429 th iteration gives loss of 0.15371372623834456\n",
      "The 33430 th iteration gives loss of 0.15371202769907208\n",
      "The 33431 th iteration gives loss of 0.15371032923586703\n",
      "The 33432 th iteration gives loss of 0.15370863084871955\n",
      "The 33433 th iteration gives loss of 0.15370693253761872\n",
      "The 33434 th iteration gives loss of 0.15370523430255817\n",
      "The 33435 th iteration gives loss of 0.15370353614351684\n",
      "The 33436 th iteration gives loss of 0.15370183806049234\n",
      "The 33437 th iteration gives loss of 0.15370014005347082\n",
      "The 33438 th iteration gives loss of 0.15369844212244474\n",
      "The 33439 th iteration gives loss of 0.15369674426740587\n",
      "The 33440 th iteration gives loss of 0.15369504648833868\n",
      "The 33441 th iteration gives loss of 0.15369334878522897\n",
      "The 33442 th iteration gives loss of 0.15369165115807773\n",
      "The 33443 th iteration gives loss of 0.15368995360686835\n",
      "The 33444 th iteration gives loss of 0.15368825613159218\n",
      "The 33445 th iteration gives loss of 0.15368655873223197\n",
      "The 33446 th iteration gives loss of 0.15368486140879092\n",
      "The 33447 th iteration gives loss of 0.1536831641612382\n",
      "The 33448 th iteration gives loss of 0.1536814669895868\n",
      "The 33449 th iteration gives loss of 0.15367976989381524\n",
      "The 33450 th iteration gives loss of 0.1536780728739118\n",
      "The 33451 th iteration gives loss of 0.15367637592986685\n",
      "The 33452 th iteration gives loss of 0.15367467906166782\n",
      "The 33453 th iteration gives loss of 0.15367298226930404\n",
      "The 33454 th iteration gives loss of 0.15367128555277884\n",
      "The 33455 th iteration gives loss of 0.1536695889120683\n",
      "The 33456 th iteration gives loss of 0.15366789234715417\n",
      "The 33457 th iteration gives loss of 0.15366619585804694\n",
      "The 33458 th iteration gives loss of 0.15366449944472838\n",
      "The 33459 th iteration gives loss of 0.15366280310717992\n",
      "The 33460 th iteration gives loss of 0.1536611068453982\n",
      "The 33461 th iteration gives loss of 0.15365941065936958\n",
      "The 33462 th iteration gives loss of 0.15365771454909136\n",
      "The 33463 th iteration gives loss of 0.15365601851454538\n",
      "The 33464 th iteration gives loss of 0.15365432255571798\n",
      "The 33465 th iteration gives loss of 0.1536526266726102\n",
      "The 33466 th iteration gives loss of 0.1536509308652032\n",
      "The 33467 th iteration gives loss of 0.15364923513349135\n",
      "The 33468 th iteration gives loss of 0.15364753947745946\n",
      "The 33469 th iteration gives loss of 0.15364584389709882\n",
      "The 33470 th iteration gives loss of 0.15364414839240764\n",
      "The 33471 th iteration gives loss of 0.1536424529633628\n",
      "The 33472 th iteration gives loss of 0.15364075760995363\n",
      "The 33473 th iteration gives loss of 0.15363906233218055\n",
      "The 33474 th iteration gives loss of 0.15363736713002982\n",
      "The 33475 th iteration gives loss of 0.1536356720034932\n",
      "The 33476 th iteration gives loss of 0.15363397695255246\n",
      "The 33477 th iteration gives loss of 0.15363228197719947\n",
      "The 33478 th iteration gives loss of 0.1536305870774254\n",
      "The 33479 th iteration gives loss of 0.15362889225322135\n",
      "The 33480 th iteration gives loss of 0.1536271975045723\n",
      "The 33481 th iteration gives loss of 0.15362550283147655\n",
      "The 33482 th iteration gives loss of 0.15362380823390662\n",
      "The 33483 th iteration gives loss of 0.1536221137118721\n",
      "The 33484 th iteration gives loss of 0.15362041926536066\n",
      "The 33485 th iteration gives loss of 0.1536187248943555\n",
      "The 33486 th iteration gives loss of 0.15361703059883736\n",
      "The 33487 th iteration gives loss of 0.15361533637881125\n",
      "The 33488 th iteration gives loss of 0.15361364223426102\n",
      "The 33489 th iteration gives loss of 0.1536119481651725\n",
      "The 33490 th iteration gives loss of 0.15361025417154267\n",
      "The 33491 th iteration gives loss of 0.1536085602533553\n",
      "The 33492 th iteration gives loss of 0.15360686641060423\n",
      "The 33493 th iteration gives loss of 0.15360517264327844\n",
      "The 33494 th iteration gives loss of 0.1536034789513662\n",
      "The 33495 th iteration gives loss of 0.1536017853348472\n",
      "The 33496 th iteration gives loss of 0.15360009179373016\n",
      "The 33497 th iteration gives loss of 0.15359839832799344\n",
      "The 33498 th iteration gives loss of 0.15359670493763375\n",
      "The 33499 th iteration gives loss of 0.15359501162263525\n",
      "The 33500 th iteration gives loss of 0.15359331838298643\n",
      "The 33501 th iteration gives loss of 0.15359162521868067\n",
      "The 33502 th iteration gives loss of 0.15358993212970304\n",
      "The 33503 th iteration gives loss of 0.15358823911604763\n",
      "The 33504 th iteration gives loss of 0.15358654617770143\n",
      "The 33505 th iteration gives loss of 0.15358485331465666\n",
      "The 33506 th iteration gives loss of 0.15358316052690177\n",
      "The 33507 th iteration gives loss of 0.15358146781442633\n",
      "The 33508 th iteration gives loss of 0.15357977517722413\n",
      "The 33509 th iteration gives loss of 0.153578082615276\n",
      "The 33510 th iteration gives loss of 0.15357639012857302\n",
      "The 33511 th iteration gives loss of 0.15357469771712043\n",
      "The 33512 th iteration gives loss of 0.15357300538088875\n",
      "The 33513 th iteration gives loss of 0.1535713131198727\n",
      "The 33514 th iteration gives loss of 0.15356962093406656\n",
      "The 33515 th iteration gives loss of 0.15356792882346296\n",
      "The 33516 th iteration gives loss of 0.1535662367880393\n",
      "The 33517 th iteration gives loss of 0.15356454482779497\n",
      "The 33518 th iteration gives loss of 0.15356285294271804\n",
      "The 33519 th iteration gives loss of 0.1535611611327949\n",
      "The 33520 th iteration gives loss of 0.15355946939801635\n",
      "The 33521 th iteration gives loss of 0.1535577777383714\n",
      "The 33522 th iteration gives loss of 0.1535560861538593\n",
      "The 33523 th iteration gives loss of 0.1535543946444624\n",
      "The 33524 th iteration gives loss of 0.15355270321016845\n",
      "The 33525 th iteration gives loss of 0.15355101185096987\n",
      "The 33526 th iteration gives loss of 0.15354932056685025\n",
      "The 33527 th iteration gives loss of 0.1535476293578049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 33528 th iteration gives loss of 0.1535459382238232\n",
      "The 33529 th iteration gives loss of 0.15354424716489845\n",
      "The 33530 th iteration gives loss of 0.15354255618101156\n",
      "The 33531 th iteration gives loss of 0.15354086527216804\n",
      "The 33532 th iteration gives loss of 0.15353917443834747\n",
      "The 33533 th iteration gives loss of 0.1535374836795304\n",
      "The 33534 th iteration gives loss of 0.15353579299571443\n",
      "The 33535 th iteration gives loss of 0.15353410238689796\n",
      "The 33536 th iteration gives loss of 0.15353241185305727\n",
      "The 33537 th iteration gives loss of 0.1535307213941876\n",
      "The 33538 th iteration gives loss of 0.15352903101028173\n",
      "The 33539 th iteration gives loss of 0.15352734070132595\n",
      "The 33540 th iteration gives loss of 0.15352565046730968\n",
      "The 33541 th iteration gives loss of 0.1535239603082322\n",
      "The 33542 th iteration gives loss of 0.15352227022407006\n",
      "The 33543 th iteration gives loss of 0.1535205802148143\n",
      "The 33544 th iteration gives loss of 0.1535188902804594\n",
      "The 33545 th iteration gives loss of 0.1535172004209938\n",
      "The 33546 th iteration gives loss of 0.15351551063640886\n",
      "The 33547 th iteration gives loss of 0.15351382092669139\n",
      "The 33548 th iteration gives loss of 0.153512131291829\n",
      "The 33549 th iteration gives loss of 0.15351044173181508\n",
      "The 33550 th iteration gives loss of 0.15350875224664579\n",
      "The 33551 th iteration gives loss of 0.15350706283630294\n",
      "The 33552 th iteration gives loss of 0.15350537350077442\n",
      "The 33553 th iteration gives loss of 0.15350368424005895\n",
      "The 33554 th iteration gives loss of 0.15350199505413384\n",
      "The 33555 th iteration gives loss of 0.15350030594300274\n",
      "The 33556 th iteration gives loss of 0.15349861690664782\n",
      "The 33557 th iteration gives loss of 0.1534969279450561\n",
      "The 33558 th iteration gives loss of 0.15349523905822307\n",
      "The 33559 th iteration gives loss of 0.15349355024613315\n",
      "The 33560 th iteration gives loss of 0.1534918615087788\n",
      "The 33561 th iteration gives loss of 0.1534901728461513\n",
      "The 33562 th iteration gives loss of 0.15348848425824413\n",
      "The 33563 th iteration gives loss of 0.15348679574503452\n",
      "The 33564 th iteration gives loss of 0.15348510730652404\n",
      "The 33565 th iteration gives loss of 0.15348341894270334\n",
      "The 33566 th iteration gives loss of 0.15348173065355383\n",
      "The 33567 th iteration gives loss of 0.15348004243906976\n",
      "The 33568 th iteration gives loss of 0.15347835429924084\n",
      "The 33569 th iteration gives loss of 0.1534766662340508\n",
      "The 33570 th iteration gives loss of 0.15347497824349876\n",
      "The 33571 th iteration gives loss of 0.15347329032756404\n",
      "The 33572 th iteration gives loss of 0.15347160248625216\n",
      "The 33573 th iteration gives loss of 0.15346991471954124\n",
      "The 33574 th iteration gives loss of 0.15346822702742471\n",
      "The 33575 th iteration gives loss of 0.1534665394098872\n",
      "The 33576 th iteration gives loss of 0.1534648518669148\n",
      "The 33577 th iteration gives loss of 0.153463164398518\n",
      "The 33578 th iteration gives loss of 0.15346147700466353\n",
      "The 33579 th iteration gives loss of 0.1534597896853614\n",
      "The 33580 th iteration gives loss of 0.15345810244058342\n",
      "The 33581 th iteration gives loss of 0.15345641527033416\n",
      "The 33582 th iteration gives loss of 0.15345472817458644\n",
      "The 33583 th iteration gives loss of 0.15345304115334704\n",
      "The 33584 th iteration gives loss of 0.15345135420659778\n",
      "The 33585 th iteration gives loss of 0.15344966733433424\n",
      "The 33586 th iteration gives loss of 0.15344798053653574\n",
      "The 33587 th iteration gives loss of 0.1534462938132112\n",
      "The 33588 th iteration gives loss of 0.15344460716432168\n",
      "The 33589 th iteration gives loss of 0.15344292058987202\n",
      "The 33590 th iteration gives loss of 0.1534412340898621\n",
      "The 33591 th iteration gives loss of 0.1534395476642708\n",
      "The 33592 th iteration gives loss of 0.15343786131308687\n",
      "The 33593 th iteration gives loss of 0.15343617503629858\n",
      "The 33594 th iteration gives loss of 0.15343448883390576\n",
      "The 33595 th iteration gives loss of 0.15343280270588858\n",
      "The 33596 th iteration gives loss of 0.1534311166522457\n",
      "The 33597 th iteration gives loss of 0.15342943067295978\n",
      "The 33598 th iteration gives loss of 0.15342774476801962\n",
      "The 33599 th iteration gives loss of 0.15342605893741726\n",
      "The 33600 th iteration gives loss of 0.15342437318115473\n",
      "The 33601 th iteration gives loss of 0.1534226874992023\n",
      "The 33602 th iteration gives loss of 0.15342100189155747\n",
      "The 33603 th iteration gives loss of 0.15341931635821743\n",
      "The 33604 th iteration gives loss of 0.15341763089915883\n",
      "The 33605 th iteration gives loss of 0.15341594551438134\n",
      "The 33606 th iteration gives loss of 0.15341426020387625\n",
      "The 33607 th iteration gives loss of 0.15341257496761737\n",
      "The 33608 th iteration gives loss of 0.15341088980561798\n",
      "The 33609 th iteration gives loss of 0.1534092047178455\n",
      "The 33610 th iteration gives loss of 0.15340751970429858\n",
      "The 33611 th iteration gives loss of 0.1534058347649794\n",
      "The 33612 th iteration gives loss of 0.15340414989986512\n",
      "The 33613 th iteration gives loss of 0.15340246510894237\n",
      "The 33614 th iteration gives loss of 0.15340078039220822\n",
      "The 33615 th iteration gives loss of 0.15339909574965813\n",
      "The 33616 th iteration gives loss of 0.15339741118127034\n",
      "The 33617 th iteration gives loss of 0.1533957266870378\n",
      "The 33618 th iteration gives loss of 0.1533940422669476\n",
      "The 33619 th iteration gives loss of 0.1533923579209957\n",
      "The 33620 th iteration gives loss of 0.1533906736491705\n",
      "The 33621 th iteration gives loss of 0.1533889894514603\n",
      "The 33622 th iteration gives loss of 0.15338730532786285\n",
      "The 33623 th iteration gives loss of 0.15338562127835656\n",
      "The 33624 th iteration gives loss of 0.15338393730293304\n",
      "The 33625 th iteration gives loss of 0.15338225340158568\n",
      "The 33626 th iteration gives loss of 0.1533805695743071\n",
      "The 33627 th iteration gives loss of 0.15337888582108386\n",
      "The 33628 th iteration gives loss of 0.15337720214190492\n",
      "The 33629 th iteration gives loss of 0.1533755185367597\n",
      "The 33630 th iteration gives loss of 0.15337383500563942\n",
      "The 33631 th iteration gives loss of 0.1533721515485272\n",
      "The 33632 th iteration gives loss of 0.15337046816543126\n",
      "The 33633 th iteration gives loss of 0.1533687848563264\n",
      "The 33634 th iteration gives loss of 0.15336710162120773\n",
      "The 33635 th iteration gives loss of 0.15336541846005525\n",
      "The 33636 th iteration gives loss of 0.15336373537287146\n",
      "The 33637 th iteration gives loss of 0.1533620523596447\n",
      "The 33638 th iteration gives loss of 0.15336036942036574\n",
      "The 33639 th iteration gives loss of 0.15335868655501284\n",
      "The 33640 th iteration gives loss of 0.15335700376358607\n",
      "The 33641 th iteration gives loss of 0.15335532104607502\n",
      "The 33642 th iteration gives loss of 0.15335363840247251\n",
      "The 33643 th iteration gives loss of 0.1533519558327486\n",
      "The 33644 th iteration gives loss of 0.15335027333692142\n",
      "The 33645 th iteration gives loss of 0.15334859091496866\n",
      "The 33646 th iteration gives loss of 0.15334690856687522\n",
      "The 33647 th iteration gives loss of 0.15334522629262576\n",
      "The 33648 th iteration gives loss of 0.1533435440922363\n",
      "The 33649 th iteration gives loss of 0.15334186196567437\n",
      "The 33650 th iteration gives loss of 0.15334017991294024\n",
      "The 33651 th iteration gives loss of 0.15333849793401205\n",
      "The 33652 th iteration gives loss of 0.15333681602888383\n",
      "The 33653 th iteration gives loss of 0.15333513419755113\n",
      "The 33654 th iteration gives loss of 0.15333345244000862\n",
      "The 33655 th iteration gives loss of 0.15333177075623336\n",
      "The 33656 th iteration gives loss of 0.1533300891462194\n",
      "The 33657 th iteration gives loss of 0.15332840760996094\n",
      "The 33658 th iteration gives loss of 0.15332672614744738\n",
      "The 33659 th iteration gives loss of 0.15332504475866446\n",
      "The 33660 th iteration gives loss of 0.1533233634436025\n",
      "The 33661 th iteration gives loss of 0.1533216822022563\n",
      "The 33662 th iteration gives loss of 0.15332000103460383\n",
      "The 33663 th iteration gives loss of 0.15331831994065176\n",
      "The 33664 th iteration gives loss of 0.1533166389203837\n",
      "The 33665 th iteration gives loss of 0.1533149579737864\n",
      "The 33666 th iteration gives loss of 0.15331327710084813\n",
      "The 33667 th iteration gives loss of 0.15331159630156804\n",
      "The 33668 th iteration gives loss of 0.153309915575926\n",
      "The 33669 th iteration gives loss of 0.15330823492391607\n",
      "The 33670 th iteration gives loss of 0.15330655434552345\n",
      "The 33671 th iteration gives loss of 0.15330487384075264\n",
      "The 33672 th iteration gives loss of 0.15330319340957896\n",
      "The 33673 th iteration gives loss of 0.1533015130519997\n",
      "The 33674 th iteration gives loss of 0.15329983276800424\n",
      "The 33675 th iteration gives loss of 0.15329815255758156\n",
      "The 33676 th iteration gives loss of 0.1532964724207136\n",
      "The 33677 th iteration gives loss of 0.15329479235739704\n",
      "The 33678 th iteration gives loss of 0.15329311236762944\n",
      "The 33679 th iteration gives loss of 0.1532914324513884\n",
      "The 33680 th iteration gives loss of 0.15328975260866873\n",
      "The 33681 th iteration gives loss of 0.15328807283946966\n",
      "The 33682 th iteration gives loss of 0.15328639314376963\n",
      "The 33683 th iteration gives loss of 0.15328471352155568\n",
      "The 33684 th iteration gives loss of 0.15328303397282997\n",
      "The 33685 th iteration gives loss of 0.15328135449757177\n",
      "The 33686 th iteration gives loss of 0.15327967509577511\n",
      "The 33687 th iteration gives loss of 0.15327799576743037\n",
      "The 33688 th iteration gives loss of 0.15327631651253257\n",
      "The 33689 th iteration gives loss of 0.1532746373310565\n",
      "The 33690 th iteration gives loss of 0.1532729582230085\n",
      "The 33691 th iteration gives loss of 0.15327127918837402\n",
      "The 33692 th iteration gives loss of 0.1532696002271355\n",
      "The 33693 th iteration gives loss of 0.15326792133929146\n",
      "The 33694 th iteration gives loss of 0.153266242524838\n",
      "The 33695 th iteration gives loss of 0.15326456378374814\n",
      "The 33696 th iteration gives loss of 0.1532628851160224\n",
      "The 33697 th iteration gives loss of 0.15326120652163944\n",
      "The 33698 th iteration gives loss of 0.15325952800060094\n",
      "The 33699 th iteration gives loss of 0.15325784955289945\n",
      "The 33700 th iteration gives loss of 0.15325617117852222\n",
      "The 33701 th iteration gives loss of 0.1532544928774551\n",
      "The 33702 th iteration gives loss of 0.15325281464968685\n",
      "The 33703 th iteration gives loss of 0.1532511364952107\n",
      "The 33704 th iteration gives loss of 0.15324945841401677\n",
      "The 33705 th iteration gives loss of 0.15324778040609746\n",
      "The 33706 th iteration gives loss of 0.15324610247143322\n",
      "The 33707 th iteration gives loss of 0.15324442461002905\n",
      "The 33708 th iteration gives loss of 0.15324274682186148\n",
      "The 33709 th iteration gives loss of 0.15324106910693083\n",
      "The 33710 th iteration gives loss of 0.15323939146521606\n",
      "The 33711 th iteration gives loss of 0.15323771389672178\n",
      "The 33712 th iteration gives loss of 0.15323603640142106\n",
      "The 33713 th iteration gives loss of 0.15323435897931986\n",
      "The 33714 th iteration gives loss of 0.15323268163038886\n",
      "The 33715 th iteration gives loss of 0.15323100435463766\n",
      "The 33716 th iteration gives loss of 0.1532293271520499\n",
      "The 33717 th iteration gives loss of 0.15322765002260774\n",
      "The 33718 th iteration gives loss of 0.15322597296631715\n",
      "The 33719 th iteration gives loss of 0.153224295983154\n",
      "The 33720 th iteration gives loss of 0.1532226190731101\n",
      "The 33721 th iteration gives loss of 0.15322094223618027\n",
      "The 33722 th iteration gives loss of 0.15321926547235504\n",
      "The 33723 th iteration gives loss of 0.15321758878162775\n",
      "The 33724 th iteration gives loss of 0.15321591216398484\n",
      "The 33725 th iteration gives loss of 0.15321423561939973\n",
      "The 33726 th iteration gives loss of 0.1532125591478773\n",
      "The 33727 th iteration gives loss of 0.15321088274942296\n",
      "The 33728 th iteration gives loss of 0.15320920642400146\n",
      "The 33729 th iteration gives loss of 0.15320753017161332\n",
      "The 33730 th iteration gives loss of 0.15320585399225098\n",
      "The 33731 th iteration gives loss of 0.15320417788589727\n",
      "The 33732 th iteration gives loss of 0.15320250185255124\n",
      "The 33733 th iteration gives loss of 0.153200825892194\n",
      "The 33734 th iteration gives loss of 0.15319915000482026\n",
      "The 33735 th iteration gives loss of 0.15319747419041854\n",
      "The 33736 th iteration gives loss of 0.1531957984489847\n",
      "The 33737 th iteration gives loss of 0.15319412278049893\n",
      "The 33738 th iteration gives loss of 0.1531924471849631\n",
      "The 33739 th iteration gives loss of 0.1531907716623557\n",
      "The 33740 th iteration gives loss of 0.15318909621267435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 33741 th iteration gives loss of 0.15318742083590653\n",
      "The 33742 th iteration gives loss of 0.15318574553203798\n",
      "The 33743 th iteration gives loss of 0.15318407030107128\n",
      "The 33744 th iteration gives loss of 0.15318239514298163\n",
      "The 33745 th iteration gives loss of 0.1531807200577669\n",
      "The 33746 th iteration gives loss of 0.15317904504542523\n",
      "The 33747 th iteration gives loss of 0.15317737010592442\n",
      "The 33748 th iteration gives loss of 0.15317569523927513\n",
      "The 33749 th iteration gives loss of 0.15317402044545456\n",
      "The 33750 th iteration gives loss of 0.15317234572446148\n",
      "The 33751 th iteration gives loss of 0.1531706710762811\n",
      "The 33752 th iteration gives loss of 0.1531689965009047\n",
      "The 33753 th iteration gives loss of 0.1531673219983262\n",
      "The 33754 th iteration gives loss of 0.15316564756852788\n",
      "The 33755 th iteration gives loss of 0.15316397321150751\n",
      "The 33756 th iteration gives loss of 0.15316229892725375\n",
      "The 33757 th iteration gives loss of 0.1531606247157543\n",
      "The 33758 th iteration gives loss of 0.1531589505770024\n",
      "The 33759 th iteration gives loss of 0.15315727651098032\n",
      "The 33760 th iteration gives loss of 0.1531556025176806\n",
      "The 33761 th iteration gives loss of 0.15315392859710367\n",
      "The 33762 th iteration gives loss of 0.15315225474923386\n",
      "The 33763 th iteration gives loss of 0.15315058097405704\n",
      "The 33764 th iteration gives loss of 0.15314890727156408\n",
      "The 33765 th iteration gives loss of 0.15314723364174387\n",
      "The 33766 th iteration gives loss of 0.15314556008459498\n",
      "The 33767 th iteration gives loss of 0.15314388660009784\n",
      "The 33768 th iteration gives loss of 0.1531422131882483\n",
      "The 33769 th iteration gives loss of 0.1531405398490368\n",
      "The 33770 th iteration gives loss of 0.15313886658245343\n",
      "The 33771 th iteration gives loss of 0.15313719338848067\n",
      "The 33772 th iteration gives loss of 0.15313552026712068\n",
      "The 33773 th iteration gives loss of 0.15313384721835643\n",
      "The 33774 th iteration gives loss of 0.15313217424217196\n",
      "The 33775 th iteration gives loss of 0.1531305013385695\n",
      "The 33776 th iteration gives loss of 0.1531288285075405\n",
      "The 33777 th iteration gives loss of 0.15312715574906005\n",
      "The 33778 th iteration gives loss of 0.1531254830631316\n",
      "The 33779 th iteration gives loss of 0.15312381044974166\n",
      "The 33780 th iteration gives loss of 0.15312213790887858\n",
      "The 33781 th iteration gives loss of 0.1531204654405363\n",
      "The 33782 th iteration gives loss of 0.1531187930446932\n",
      "The 33783 th iteration gives loss of 0.1531171207213492\n",
      "The 33784 th iteration gives loss of 0.1531154484704967\n",
      "The 33785 th iteration gives loss of 0.15311377629212042\n",
      "The 33786 th iteration gives loss of 0.1531121041862128\n",
      "The 33787 th iteration gives loss of 0.15311043215276302\n",
      "The 33788 th iteration gives loss of 0.15310876019177075\n",
      "The 33789 th iteration gives loss of 0.15310708830321637\n",
      "The 33790 th iteration gives loss of 0.1531054164870857\n",
      "The 33791 th iteration gives loss of 0.1531037447433767\n",
      "The 33792 th iteration gives loss of 0.1531020730720801\n",
      "The 33793 th iteration gives loss of 0.15310040147316792\n",
      "The 33794 th iteration gives loss of 0.15309872994665827\n",
      "The 33795 th iteration gives loss of 0.15309705849253066\n",
      "The 33796 th iteration gives loss of 0.15309538711076595\n",
      "The 33797 th iteration gives loss of 0.1530937158013614\n",
      "The 33798 th iteration gives loss of 0.15309204456431894\n",
      "The 33799 th iteration gives loss of 0.1530903733996051\n",
      "The 33800 th iteration gives loss of 0.1530887023072255\n",
      "The 33801 th iteration gives loss of 0.15308703128716905\n",
      "The 33802 th iteration gives loss of 0.15308536033942052\n",
      "The 33803 th iteration gives loss of 0.15308368946397294\n",
      "The 33804 th iteration gives loss of 0.1530820186608215\n",
      "The 33805 th iteration gives loss of 0.153080347929949\n",
      "The 33806 th iteration gives loss of 0.15307867727135238\n",
      "The 33807 th iteration gives loss of 0.15307700668500876\n",
      "The 33808 th iteration gives loss of 0.15307533617091862\n",
      "The 33809 th iteration gives loss of 0.15307366572907855\n",
      "The 33810 th iteration gives loss of 0.15307199535946617\n",
      "The 33811 th iteration gives loss of 0.1530703250620805\n",
      "The 33812 th iteration gives loss of 0.1530686548368968\n",
      "The 33813 th iteration gives loss of 0.15306698468392724\n",
      "The 33814 th iteration gives loss of 0.15306531460314218\n",
      "The 33815 th iteration gives loss of 0.15306364459455163\n",
      "The 33816 th iteration gives loss of 0.15306197465812788\n",
      "The 33817 th iteration gives loss of 0.15306030479386662\n",
      "The 33818 th iteration gives loss of 0.1530586350017599\n",
      "The 33819 th iteration gives loss of 0.15305696528179566\n",
      "The 33820 th iteration gives loss of 0.15305529563396877\n",
      "The 33821 th iteration gives loss of 0.15305362605827152\n",
      "The 33822 th iteration gives loss of 0.1530519565546866\n",
      "The 33823 th iteration gives loss of 0.15305028712320567\n",
      "The 33824 th iteration gives loss of 0.15304861776381748\n",
      "The 33825 th iteration gives loss of 0.15304694847651887\n",
      "The 33826 th iteration gives loss of 0.15304527926128617\n",
      "The 33827 th iteration gives loss of 0.1530436101181304\n",
      "The 33828 th iteration gives loss of 0.15304194104702934\n",
      "The 33829 th iteration gives loss of 0.15304027204797505\n",
      "The 33830 th iteration gives loss of 0.1530386031209475\n",
      "The 33831 th iteration gives loss of 0.15303693426595438\n",
      "The 33832 th iteration gives loss of 0.15303526548297453\n",
      "The 33833 th iteration gives loss of 0.15303359677200615\n",
      "The 33834 th iteration gives loss of 0.15303192813303826\n",
      "The 33835 th iteration gives loss of 0.15303025956604896\n",
      "The 33836 th iteration gives loss of 0.15302859107104233\n",
      "The 33837 th iteration gives loss of 0.15302692264800236\n",
      "The 33838 th iteration gives loss of 0.15302525429692526\n",
      "The 33839 th iteration gives loss of 0.15302358601779567\n",
      "The 33840 th iteration gives loss of 0.15302191781059984\n",
      "The 33841 th iteration gives loss of 0.1530202496753353\n",
      "The 33842 th iteration gives loss of 0.15301858161198797\n",
      "The 33843 th iteration gives loss of 0.15301691362055592\n",
      "The 33844 th iteration gives loss of 0.15301524570102454\n",
      "The 33845 th iteration gives loss of 0.15301357785337158\n",
      "The 33846 th iteration gives loss of 0.1530119100776059\n",
      "The 33847 th iteration gives loss of 0.15301024237372157\n",
      "The 33848 th iteration gives loss of 0.1530085747416849\n",
      "The 33849 th iteration gives loss of 0.1530069071815017\n",
      "The 33850 th iteration gives loss of 0.15300523969315058\n",
      "The 33851 th iteration gives loss of 0.15300357227664257\n",
      "The 33852 th iteration gives loss of 0.15300190493196017\n",
      "The 33853 th iteration gives loss of 0.15300023765907816\n",
      "The 33854 th iteration gives loss of 0.15299857045800372\n",
      "The 33855 th iteration gives loss of 0.15299690332872132\n",
      "The 33856 th iteration gives loss of 0.15299523627121914\n",
      "The 33857 th iteration gives loss of 0.15299356928548793\n",
      "The 33858 th iteration gives loss of 0.15299190237152888\n",
      "The 33859 th iteration gives loss of 0.15299023552931182\n",
      "The 33860 th iteration gives loss of 0.1529885687588439\n",
      "The 33861 th iteration gives loss of 0.15298690206011006\n",
      "The 33862 th iteration gives loss of 0.15298523543310105\n",
      "The 33863 th iteration gives loss of 0.1529835688778108\n",
      "The 33864 th iteration gives loss of 0.15298190239422027\n",
      "The 33865 th iteration gives loss of 0.1529802359823317\n",
      "The 33866 th iteration gives loss of 0.15297856964211493\n",
      "The 33867 th iteration gives loss of 0.15297690337358508\n",
      "The 33868 th iteration gives loss of 0.1529752371767191\n",
      "The 33869 th iteration gives loss of 0.15297357105150278\n",
      "The 33870 th iteration gives loss of 0.1529719049979366\n",
      "The 33871 th iteration gives loss of 0.15297023901601062\n",
      "The 33872 th iteration gives loss of 0.15296857310570583\n",
      "The 33873 th iteration gives loss of 0.15296690726702125\n",
      "The 33874 th iteration gives loss of 0.15296524149994256\n",
      "The 33875 th iteration gives loss of 0.15296357580446918\n",
      "The 33876 th iteration gives loss of 0.15296191018057112\n",
      "The 33877 th iteration gives loss of 0.15296024462825958\n",
      "The 33878 th iteration gives loss of 0.15295857914751462\n",
      "The 33879 th iteration gives loss of 0.1529569137383312\n",
      "The 33880 th iteration gives loss of 0.1529552484006983\n",
      "The 33881 th iteration gives loss of 0.15295358313459648\n",
      "The 33882 th iteration gives loss of 0.15295191794003035\n",
      "The 33883 th iteration gives loss of 0.15295025281698293\n",
      "The 33884 th iteration gives loss of 0.15294858776544637\n",
      "The 33885 th iteration gives loss of 0.15294692278541536\n",
      "The 33886 th iteration gives loss of 0.15294525787686145\n",
      "The 33887 th iteration gives loss of 0.15294359303979638\n",
      "The 33888 th iteration gives loss of 0.1529419282742129\n",
      "The 33889 th iteration gives loss of 0.15294026358007679\n",
      "The 33890 th iteration gives loss of 0.1529385989574074\n",
      "The 33891 th iteration gives loss of 0.15293693440616743\n",
      "The 33892 th iteration gives loss of 0.15293526992636136\n",
      "The 33893 th iteration gives loss of 0.15293360551798313\n",
      "The 33894 th iteration gives loss of 0.1529319411810172\n",
      "The 33895 th iteration gives loss of 0.15293027691545816\n",
      "The 33896 th iteration gives loss of 0.15292861272129657\n",
      "The 33897 th iteration gives loss of 0.1529269485985114\n",
      "The 33898 th iteration gives loss of 0.15292528454710233\n",
      "The 33899 th iteration gives loss of 0.1529236205670558\n",
      "The 33900 th iteration gives loss of 0.15292195665837277\n",
      "The 33901 th iteration gives loss of 0.15292029282102496\n",
      "The 33902 th iteration gives loss of 0.15291862905501982\n",
      "The 33903 th iteration gives loss of 0.15291696536033797\n",
      "The 33904 th iteration gives loss of 0.15291530173697365\n",
      "The 33905 th iteration gives loss of 0.15291363818491374\n",
      "The 33906 th iteration gives loss of 0.15291197470415835\n",
      "The 33907 th iteration gives loss of 0.15291031129469146\n",
      "The 33908 th iteration gives loss of 0.1529086479564946\n",
      "The 33909 th iteration gives loss of 0.15290698468957023\n",
      "The 33910 th iteration gives loss of 0.15290532149390387\n",
      "The 33911 th iteration gives loss of 0.1529036583694867\n",
      "The 33912 th iteration gives loss of 0.15290199531630497\n",
      "The 33913 th iteration gives loss of 0.15290033233436137\n",
      "The 33914 th iteration gives loss of 0.1528986694236267\n",
      "The 33915 th iteration gives loss of 0.15289700658411026\n",
      "The 33916 th iteration gives loss of 0.15289534381580128\n",
      "The 33917 th iteration gives loss of 0.1528936811186755\n",
      "The 33918 th iteration gives loss of 0.15289201849272777\n",
      "The 33919 th iteration gives loss of 0.1528903559379587\n",
      "The 33920 th iteration gives loss of 0.1528886934543503\n",
      "The 33921 th iteration gives loss of 0.1528870310418874\n",
      "The 33922 th iteration gives loss of 0.15288536870057218\n",
      "The 33923 th iteration gives loss of 0.15288370643039292\n",
      "The 33924 th iteration gives loss of 0.15288204423133217\n",
      "The 33925 th iteration gives loss of 0.15288038210338992\n",
      "The 33926 th iteration gives loss of 0.15287872004655073\n",
      "The 33927 th iteration gives loss of 0.15287705806080631\n",
      "The 33928 th iteration gives loss of 0.15287539614615112\n",
      "The 33929 th iteration gives loss of 0.15287373430256634\n",
      "The 33930 th iteration gives loss of 0.15287207253004262\n",
      "The 33931 th iteration gives loss of 0.15287041082858382\n",
      "The 33932 th iteration gives loss of 0.1528687491981703\n",
      "The 33933 th iteration gives loss of 0.15286708763878762\n",
      "The 33934 th iteration gives loss of 0.15286542615043763\n",
      "The 33935 th iteration gives loss of 0.15286376473310595\n",
      "The 33936 th iteration gives loss of 0.152862103386773\n",
      "The 33937 th iteration gives loss of 0.15286044211144612\n",
      "The 33938 th iteration gives loss of 0.1528587809071098\n",
      "The 33939 th iteration gives loss of 0.15285711977375105\n",
      "The 33940 th iteration gives loss of 0.15285545871136064\n",
      "The 33941 th iteration gives loss of 0.15285379771993135\n",
      "The 33942 th iteration gives loss of 0.15285213679944668\n",
      "The 33943 th iteration gives loss of 0.15285047594991574\n",
      "The 33944 th iteration gives loss of 0.15284881517130852\n",
      "The 33945 th iteration gives loss of 0.15284715446361694\n",
      "The 33946 th iteration gives loss of 0.1528454938268444\n",
      "The 33947 th iteration gives loss of 0.15284383326096684\n",
      "The 33948 th iteration gives loss of 0.152842172765987\n",
      "The 33949 th iteration gives loss of 0.15284051234189516\n",
      "The 33950 th iteration gives loss of 0.15283885198866937\n",
      "The 33951 th iteration gives loss of 0.1528371917063125\n",
      "The 33952 th iteration gives loss of 0.15283553149480797\n",
      "The 33953 th iteration gives loss of 0.15283387135414797\n",
      "The 33954 th iteration gives loss of 0.15283221128432936\n",
      "The 33955 th iteration gives loss of 0.15283055128533016\n",
      "The 33956 th iteration gives loss of 0.15282889135713923\n",
      "The 33957 th iteration gives loss of 0.15282723149976432\n",
      "The 33958 th iteration gives loss of 0.15282557171318556\n",
      "The 33959 th iteration gives loss of 0.15282391199739115\n",
      "The 33960 th iteration gives loss of 0.15282225235237187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 33961 th iteration gives loss of 0.15282059277813287\n",
      "The 33962 th iteration gives loss of 0.15281893327464036\n",
      "The 33963 th iteration gives loss of 0.15281727384189697\n",
      "The 33964 th iteration gives loss of 0.15281561447989672\n",
      "The 33965 th iteration gives loss of 0.15281395518862376\n",
      "The 33966 th iteration gives loss of 0.15281229596807475\n",
      "The 33967 th iteration gives loss of 0.15281063681823284\n",
      "The 33968 th iteration gives loss of 0.15280897773908972\n",
      "The 33969 th iteration gives loss of 0.15280731873064396\n",
      "The 33970 th iteration gives loss of 0.1528056597928785\n",
      "The 33971 th iteration gives loss of 0.15280400092578642\n",
      "The 33972 th iteration gives loss of 0.15280234212935828\n",
      "The 33973 th iteration gives loss of 0.15280068340357666\n",
      "The 33974 th iteration gives loss of 0.15279902474844137\n",
      "The 33975 th iteration gives loss of 0.15279736616393663\n",
      "The 33976 th iteration gives loss of 0.15279570765005823\n",
      "The 33977 th iteration gives loss of 0.15279404920680253\n",
      "The 33978 th iteration gives loss of 0.152792390834146\n",
      "The 33979 th iteration gives loss of 0.1527907325320851\n",
      "The 33980 th iteration gives loss of 0.15278907430060779\n",
      "The 33981 th iteration gives loss of 0.15278741613971164\n",
      "The 33982 th iteration gives loss of 0.15278575804938055\n",
      "The 33983 th iteration gives loss of 0.15278410002960516\n",
      "The 33984 th iteration gives loss of 0.15278244208038178\n",
      "The 33985 th iteration gives loss of 0.1527807842016939\n",
      "The 33986 th iteration gives loss of 0.15277912639353425\n",
      "The 33987 th iteration gives loss of 0.15277746865589786\n",
      "The 33988 th iteration gives loss of 0.1527758109887698\n",
      "The 33989 th iteration gives loss of 0.15277415339214126\n",
      "The 33990 th iteration gives loss of 0.15277249586600403\n",
      "The 33991 th iteration gives loss of 0.15277083841035258\n",
      "The 33992 th iteration gives loss of 0.152769181025159\n",
      "The 33993 th iteration gives loss of 0.15276752371043859\n",
      "The 33994 th iteration gives loss of 0.15276586646617266\n",
      "The 33995 th iteration gives loss of 0.15276420929233797\n",
      "The 33996 th iteration gives loss of 0.15276255218894236\n",
      "The 33997 th iteration gives loss of 0.15276089515597568\n",
      "The 33998 th iteration gives loss of 0.15275923819342122\n",
      "The 33999 th iteration gives loss of 0.1527575813012798\n",
      "The 34000 th iteration gives loss of 0.15275592447952424\n",
      "The 34001 th iteration gives loss of 0.15275426772815037\n",
      "The 34002 th iteration gives loss of 0.15275261104715546\n",
      "The 34003 th iteration gives loss of 0.15275095443652964\n",
      "The 34004 th iteration gives loss of 0.15274929789626596\n",
      "The 34005 th iteration gives loss of 0.15274764142633876\n",
      "The 34006 th iteration gives loss of 0.15274598502676073\n",
      "The 34007 th iteration gives loss of 0.1527443286975072\n",
      "The 34008 th iteration gives loss of 0.15274267243857018\n",
      "The 34009 th iteration gives loss of 0.1527410162499481\n",
      "The 34010 th iteration gives loss of 0.15273936013161818\n",
      "The 34011 th iteration gives loss of 0.15273770408359363\n",
      "The 34012 th iteration gives loss of 0.15273604810584449\n",
      "The 34013 th iteration gives loss of 0.15273439219836038\n",
      "The 34014 th iteration gives loss of 0.1527327363611403\n",
      "The 34015 th iteration gives loss of 0.1527310805941743\n",
      "The 34016 th iteration gives loss of 0.15272942489744737\n",
      "The 34017 th iteration gives loss of 0.15272776927096163\n",
      "The 34018 th iteration gives loss of 0.15272611371469866\n",
      "The 34019 th iteration gives loss of 0.15272445822864578\n",
      "The 34020 th iteration gives loss of 0.15272280281280004\n",
      "The 34021 th iteration gives loss of 0.15272114746714494\n",
      "The 34022 th iteration gives loss of 0.15271949219169292\n",
      "The 34023 th iteration gives loss of 0.1527178369864032\n",
      "The 34024 th iteration gives loss of 0.15271618185128888\n",
      "The 34025 th iteration gives loss of 0.15271452678632863\n",
      "The 34026 th iteration gives loss of 0.15271287179151782\n",
      "The 34027 th iteration gives loss of 0.1527112168668449\n",
      "The 34028 th iteration gives loss of 0.15270956201230634\n",
      "The 34029 th iteration gives loss of 0.1527079072278776\n",
      "The 34030 th iteration gives loss of 0.15270625251356354\n",
      "The 34031 th iteration gives loss of 0.15270459786935095\n",
      "The 34032 th iteration gives loss of 0.15270294329522674\n",
      "The 34033 th iteration gives loss of 0.1527012887911865\n",
      "The 34034 th iteration gives loss of 0.15269963435722467\n",
      "The 34035 th iteration gives loss of 0.1526979799933146\n",
      "The 34036 th iteration gives loss of 0.1526963256994621\n",
      "The 34037 th iteration gives loss of 0.1526946714756629\n",
      "The 34038 th iteration gives loss of 0.1526930173218935\n",
      "The 34039 th iteration gives loss of 0.15269136323814722\n",
      "The 34040 th iteration gives loss of 0.15268970922441524\n",
      "The 34041 th iteration gives loss of 0.15268805528069243\n",
      "The 34042 th iteration gives loss of 0.15268640140696654\n",
      "The 34043 th iteration gives loss of 0.15268474760322223\n",
      "The 34044 th iteration gives loss of 0.15268309386946163\n",
      "The 34045 th iteration gives loss of 0.15268144020566815\n",
      "The 34046 th iteration gives loss of 0.1526797866118297\n",
      "The 34047 th iteration gives loss of 0.15267813308794514\n",
      "The 34048 th iteration gives loss of 0.1526764796340037\n",
      "The 34049 th iteration gives loss of 0.15267482624998882\n",
      "The 34050 th iteration gives loss of 0.15267317293589103\n",
      "The 34051 th iteration gives loss of 0.15267151969170953\n",
      "The 34052 th iteration gives loss of 0.1526698665174299\n",
      "The 34053 th iteration gives loss of 0.15266821341303954\n",
      "The 34054 th iteration gives loss of 0.15266656037853812\n",
      "The 34055 th iteration gives loss of 0.15266490741391284\n",
      "The 34056 th iteration gives loss of 0.15266325451914547\n",
      "The 34057 th iteration gives loss of 0.1526616016942343\n",
      "The 34058 th iteration gives loss of 0.15265994893916518\n",
      "The 34059 th iteration gives loss of 0.15265829625393762\n",
      "The 34060 th iteration gives loss of 0.15265664363853146\n",
      "The 34061 th iteration gives loss of 0.15265499109295214\n",
      "The 34062 th iteration gives loss of 0.15265333861716926\n",
      "The 34063 th iteration gives loss of 0.15265168621119585\n",
      "The 34064 th iteration gives loss of 0.1526500338750072\n",
      "The 34065 th iteration gives loss of 0.15264838160859412\n",
      "The 34066 th iteration gives loss of 0.1526467294119573\n",
      "The 34067 th iteration gives loss of 0.15264507728507476\n",
      "The 34068 th iteration gives loss of 0.15264342522795202\n",
      "The 34069 th iteration gives loss of 0.15264177324057265\n",
      "The 34070 th iteration gives loss of 0.15264012132291507\n",
      "The 34071 th iteration gives loss of 0.15263846947497683\n",
      "The 34072 th iteration gives loss of 0.1526368176967653\n",
      "The 34073 th iteration gives loss of 0.1526351659882571\n",
      "The 34074 th iteration gives loss of 0.15263351434943298\n",
      "The 34075 th iteration gives loss of 0.15263186278030216\n",
      "The 34076 th iteration gives loss of 0.15263021128084858\n",
      "The 34077 th iteration gives loss of 0.15262855985105372\n",
      "The 34078 th iteration gives loss of 0.15262690849092392\n",
      "The 34079 th iteration gives loss of 0.1526252572004381\n",
      "The 34080 th iteration gives loss of 0.15262360597959168\n",
      "The 34081 th iteration gives loss of 0.15262195482837412\n",
      "The 34082 th iteration gives loss of 0.15262030374678334\n",
      "The 34083 th iteration gives loss of 0.15261865273478892\n",
      "The 34084 th iteration gives loss of 0.15261700179240711\n",
      "The 34085 th iteration gives loss of 0.1526153509196091\n",
      "The 34086 th iteration gives loss of 0.1526137001163956\n",
      "The 34087 th iteration gives loss of 0.15261204938275028\n",
      "The 34088 th iteration gives loss of 0.15261039871867166\n",
      "The 34089 th iteration gives loss of 0.152608748124151\n",
      "The 34090 th iteration gives loss of 0.15260709759916613\n",
      "The 34091 th iteration gives loss of 0.15260544714372193\n",
      "The 34092 th iteration gives loss of 0.15260379675780825\n",
      "The 34093 th iteration gives loss of 0.15260214644140432\n",
      "The 34094 th iteration gives loss of 0.1526004961945076\n",
      "The 34095 th iteration gives loss of 0.15259884601710877\n",
      "The 34096 th iteration gives loss of 0.1525971959091961\n",
      "The 34097 th iteration gives loss of 0.15259554587076338\n",
      "The 34098 th iteration gives loss of 0.15259389590180758\n",
      "The 34099 th iteration gives loss of 0.15259224600230897\n",
      "The 34100 th iteration gives loss of 0.15259059617225762\n",
      "The 34101 th iteration gives loss of 0.1525889464116469\n",
      "The 34102 th iteration gives loss of 0.15258729672046273\n",
      "The 34103 th iteration gives loss of 0.1525856470987073\n",
      "The 34104 th iteration gives loss of 0.15258399754636565\n",
      "The 34105 th iteration gives loss of 0.15258234806342885\n",
      "The 34106 th iteration gives loss of 0.15258069864988572\n",
      "The 34107 th iteration gives loss of 0.1525790493057232\n",
      "The 34108 th iteration gives loss of 0.15257740003093265\n",
      "The 34109 th iteration gives loss of 0.1525757508255218\n",
      "The 34110 th iteration gives loss of 0.1525741016894585\n",
      "The 34111 th iteration gives loss of 0.1525724526227442\n",
      "The 34112 th iteration gives loss of 0.15257080362536565\n",
      "The 34113 th iteration gives loss of 0.15256915469732463\n",
      "The 34114 th iteration gives loss of 0.15256750583859913\n",
      "The 34115 th iteration gives loss of 0.15256585704918654\n",
      "The 34116 th iteration gives loss of 0.15256420832906484\n",
      "The 34117 th iteration gives loss of 0.15256255967824098\n",
      "The 34118 th iteration gives loss of 0.15256091109670272\n",
      "The 34119 th iteration gives loss of 0.152559262584432\n",
      "The 34120 th iteration gives loss of 0.1525576141414235\n",
      "The 34121 th iteration gives loss of 0.15255596576767588\n",
      "The 34122 th iteration gives loss of 0.1525543174631691\n",
      "The 34123 th iteration gives loss of 0.1525526692278968\n",
      "The 34124 th iteration gives loss of 0.15255102106185506\n",
      "The 34125 th iteration gives loss of 0.15254937296502788\n",
      "The 34126 th iteration gives loss of 0.15254772493739918\n",
      "The 34127 th iteration gives loss of 0.15254607697898015\n",
      "The 34128 th iteration gives loss of 0.1525444290897463\n",
      "The 34129 th iteration gives loss of 0.1525427812696918\n",
      "The 34130 th iteration gives loss of 0.15254113351881043\n",
      "The 34131 th iteration gives loss of 0.15253948583708338\n",
      "The 34132 th iteration gives loss of 0.15253783822450942\n",
      "The 34133 th iteration gives loss of 0.1525361906810765\n",
      "The 34134 th iteration gives loss of 0.15253454320678184\n",
      "The 34135 th iteration gives loss of 0.15253289580160426\n",
      "The 34136 th iteration gives loss of 0.1525312484655458\n",
      "The 34137 th iteration gives loss of 0.15252960119859033\n",
      "The 34138 th iteration gives loss of 0.1525279540007237\n",
      "The 34139 th iteration gives loss of 0.15252630687195373\n",
      "The 34140 th iteration gives loss of 0.15252465981225302\n",
      "The 34141 th iteration gives loss of 0.15252301282162242\n",
      "The 34142 th iteration gives loss of 0.15252136590005444\n",
      "The 34143 th iteration gives loss of 0.15251971904753645\n",
      "The 34144 th iteration gives loss of 0.1525180722640502\n",
      "The 34145 th iteration gives loss of 0.15251642554960324\n",
      "The 34146 th iteration gives loss of 0.15251477890417153\n",
      "The 34147 th iteration gives loss of 0.15251313232775163\n",
      "The 34148 th iteration gives loss of 0.15251148582033436\n",
      "The 34149 th iteration gives loss of 0.15250983938191426\n",
      "The 34150 th iteration gives loss of 0.152508193012475\n",
      "The 34151 th iteration gives loss of 0.15250654671200714\n",
      "The 34152 th iteration gives loss of 0.1525049004805022\n",
      "The 34153 th iteration gives loss of 0.15250325431796385\n",
      "The 34154 th iteration gives loss of 0.15250160822436248\n",
      "The 34155 th iteration gives loss of 0.15249996219970027\n",
      "The 34156 th iteration gives loss of 0.15249831624396723\n",
      "The 34157 th iteration gives loss of 0.15249667035715284\n",
      "The 34158 th iteration gives loss of 0.15249502453924593\n",
      "The 34159 th iteration gives loss of 0.15249337879024807\n",
      "The 34160 th iteration gives loss of 0.15249173311013509\n",
      "The 34161 th iteration gives loss of 0.15249008749889864\n",
      "The 34162 th iteration gives loss of 0.15248844195654557\n",
      "The 34163 th iteration gives loss of 0.15248679648304914\n",
      "The 34164 th iteration gives loss of 0.1524851510784043\n",
      "The 34165 th iteration gives loss of 0.15248350574260514\n",
      "The 34166 th iteration gives loss of 0.15248186047564072\n",
      "The 34167 th iteration gives loss of 0.15248021527750152\n",
      "The 34168 th iteration gives loss of 0.15247857014818939\n",
      "The 34169 th iteration gives loss of 0.15247692508767957\n",
      "The 34170 th iteration gives loss of 0.15247528009595515\n",
      "The 34171 th iteration gives loss of 0.15247363517302814\n",
      "The 34172 th iteration gives loss of 0.15247199031888867\n",
      "The 34173 th iteration gives loss of 0.15247034553351163\n",
      "The 34174 th iteration gives loss of 0.15246870081689412\n",
      "The 34175 th iteration gives loss of 0.15246705616903267\n",
      "The 34176 th iteration gives loss of 0.1524654115899051\n",
      "The 34177 th iteration gives loss of 0.15246376707952403\n",
      "The 34178 th iteration gives loss of 0.15246212263786682\n",
      "The 34179 th iteration gives loss of 0.15246047826491582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 34180 th iteration gives loss of 0.1524588339606704\n",
      "The 34181 th iteration gives loss of 0.152457189725122\n",
      "The 34182 th iteration gives loss of 0.15245554555826102\n",
      "The 34183 th iteration gives loss of 0.1524539014600836\n",
      "The 34184 th iteration gives loss of 0.15245225743056315\n",
      "The 34185 th iteration gives loss of 0.1524506134697112\n",
      "The 34186 th iteration gives loss of 0.15244896957750448\n",
      "The 34187 th iteration gives loss of 0.15244732575394151\n",
      "The 34188 th iteration gives loss of 0.15244568199901057\n",
      "The 34189 th iteration gives loss of 0.15244403831270642\n",
      "The 34190 th iteration gives loss of 0.15244239469500487\n",
      "The 34191 th iteration gives loss of 0.15244075114590816\n",
      "The 34192 th iteration gives loss of 0.15243910766540997\n",
      "The 34193 th iteration gives loss of 0.15243746425350121\n",
      "The 34194 th iteration gives loss of 0.15243582091016206\n",
      "The 34195 th iteration gives loss of 0.15243417763539951\n",
      "The 34196 th iteration gives loss of 0.152432534429183\n",
      "The 34197 th iteration gives loss of 0.15243089129151843\n",
      "The 34198 th iteration gives loss of 0.15242924822239523\n",
      "The 34199 th iteration gives loss of 0.1524276052218001\n",
      "The 34200 th iteration gives loss of 0.15242596228972116\n",
      "The 34201 th iteration gives loss of 0.15242431942615906\n",
      "The 34202 th iteration gives loss of 0.15242267663110198\n",
      "The 34203 th iteration gives loss of 0.15242103390452877\n",
      "The 34204 th iteration gives loss of 0.1524193912464522\n",
      "The 34205 th iteration gives loss of 0.1524177486568418\n",
      "The 34206 th iteration gives loss of 0.15241610613569304\n",
      "The 34207 th iteration gives loss of 0.15241446368301087\n",
      "The 34208 th iteration gives loss of 0.15241282129877198\n",
      "The 34209 th iteration gives loss of 0.15241117898297232\n",
      "The 34210 th iteration gives loss of 0.15240953673559557\n",
      "The 34211 th iteration gives loss of 0.15240789455663972\n",
      "The 34212 th iteration gives loss of 0.15240625244609718\n",
      "The 34213 th iteration gives loss of 0.1524046104039521\n",
      "The 34214 th iteration gives loss of 0.15240296843019488\n",
      "The 34215 th iteration gives loss of 0.1524013265248292\n",
      "The 34216 th iteration gives loss of 0.15239968468783657\n",
      "The 34217 th iteration gives loss of 0.15239804291919856\n",
      "The 34218 th iteration gives loss of 0.15239640121891831\n",
      "The 34219 th iteration gives loss of 0.15239475958698567\n",
      "The 34220 th iteration gives loss of 0.15239311802339198\n",
      "The 34221 th iteration gives loss of 0.15239147652811807\n",
      "The 34222 th iteration gives loss of 0.15238983510117352\n",
      "The 34223 th iteration gives loss of 0.15238819374253165\n",
      "The 34224 th iteration gives loss of 0.15238655245219127\n",
      "The 34225 th iteration gives loss of 0.15238491123014108\n",
      "The 34226 th iteration gives loss of 0.1523832700763695\n",
      "The 34227 th iteration gives loss of 0.15238162899087487\n",
      "The 34228 th iteration gives loss of 0.15237998797363966\n",
      "The 34229 th iteration gives loss of 0.1523783470246584\n",
      "The 34230 th iteration gives loss of 0.15237670614391924\n",
      "The 34231 th iteration gives loss of 0.15237506533141806\n",
      "The 34232 th iteration gives loss of 0.15237342458714645\n",
      "The 34233 th iteration gives loss of 0.1523717839110939\n",
      "The 34234 th iteration gives loss of 0.15237014330324333\n",
      "The 34235 th iteration gives loss of 0.1523685027635928\n",
      "The 34236 th iteration gives loss of 0.15236686229212928\n",
      "The 34237 th iteration gives loss of 0.15236522188884483\n",
      "The 34238 th iteration gives loss of 0.15236358155373972\n",
      "The 34239 th iteration gives loss of 0.15236194128679148\n",
      "The 34240 th iteration gives loss of 0.15236030108799895\n",
      "The 34241 th iteration gives loss of 0.15235866095734585\n",
      "The 34242 th iteration gives loss of 0.1523570208948263\n",
      "The 34243 th iteration gives loss of 0.15235538090043677\n",
      "The 34244 th iteration gives loss of 0.1523537409741639\n",
      "The 34245 th iteration gives loss of 0.15235210111598985\n",
      "The 34246 th iteration gives loss of 0.15235046132592692\n",
      "The 34247 th iteration gives loss of 0.15234882160394686\n",
      "The 34248 th iteration gives loss of 0.15234718195003935\n",
      "The 34249 th iteration gives loss of 0.1523455423642101\n",
      "The 34250 th iteration gives loss of 0.15234390284644458\n",
      "The 34251 th iteration gives loss of 0.1523422633967241\n",
      "The 34252 th iteration gives loss of 0.15234062401504897\n",
      "The 34253 th iteration gives loss of 0.15233898470141125\n",
      "The 34254 th iteration gives loss of 0.15233734545579544\n",
      "The 34255 th iteration gives loss of 0.15233570627819717\n",
      "The 34256 th iteration gives loss of 0.15233406716859862\n",
      "The 34257 th iteration gives loss of 0.15233242812700273\n",
      "The 34258 th iteration gives loss of 0.15233078915338838\n",
      "The 34259 th iteration gives loss of 0.1523291502477716\n",
      "The 34260 th iteration gives loss of 0.1523275114101097\n",
      "The 34261 th iteration gives loss of 0.1523258726404091\n",
      "The 34262 th iteration gives loss of 0.1523242339386617\n",
      "The 34263 th iteration gives loss of 0.1523225953048521\n",
      "The 34264 th iteration gives loss of 0.15232095673898538\n",
      "The 34265 th iteration gives loss of 0.15231931824103362\n",
      "The 34266 th iteration gives loss of 0.15231767981100167\n",
      "The 34267 th iteration gives loss of 0.15231604144887653\n",
      "The 34268 th iteration gives loss of 0.1523144031546421\n",
      "The 34269 th iteration gives loss of 0.15231276492830623\n",
      "The 34270 th iteration gives loss of 0.1523111267698433\n",
      "The 34271 th iteration gives loss of 0.15230948867924413\n",
      "The 34272 th iteration gives loss of 0.1523078506565125\n",
      "The 34273 th iteration gives loss of 0.15230621270162548\n",
      "The 34274 th iteration gives loss of 0.15230457481458606\n",
      "The 34275 th iteration gives loss of 0.15230293699537326\n",
      "The 34276 th iteration gives loss of 0.15230129924398797\n",
      "The 34277 th iteration gives loss of 0.15229966156042454\n",
      "The 34278 th iteration gives loss of 0.1522980239446585\n",
      "The 34279 th iteration gives loss of 0.15229638639668971\n",
      "The 34280 th iteration gives loss of 0.1522947489165141\n",
      "The 34281 th iteration gives loss of 0.15229311150411073\n",
      "The 34282 th iteration gives loss of 0.15229147415947625\n",
      "The 34283 th iteration gives loss of 0.1522898368826038\n",
      "The 34284 th iteration gives loss of 0.15228819967348506\n",
      "The 34285 th iteration gives loss of 0.15228656253210884\n",
      "The 34286 th iteration gives loss of 0.15228492545845895\n",
      "The 34287 th iteration gives loss of 0.15228328845253258\n",
      "The 34288 th iteration gives loss of 0.15228165151433334\n",
      "The 34289 th iteration gives loss of 0.15228001464382288\n",
      "The 34290 th iteration gives loss of 0.15227837784101225\n",
      "The 34291 th iteration gives loss of 0.1522767411058988\n",
      "The 34292 th iteration gives loss of 0.15227510443846037\n",
      "The 34293 th iteration gives loss of 0.15227346783867823\n",
      "The 34294 th iteration gives loss of 0.15227183130656824\n",
      "The 34295 th iteration gives loss of 0.152270194842111\n",
      "The 34296 th iteration gives loss of 0.15226855844528964\n",
      "The 34297 th iteration gives loss of 0.1522669221161034\n",
      "The 34298 th iteration gives loss of 0.15226528585454138\n",
      "The 34299 th iteration gives loss of 0.1522636496605885\n",
      "The 34300 th iteration gives loss of 0.15226201353424565\n",
      "The 34301 th iteration gives loss of 0.15226037747550342\n",
      "The 34302 th iteration gives loss of 0.15225874148434193\n",
      "The 34303 th iteration gives loss of 0.15225710556076347\n",
      "The 34304 th iteration gives loss of 0.15225546970475107\n",
      "The 34305 th iteration gives loss of 0.1522538339162962\n",
      "The 34306 th iteration gives loss of 0.15225219819539934\n",
      "The 34307 th iteration gives loss of 0.15225056254203123\n",
      "The 34308 th iteration gives loss of 0.15224892695620615\n",
      "The 34309 th iteration gives loss of 0.15224729143790683\n",
      "The 34310 th iteration gives loss of 0.15224565598711934\n",
      "The 34311 th iteration gives loss of 0.15224402060384104\n",
      "The 34312 th iteration gives loss of 0.15224238528805173\n",
      "The 34313 th iteration gives loss of 0.1522407500397585\n",
      "The 34314 th iteration gives loss of 0.15223911485893848\n",
      "The 34315 th iteration gives loss of 0.15223747974558927\n",
      "The 34316 th iteration gives loss of 0.15223584469969526\n",
      "The 34317 th iteration gives loss of 0.15223420972125842\n",
      "The 34318 th iteration gives loss of 0.1522325748102636\n",
      "The 34319 th iteration gives loss of 0.1522309399667003\n",
      "The 34320 th iteration gives loss of 0.15222930519057024\n",
      "The 34321 th iteration gives loss of 0.15222767048184524\n",
      "The 34322 th iteration gives loss of 0.15222603584053268\n",
      "The 34323 th iteration gives loss of 0.15222440126661027\n",
      "The 34324 th iteration gives loss of 0.1522227667600748\n",
      "The 34325 th iteration gives loss of 0.1522211323209257\n",
      "The 34326 th iteration gives loss of 0.152219497949145\n",
      "The 34327 th iteration gives loss of 0.15221786364472079\n",
      "The 34328 th iteration gives loss of 0.1522162294076508\n",
      "The 34329 th iteration gives loss of 0.15221459523792083\n",
      "The 34330 th iteration gives loss of 0.15221296113552962\n",
      "The 34331 th iteration gives loss of 0.1522113271004622\n",
      "The 34332 th iteration gives loss of 0.15220969313271204\n",
      "The 34333 th iteration gives loss of 0.15220805923226344\n",
      "The 34334 th iteration gives loss of 0.1522064253991164\n",
      "The 34335 th iteration gives loss of 0.15220479163324768\n",
      "The 34336 th iteration gives loss of 0.15220315793467007\n",
      "The 34337 th iteration gives loss of 0.15220152430336126\n",
      "The 34338 th iteration gives loss of 0.15219989073931273\n",
      "The 34339 th iteration gives loss of 0.15219825724251965\n",
      "The 34340 th iteration gives loss of 0.15219662381297308\n",
      "The 34341 th iteration gives loss of 0.15219499045065174\n",
      "The 34342 th iteration gives loss of 0.15219335715555327\n",
      "The 34343 th iteration gives loss of 0.15219172392767644\n",
      "The 34344 th iteration gives loss of 0.1521900907670106\n",
      "The 34345 th iteration gives loss of 0.15218845767353886\n",
      "The 34346 th iteration gives loss of 0.15218682464726302\n",
      "The 34347 th iteration gives loss of 0.15218519168815917\n",
      "The 34348 th iteration gives loss of 0.15218355879623127\n",
      "The 34349 th iteration gives loss of 0.15218192597146418\n",
      "The 34350 th iteration gives loss of 0.15218029321385035\n",
      "The 34351 th iteration gives loss of 0.1521786605233789\n",
      "The 34352 th iteration gives loss of 0.15217702790004914\n",
      "The 34353 th iteration gives loss of 0.15217539534383523\n",
      "The 34354 th iteration gives loss of 0.15217376285473752\n",
      "The 34355 th iteration gives loss of 0.15217213043276043\n",
      "The 34356 th iteration gives loss of 0.1521704980778828\n",
      "The 34357 th iteration gives loss of 0.1521688657900878\n",
      "The 34358 th iteration gives loss of 0.1521672335693746\n",
      "The 34359 th iteration gives loss of 0.15216560141573762\n",
      "The 34360 th iteration gives loss of 0.15216396932916754\n",
      "The 34361 th iteration gives loss of 0.1521623373096413\n",
      "The 34362 th iteration gives loss of 0.15216070535716597\n",
      "The 34363 th iteration gives loss of 0.15215907347173674\n",
      "The 34364 th iteration gives loss of 0.15215744165331963\n",
      "The 34365 th iteration gives loss of 0.15215580990192448\n",
      "The 34366 th iteration gives loss of 0.15215417821753735\n",
      "The 34367 th iteration gives loss of 0.15215254660015382\n",
      "The 34368 th iteration gives loss of 0.1521509150497615\n",
      "The 34369 th iteration gives loss of 0.15214928356634932\n",
      "The 34370 th iteration gives loss of 0.152147652149905\n",
      "The 34371 th iteration gives loss of 0.15214602080043935\n",
      "The 34372 th iteration gives loss of 0.1521443895179347\n",
      "The 34373 th iteration gives loss of 0.15214275830235416\n",
      "The 34374 th iteration gives loss of 0.15214112715372655\n",
      "The 34375 th iteration gives loss of 0.15213949607201757\n",
      "The 34376 th iteration gives loss of 0.15213786505723148\n",
      "The 34377 th iteration gives loss of 0.1521362341093541\n",
      "The 34378 th iteration gives loss of 0.15213460322838634\n",
      "The 34379 th iteration gives loss of 0.15213297241430818\n",
      "The 34380 th iteration gives loss of 0.15213134166711423\n",
      "The 34381 th iteration gives loss of 0.15212971098679298\n",
      "The 34382 th iteration gives loss of 0.15212808037333506\n",
      "The 34383 th iteration gives loss of 0.1521264498267374\n",
      "The 34384 th iteration gives loss of 0.1521248193469848\n",
      "The 34385 th iteration gives loss of 0.1521231889340712\n",
      "The 34386 th iteration gives loss of 0.15212155858799314\n",
      "The 34387 th iteration gives loss of 0.15211992830873086\n",
      "The 34388 th iteration gives loss of 0.15211829809628272\n",
      "The 34389 th iteration gives loss of 0.1521166679506433\n",
      "The 34390 th iteration gives loss of 0.15211503787179256\n",
      "The 34391 th iteration gives loss of 0.1521134078597271\n",
      "The 34392 th iteration gives loss of 0.15211177791443473\n",
      "The 34393 th iteration gives loss of 0.15211014803591227\n",
      "The 34394 th iteration gives loss of 0.15210851822414537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 34395 th iteration gives loss of 0.15210688847913287\n",
      "The 34396 th iteration gives loss of 0.15210525880085923\n",
      "The 34397 th iteration gives loss of 0.15210362918932077\n",
      "The 34398 th iteration gives loss of 0.15210199964449514\n",
      "The 34399 th iteration gives loss of 0.15210037016638767\n",
      "The 34400 th iteration gives loss of 0.15209874075499144\n",
      "The 34401 th iteration gives loss of 0.15209711141028426\n",
      "The 34402 th iteration gives loss of 0.1520954821322668\n",
      "The 34403 th iteration gives loss of 0.15209385292092722\n",
      "The 34404 th iteration gives loss of 0.15209222377625461\n",
      "The 34405 th iteration gives loss of 0.1520905946982451\n",
      "The 34406 th iteration gives loss of 0.1520889656868805\n",
      "The 34407 th iteration gives loss of 0.1520873367421693\n",
      "The 34408 th iteration gives loss of 0.15208570786408324\n",
      "The 34409 th iteration gives loss of 0.15208407905262009\n",
      "The 34410 th iteration gives loss of 0.15208245030777576\n",
      "The 34411 th iteration gives loss of 0.15208082162954203\n",
      "The 34412 th iteration gives loss of 0.15207919301790168\n",
      "The 34413 th iteration gives loss of 0.15207756447283946\n",
      "The 34414 th iteration gives loss of 0.15207593599437289\n",
      "The 34415 th iteration gives loss of 0.1520743075824731\n",
      "The 34416 th iteration gives loss of 0.15207267923713336\n",
      "The 34417 th iteration gives loss of 0.15207105095834653\n",
      "The 34418 th iteration gives loss of 0.1520694227460988\n",
      "The 34419 th iteration gives loss of 0.1520677946003951\n",
      "The 34420 th iteration gives loss of 0.15206616652121685\n",
      "The 34421 th iteration gives loss of 0.15206453850855242\n",
      "The 34422 th iteration gives loss of 0.1520629105623983\n",
      "The 34423 th iteration gives loss of 0.15206128268274277\n",
      "The 34424 th iteration gives loss of 0.15205965486958112\n",
      "The 34425 th iteration gives loss of 0.1520580271228997\n",
      "The 34426 th iteration gives loss of 0.15205639944268784\n",
      "The 34427 th iteration gives loss of 0.15205477182894264\n",
      "The 34428 th iteration gives loss of 0.15205314428164268\n",
      "The 34429 th iteration gives loss of 0.1520515168008043\n",
      "The 34430 th iteration gives loss of 0.152049889386398\n",
      "The 34431 th iteration gives loss of 0.15204826203841804\n",
      "The 34432 th iteration gives loss of 0.15204663475685717\n",
      "The 34433 th iteration gives loss of 0.15204500754170897\n",
      "The 34434 th iteration gives loss of 0.15204338039295567\n",
      "The 34435 th iteration gives loss of 0.15204175331060465\n",
      "The 34436 th iteration gives loss of 0.15204012629463057\n",
      "The 34437 th iteration gives loss of 0.1520384993450453\n",
      "The 34438 th iteration gives loss of 0.15203687246180767\n",
      "The 34439 th iteration gives loss of 0.15203524564493484\n",
      "The 34440 th iteration gives loss of 0.1520336188944114\n",
      "The 34441 th iteration gives loss of 0.15203199221023136\n",
      "The 34442 th iteration gives loss of 0.15203036559237698\n",
      "The 34443 th iteration gives loss of 0.15202873904084768\n",
      "The 34444 th iteration gives loss of 0.15202711255562873\n",
      "The 34445 th iteration gives loss of 0.1520254861367078\n",
      "The 34446 th iteration gives loss of 0.1520238597840936\n",
      "The 34447 th iteration gives loss of 0.15202223349776248\n",
      "The 34448 th iteration gives loss of 0.1520206072777071\n",
      "The 34449 th iteration gives loss of 0.15201898112391624\n",
      "The 34450 th iteration gives loss of 0.15201735503639252\n",
      "The 34451 th iteration gives loss of 0.1520157290151199\n",
      "The 34452 th iteration gives loss of 0.152014103060087\n",
      "The 34453 th iteration gives loss of 0.15201247717128452\n",
      "The 34454 th iteration gives loss of 0.15201085134870568\n",
      "The 34455 th iteration gives loss of 0.15200922559234367\n",
      "The 34456 th iteration gives loss of 0.15200759990219007\n",
      "The 34457 th iteration gives loss of 0.1520059742782292\n",
      "The 34458 th iteration gives loss of 0.15200434872046312\n",
      "The 34459 th iteration gives loss of 0.15200272322887323\n",
      "The 34460 th iteration gives loss of 0.1520010978034557\n",
      "The 34461 th iteration gives loss of 0.15199947244419473\n",
      "The 34462 th iteration gives loss of 0.15199784715109943\n",
      "The 34463 th iteration gives loss of 0.15199622192413673\n",
      "The 34464 th iteration gives loss of 0.15199459676332108\n",
      "The 34465 th iteration gives loss of 0.15199297166863035\n",
      "The 34466 th iteration gives loss of 0.1519913466400531\n",
      "The 34467 th iteration gives loss of 0.15198972167758243\n",
      "The 34468 th iteration gives loss of 0.1519880967812139\n",
      "The 34469 th iteration gives loss of 0.15198647195093978\n",
      "The 34470 th iteration gives loss of 0.15198484718675023\n",
      "The 34471 th iteration gives loss of 0.15198322248862678\n",
      "The 34472 th iteration gives loss of 0.15198159785656926\n",
      "The 34473 th iteration gives loss of 0.1519799732905758\n",
      "The 34474 th iteration gives loss of 0.15197834879062525\n",
      "The 34475 th iteration gives loss of 0.15197672435671575\n",
      "The 34476 th iteration gives loss of 0.15197509998883066\n",
      "The 34477 th iteration gives loss of 0.15197347568697703\n",
      "The 34478 th iteration gives loss of 0.1519718514511265\n",
      "The 34479 th iteration gives loss of 0.15197022728128093\n",
      "The 34480 th iteration gives loss of 0.1519686031774259\n",
      "The 34481 th iteration gives loss of 0.15196697913955765\n",
      "The 34482 th iteration gives loss of 0.15196535516766538\n",
      "The 34483 th iteration gives loss of 0.1519637312617453\n",
      "The 34484 th iteration gives loss of 0.15196210742178373\n",
      "The 34485 th iteration gives loss of 0.15196048364777445\n",
      "The 34486 th iteration gives loss of 0.1519588599397016\n",
      "The 34487 th iteration gives loss of 0.15195723629757235\n",
      "The 34488 th iteration gives loss of 0.15195561272135452\n",
      "The 34489 th iteration gives loss of 0.15195398921105482\n",
      "The 34490 th iteration gives loss of 0.15195236576666124\n",
      "The 34491 th iteration gives loss of 0.15195074238816303\n",
      "The 34492 th iteration gives loss of 0.15194911907556544\n",
      "The 34493 th iteration gives loss of 0.15194749582883693\n",
      "The 34494 th iteration gives loss of 0.15194587264797738\n",
      "The 34495 th iteration gives loss of 0.15194424953298336\n",
      "The 34496 th iteration gives loss of 0.15194262648384224\n",
      "The 34497 th iteration gives loss of 0.15194100350054549\n",
      "The 34498 th iteration gives loss of 0.15193938058308876\n",
      "The 34499 th iteration gives loss of 0.15193775773145954\n",
      "The 34500 th iteration gives loss of 0.1519361349456417\n",
      "The 34501 th iteration gives loss of 0.15193451222563745\n",
      "The 34502 th iteration gives loss of 0.1519328895714349\n",
      "The 34503 th iteration gives loss of 0.15193126698301979\n",
      "The 34504 th iteration gives loss of 0.15192964446038446\n",
      "The 34505 th iteration gives loss of 0.15192802200353273\n",
      "The 34506 th iteration gives loss of 0.15192639961244306\n",
      "The 34507 th iteration gives loss of 0.1519247772871135\n",
      "The 34508 th iteration gives loss of 0.15192315502752793\n",
      "The 34509 th iteration gives loss of 0.15192153283368537\n",
      "The 34510 th iteration gives loss of 0.15191991070556027\n",
      "The 34511 th iteration gives loss of 0.15191828864316861\n",
      "The 34512 th iteration gives loss of 0.15191666664648684\n",
      "The 34513 th iteration gives loss of 0.15191504471551087\n",
      "The 34514 th iteration gives loss of 0.15191342285022894\n",
      "The 34515 th iteration gives loss of 0.15191180105062826\n",
      "The 34516 th iteration gives loss of 0.15191017931671738\n",
      "The 34517 th iteration gives loss of 0.1519085576484588\n",
      "The 34518 th iteration gives loss of 0.1519069360458798\n",
      "The 34519 th iteration gives loss of 0.15190531450893832\n",
      "The 34520 th iteration gives loss of 0.15190369303763904\n",
      "The 34521 th iteration gives loss of 0.15190207163198277\n",
      "The 34522 th iteration gives loss of 0.15190045029194182\n",
      "The 34523 th iteration gives loss of 0.15189882901752716\n",
      "The 34524 th iteration gives loss of 0.15189720780870994\n",
      "The 34525 th iteration gives loss of 0.15189558666549666\n",
      "The 34526 th iteration gives loss of 0.15189396558786777\n",
      "The 34527 th iteration gives loss of 0.15189234457583414\n",
      "The 34528 th iteration gives loss of 0.15189072362936468\n",
      "The 34529 th iteration gives loss of 0.15188910274845627\n",
      "The 34530 th iteration gives loss of 0.15188748193309967\n",
      "The 34531 th iteration gives loss of 0.15188586118329644\n",
      "The 34532 th iteration gives loss of 0.1518842404990268\n",
      "The 34533 th iteration gives loss of 0.15188261988029217\n",
      "The 34534 th iteration gives loss of 0.15188099932706894\n",
      "The 34535 th iteration gives loss of 0.15187937883936214\n",
      "The 34536 th iteration gives loss of 0.15187775841715193\n",
      "The 34537 th iteration gives loss of 0.15187613806044273\n",
      "The 34538 th iteration gives loss of 0.15187451776921618\n",
      "The 34539 th iteration gives loss of 0.15187289754346675\n",
      "The 34540 th iteration gives loss of 0.15187127738318784\n",
      "The 34541 th iteration gives loss of 0.1518696572883613\n",
      "The 34542 th iteration gives loss of 0.15186803725898873\n",
      "The 34543 th iteration gives loss of 0.15186641729505823\n",
      "The 34544 th iteration gives loss of 0.15186479739655714\n",
      "The 34545 th iteration gives loss of 0.1518631775634854\n",
      "The 34546 th iteration gives loss of 0.1518615577958246\n",
      "The 34547 th iteration gives loss of 0.1518599380935686\n",
      "The 34548 th iteration gives loss of 0.1518583184567116\n",
      "The 34549 th iteration gives loss of 0.1518566988852402\n",
      "The 34550 th iteration gives loss of 0.15185507937914816\n",
      "The 34551 th iteration gives loss of 0.15185345993844201\n",
      "The 34552 th iteration gives loss of 0.1518518405630897\n",
      "The 34553 th iteration gives loss of 0.15185022125309358\n",
      "The 34554 th iteration gives loss of 0.15184860200843228\n",
      "The 34555 th iteration gives loss of 0.1518469828291097\n",
      "The 34556 th iteration gives loss of 0.15184536371511995\n",
      "The 34557 th iteration gives loss of 0.15184374466645328\n",
      "The 34558 th iteration gives loss of 0.15184212568309435\n",
      "The 34559 th iteration gives loss of 0.1518405067650361\n",
      "The 34560 th iteration gives loss of 0.15183888791227054\n",
      "The 34561 th iteration gives loss of 0.1518372691247914\n",
      "The 34562 th iteration gives loss of 0.15183565040258584\n",
      "The 34563 th iteration gives loss of 0.1518340317456486\n",
      "The 34564 th iteration gives loss of 0.1518324131539588\n",
      "The 34565 th iteration gives loss of 0.1518307946275301\n",
      "The 34566 th iteration gives loss of 0.15182917616634312\n",
      "The 34567 th iteration gives loss of 0.15182755777038884\n",
      "The 34568 th iteration gives loss of 0.15182593943964845\n",
      "The 34569 th iteration gives loss of 0.15182432117413153\n",
      "The 34570 th iteration gives loss of 0.15182270297382094\n",
      "The 34571 th iteration gives loss of 0.1518210848387069\n",
      "The 34572 th iteration gives loss of 0.15181946676877492\n",
      "The 34573 th iteration gives loss of 0.15181784876403112\n",
      "The 34574 th iteration gives loss of 0.15181623082445025\n",
      "The 34575 th iteration gives loss of 0.1518146129500347\n",
      "The 34576 th iteration gives loss of 0.15181299514077454\n",
      "The 34577 th iteration gives loss of 0.15181137739665987\n",
      "The 34578 th iteration gives loss of 0.15180975971768645\n",
      "The 34579 th iteration gives loss of 0.15180814210383425\n",
      "The 34580 th iteration gives loss of 0.15180652455509444\n",
      "The 34581 th iteration gives loss of 0.15180490707147234\n",
      "The 34582 th iteration gives loss of 0.1518032896529491\n",
      "The 34583 th iteration gives loss of 0.15180167229952857\n",
      "The 34584 th iteration gives loss of 0.15180005501118676\n",
      "The 34585 th iteration gives loss of 0.1517984377879153\n",
      "The 34586 th iteration gives loss of 0.1517968206297179\n",
      "The 34587 th iteration gives loss of 0.15179520353657167\n",
      "The 34588 th iteration gives loss of 0.15179358650848818\n",
      "The 34589 th iteration gives loss of 0.15179196954543495\n",
      "The 34590 th iteration gives loss of 0.15179035264740923\n",
      "The 34591 th iteration gives loss of 0.15178873581441044\n",
      "The 34592 th iteration gives loss of 0.15178711904643585\n",
      "The 34593 th iteration gives loss of 0.15178550234345972\n",
      "The 34594 th iteration gives loss of 0.15178388570548637\n",
      "The 34595 th iteration gives loss of 0.1517822691324994\n",
      "The 34596 th iteration gives loss of 0.15178065262449328\n",
      "The 34597 th iteration gives loss of 0.15177903618145056\n",
      "The 34598 th iteration gives loss of 0.1517774198033844\n",
      "The 34599 th iteration gives loss of 0.15177580349026445\n",
      "The 34600 th iteration gives loss of 0.15177418724209643\n",
      "The 34601 th iteration gives loss of 0.15177257105886546\n",
      "The 34602 th iteration gives loss of 0.15177095494055248\n",
      "The 34603 th iteration gives loss of 0.15176933888716548\n",
      "The 34604 th iteration gives loss of 0.15176772289868992\n",
      "The 34605 th iteration gives loss of 0.15176610697511383\n",
      "The 34606 th iteration gives loss of 0.15176449111643825\n",
      "The 34607 th iteration gives loss of 0.15176287532264335\n",
      "The 34608 th iteration gives loss of 0.15176125959372488\n",
      "The 34609 th iteration gives loss of 0.1517596439296759\n",
      "The 34610 th iteration gives loss of 0.1517580283304817\n",
      "The 34611 th iteration gives loss of 0.1517564127961417\n",
      "The 34612 th iteration gives loss of 0.15175479732664782\n",
      "The 34613 th iteration gives loss of 0.15175318192197526\n",
      "The 34614 th iteration gives loss of 0.15175156658213074\n",
      "The 34615 th iteration gives loss of 0.15174995130710897\n",
      "The 34616 th iteration gives loss of 0.15174833609688795\n",
      "The 34617 th iteration gives loss of 0.1517467209514712\n",
      "The 34618 th iteration gives loss of 0.15174510587084228\n",
      "The 34619 th iteration gives loss of 0.1517434908549982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 34620 th iteration gives loss of 0.1517418759039296\n",
      "The 34621 th iteration gives loss of 0.1517402610176153\n",
      "The 34622 th iteration gives loss of 0.15173864619605906\n",
      "The 34623 th iteration gives loss of 0.15173703143925102\n",
      "The 34624 th iteration gives loss of 0.15173541674718055\n",
      "The 34625 th iteration gives loss of 0.151733802119835\n",
      "The 34626 th iteration gives loss of 0.15173218755722037\n",
      "The 34627 th iteration gives loss of 0.15173057305931426\n",
      "The 34628 th iteration gives loss of 0.15172895862611166\n",
      "The 34629 th iteration gives loss of 0.15172734425760412\n",
      "The 34630 th iteration gives loss of 0.15172572995378508\n",
      "The 34631 th iteration gives loss of 0.15172411571464153\n",
      "The 34632 th iteration gives loss of 0.151722501540171\n",
      "The 34633 th iteration gives loss of 0.1517208874303557\n",
      "The 34634 th iteration gives loss of 0.15171927338519345\n",
      "The 34635 th iteration gives loss of 0.1517176594046781\n",
      "The 34636 th iteration gives loss of 0.15171604548879994\n",
      "The 34637 th iteration gives loss of 0.15171443163753895\n",
      "The 34638 th iteration gives loss of 0.15171281785090127\n",
      "The 34639 th iteration gives loss of 0.15171120412887493\n",
      "The 34640 th iteration gives loss of 0.15170959047144783\n",
      "The 34641 th iteration gives loss of 0.15170797687861554\n",
      "The 34642 th iteration gives loss of 0.15170636335036147\n",
      "The 34643 th iteration gives loss of 0.15170474988668073\n",
      "The 34644 th iteration gives loss of 0.15170313648757242\n",
      "The 34645 th iteration gives loss of 0.15170152315301452\n",
      "The 34646 th iteration gives loss of 0.15169990988300755\n",
      "The 34647 th iteration gives loss of 0.15169829667754456\n",
      "The 34648 th iteration gives loss of 0.15169668353661994\n",
      "The 34649 th iteration gives loss of 0.1516950704601984\n",
      "The 34650 th iteration gives loss of 0.15169345744831247\n",
      "The 34651 th iteration gives loss of 0.151691844500921\n",
      "The 34652 th iteration gives loss of 0.15169023161801967\n",
      "The 34653 th iteration gives loss of 0.1516886187996211\n",
      "The 34654 th iteration gives loss of 0.15168700604570032\n",
      "The 34655 th iteration gives loss of 0.15168539335625034\n",
      "The 34656 th iteration gives loss of 0.15168378073125938\n",
      "The 34657 th iteration gives loss of 0.1516821681707285\n",
      "The 34658 th iteration gives loss of 0.15168055567464292\n",
      "The 34659 th iteration gives loss of 0.15167894324299297\n",
      "The 34660 th iteration gives loss of 0.15167733087576787\n",
      "The 34661 th iteration gives loss of 0.15167571857296896\n",
      "The 34662 th iteration gives loss of 0.1516741063345821\n",
      "The 34663 th iteration gives loss of 0.1516724941605926\n",
      "The 34664 th iteration gives loss of 0.15167088205100152\n",
      "The 34665 th iteration gives loss of 0.15166927000579195\n",
      "The 34666 th iteration gives loss of 0.15166765802496884\n",
      "The 34667 th iteration gives loss of 0.15166604610850562\n",
      "The 34668 th iteration gives loss of 0.15166443425640386\n",
      "The 34669 th iteration gives loss of 0.1516628224686593\n",
      "The 34670 th iteration gives loss of 0.15166121074524913\n",
      "The 34671 th iteration gives loss of 0.15165959908618162\n",
      "The 34672 th iteration gives loss of 0.15165798749143541\n",
      "The 34673 th iteration gives loss of 0.15165637596101617\n",
      "The 34674 th iteration gives loss of 0.15165476449489568\n",
      "The 34675 th iteration gives loss of 0.1516531530930729\n",
      "The 34676 th iteration gives loss of 0.1516515417555526\n",
      "The 34677 th iteration gives loss of 0.1516499304823046\n",
      "The 34678 th iteration gives loss of 0.15164831927333647\n",
      "The 34679 th iteration gives loss of 0.151646708128638\n",
      "The 34680 th iteration gives loss of 0.1516450970481953\n",
      "The 34681 th iteration gives loss of 0.15164348603200048\n",
      "The 34682 th iteration gives loss of 0.15164187508004165\n",
      "The 34683 th iteration gives loss of 0.15164026419231957\n",
      "The 34684 th iteration gives loss of 0.1516386533688237\n",
      "The 34685 th iteration gives loss of 0.15163704260953356\n",
      "The 34686 th iteration gives loss of 0.15163543191445797\n",
      "The 34687 th iteration gives loss of 0.1516338212835812\n",
      "The 34688 th iteration gives loss of 0.151632210716888\n",
      "The 34689 th iteration gives loss of 0.15163060021437094\n",
      "The 34690 th iteration gives loss of 0.15162898977603229\n",
      "The 34691 th iteration gives loss of 0.1516273794018551\n",
      "The 34692 th iteration gives loss of 0.1516257690918361\n",
      "The 34693 th iteration gives loss of 0.15162415884596903\n",
      "The 34694 th iteration gives loss of 0.15162254866422734\n",
      "The 34695 th iteration gives loss of 0.15162093854663067\n",
      "The 34696 th iteration gives loss of 0.15161932849315155\n",
      "The 34697 th iteration gives loss of 0.15161771850378045\n",
      "The 34698 th iteration gives loss of 0.15161610857851296\n",
      "The 34699 th iteration gives loss of 0.15161449871733432\n",
      "The 34700 th iteration gives loss of 0.1516128889202573\n",
      "The 34701 th iteration gives loss of 0.15161127918724399\n",
      "The 34702 th iteration gives loss of 0.15160966951830948\n",
      "The 34703 th iteration gives loss of 0.15160805991343243\n",
      "The 34704 th iteration gives loss of 0.1516064503726127\n",
      "The 34705 th iteration gives loss of 0.1516048408958306\n",
      "The 34706 th iteration gives loss of 0.15160323148309351\n",
      "The 34707 th iteration gives loss of 0.15160162213437212\n",
      "The 34708 th iteration gives loss of 0.1516000128496806\n",
      "The 34709 th iteration gives loss of 0.15159840362898844\n",
      "The 34710 th iteration gives loss of 0.15159679447230562\n",
      "The 34711 th iteration gives loss of 0.15159518537961172\n",
      "The 34712 th iteration gives loss of 0.15159357635090187\n",
      "The 34713 th iteration gives loss of 0.15159196738617722\n",
      "The 34714 th iteration gives loss of 0.1515903584854119\n",
      "The 34715 th iteration gives loss of 0.15158874964861105\n",
      "The 34716 th iteration gives loss of 0.151587140875756\n",
      "The 34717 th iteration gives loss of 0.15158553216684753\n",
      "The 34718 th iteration gives loss of 0.15158392352187378\n",
      "The 34719 th iteration gives loss of 0.15158231494082647\n",
      "The 34720 th iteration gives loss of 0.15158070642369031\n",
      "The 34721 th iteration gives loss of 0.15157909797046681\n",
      "The 34722 th iteration gives loss of 0.15157748958113837\n",
      "The 34723 th iteration gives loss of 0.1515758812556994\n",
      "The 34724 th iteration gives loss of 0.15157427299415144\n",
      "The 34725 th iteration gives loss of 0.15157266479647988\n",
      "The 34726 th iteration gives loss of 0.15157105666266624\n",
      "The 34727 th iteration gives loss of 0.15156944859271476\n",
      "The 34728 th iteration gives loss of 0.15156784058660444\n",
      "The 34729 th iteration gives loss of 0.1515662326443417\n",
      "The 34730 th iteration gives loss of 0.1515646247659109\n",
      "The 34731 th iteration gives loss of 0.15156301695130384\n",
      "The 34732 th iteration gives loss of 0.15156140920049802\n",
      "The 34733 th iteration gives loss of 0.15155980151351434\n",
      "The 34734 th iteration gives loss of 0.15155819389032724\n",
      "The 34735 th iteration gives loss of 0.1515565863309272\n",
      "The 34736 th iteration gives loss of 0.1515549788353114\n",
      "The 34737 th iteration gives loss of 0.15155337140346378\n",
      "The 34738 th iteration gives loss of 0.1515517640353802\n",
      "The 34739 th iteration gives loss of 0.1515501567310578\n",
      "The 34740 th iteration gives loss of 0.15154854949047883\n",
      "The 34741 th iteration gives loss of 0.15154694231363433\n",
      "The 34742 th iteration gives loss of 0.15154533520052435\n",
      "The 34743 th iteration gives loss of 0.15154372815113798\n",
      "The 34744 th iteration gives loss of 0.1515421211654646\n",
      "The 34745 th iteration gives loss of 0.15154051424349171\n",
      "The 34746 th iteration gives loss of 0.1515389073852194\n",
      "The 34747 th iteration gives loss of 0.1515373005906308\n",
      "The 34748 th iteration gives loss of 0.15153569385972998\n",
      "The 34749 th iteration gives loss of 0.15153408719249697\n",
      "The 34750 th iteration gives loss of 0.15153248058892388\n",
      "The 34751 th iteration gives loss of 0.151530874049002\n",
      "The 34752 th iteration gives loss of 0.15152926757273585\n",
      "The 34753 th iteration gives loss of 0.15152766116010005\n",
      "The 34754 th iteration gives loss of 0.1515260548110919\n",
      "The 34755 th iteration gives loss of 0.15152444852570152\n",
      "The 34756 th iteration gives loss of 0.1515228423039319\n",
      "The 34757 th iteration gives loss of 0.15152123614576357\n",
      "The 34758 th iteration gives loss of 0.15151963005119212\n",
      "The 34759 th iteration gives loss of 0.15151802402019693\n",
      "The 34760 th iteration gives loss of 0.15151641805279026\n",
      "The 34761 th iteration gives loss of 0.1515148121489455\n",
      "The 34762 th iteration gives loss of 0.15151320630866455\n",
      "The 34763 th iteration gives loss of 0.1515116005319449\n",
      "The 34764 th iteration gives loss of 0.15150999481876043\n",
      "The 34765 th iteration gives loss of 0.15150838916911027\n",
      "The 34766 th iteration gives loss of 0.15150678358299244\n",
      "The 34767 th iteration gives loss of 0.1515051780603963\n",
      "The 34768 th iteration gives loss of 0.1515035726013076\n",
      "The 34769 th iteration gives loss of 0.15150196720572612\n",
      "The 34770 th iteration gives loss of 0.15150036187363358\n",
      "The 34771 th iteration gives loss of 0.1514987566050221\n",
      "The 34772 th iteration gives loss of 0.15149715139989678\n",
      "The 34773 th iteration gives loss of 0.15149554625822886\n",
      "The 34774 th iteration gives loss of 0.1514939411800348\n",
      "The 34775 th iteration gives loss of 0.15149233616528573\n",
      "The 34776 th iteration gives loss of 0.15149073121397563\n",
      "The 34777 th iteration gives loss of 0.15148912632610287\n",
      "The 34778 th iteration gives loss of 0.15148752150166\n",
      "The 34779 th iteration gives loss of 0.15148591674063566\n",
      "The 34780 th iteration gives loss of 0.1514843120430178\n",
      "The 34781 th iteration gives loss of 0.15148270740880712\n",
      "The 34782 th iteration gives loss of 0.15148110283798266\n",
      "The 34783 th iteration gives loss of 0.15147949833054808\n",
      "The 34784 th iteration gives loss of 0.15147789388649174\n",
      "The 34785 th iteration gives loss of 0.15147628950578862\n",
      "The 34786 th iteration gives loss of 0.1514746851884589\n",
      "The 34787 th iteration gives loss of 0.1514730809344738\n",
      "The 34788 th iteration gives loss of 0.15147147674383368\n",
      "The 34789 th iteration gives loss of 0.1514698726165219\n",
      "The 34790 th iteration gives loss of 0.1514682685525405\n",
      "The 34791 th iteration gives loss of 0.15146666455187383\n",
      "The 34792 th iteration gives loss of 0.15146506061452422\n",
      "The 34793 th iteration gives loss of 0.15146345674047018\n",
      "The 34794 th iteration gives loss of 0.15146185292970651\n",
      "The 34795 th iteration gives loss of 0.15146024918222853\n",
      "The 34796 th iteration gives loss of 0.1514586454980289\n",
      "The 34797 th iteration gives loss of 0.1514570418770928\n",
      "The 34798 th iteration gives loss of 0.1514554383194099\n",
      "The 34799 th iteration gives loss of 0.15145383482498076\n",
      "The 34800 th iteration gives loss of 0.15145223139379244\n",
      "The 34801 th iteration gives loss of 0.15145062802584014\n",
      "The 34802 th iteration gives loss of 0.15144902472110938\n",
      "The 34803 th iteration gives loss of 0.15144742147959728\n",
      "The 34804 th iteration gives loss of 0.15144581830129508\n",
      "The 34805 th iteration gives loss of 0.15144421518618983\n",
      "The 34806 th iteration gives loss of 0.1514426121342824\n",
      "The 34807 th iteration gives loss of 0.15144100914554903\n",
      "The 34808 th iteration gives loss of 0.15143940621999566\n",
      "The 34809 th iteration gives loss of 0.15143780335761275\n",
      "The 34810 th iteration gives loss of 0.15143620055837745\n",
      "The 34811 th iteration gives loss of 0.15143459782230023\n",
      "The 34812 th iteration gives loss of 0.15143299514935937\n",
      "The 34813 th iteration gives loss of 0.1514313925395469\n",
      "The 34814 th iteration gives loss of 0.15142978999286372\n",
      "The 34815 th iteration gives loss of 0.15142818750930145\n",
      "The 34816 th iteration gives loss of 0.15142658508884188\n",
      "The 34817 th iteration gives loss of 0.1514249827314782\n",
      "The 34818 th iteration gives loss of 0.15142338043721848\n",
      "The 34819 th iteration gives loss of 0.15142177820603536\n",
      "The 34820 th iteration gives loss of 0.15142017603792227\n",
      "The 34821 th iteration gives loss of 0.1514185739328752\n",
      "The 34822 th iteration gives loss of 0.15141697189089107\n",
      "The 34823 th iteration gives loss of 0.1514153699119545\n",
      "The 34824 th iteration gives loss of 0.1514137679960615\n",
      "The 34825 th iteration gives loss of 0.15141216614319586\n",
      "The 34826 th iteration gives loss of 0.15141056435334432\n",
      "The 34827 th iteration gives loss of 0.15140896262652526\n",
      "The 34828 th iteration gives loss of 0.15140736096270377\n",
      "The 34829 th iteration gives loss of 0.15140575936189557\n",
      "The 34830 th iteration gives loss of 0.15140415782406783\n",
      "The 34831 th iteration gives loss of 0.15140255634921845\n",
      "The 34832 th iteration gives loss of 0.1514009549373476\n",
      "The 34833 th iteration gives loss of 0.15139935358844434\n",
      "The 34834 th iteration gives loss of 0.15139775230249655\n",
      "The 34835 th iteration gives loss of 0.15139615107949603\n",
      "The 34836 th iteration gives loss of 0.15139454991943746\n",
      "The 34837 th iteration gives loss of 0.15139294882231466\n",
      "The 34838 th iteration gives loss of 0.15139134778811325\n",
      "The 34839 th iteration gives loss of 0.15138974681682893\n",
      "The 34840 th iteration gives loss of 0.15138814590845326\n",
      "The 34841 th iteration gives loss of 0.15138654506297233\n",
      "The 34842 th iteration gives loss of 0.1513849442803932\n",
      "The 34843 th iteration gives loss of 0.15138334356067504\n",
      "The 34844 th iteration gives loss of 0.15138174290384523\n",
      "The 34845 th iteration gives loss of 0.15138014230988106\n",
      "The 34846 th iteration gives loss of 0.15137854177877497\n",
      "The 34847 th iteration gives loss of 0.15137694131051632\n",
      "The 34848 th iteration gives loss of 0.15137534090509447\n",
      "The 34849 th iteration gives loss of 0.15137374056251263\n",
      "The 34850 th iteration gives loss of 0.1513721402827465\n",
      "The 34851 th iteration gives loss of 0.15137054006580794\n",
      "The 34852 th iteration gives loss of 0.1513689399116639\n",
      "The 34853 th iteration gives loss of 0.15136733982032455\n",
      "The 34854 th iteration gives loss of 0.1513657397917754\n",
      "The 34855 th iteration gives loss of 0.1513641398260117\n",
      "The 34856 th iteration gives loss of 0.15136253992301357\n",
      "The 34857 th iteration gives loss of 0.15136094008279297\n",
      "The 34858 th iteration gives loss of 0.1513593403053256\n",
      "The 34859 th iteration gives loss of 0.1513577405906047\n",
      "The 34860 th iteration gives loss of 0.15135614093862954\n",
      "The 34861 th iteration gives loss of 0.15135454134938242\n",
      "The 34862 th iteration gives loss of 0.15135294182286796\n",
      "The 34863 th iteration gives loss of 0.1513513423590635\n",
      "The 34864 th iteration gives loss of 0.15134974295796275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 34865 th iteration gives loss of 0.15134814361956733\n",
      "The 34866 th iteration gives loss of 0.1513465443438631\n",
      "The 34867 th iteration gives loss of 0.1513449451308387\n",
      "The 34868 th iteration gives loss of 0.15134334598049295\n",
      "The 34869 th iteration gives loss of 0.15134174689281107\n",
      "The 34870 th iteration gives loss of 0.151340147867791\n",
      "The 34871 th iteration gives loss of 0.1513385489054175\n",
      "The 34872 th iteration gives loss of 0.15133695000567776\n",
      "The 34873 th iteration gives loss of 0.1513353511685827\n",
      "The 34874 th iteration gives loss of 0.15133375239410557\n",
      "The 34875 th iteration gives loss of 0.15133215368224687\n",
      "The 34876 th iteration gives loss of 0.15133055503300077\n",
      "The 34877 th iteration gives loss of 0.15132895644635372\n",
      "The 34878 th iteration gives loss of 0.15132735792229376\n",
      "The 34879 th iteration gives loss of 0.1513257594608229\n",
      "The 34880 th iteration gives loss of 0.15132416106191943\n",
      "The 34881 th iteration gives loss of 0.15132256272558994\n",
      "The 34882 th iteration gives loss of 0.15132096445182466\n",
      "The 34883 th iteration gives loss of 0.15131936624060138\n",
      "The 34884 th iteration gives loss of 0.15131776809191708\n",
      "The 34885 th iteration gives loss of 0.15131617000576997\n",
      "The 34886 th iteration gives loss of 0.15131457198214684\n",
      "The 34887 th iteration gives loss of 0.1513129740210453\n",
      "The 34888 th iteration gives loss of 0.15131137612244372\n",
      "The 34889 th iteration gives loss of 0.15130977828635517\n",
      "The 34890 th iteration gives loss of 0.1513081805127631\n",
      "The 34891 th iteration gives loss of 0.15130658280164028\n",
      "The 34892 th iteration gives loss of 0.15130498515299687\n",
      "The 34893 th iteration gives loss of 0.15130338756681933\n",
      "The 34894 th iteration gives loss of 0.15130179004311364\n",
      "The 34895 th iteration gives loss of 0.15130019258184393\n",
      "The 34896 th iteration gives loss of 0.15129859518302308\n",
      "The 34897 th iteration gives loss of 0.1512969978466387\n",
      "The 34898 th iteration gives loss of 0.15129540057267754\n",
      "The 34899 th iteration gives loss of 0.1512938033611356\n",
      "The 34900 th iteration gives loss of 0.1512922062120044\n",
      "The 34901 th iteration gives loss of 0.15129060912527995\n",
      "The 34902 th iteration gives loss of 0.1512890121009421\n",
      "The 34903 th iteration gives loss of 0.15128741513899135\n",
      "The 34904 th iteration gives loss of 0.15128581823941592\n",
      "The 34905 th iteration gives loss of 0.1512842214022096\n",
      "The 34906 th iteration gives loss of 0.15128262462736675\n",
      "The 34907 th iteration gives loss of 0.15128102791487677\n",
      "The 34908 th iteration gives loss of 0.15127943126472568\n",
      "The 34909 th iteration gives loss of 0.1512778346769108\n",
      "The 34910 th iteration gives loss of 0.15127623815142838\n",
      "The 34911 th iteration gives loss of 0.15127464168826502\n",
      "The 34912 th iteration gives loss of 0.15127304528740676\n",
      "The 34913 th iteration gives loss of 0.15127144894884723\n",
      "The 34914 th iteration gives loss of 0.15126985267259416\n",
      "The 34915 th iteration gives loss of 0.151268256458618\n",
      "The 34916 th iteration gives loss of 0.1512666603069274\n",
      "The 34917 th iteration gives loss of 0.15126506421751013\n",
      "The 34918 th iteration gives loss of 0.1512634681903446\n",
      "The 34919 th iteration gives loss of 0.15126187222543822\n",
      "The 34920 th iteration gives loss of 0.1512602763227726\n",
      "The 34921 th iteration gives loss of 0.15125868048234373\n",
      "The 34922 th iteration gives loss of 0.15125708470414245\n",
      "The 34923 th iteration gives loss of 0.15125548898816332\n",
      "The 34924 th iteration gives loss of 0.15125389333439831\n",
      "The 34925 th iteration gives loss of 0.15125229774283983\n",
      "The 34926 th iteration gives loss of 0.15125070221346928\n",
      "The 34927 th iteration gives loss of 0.15124910674628958\n",
      "The 34928 th iteration gives loss of 0.15124751134129233\n",
      "The 34929 th iteration gives loss of 0.15124591599846784\n",
      "The 34930 th iteration gives loss of 0.1512443207178036\n",
      "The 34931 th iteration gives loss of 0.1512427254992862\n",
      "The 34932 th iteration gives loss of 0.15124113034291925\n",
      "The 34933 th iteration gives loss of 0.1512395352486999\n",
      "The 34934 th iteration gives loss of 0.15123794021659617\n",
      "The 34935 th iteration gives loss of 0.15123634524662902\n",
      "The 34936 th iteration gives loss of 0.1512347503387674\n",
      "The 34937 th iteration gives loss of 0.15123315549301647\n",
      "The 34938 th iteration gives loss of 0.15123156070936392\n",
      "The 34939 th iteration gives loss of 0.151229965987801\n",
      "The 34940 th iteration gives loss of 0.15122837132831307\n",
      "The 34941 th iteration gives loss of 0.1512267767308944\n",
      "The 34942 th iteration gives loss of 0.15122518219554212\n",
      "The 34943 th iteration gives loss of 0.15122358772224515\n",
      "The 34944 th iteration gives loss of 0.15122199331099942\n",
      "The 34945 th iteration gives loss of 0.15122039896178904\n",
      "The 34946 th iteration gives loss of 0.15121880467461823\n",
      "The 34947 th iteration gives loss of 0.15121721044947273\n",
      "The 34948 th iteration gives loss of 0.15121561628633529\n",
      "The 34949 th iteration gives loss of 0.15121402218520735\n",
      "The 34950 th iteration gives loss of 0.15121242814607877\n",
      "The 34951 th iteration gives loss of 0.1512108341689356\n",
      "The 34952 th iteration gives loss of 0.15120924025378246\n",
      "The 34953 th iteration gives loss of 0.15120764640060075\n",
      "The 34954 th iteration gives loss of 0.15120605260937722\n",
      "The 34955 th iteration gives loss of 0.15120445888012993\n",
      "The 34956 th iteration gives loss of 0.15120286521281454\n",
      "The 34957 th iteration gives loss of 0.1512012716074542\n",
      "The 34958 th iteration gives loss of 0.15119967806401627\n",
      "The 34959 th iteration gives loss of 0.1511980845825094\n",
      "The 34960 th iteration gives loss of 0.15119649116292067\n",
      "The 34961 th iteration gives loss of 0.15119489780524178\n",
      "The 34962 th iteration gives loss of 0.15119330450945603\n",
      "The 34963 th iteration gives loss of 0.15119171127556716\n",
      "The 34964 th iteration gives loss of 0.1511901181035658\n",
      "The 34965 th iteration gives loss of 0.1511885249934405\n",
      "The 34966 th iteration gives loss of 0.15118693194518015\n",
      "The 34967 th iteration gives loss of 0.15118533895878505\n",
      "The 34968 th iteration gives loss of 0.15118374603423396\n",
      "The 34969 th iteration gives loss of 0.15118215317152597\n",
      "The 34970 th iteration gives loss of 0.15118056037065847\n",
      "The 34971 th iteration gives loss of 0.15117896763162594\n",
      "The 34972 th iteration gives loss of 0.15117737495440325\n",
      "The 34973 th iteration gives loss of 0.15117578233898796\n",
      "The 34974 th iteration gives loss of 0.1511741897853751\n",
      "The 34975 th iteration gives loss of 0.15117259729356586\n",
      "The 34976 th iteration gives loss of 0.15117100486354798\n",
      "The 34977 th iteration gives loss of 0.15116941249529658\n",
      "The 34978 th iteration gives loss of 0.15116782018881486\n",
      "The 34979 th iteration gives loss of 0.1511662279440983\n",
      "The 34980 th iteration gives loss of 0.1511646357611322\n",
      "The 34981 th iteration gives loss of 0.15116304363991157\n",
      "The 34982 th iteration gives loss of 0.15116145158043343\n",
      "The 34983 th iteration gives loss of 0.15115985958268738\n",
      "The 34984 th iteration gives loss of 0.15115826764665727\n",
      "The 34985 th iteration gives loss of 0.15115667577233663\n",
      "The 34986 th iteration gives loss of 0.15115508395973395\n",
      "The 34987 th iteration gives loss of 0.15115349220882165\n",
      "The 34988 th iteration gives loss of 0.15115190051959723\n",
      "The 34989 th iteration gives loss of 0.15115030889204695\n",
      "The 34990 th iteration gives loss of 0.151148717326174\n",
      "The 34991 th iteration gives loss of 0.15114712582196696\n",
      "The 34992 th iteration gives loss of 0.15114553437941816\n",
      "The 34993 th iteration gives loss of 0.151143942998517\n",
      "The 34994 th iteration gives loss of 0.15114235167925383\n",
      "The 34995 th iteration gives loss of 0.15114076042162497\n",
      "The 34996 th iteration gives loss of 0.15113916922561937\n",
      "The 34997 th iteration gives loss of 0.1511375780912282\n",
      "The 34998 th iteration gives loss of 0.15113598701844333\n",
      "The 34999 th iteration gives loss of 0.15113439600725426\n",
      "The 35000 th iteration gives loss of 0.15113280505765728\n",
      "The 35001 th iteration gives loss of 0.15113121416964942\n",
      "The 35002 th iteration gives loss of 0.1511296233432165\n",
      "The 35003 th iteration gives loss of 0.15112803257834473\n",
      "The 35004 th iteration gives loss of 0.15112644187503274\n",
      "The 35005 th iteration gives loss of 0.15112485123327468\n",
      "The 35006 th iteration gives loss of 0.15112326065305776\n",
      "The 35007 th iteration gives loss of 0.15112167013437613\n",
      "The 35008 th iteration gives loss of 0.15112007967722588\n",
      "The 35009 th iteration gives loss of 0.15111848928158647\n",
      "The 35010 th iteration gives loss of 0.15111689894745434\n",
      "The 35011 th iteration gives loss of 0.15111530867483045\n",
      "The 35012 th iteration gives loss of 0.1511137184636921\n",
      "The 35013 th iteration gives loss of 0.15111212831404255\n",
      "The 35014 th iteration gives loss of 0.1511105382258771\n",
      "The 35015 th iteration gives loss of 0.15110894819918064\n",
      "The 35016 th iteration gives loss of 0.15110735823393423\n",
      "The 35017 th iteration gives loss of 0.15110576833014996\n",
      "The 35018 th iteration gives loss of 0.1511041784878116\n",
      "The 35019 th iteration gives loss of 0.1511025887069091\n",
      "The 35020 th iteration gives loss of 0.15110099898743687\n",
      "The 35021 th iteration gives loss of 0.15109940932938318\n",
      "The 35022 th iteration gives loss of 0.1510978197327413\n",
      "The 35023 th iteration gives loss of 0.15109623019750879\n",
      "The 35024 th iteration gives loss of 0.1510946407236739\n",
      "The 35025 th iteration gives loss of 0.1510930513112223\n",
      "The 35026 th iteration gives loss of 0.1510914619601457\n",
      "The 35027 th iteration gives loss of 0.15108987267044655\n",
      "The 35028 th iteration gives loss of 0.15108828344212077\n",
      "The 35029 th iteration gives loss of 0.1510866942751421\n",
      "The 35030 th iteration gives loss of 0.15108510516951146\n",
      "The 35031 th iteration gives loss of 0.15108351612522306\n",
      "The 35032 th iteration gives loss of 0.15108192714227053\n",
      "The 35033 th iteration gives loss of 0.15108033822063563\n",
      "The 35034 th iteration gives loss of 0.15107874936031881\n",
      "The 35035 th iteration gives loss of 0.15107716056131182\n",
      "The 35036 th iteration gives loss of 0.15107557182360232\n",
      "The 35037 th iteration gives loss of 0.15107398314718054\n",
      "The 35038 th iteration gives loss of 0.15107239453204632\n",
      "The 35039 th iteration gives loss of 0.15107080597819053\n",
      "The 35040 th iteration gives loss of 0.1510692174855982\n",
      "The 35041 th iteration gives loss of 0.15106762905427076\n",
      "The 35042 th iteration gives loss of 0.15106604068418697\n",
      "The 35043 th iteration gives loss of 0.15106445237535465\n",
      "The 35044 th iteration gives loss of 0.15106286412774647\n",
      "The 35045 th iteration gives loss of 0.151061275941367\n",
      "The 35046 th iteration gives loss of 0.15105968781620333\n",
      "The 35047 th iteration gives loss of 0.15105809975226814\n",
      "The 35048 th iteration gives loss of 0.1510565117495259\n",
      "The 35049 th iteration gives loss of 0.15105492380796898\n",
      "The 35050 th iteration gives loss of 0.15105333592761194\n",
      "The 35051 th iteration gives loss of 0.1510517481084238\n",
      "The 35052 th iteration gives loss of 0.15105016035041569\n",
      "The 35053 th iteration gives loss of 0.1510485726535668\n",
      "The 35054 th iteration gives loss of 0.1510469850178695\n",
      "The 35055 th iteration gives loss of 0.15104539744331932\n",
      "The 35056 th iteration gives loss of 0.15104380992990213\n",
      "The 35057 th iteration gives loss of 0.151042222477627\n",
      "The 35058 th iteration gives loss of 0.15104063508646814\n",
      "The 35059 th iteration gives loss of 0.15103904775642593\n",
      "The 35060 th iteration gives loss of 0.1510374604874945\n",
      "The 35061 th iteration gives loss of 0.15103587327965232\n",
      "The 35062 th iteration gives loss of 0.15103428613289915\n",
      "The 35063 th iteration gives loss of 0.15103269904723124\n",
      "The 35064 th iteration gives loss of 0.15103111202263925\n",
      "The 35065 th iteration gives loss of 0.15102952505911615\n",
      "The 35066 th iteration gives loss of 0.1510279381566396\n",
      "The 35067 th iteration gives loss of 0.15102635131522146\n",
      "The 35068 th iteration gives loss of 0.1510247645348385\n",
      "The 35069 th iteration gives loss of 0.1510231778154959\n",
      "The 35070 th iteration gives loss of 0.1510215911571879\n",
      "The 35071 th iteration gives loss of 0.1510200045598886\n",
      "The 35072 th iteration gives loss of 0.15101841802359692\n",
      "The 35073 th iteration gives loss of 0.1510168315483033\n",
      "The 35074 th iteration gives loss of 0.1510152451340145\n",
      "The 35075 th iteration gives loss of 0.15101365878070327\n",
      "The 35076 th iteration gives loss of 0.1510120724883661\n",
      "The 35077 th iteration gives loss of 0.15101048625700744\n",
      "The 35078 th iteration gives loss of 0.15100890008660942\n",
      "The 35079 th iteration gives loss of 0.1510073139771628\n",
      "The 35080 th iteration gives loss of 0.15100572792865832\n",
      "The 35081 th iteration gives loss of 0.1510041419410934\n",
      "The 35082 th iteration gives loss of 0.15100255601445653\n",
      "The 35083 th iteration gives loss of 0.15100097014874794\n",
      "The 35084 th iteration gives loss of 0.1509993843439485\n",
      "The 35085 th iteration gives loss of 0.1509977986000562\n",
      "The 35086 th iteration gives loss of 0.15099621291706203\n",
      "The 35087 th iteration gives loss of 0.15099462729495308\n",
      "The 35088 th iteration gives loss of 0.15099304173372796\n",
      "The 35089 th iteration gives loss of 0.15099145623337196\n",
      "The 35090 th iteration gives loss of 0.15098987079388962\n",
      "The 35091 th iteration gives loss of 0.15098828541525783\n",
      "The 35092 th iteration gives loss of 0.15098670009748236\n",
      "The 35093 th iteration gives loss of 0.15098511484054336\n",
      "The 35094 th iteration gives loss of 0.15098352964443656\n",
      "The 35095 th iteration gives loss of 0.1509819445091602\n",
      "The 35096 th iteration gives loss of 0.15098035943469584\n",
      "The 35097 th iteration gives loss of 0.150978774421044\n",
      "The 35098 th iteration gives loss of 0.15097718946820024\n",
      "The 35099 th iteration gives loss of 0.15097560457613193\n",
      "The 35100 th iteration gives loss of 0.15097401974486208\n",
      "The 35101 th iteration gives loss of 0.15097243497436594\n",
      "The 35102 th iteration gives loss of 0.15097085026464058\n",
      "The 35103 th iteration gives loss of 0.15096926561567567\n",
      "The 35104 th iteration gives loss of 0.1509676810274662\n",
      "The 35105 th iteration gives loss of 0.15096609650000123\n",
      "The 35106 th iteration gives loss of 0.15096451203327124\n",
      "The 35107 th iteration gives loss of 0.15096292762727617\n",
      "The 35108 th iteration gives loss of 0.1509613432820003\n",
      "The 35109 th iteration gives loss of 0.1509597589974302\n",
      "The 35110 th iteration gives loss of 0.15095817477357146\n",
      "The 35111 th iteration gives loss of 0.15095659061041544\n",
      "The 35112 th iteration gives loss of 0.1509550065079482\n",
      "The 35113 th iteration gives loss of 0.1509534224661555\n",
      "The 35114 th iteration gives loss of 0.15095183848504545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 35115 th iteration gives loss of 0.15095025456459882\n",
      "The 35116 th iteration gives loss of 0.1509486707048057\n",
      "The 35117 th iteration gives loss of 0.15094708690566444\n",
      "The 35118 th iteration gives loss of 0.1509455031671625\n",
      "The 35119 th iteration gives loss of 0.15094391948929214\n",
      "The 35120 th iteration gives loss of 0.1509423358720546\n",
      "The 35121 th iteration gives loss of 0.15094075231543141\n",
      "The 35122 th iteration gives loss of 0.1509391688194207\n",
      "The 35123 th iteration gives loss of 0.15093758538400795\n",
      "The 35124 th iteration gives loss of 0.1509360020091887\n",
      "The 35125 th iteration gives loss of 0.1509344186949585\n",
      "The 35126 th iteration gives loss of 0.15093283544130756\n",
      "The 35127 th iteration gives loss of 0.15093125224822354\n",
      "The 35128 th iteration gives loss of 0.15092966911570657\n",
      "The 35129 th iteration gives loss of 0.15092808604373698\n",
      "The 35130 th iteration gives loss of 0.1509265030323103\n",
      "The 35131 th iteration gives loss of 0.1509249200814318\n",
      "The 35132 th iteration gives loss of 0.15092333719108417\n",
      "The 35133 th iteration gives loss of 0.15092175436125133\n",
      "The 35134 th iteration gives loss of 0.1509201715919357\n",
      "The 35135 th iteration gives loss of 0.15091858888313134\n",
      "The 35136 th iteration gives loss of 0.15091700623481516\n",
      "The 35137 th iteration gives loss of 0.1509154236469951\n",
      "The 35138 th iteration gives loss of 0.1509138411196656\n",
      "The 35139 th iteration gives loss of 0.1509122586528015\n",
      "The 35140 th iteration gives loss of 0.15091067624640273\n",
      "The 35141 th iteration gives loss of 0.1509090939004707\n",
      "The 35142 th iteration gives loss of 0.15090751161497906\n",
      "The 35143 th iteration gives loss of 0.15090592938993821\n",
      "The 35144 th iteration gives loss of 0.15090434722533264\n",
      "The 35145 th iteration gives loss of 0.15090276512115203\n",
      "The 35146 th iteration gives loss of 0.1509011830773886\n",
      "The 35147 th iteration gives loss of 0.15089960109403344\n",
      "The 35148 th iteration gives loss of 0.15089801917108114\n",
      "The 35149 th iteration gives loss of 0.15089643730852664\n",
      "The 35150 th iteration gives loss of 0.15089485550636017\n",
      "The 35151 th iteration gives loss of 0.15089327376458309\n",
      "The 35152 th iteration gives loss of 0.15089169208316414\n",
      "The 35153 th iteration gives loss of 0.15089011046211048\n",
      "The 35154 th iteration gives loss of 0.15088852890141727\n",
      "The 35155 th iteration gives loss of 0.1508869474010745\n",
      "The 35156 th iteration gives loss of 0.15088536596106744\n",
      "The 35157 th iteration gives loss of 0.15088378458138563\n",
      "The 35158 th iteration gives loss of 0.15088220326203056\n",
      "The 35159 th iteration gives loss of 0.1508806220029909\n",
      "The 35160 th iteration gives loss of 0.15087904080426537\n",
      "The 35161 th iteration gives loss of 0.15087745966583835\n",
      "The 35162 th iteration gives loss of 0.1508758785877029\n",
      "The 35163 th iteration gives loss of 0.15087429756984783\n",
      "The 35164 th iteration gives loss of 0.15087271661227758\n",
      "The 35165 th iteration gives loss of 0.15087113571497007\n",
      "The 35166 th iteration gives loss of 0.1508695548779233\n",
      "The 35167 th iteration gives loss of 0.15086797410113242\n",
      "The 35168 th iteration gives loss of 0.15086639338458296\n",
      "The 35169 th iteration gives loss of 0.15086481272827346\n",
      "The 35170 th iteration gives loss of 0.15086323213219\n",
      "The 35171 th iteration gives loss of 0.15086165159632653\n",
      "The 35172 th iteration gives loss of 0.15086007112068128\n",
      "The 35173 th iteration gives loss of 0.1508584907052352\n",
      "The 35174 th iteration gives loss of 0.15085691034998727\n",
      "The 35175 th iteration gives loss of 0.1508553300549337\n",
      "The 35176 th iteration gives loss of 0.15085374982005761\n",
      "The 35177 th iteration gives loss of 0.15085216964535247\n",
      "The 35178 th iteration gives loss of 0.15085058953082298\n",
      "The 35179 th iteration gives loss of 0.15084900947644533\n",
      "The 35180 th iteration gives loss of 0.15084742948221644\n",
      "The 35181 th iteration gives loss of 0.15084584954813202\n",
      "The 35182 th iteration gives loss of 0.15084426967418224\n",
      "The 35183 th iteration gives loss of 0.15084268986035174\n",
      "The 35184 th iteration gives loss of 0.15084111010664752\n",
      "The 35185 th iteration gives loss of 0.15083953041305473\n",
      "The 35186 th iteration gives loss of 0.15083795077955997\n",
      "The 35187 th iteration gives loss of 0.15083637120616541\n",
      "The 35188 th iteration gives loss of 0.15083479169285643\n",
      "The 35189 th iteration gives loss of 0.15083321223961893\n",
      "The 35190 th iteration gives loss of 0.1508316328464649\n",
      "The 35191 th iteration gives loss of 0.1508300535133666\n",
      "The 35192 th iteration gives loss of 0.1508284742403243\n",
      "The 35193 th iteration gives loss of 0.15082689502733565\n",
      "The 35194 th iteration gives loss of 0.15082531587437126\n",
      "The 35195 th iteration gives loss of 0.15082373678144556\n",
      "The 35196 th iteration gives loss of 0.15082215774854763\n",
      "The 35197 th iteration gives loss of 0.15082057877565858\n",
      "The 35198 th iteration gives loss of 0.1508189998627854\n",
      "The 35199 th iteration gives loss of 0.15081742100990744\n",
      "The 35200 th iteration gives loss of 0.15081584221702465\n",
      "The 35201 th iteration gives loss of 0.15081426348412877\n",
      "The 35202 th iteration gives loss of 0.15081268481120783\n",
      "The 35203 th iteration gives loss of 0.1508111061982545\n",
      "The 35204 th iteration gives loss of 0.1508095276452621\n",
      "The 35205 th iteration gives loss of 0.15080794915222323\n",
      "The 35206 th iteration gives loss of 0.15080637071912445\n",
      "The 35207 th iteration gives loss of 0.1508047923459674\n",
      "The 35208 th iteration gives loss of 0.15080321403274227\n",
      "The 35209 th iteration gives loss of 0.15080163577943387\n",
      "The 35210 th iteration gives loss of 0.15080005758604817\n",
      "The 35211 th iteration gives loss of 0.15079847945256108\n",
      "The 35212 th iteration gives loss of 0.15079690137897303\n",
      "The 35213 th iteration gives loss of 0.15079532336527732\n",
      "The 35214 th iteration gives loss of 0.1507937454114588\n",
      "The 35215 th iteration gives loss of 0.15079216751751273\n",
      "The 35216 th iteration gives loss of 0.1507905896834372\n",
      "The 35217 th iteration gives loss of 0.1507890119092212\n",
      "The 35218 th iteration gives loss of 0.15078743419485482\n",
      "The 35219 th iteration gives loss of 0.15078585654033774\n",
      "The 35220 th iteration gives loss of 0.15078427894565113\n",
      "The 35221 th iteration gives loss of 0.15078270141079908\n",
      "The 35222 th iteration gives loss of 0.1507811239357578\n",
      "The 35223 th iteration gives loss of 0.15077954652053338\n",
      "The 35224 th iteration gives loss of 0.15077796916510688\n",
      "The 35225 th iteration gives loss of 0.15077639186948333\n",
      "The 35226 th iteration gives loss of 0.1507748146336463\n",
      "The 35227 th iteration gives loss of 0.15077323745758908\n",
      "The 35228 th iteration gives loss of 0.15077166034129763\n",
      "The 35229 th iteration gives loss of 0.15077008328477337\n",
      "The 35230 th iteration gives loss of 0.15076850628801144\n",
      "The 35231 th iteration gives loss of 0.15076692935099337\n",
      "The 35232 th iteration gives loss of 0.1507653524737145\n",
      "The 35233 th iteration gives loss of 0.15076377565618296\n",
      "The 35234 th iteration gives loss of 0.15076219889836287\n",
      "The 35235 th iteration gives loss of 0.1507606222002629\n",
      "The 35236 th iteration gives loss of 0.15075904556188052\n",
      "The 35237 th iteration gives loss of 0.15075746898319928\n",
      "The 35238 th iteration gives loss of 0.15075589246421167\n",
      "The 35239 th iteration gives loss of 0.1507543160049089\n",
      "The 35240 th iteration gives loss of 0.15075273960528215\n",
      "The 35241 th iteration gives loss of 0.15075116326532614\n",
      "The 35242 th iteration gives loss of 0.1507495869850375\n",
      "The 35243 th iteration gives loss of 0.15074801076439845\n",
      "The 35244 th iteration gives loss of 0.1507464346034085\n",
      "The 35245 th iteration gives loss of 0.15074485850205888\n",
      "The 35246 th iteration gives loss of 0.15074328246034333\n",
      "The 35247 th iteration gives loss of 0.15074170647825125\n",
      "The 35248 th iteration gives loss of 0.15074013055576937\n",
      "The 35249 th iteration gives loss of 0.15073855469290592\n",
      "The 35250 th iteration gives loss of 0.15073697888963536\n",
      "The 35251 th iteration gives loss of 0.1507354031459604\n",
      "The 35252 th iteration gives loss of 0.15073382746187478\n",
      "The 35253 th iteration gives loss of 0.15073225183735744\n",
      "The 35254 th iteration gives loss of 0.15073067627241343\n",
      "The 35255 th iteration gives loss of 0.15072910076702928\n",
      "The 35256 th iteration gives loss of 0.15072752532120545\n",
      "The 35257 th iteration gives loss of 0.15072594993491598\n",
      "The 35258 th iteration gives loss of 0.15072437460817806\n",
      "The 35259 th iteration gives loss of 0.15072279934095628\n",
      "The 35260 th iteration gives loss of 0.15072122413326403\n",
      "The 35261 th iteration gives loss of 0.15071964898508788\n",
      "The 35262 th iteration gives loss of 0.15071807389641637\n",
      "The 35263 th iteration gives loss of 0.15071649886725066\n",
      "The 35264 th iteration gives loss of 0.15071492389757338\n",
      "The 35265 th iteration gives loss of 0.15071334898737807\n",
      "The 35266 th iteration gives loss of 0.1507117741366592\n",
      "The 35267 th iteration gives loss of 0.15071019934541238\n",
      "The 35268 th iteration gives loss of 0.1507086246136106\n",
      "The 35269 th iteration gives loss of 0.15070704994127687\n",
      "The 35270 th iteration gives loss of 0.15070547532838097\n",
      "The 35271 th iteration gives loss of 0.1507039007749277\n",
      "The 35272 th iteration gives loss of 0.1507023262809\n",
      "The 35273 th iteration gives loss of 0.15070075184629564\n",
      "The 35274 th iteration gives loss of 0.15069917747109693\n",
      "The 35275 th iteration gives loss of 0.1506976031553073\n",
      "The 35276 th iteration gives loss of 0.15069602889892172\n",
      "The 35277 th iteration gives loss of 0.15069445470192122\n",
      "The 35278 th iteration gives loss of 0.15069288056431154\n",
      "The 35279 th iteration gives loss of 0.15069130648607157\n",
      "The 35280 th iteration gives loss of 0.15068973246719816\n",
      "The 35281 th iteration gives loss of 0.15068815850767955\n",
      "The 35282 th iteration gives loss of 0.15068658460751586\n",
      "The 35283 th iteration gives loss of 0.15068501076669577\n",
      "The 35284 th iteration gives loss of 0.15068343698521106\n",
      "The 35285 th iteration gives loss of 0.15068186326305294\n",
      "The 35286 th iteration gives loss of 0.1506802896002168\n",
      "The 35287 th iteration gives loss of 0.15067871599669808\n",
      "The 35288 th iteration gives loss of 0.15067714245247774\n",
      "The 35289 th iteration gives loss of 0.15067556896755616\n",
      "The 35290 th iteration gives loss of 0.15067399554192767\n",
      "The 35291 th iteration gives loss of 0.15067242217557505\n",
      "The 35292 th iteration gives loss of 0.15067084886849705\n",
      "The 35293 th iteration gives loss of 0.1506692756206861\n",
      "The 35294 th iteration gives loss of 0.1506677024321352\n",
      "The 35295 th iteration gives loss of 0.15066612930283022\n",
      "The 35296 th iteration gives loss of 0.15066455623278044\n",
      "The 35297 th iteration gives loss of 0.15066298322195384\n",
      "The 35298 th iteration gives loss of 0.15066141027035812\n",
      "The 35299 th iteration gives loss of 0.1506598373779833\n",
      "The 35300 th iteration gives loss of 0.15065826454481576\n",
      "The 35301 th iteration gives loss of 0.1506566917708557\n",
      "The 35302 th iteration gives loss of 0.15065511905608717\n",
      "The 35303 th iteration gives loss of 0.15065354640051515\n",
      "The 35304 th iteration gives loss of 0.15065197380411766\n",
      "The 35305 th iteration gives loss of 0.15065040126690074\n",
      "The 35306 th iteration gives loss of 0.15064882878884392\n",
      "The 35307 th iteration gives loss of 0.15064725636994347\n",
      "The 35308 th iteration gives loss of 0.15064568401019715\n",
      "The 35309 th iteration gives loss of 0.1506441117095874\n",
      "The 35310 th iteration gives loss of 0.15064253946812275\n",
      "The 35311 th iteration gives loss of 0.15064096728577503\n",
      "The 35312 th iteration gives loss of 0.15063939516254923\n",
      "The 35313 th iteration gives loss of 0.15063782309843582\n",
      "The 35314 th iteration gives loss of 0.15063625109342255\n",
      "The 35315 th iteration gives loss of 0.15063467914751266\n",
      "The 35316 th iteration gives loss of 0.15063310726068743\n",
      "The 35317 th iteration gives loss of 0.1506315354329387\n",
      "The 35318 th iteration gives loss of 0.15062996366426237\n",
      "The 35319 th iteration gives loss of 0.1506283919546576\n",
      "The 35320 th iteration gives loss of 0.15062682030410365\n",
      "The 35321 th iteration gives loss of 0.1506252487126018\n",
      "The 35322 th iteration gives loss of 0.15062367718013864\n",
      "The 35323 th iteration gives loss of 0.1506221057067137\n",
      "The 35324 th iteration gives loss of 0.15062053429231817\n",
      "The 35325 th iteration gives loss of 0.15061896293693283\n",
      "The 35326 th iteration gives loss of 0.15061739164056503\n",
      "The 35327 th iteration gives loss of 0.15061582040320015\n",
      "The 35328 th iteration gives loss of 0.150614249224828\n",
      "The 35329 th iteration gives loss of 0.15061267810544038\n",
      "The 35330 th iteration gives loss of 0.15061110704504033\n",
      "The 35331 th iteration gives loss of 0.15060953604360722\n",
      "The 35332 th iteration gives loss of 0.1506079651011375\n",
      "The 35333 th iteration gives loss of 0.15060639421762997\n",
      "The 35334 th iteration gives loss of 0.15060482339306805\n",
      "The 35335 th iteration gives loss of 0.15060325262745508\n",
      "The 35336 th iteration gives loss of 0.15060168192077253\n",
      "The 35337 th iteration gives loss of 0.15060011127301384\n",
      "The 35338 th iteration gives loss of 0.1505985406841755\n",
      "The 35339 th iteration gives loss of 0.15059697015425083\n",
      "The 35340 th iteration gives loss of 0.15059539968321947\n",
      "The 35341 th iteration gives loss of 0.1505938292710896\n",
      "The 35342 th iteration gives loss of 0.15059225891784836\n",
      "The 35343 th iteration gives loss of 0.15059068862348943\n",
      "The 35344 th iteration gives loss of 0.15058911838799557\n",
      "The 35345 th iteration gives loss of 0.15058754821137185\n",
      "The 35346 th iteration gives loss of 0.15058597809359858\n",
      "The 35347 th iteration gives loss of 0.15058440803468306\n",
      "The 35348 th iteration gives loss of 0.150582838034608\n",
      "The 35349 th iteration gives loss of 0.15058126809336425\n",
      "The 35350 th iteration gives loss of 0.15057969821094866\n",
      "The 35351 th iteration gives loss of 0.1505781283873469\n",
      "The 35352 th iteration gives loss of 0.1505765586225543\n",
      "The 35353 th iteration gives loss of 0.15057498891656962\n",
      "The 35354 th iteration gives loss of 0.1505734192693831\n",
      "The 35355 th iteration gives loss of 0.15057184968098553\n",
      "The 35356 th iteration gives loss of 0.1505702801513658\n",
      "The 35357 th iteration gives loss of 0.1505687106805142\n",
      "The 35358 th iteration gives loss of 0.1505671412684276\n",
      "The 35359 th iteration gives loss of 0.1505655719151062\n",
      "The 35360 th iteration gives loss of 0.1505640026205231\n",
      "The 35361 th iteration gives loss of 0.1505624333846939\n",
      "The 35362 th iteration gives loss of 0.150560864207588\n",
      "The 35363 th iteration gives loss of 0.15055929508921168\n",
      "The 35364 th iteration gives loss of 0.15055772602956247\n",
      "The 35365 th iteration gives loss of 0.15055615702861544\n",
      "The 35366 th iteration gives loss of 0.1505545880863785\n",
      "The 35367 th iteration gives loss of 0.1505530192028246\n",
      "The 35368 th iteration gives loss of 0.15055145037797432\n",
      "The 35369 th iteration gives loss of 0.15054988161179342\n",
      "The 35370 th iteration gives loss of 0.15054831290429477\n",
      "The 35371 th iteration gives loss of 0.1505467442554505\n",
      "The 35372 th iteration gives loss of 0.150545175665268\n",
      "The 35373 th iteration gives loss of 0.15054360713373632\n",
      "The 35374 th iteration gives loss of 0.15054203866085142\n",
      "The 35375 th iteration gives loss of 0.15054047024659986\n",
      "The 35376 th iteration gives loss of 0.1505389018909641\n",
      "The 35377 th iteration gives loss of 0.15053733359395924\n",
      "The 35378 th iteration gives loss of 0.1505357653555615\n",
      "The 35379 th iteration gives loss of 0.15053419717576638\n",
      "The 35380 th iteration gives loss of 0.15053262905457357\n",
      "The 35381 th iteration gives loss of 0.15053106099196814\n",
      "The 35382 th iteration gives loss of 0.15052949298794394\n",
      "The 35383 th iteration gives loss of 0.15052792504248877\n",
      "The 35384 th iteration gives loss of 0.15052635715560486\n",
      "The 35385 th iteration gives loss of 0.15052478932726743\n",
      "The 35386 th iteration gives loss of 0.15052322155749365\n",
      "The 35387 th iteration gives loss of 0.15052165384625907\n",
      "The 35388 th iteration gives loss of 0.15052008619354731\n",
      "The 35389 th iteration gives loss of 0.15051851859938847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 35390 th iteration gives loss of 0.15051695106373358\n",
      "The 35391 th iteration gives loss of 0.15051538358658217\n",
      "The 35392 th iteration gives loss of 0.15051381616794968\n",
      "The 35393 th iteration gives loss of 0.1505122488078165\n",
      "The 35394 th iteration gives loss of 0.15051068150616856\n",
      "The 35395 th iteration gives loss of 0.15050911426300123\n",
      "The 35396 th iteration gives loss of 0.1505075470783029\n",
      "The 35397 th iteration gives loss of 0.15050597995207696\n",
      "The 35398 th iteration gives loss of 0.15050441288431043\n",
      "The 35399 th iteration gives loss of 0.1505028458749951\n",
      "The 35400 th iteration gives loss of 0.15050127892411863\n",
      "The 35401 th iteration gives loss of 0.150499712031681\n",
      "The 35402 th iteration gives loss of 0.15049814519766477\n",
      "The 35403 th iteration gives loss of 0.15049657842208047\n",
      "The 35404 th iteration gives loss of 0.15049501170491103\n",
      "The 35405 th iteration gives loss of 0.15049344504614384\n",
      "The 35406 th iteration gives loss of 0.15049187844576326\n",
      "The 35407 th iteration gives loss of 0.15049031190378598\n",
      "The 35408 th iteration gives loss of 0.15048874542018675\n",
      "The 35409 th iteration gives loss of 0.15048717899496866\n",
      "The 35410 th iteration gives loss of 0.15048561262810953\n",
      "The 35411 th iteration gives loss of 0.15048404631961365\n",
      "The 35412 th iteration gives loss of 0.15048248006947698\n",
      "The 35413 th iteration gives loss of 0.15048091387767662\n",
      "The 35414 th iteration gives loss of 0.15047934774421698\n",
      "The 35415 th iteration gives loss of 0.15047778166908013\n",
      "The 35416 th iteration gives loss of 0.15047621565227018\n",
      "The 35417 th iteration gives loss of 0.15047464969377616\n",
      "The 35418 th iteration gives loss of 0.15047308379358543\n",
      "The 35419 th iteration gives loss of 0.15047151795169822\n",
      "The 35420 th iteration gives loss of 0.15046995216810155\n",
      "The 35421 th iteration gives loss of 0.15046838644278723\n",
      "The 35422 th iteration gives loss of 0.15046682077575324\n",
      "The 35423 th iteration gives loss of 0.1504652551669829\n",
      "The 35424 th iteration gives loss of 0.1504636896164671\n",
      "The 35425 th iteration gives loss of 0.15046212412421545\n",
      "The 35426 th iteration gives loss of 0.1504605586902023\n",
      "The 35427 th iteration gives loss of 0.15045899331442744\n",
      "The 35428 th iteration gives loss of 0.15045742799688797\n",
      "The 35429 th iteration gives loss of 0.15045586273758216\n",
      "The 35430 th iteration gives loss of 0.1504542975364812\n",
      "The 35431 th iteration gives loss of 0.15045273239358584\n",
      "The 35432 th iteration gives loss of 0.15045116730889446\n",
      "The 35433 th iteration gives loss of 0.15044960228239646\n",
      "The 35434 th iteration gives loss of 0.15044803731407905\n",
      "The 35435 th iteration gives loss of 0.15044647240394582\n",
      "The 35436 th iteration gives loss of 0.15044490755197465\n",
      "The 35437 th iteration gives loss of 0.15044334275817006\n",
      "The 35438 th iteration gives loss of 0.15044177802252426\n",
      "The 35439 th iteration gives loss of 0.1504402133450205\n",
      "The 35440 th iteration gives loss of 0.1504386487256618\n",
      "The 35441 th iteration gives loss of 0.15043708416443424\n",
      "The 35442 th iteration gives loss of 0.15043551966132956\n",
      "The 35443 th iteration gives loss of 0.15043395521633632\n",
      "The 35444 th iteration gives loss of 0.15043239082945833\n",
      "The 35445 th iteration gives loss of 0.15043082650068532\n",
      "The 35446 th iteration gives loss of 0.15042926223000314\n",
      "The 35447 th iteration gives loss of 0.15042769801740227\n",
      "The 35448 th iteration gives loss of 0.1504261338628899\n",
      "The 35449 th iteration gives loss of 0.150424569766445\n",
      "The 35450 th iteration gives loss of 0.15042300572806797\n",
      "The 35451 th iteration gives loss of 0.15042144174774116\n",
      "The 35452 th iteration gives loss of 0.15041987782546598\n",
      "The 35453 th iteration gives loss of 0.1504183139612352\n",
      "The 35454 th iteration gives loss of 0.1504167501550328\n",
      "The 35455 th iteration gives loss of 0.15041518640686252\n",
      "The 35456 th iteration gives loss of 0.1504136227167056\n",
      "The 35457 th iteration gives loss of 0.15041205908456423\n",
      "The 35458 th iteration gives loss of 0.15041049551042007\n",
      "The 35459 th iteration gives loss of 0.15040893199428504\n",
      "The 35460 th iteration gives loss of 0.15040736853613226\n",
      "The 35461 th iteration gives loss of 0.15040580513595844\n",
      "The 35462 th iteration gives loss of 0.15040424179375597\n",
      "The 35463 th iteration gives loss of 0.150402678509526\n",
      "The 35464 th iteration gives loss of 0.15040111528324598\n",
      "The 35465 th iteration gives loss of 0.15039955211492584\n",
      "The 35466 th iteration gives loss of 0.15039798900454282\n",
      "The 35467 th iteration gives loss of 0.15039642595209426\n",
      "The 35468 th iteration gives loss of 0.15039486295757543\n",
      "The 35469 th iteration gives loss of 0.15039330002097961\n",
      "The 35470 th iteration gives loss of 0.150391737142299\n",
      "The 35471 th iteration gives loss of 0.15039017432152388\n",
      "The 35472 th iteration gives loss of 0.150388611558646\n",
      "The 35473 th iteration gives loss of 0.15038704885366028\n",
      "The 35474 th iteration gives loss of 0.15038548620654663\n",
      "The 35475 th iteration gives loss of 0.15038392361732042\n",
      "The 35476 th iteration gives loss of 0.15038236108595673\n",
      "The 35477 th iteration gives loss of 0.15038079861245582\n",
      "The 35478 th iteration gives loss of 0.15037923619680454\n",
      "The 35479 th iteration gives loss of 0.1503776738389991\n",
      "The 35480 th iteration gives loss of 0.15037611153903593\n",
      "The 35481 th iteration gives loss of 0.15037454929689886\n",
      "The 35482 th iteration gives loss of 0.15037298711258854\n",
      "The 35483 th iteration gives loss of 0.15037142498609163\n",
      "The 35484 th iteration gives loss of 0.15036986291739987\n",
      "The 35485 th iteration gives loss of 0.15036830090651035\n",
      "The 35486 th iteration gives loss of 0.1503667389534165\n",
      "The 35487 th iteration gives loss of 0.15036517705810393\n",
      "The 35488 th iteration gives loss of 0.15036361522056405\n",
      "The 35489 th iteration gives loss of 0.1503620534408039\n",
      "The 35490 th iteration gives loss of 0.1503604917188073\n",
      "The 35491 th iteration gives loss of 0.15035893005456244\n",
      "The 35492 th iteration gives loss of 0.15035736844805933\n",
      "The 35493 th iteration gives loss of 0.15035580689930106\n",
      "The 35494 th iteration gives loss of 0.15035424540828324\n",
      "The 35495 th iteration gives loss of 0.1503526839749817\n",
      "The 35496 th iteration gives loss of 0.1503511225994019\n",
      "The 35497 th iteration gives loss of 0.15034956128153004\n",
      "The 35498 th iteration gives loss of 0.15034800002135815\n",
      "The 35499 th iteration gives loss of 0.15034643881888168\n",
      "The 35500 th iteration gives loss of 0.1503448776740995\n",
      "The 35501 th iteration gives loss of 0.15034331658698788\n",
      "The 35502 th iteration gives loss of 0.15034175555755444\n",
      "The 35503 th iteration gives loss of 0.15034019458578982\n",
      "The 35504 th iteration gives loss of 0.15033863367167602\n",
      "The 35505 th iteration gives loss of 0.1503370728152151\n",
      "The 35506 th iteration gives loss of 0.15033551201640077\n",
      "The 35507 th iteration gives loss of 0.15033395127521498\n",
      "The 35508 th iteration gives loss of 0.15033239059164957\n",
      "The 35509 th iteration gives loss of 0.15033082996572328\n",
      "The 35510 th iteration gives loss of 0.1503292693973945\n",
      "The 35511 th iteration gives loss of 0.15032770888668165\n",
      "The 35512 th iteration gives loss of 0.15032614843356726\n",
      "The 35513 th iteration gives loss of 0.15032458803803272\n",
      "The 35514 th iteration gives loss of 0.15032302770008485\n",
      "The 35515 th iteration gives loss of 0.1503214674197169\n",
      "The 35516 th iteration gives loss of 0.1503199071969105\n",
      "The 35517 th iteration gives loss of 0.1503183470316701\n",
      "The 35518 th iteration gives loss of 0.1503167869239761\n",
      "The 35519 th iteration gives loss of 0.1503152268738274\n",
      "The 35520 th iteration gives loss of 0.15031366688121786\n",
      "The 35521 th iteration gives loss of 0.15031210694614308\n",
      "The 35522 th iteration gives loss of 0.15031054706859218\n",
      "The 35523 th iteration gives loss of 0.1503089872485491\n",
      "The 35524 th iteration gives loss of 0.15030742748602\n",
      "The 35525 th iteration gives loss of 0.15030586778099012\n",
      "The 35526 th iteration gives loss of 0.1503043081334504\n",
      "The 35527 th iteration gives loss of 0.1503027485433983\n",
      "The 35528 th iteration gives loss of 0.15030118901083142\n",
      "The 35529 th iteration gives loss of 0.15029962953572507\n",
      "The 35530 th iteration gives loss of 0.15029807011808163\n",
      "The 35531 th iteration gives loss of 0.15029651075789285\n",
      "The 35532 th iteration gives loss of 0.15029495145515995\n",
      "The 35533 th iteration gives loss of 0.15029339220986535\n",
      "The 35534 th iteration gives loss of 0.15029183302200302\n",
      "The 35535 th iteration gives loss of 0.15029027389156965\n",
      "The 35536 th iteration gives loss of 0.15028871481854894\n",
      "The 35537 th iteration gives loss of 0.15028715580294313\n",
      "The 35538 th iteration gives loss of 0.1502855968447329\n",
      "The 35539 th iteration gives loss of 0.15028403794392772\n",
      "The 35540 th iteration gives loss of 0.15028247910050824\n",
      "The 35541 th iteration gives loss of 0.15028092031447787\n",
      "The 35542 th iteration gives loss of 0.15027936158580518\n",
      "The 35543 th iteration gives loss of 0.15027780291451046\n",
      "The 35544 th iteration gives loss of 0.15027624430057165\n",
      "The 35545 th iteration gives loss of 0.15027468574398126\n",
      "The 35546 th iteration gives loss of 0.15027312724473516\n",
      "The 35547 th iteration gives loss of 0.15027156880283402\n",
      "The 35548 th iteration gives loss of 0.15027001041825203\n",
      "The 35549 th iteration gives loss of 0.1502684520909891\n",
      "The 35550 th iteration gives loss of 0.15026689382104697\n",
      "The 35551 th iteration gives loss of 0.15026533560840769\n",
      "The 35552 th iteration gives loss of 0.15026377745307035\n",
      "The 35553 th iteration gives loss of 0.1502622193550278\n",
      "The 35554 th iteration gives loss of 0.15026066131427476\n",
      "The 35555 th iteration gives loss of 0.15025910333078882\n",
      "The 35556 th iteration gives loss of 0.15025754540457256\n",
      "The 35557 th iteration gives loss of 0.1502559875356181\n",
      "The 35558 th iteration gives loss of 0.1502544297239217\n",
      "The 35559 th iteration gives loss of 0.1502528719694691\n",
      "The 35560 th iteration gives loss of 0.15025131427225097\n",
      "The 35561 th iteration gives loss of 0.1502497566322792\n",
      "The 35562 th iteration gives loss of 0.1502481990495245\n",
      "The 35563 th iteration gives loss of 0.15024664152398876\n",
      "The 35564 th iteration gives loss of 0.15024508405566306\n",
      "The 35565 th iteration gives loss of 0.1502435266445363\n",
      "The 35566 th iteration gives loss of 0.1502419692906036\n",
      "The 35567 th iteration gives loss of 0.15024041199386698\n",
      "The 35568 th iteration gives loss of 0.15023885475430268\n",
      "The 35569 th iteration gives loss of 0.15023729757191628\n",
      "The 35570 th iteration gives loss of 0.1502357404466953\n",
      "The 35571 th iteration gives loss of 0.15023418337862857\n",
      "The 35572 th iteration gives loss of 0.1502326263677131\n",
      "The 35573 th iteration gives loss of 0.15023106941394607\n",
      "The 35574 th iteration gives loss of 0.15022951251730843\n",
      "The 35575 th iteration gives loss of 0.15022795567779818\n",
      "The 35576 th iteration gives loss of 0.15022639889541722\n",
      "The 35577 th iteration gives loss of 0.15022484217014673\n",
      "The 35578 th iteration gives loss of 0.1502232855019801\n",
      "The 35579 th iteration gives loss of 0.1502217288909103\n",
      "The 35580 th iteration gives loss of 0.15022017233693516\n",
      "The 35581 th iteration gives loss of 0.15021861584004437\n",
      "The 35582 th iteration gives loss of 0.15021705940022703\n",
      "The 35583 th iteration gives loss of 0.15021550301749478\n",
      "The 35584 th iteration gives loss of 0.1502139466918082\n",
      "The 35585 th iteration gives loss of 0.15021239042317586\n",
      "The 35586 th iteration gives loss of 0.1502108342115933\n",
      "The 35587 th iteration gives loss of 0.1502092780570476\n",
      "The 35588 th iteration gives loss of 0.15020772195954096\n",
      "The 35589 th iteration gives loss of 0.1502061659190516\n",
      "The 35590 th iteration gives loss of 0.15020460993558435\n",
      "The 35591 th iteration gives loss of 0.1502030540091234\n",
      "The 35592 th iteration gives loss of 0.1502014981396625\n",
      "The 35593 th iteration gives loss of 0.15019994232720316\n",
      "The 35594 th iteration gives loss of 0.15019838657172901\n",
      "The 35595 th iteration gives loss of 0.15019683087323896\n",
      "The 35596 th iteration gives loss of 0.1501952752317206\n",
      "The 35597 th iteration gives loss of 0.1501937196471652\n",
      "The 35598 th iteration gives loss of 0.15019216411957592\n",
      "The 35599 th iteration gives loss of 0.15019060864892964\n",
      "The 35600 th iteration gives loss of 0.15018905323522966\n",
      "The 35601 th iteration gives loss of 0.15018749787846494\n",
      "The 35602 th iteration gives loss of 0.15018594257862536\n",
      "The 35603 th iteration gives loss of 0.15018438733570966\n",
      "The 35604 th iteration gives loss of 0.15018283214970726\n",
      "The 35605 th iteration gives loss of 0.1501812770206137\n",
      "The 35606 th iteration gives loss of 0.1501797219484223\n",
      "The 35607 th iteration gives loss of 0.15017816693311617\n",
      "The 35608 th iteration gives loss of 0.1501766119747029\n",
      "The 35609 th iteration gives loss of 0.15017505707315346\n",
      "The 35610 th iteration gives loss of 0.15017350222848833\n",
      "The 35611 th iteration gives loss of 0.15017194744067097\n",
      "The 35612 th iteration gives loss of 0.15017039270971588\n",
      "The 35613 th iteration gives loss of 0.15016883803561284\n",
      "The 35614 th iteration gives loss of 0.15016728341835064\n",
      "The 35615 th iteration gives loss of 0.1501657288579168\n",
      "The 35616 th iteration gives loss of 0.1501641743543036\n",
      "The 35617 th iteration gives loss of 0.15016261990751986\n",
      "The 35618 th iteration gives loss of 0.15016106551753738\n",
      "The 35619 th iteration gives loss of 0.15015951118435952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 35620 th iteration gives loss of 0.15015795690797845\n",
      "The 35621 th iteration gives loss of 0.1501564026883871\n",
      "The 35622 th iteration gives loss of 0.15015484852557345\n",
      "The 35623 th iteration gives loss of 0.1501532944195426\n",
      "The 35624 th iteration gives loss of 0.15015174037027268\n",
      "The 35625 th iteration gives loss of 0.15015018637776617\n",
      "The 35626 th iteration gives loss of 0.15014863244200502\n",
      "The 35627 th iteration gives loss of 0.15014707856299578\n",
      "The 35628 th iteration gives loss of 0.150145524740722\n",
      "The 35629 th iteration gives loss of 0.1501439709751735\n",
      "The 35630 th iteration gives loss of 0.15014241726634875\n",
      "The 35631 th iteration gives loss of 0.1501408636142468\n",
      "The 35632 th iteration gives loss of 0.1501393100188433\n",
      "The 35633 th iteration gives loss of 0.15013775648014102\n",
      "The 35634 th iteration gives loss of 0.1501362029981378\n",
      "The 35635 th iteration gives loss of 0.1501346495728167\n",
      "The 35636 th iteration gives loss of 0.15013309620417598\n",
      "The 35637 th iteration gives loss of 0.15013154289219935\n",
      "The 35638 th iteration gives loss of 0.1501299896368918\n",
      "The 35639 th iteration gives loss of 0.15012843643824372\n",
      "The 35640 th iteration gives loss of 0.1501268832962434\n",
      "The 35641 th iteration gives loss of 0.15012533021088792\n",
      "The 35642 th iteration gives loss of 0.15012377718216252\n",
      "The 35643 th iteration gives loss of 0.15012222421006768\n",
      "The 35644 th iteration gives loss of 0.15012067129459022\n",
      "The 35645 th iteration gives loss of 0.15011911843572626\n",
      "The 35646 th iteration gives loss of 0.15011756563346176\n",
      "The 35647 th iteration gives loss of 0.1501160128877977\n",
      "The 35648 th iteration gives loss of 0.15011446019873065\n",
      "The 35649 th iteration gives loss of 0.15011290756623927\n",
      "The 35650 th iteration gives loss of 0.150111354990326\n",
      "The 35651 th iteration gives loss of 0.15010980247098224\n",
      "The 35652 th iteration gives loss of 0.1501082500081991\n",
      "The 35653 th iteration gives loss of 0.15010669760196932\n",
      "The 35654 th iteration gives loss of 0.15010514525229351\n",
      "The 35655 th iteration gives loss of 0.15010359295914397\n",
      "The 35656 th iteration gives loss of 0.15010204072253322\n",
      "The 35657 th iteration gives loss of 0.15010048854244115\n",
      "The 35658 th iteration gives loss of 0.150098936418874\n",
      "The 35659 th iteration gives loss of 0.15009738435181316\n",
      "The 35660 th iteration gives loss of 0.15009583234125345\n",
      "The 35661 th iteration gives loss of 0.15009428038719758\n",
      "The 35662 th iteration gives loss of 0.15009272848962554\n",
      "The 35663 th iteration gives loss of 0.1500911766485304\n",
      "The 35664 th iteration gives loss of 0.15008962486391136\n",
      "The 35665 th iteration gives loss of 0.15008807313576042\n",
      "The 35666 th iteration gives loss of 0.15008652146406307\n",
      "The 35667 th iteration gives loss of 0.15008496984881814\n",
      "The 35668 th iteration gives loss of 0.15008341829002064\n",
      "The 35669 th iteration gives loss of 0.15008186678766258\n",
      "The 35670 th iteration gives loss of 0.15008031534172822\n",
      "The 35671 th iteration gives loss of 0.15007876395222267\n",
      "The 35672 th iteration gives loss of 0.15007721261913048\n",
      "The 35673 th iteration gives loss of 0.15007566134244743\n",
      "The 35674 th iteration gives loss of 0.15007411012215527\n",
      "The 35675 th iteration gives loss of 0.15007255895826174\n",
      "The 35676 th iteration gives loss of 0.15007100785075056\n",
      "The 35677 th iteration gives loss of 0.15006945679962636\n",
      "The 35678 th iteration gives loss of 0.15006790580486737\n",
      "The 35679 th iteration gives loss of 0.15006635486647404\n",
      "The 35680 th iteration gives loss of 0.15006480398443528\n",
      "The 35681 th iteration gives loss of 0.1500632531587501\n",
      "The 35682 th iteration gives loss of 0.15006170238940397\n",
      "The 35683 th iteration gives loss of 0.15006015167639328\n",
      "The 35684 th iteration gives loss of 0.1500586010197098\n",
      "The 35685 th iteration gives loss of 0.15005705041935183\n",
      "The 35686 th iteration gives loss of 0.15005549987530298\n",
      "The 35687 th iteration gives loss of 0.15005394938755506\n",
      "The 35688 th iteration gives loss of 0.15005239895610914\n",
      "The 35689 th iteration gives loss of 0.15005084858095666\n",
      "The 35690 th iteration gives loss of 0.1500492982620845\n",
      "The 35691 th iteration gives loss of 0.15004774799948944\n",
      "The 35692 th iteration gives loss of 0.15004619779316\n",
      "The 35693 th iteration gives loss of 0.1500446476431016\n",
      "The 35694 th iteration gives loss of 0.1500430975492931\n",
      "The 35695 th iteration gives loss of 0.15004154751173157\n",
      "The 35696 th iteration gives loss of 0.15003999753041258\n",
      "The 35697 th iteration gives loss of 0.15003844760532398\n",
      "The 35698 th iteration gives loss of 0.15003689773646536\n",
      "The 35699 th iteration gives loss of 0.1500353479238246\n",
      "The 35700 th iteration gives loss of 0.15003379816738988\n",
      "The 35701 th iteration gives loss of 0.15003224846716137\n",
      "The 35702 th iteration gives loss of 0.15003069882312503\n",
      "The 35703 th iteration gives loss of 0.15002914923528973\n",
      "The 35704 th iteration gives loss of 0.15002759970363264\n",
      "The 35705 th iteration gives loss of 0.1500260502281461\n",
      "The 35706 th iteration gives loss of 0.1500245008088275\n",
      "The 35707 th iteration gives loss of 0.15002295144567646\n",
      "The 35708 th iteration gives loss of 0.15002140213866055\n",
      "The 35709 th iteration gives loss of 0.1500198528878014\n",
      "The 35710 th iteration gives loss of 0.1500183036930803\n",
      "The 35711 th iteration gives loss of 0.1500167545544908\n",
      "The 35712 th iteration gives loss of 0.15001520547202543\n",
      "The 35713 th iteration gives loss of 0.15001365644567594\n",
      "The 35714 th iteration gives loss of 0.15001210747544563\n",
      "The 35715 th iteration gives loss of 0.15001055856130865\n",
      "The 35716 th iteration gives loss of 0.15000900970326717\n",
      "The 35717 th iteration gives loss of 0.15000746090132042\n",
      "The 35718 th iteration gives loss of 0.15000591215544595\n",
      "The 35719 th iteration gives loss of 0.15000436346564208\n",
      "The 35720 th iteration gives loss of 0.1500028148319113\n",
      "The 35721 th iteration gives loss of 0.1500012662542363\n",
      "The 35722 th iteration gives loss of 0.14999971773261692\n",
      "The 35723 th iteration gives loss of 0.14999816926703421\n",
      "The 35724 th iteration gives loss of 0.14999662085749665\n",
      "The 35725 th iteration gives loss of 0.14999507250398159\n",
      "The 35726 th iteration gives loss of 0.14999352420648965\n",
      "The 35727 th iteration gives loss of 0.149991975965018\n",
      "The 35728 th iteration gives loss of 0.1499904277795554\n",
      "The 35729 th iteration gives loss of 0.14998887965008995\n",
      "The 35730 th iteration gives loss of 0.1499873315766192\n",
      "The 35731 th iteration gives loss of 0.14998578355913617\n",
      "The 35732 th iteration gives loss of 0.1499842355976268\n",
      "The 35733 th iteration gives loss of 0.14998268769209744\n",
      "The 35734 th iteration gives loss of 0.14998113984252384\n",
      "The 35735 th iteration gives loss of 0.14997959204891465\n",
      "The 35736 th iteration gives loss of 0.1499780443112559\n",
      "The 35737 th iteration gives loss of 0.1499764966295425\n",
      "The 35738 th iteration gives loss of 0.1499749490037649\n",
      "The 35739 th iteration gives loss of 0.1499734014339027\n",
      "The 35740 th iteration gives loss of 0.1499718539199754\n",
      "The 35741 th iteration gives loss of 0.14997030646195417\n",
      "The 35742 th iteration gives loss of 0.14996875905984805\n",
      "The 35743 th iteration gives loss of 0.1499672117136377\n",
      "The 35744 th iteration gives loss of 0.14996566442332201\n",
      "The 35745 th iteration gives loss of 0.14996411718888508\n",
      "The 35746 th iteration gives loss of 0.1499625700103325\n",
      "The 35747 th iteration gives loss of 0.14996102288764698\n",
      "The 35748 th iteration gives loss of 0.14995947582083163\n",
      "The 35749 th iteration gives loss of 0.14995792880986422\n",
      "The 35750 th iteration gives loss of 0.1499563818547604\n",
      "The 35751 th iteration gives loss of 0.1499548349554893\n",
      "The 35752 th iteration gives loss of 0.1499532881120505\n",
      "The 35753 th iteration gives loss of 0.14995174132443814\n",
      "The 35754 th iteration gives loss of 0.1499501945926495\n",
      "The 35755 th iteration gives loss of 0.14994864791667706\n",
      "The 35756 th iteration gives loss of 0.14994710129650646\n",
      "The 35757 th iteration gives loss of 0.14994555473213508\n",
      "The 35758 th iteration gives loss of 0.14994400822355558\n",
      "The 35759 th iteration gives loss of 0.14994246177076515\n",
      "The 35760 th iteration gives loss of 0.14994091537374954\n",
      "The 35761 th iteration gives loss of 0.1499393690324997\n",
      "The 35762 th iteration gives loss of 0.14993782274701764\n",
      "The 35763 th iteration gives loss of 0.14993627651729252\n",
      "The 35764 th iteration gives loss of 0.14993473034330798\n",
      "The 35765 th iteration gives loss of 0.14993318422506696\n",
      "The 35766 th iteration gives loss of 0.149931638162564\n",
      "The 35767 th iteration gives loss of 0.1499300921557911\n",
      "The 35768 th iteration gives loss of 0.14992854620473484\n",
      "The 35769 th iteration gives loss of 0.14992700030938724\n",
      "The 35770 th iteration gives loss of 0.14992545446974997\n",
      "The 35771 th iteration gives loss of 0.14992390868580693\n",
      "The 35772 th iteration gives loss of 0.1499223629575625\n",
      "The 35773 th iteration gives loss of 0.14992081728499107\n",
      "The 35774 th iteration gives loss of 0.14991927166809946\n",
      "The 35775 th iteration gives loss of 0.1499177261068806\n",
      "The 35776 th iteration gives loss of 0.1499161806013246\n",
      "The 35777 th iteration gives loss of 0.1499146351514207\n",
      "The 35778 th iteration gives loss of 0.1499130897571634\n",
      "The 35779 th iteration gives loss of 0.14991154441855367\n",
      "The 35780 th iteration gives loss of 0.1499099991355723\n",
      "The 35781 th iteration gives loss of 0.14990845390821628\n",
      "The 35782 th iteration gives loss of 0.14990690873648357\n",
      "The 35783 th iteration gives loss of 0.14990536362036075\n",
      "The 35784 th iteration gives loss of 0.14990381855984075\n",
      "The 35785 th iteration gives loss of 0.14990227355492458\n",
      "The 35786 th iteration gives loss of 0.14990072860559736\n",
      "The 35787 th iteration gives loss of 0.14989918371185396\n",
      "The 35788 th iteration gives loss of 0.14989763887368407\n",
      "The 35789 th iteration gives loss of 0.1498960940910777\n",
      "The 35790 th iteration gives loss of 0.14989454936403723\n",
      "The 35791 th iteration gives loss of 0.14989300469255246\n",
      "The 35792 th iteration gives loss of 0.14989146007661538\n",
      "The 35793 th iteration gives loss of 0.14988991551622383\n",
      "The 35794 th iteration gives loss of 0.1498883710113604\n",
      "The 35795 th iteration gives loss of 0.1498868265620234\n",
      "The 35796 th iteration gives loss of 0.14988528216820365\n",
      "The 35797 th iteration gives loss of 0.14988373782989997\n",
      "The 35798 th iteration gives loss of 0.14988219354709773\n",
      "The 35799 th iteration gives loss of 0.14988064931979622\n",
      "The 35800 th iteration gives loss of 0.14987910514797886\n",
      "The 35801 th iteration gives loss of 0.14987756103164668\n",
      "The 35802 th iteration gives loss of 0.14987601697079644\n",
      "The 35803 th iteration gives loss of 0.1498744729654093\n",
      "The 35804 th iteration gives loss of 0.149872929015493\n",
      "The 35805 th iteration gives loss of 0.14987138512101747\n",
      "The 35806 th iteration gives loss of 0.1498698412819989\n",
      "The 35807 th iteration gives loss of 0.14986829749841554\n",
      "The 35808 th iteration gives loss of 0.14986675377026706\n",
      "The 35809 th iteration gives loss of 0.14986521009754353\n",
      "The 35810 th iteration gives loss of 0.14986366648024627\n",
      "The 35811 th iteration gives loss of 0.14986212291835335\n",
      "The 35812 th iteration gives loss of 0.14986057941186098\n",
      "The 35813 th iteration gives loss of 0.14985903596077535\n",
      "The 35814 th iteration gives loss of 0.14985749256507813\n",
      "The 35815 th iteration gives loss of 0.1498559492247611\n",
      "The 35816 th iteration gives loss of 0.14985440593981914\n",
      "The 35817 th iteration gives loss of 0.14985286271024834\n",
      "The 35818 th iteration gives loss of 0.14985131953604316\n",
      "The 35819 th iteration gives loss of 0.149849776417188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 35820 th iteration gives loss of 0.14984823335368425\n",
      "The 35821 th iteration gives loss of 0.14984669034551487\n",
      "The 35822 th iteration gives loss of 0.14984514739268273\n",
      "The 35823 th iteration gives loss of 0.14984360449517076\n",
      "The 35824 th iteration gives loss of 0.14984206165298736\n",
      "The 35825 th iteration gives loss of 0.14984051886610897\n",
      "The 35826 th iteration gives loss of 0.1498389761345439\n",
      "The 35827 th iteration gives loss of 0.14983743345826858\n",
      "The 35828 th iteration gives loss of 0.14983589083728882\n",
      "The 35829 th iteration gives loss of 0.1498343482715922\n",
      "The 35830 th iteration gives loss of 0.14983280576117\n",
      "The 35831 th iteration gives loss of 0.14983126330601637\n",
      "The 35832 th iteration gives loss of 0.1498297209061244\n",
      "The 35833 th iteration gives loss of 0.14982817856148878\n",
      "The 35834 th iteration gives loss of 0.14982663627209672\n",
      "The 35835 th iteration gives loss of 0.14982509403795274\n",
      "The 35836 th iteration gives loss of 0.14982355185903523\n",
      "The 35837 th iteration gives loss of 0.1498220097353442\n",
      "The 35838 th iteration gives loss of 0.1498204676668783\n",
      "The 35839 th iteration gives loss of 0.14981892565361912\n",
      "The 35840 th iteration gives loss of 0.14981738369557004\n",
      "The 35841 th iteration gives loss of 0.1498158417927159\n",
      "The 35842 th iteration gives loss of 0.14981429994506085\n",
      "The 35843 th iteration gives loss of 0.1498127581525808\n",
      "The 35844 th iteration gives loss of 0.14981121641528425\n",
      "The 35845 th iteration gives loss of 0.14980967473316376\n",
      "The 35846 th iteration gives loss of 0.14980813310618493\n",
      "The 35847 th iteration gives loss of 0.1498065915343737\n",
      "The 35848 th iteration gives loss of 0.1498050500177055\n",
      "The 35849 th iteration gives loss of 0.1498035085561902\n",
      "The 35850 th iteration gives loss of 0.14980196714980065\n",
      "The 35851 th iteration gives loss of 0.1498004257985405\n",
      "The 35852 th iteration gives loss of 0.14979888450239595\n",
      "The 35853 th iteration gives loss of 0.14979734326136485\n",
      "The 35854 th iteration gives loss of 0.14979580207544083\n",
      "The 35855 th iteration gives loss of 0.1497942609446188\n",
      "The 35856 th iteration gives loss of 0.14979271986888776\n",
      "The 35857 th iteration gives loss of 0.14979117884824206\n",
      "The 35858 th iteration gives loss of 0.14978963788267047\n",
      "The 35859 th iteration gives loss of 0.1497880969721724\n",
      "The 35860 th iteration gives loss of 0.14978655611673877\n",
      "The 35861 th iteration gives loss of 0.14978501531635835\n",
      "The 35862 th iteration gives loss of 0.14978347457103036\n",
      "The 35863 th iteration gives loss of 0.1497819338807364\n",
      "The 35864 th iteration gives loss of 0.1497803932454848\n",
      "The 35865 th iteration gives loss of 0.1497788526652492\n",
      "The 35866 th iteration gives loss of 0.14977731214004733\n",
      "The 35867 th iteration gives loss of 0.14977577166985626\n",
      "The 35868 th iteration gives loss of 0.14977423125467107\n",
      "The 35869 th iteration gives loss of 0.14977269089448866\n",
      "The 35870 th iteration gives loss of 0.1497711505892969\n",
      "The 35871 th iteration gives loss of 0.1497696103390905\n",
      "The 35872 th iteration gives loss of 0.14976807014386578\n",
      "The 35873 th iteration gives loss of 0.149766530003612\n",
      "The 35874 th iteration gives loss of 0.1497649899183145\n",
      "The 35875 th iteration gives loss of 0.14976344988798046\n",
      "The 35876 th iteration gives loss of 0.14976190991259775\n",
      "The 35877 th iteration gives loss of 0.14976036999215106\n",
      "The 35878 th iteration gives loss of 0.1497588301266442\n",
      "The 35879 th iteration gives loss of 0.14975729031606494\n",
      "The 35880 th iteration gives loss of 0.14975575056041462\n",
      "The 35881 th iteration gives loss of 0.1497542108596759\n",
      "The 35882 th iteration gives loss of 0.14975267121384006\n",
      "The 35883 th iteration gives loss of 0.14975113162290613\n",
      "The 35884 th iteration gives loss of 0.14974959208687016\n",
      "The 35885 th iteration gives loss of 0.14974805260571689\n",
      "The 35886 th iteration gives loss of 0.14974651317944615\n",
      "The 35887 th iteration gives loss of 0.14974497380805227\n",
      "The 35888 th iteration gives loss of 0.1497434344915148\n",
      "The 35889 th iteration gives loss of 0.1497418952298374\n",
      "The 35890 th iteration gives loss of 0.14974035602302388\n",
      "The 35891 th iteration gives loss of 0.14973881687103244\n",
      "The 35892 th iteration gives loss of 0.14973727777388834\n",
      "The 35893 th iteration gives loss of 0.14973573873157267\n",
      "The 35894 th iteration gives loss of 0.14973419974409147\n",
      "The 35895 th iteration gives loss of 0.14973266081142458\n",
      "The 35896 th iteration gives loss of 0.1497311219335505\n",
      "The 35897 th iteration gives loss of 0.14972958311049242\n",
      "The 35898 th iteration gives loss of 0.1497280443422275\n",
      "The 35899 th iteration gives loss of 0.1497265056287452\n",
      "The 35900 th iteration gives loss of 0.14972496697004964\n",
      "The 35901 th iteration gives loss of 0.149723428366125\n",
      "The 35902 th iteration gives loss of 0.14972188981696702\n",
      "The 35903 th iteration gives loss of 0.14972035132256767\n",
      "The 35904 th iteration gives loss of 0.149718812882926\n",
      "The 35905 th iteration gives loss of 0.1497172744980348\n",
      "The 35906 th iteration gives loss of 0.14971573616786976\n",
      "The 35907 th iteration gives loss of 0.14971419789243837\n",
      "The 35908 th iteration gives loss of 0.14971265967173908\n",
      "The 35909 th iteration gives loss of 0.14971112150574636\n",
      "The 35910 th iteration gives loss of 0.14970958339447757\n",
      "The 35911 th iteration gives loss of 0.14970804533790458\n",
      "The 35912 th iteration gives loss of 0.1497065073360312\n",
      "The 35913 th iteration gives loss of 0.14970496938883537\n",
      "The 35914 th iteration gives loss of 0.14970343149633916\n",
      "The 35915 th iteration gives loss of 0.14970189365851136\n",
      "The 35916 th iteration gives loss of 0.14970035587535888\n",
      "The 35917 th iteration gives loss of 0.1496988181468586\n",
      "The 35918 th iteration gives loss of 0.14969728047301217\n",
      "The 35919 th iteration gives loss of 0.14969574285381518\n",
      "The 35920 th iteration gives loss of 0.14969420528926322\n",
      "The 35921 th iteration gives loss of 0.1496926677793424\n",
      "The 35922 th iteration gives loss of 0.14969113032404807\n",
      "The 35923 th iteration gives loss of 0.1496895929233733\n",
      "The 35924 th iteration gives loss of 0.14968805557730996\n",
      "The 35925 th iteration gives loss of 0.14968651828584706\n",
      "The 35926 th iteration gives loss of 0.14968498104899117\n",
      "The 35927 th iteration gives loss of 0.14968344386672056\n",
      "The 35928 th iteration gives loss of 0.14968190673903453\n",
      "The 35929 th iteration gives loss of 0.14968036966592338\n",
      "The 35930 th iteration gives loss of 0.14967883264739254\n",
      "The 35931 th iteration gives loss of 0.14967729568341973\n",
      "The 35932 th iteration gives loss of 0.14967575877400277\n",
      "The 35933 th iteration gives loss of 0.14967422191912882\n",
      "The 35934 th iteration gives loss of 0.14967268511880155\n",
      "The 35935 th iteration gives loss of 0.14967114837301693\n",
      "The 35936 th iteration gives loss of 0.1496696116817594\n",
      "The 35937 th iteration gives loss of 0.14966807504501445\n",
      "The 35938 th iteration gives loss of 0.1496665384627902\n",
      "The 35939 th iteration gives loss of 0.14966500193507035\n",
      "The 35940 th iteration gives loss of 0.14966346546185053\n",
      "The 35941 th iteration gives loss of 0.14966192904313028\n",
      "The 35942 th iteration gives loss of 0.14966039267888726\n",
      "The 35943 th iteration gives loss of 0.14965885636912465\n",
      "The 35944 th iteration gives loss of 0.14965732011384225\n",
      "The 35945 th iteration gives loss of 0.1496557839130169\n",
      "The 35946 th iteration gives loss of 0.1496542477666497\n",
      "The 35947 th iteration gives loss of 0.14965271167474067\n",
      "The 35948 th iteration gives loss of 0.14965117563727062\n",
      "The 35949 th iteration gives loss of 0.14964963965424313\n",
      "The 35950 th iteration gives loss of 0.14964810372564244\n",
      "The 35951 th iteration gives loss of 0.14964656785146405\n",
      "The 35952 th iteration gives loss of 0.14964503203171214\n",
      "The 35953 th iteration gives loss of 0.14964349626635917\n",
      "The 35954 th iteration gives loss of 0.14964196055540982\n",
      "The 35955 th iteration gives loss of 0.1496404248988584\n",
      "The 35956 th iteration gives loss of 0.14963888929669464\n",
      "The 35957 th iteration gives loss of 0.14963735374891518\n",
      "The 35958 th iteration gives loss of 0.14963581825550454\n",
      "The 35959 th iteration gives loss of 0.14963428281646513\n",
      "The 35960 th iteration gives loss of 0.14963274743178995\n",
      "The 35961 th iteration gives loss of 0.1496312121014651\n",
      "The 35962 th iteration gives loss of 0.14962967682548312\n",
      "The 35963 th iteration gives loss of 0.14962814160384696\n",
      "The 35964 th iteration gives loss of 0.14962660643653944\n",
      "The 35965 th iteration gives loss of 0.14962507132355138\n",
      "The 35966 th iteration gives loss of 0.14962353626489525\n",
      "The 35967 th iteration gives loss of 0.14962200126054653\n",
      "The 35968 th iteration gives loss of 0.1496204663105007\n",
      "The 35969 th iteration gives loss of 0.14961893141475446\n",
      "The 35970 th iteration gives loss of 0.14961739657329645\n",
      "The 35971 th iteration gives loss of 0.14961586178613054\n",
      "The 35972 th iteration gives loss of 0.14961432705323782\n",
      "The 35973 th iteration gives loss of 0.14961279237460742\n",
      "The 35974 th iteration gives loss of 0.14961125775024825\n",
      "The 35975 th iteration gives loss of 0.14960972318014418\n",
      "The 35976 th iteration gives loss of 0.14960818866429135\n",
      "The 35977 th iteration gives loss of 0.14960665420268068\n",
      "The 35978 th iteration gives loss of 0.1496051197953015\n",
      "The 35979 th iteration gives loss of 0.14960358544215255\n",
      "The 35980 th iteration gives loss of 0.1496020511432202\n",
      "The 35981 th iteration gives loss of 0.1496005168985054\n",
      "The 35982 th iteration gives loss of 0.14959898270800703\n",
      "The 35983 th iteration gives loss of 0.14959744857170085\n",
      "The 35984 th iteration gives loss of 0.14959591448959114\n",
      "The 35985 th iteration gives loss of 0.14959438046166293\n",
      "The 35986 th iteration gives loss of 0.14959284648792096\n",
      "The 35987 th iteration gives loss of 0.14959131256835118\n",
      "The 35988 th iteration gives loss of 0.14958977870294526\n",
      "The 35989 th iteration gives loss of 0.14958824489169098\n",
      "The 35990 th iteration gives loss of 0.1495867111345958\n",
      "The 35991 th iteration gives loss of 0.14958517743164862\n",
      "The 35992 th iteration gives loss of 0.14958364378283998\n",
      "The 35993 th iteration gives loss of 0.14958211018815448\n",
      "The 35994 th iteration gives loss of 0.14958057664760452\n",
      "The 35995 th iteration gives loss of 0.14957904316116452\n",
      "The 35996 th iteration gives loss of 0.14957750972883962\n",
      "The 35997 th iteration gives loss of 0.14957597635061523\n",
      "The 35998 th iteration gives loss of 0.14957444302648104\n",
      "The 35999 th iteration gives loss of 0.14957290975644602\n",
      "The 36000 th iteration gives loss of 0.1495713765404907\n",
      "The 36001 th iteration gives loss of 0.14956984337861248\n",
      "The 36002 th iteration gives loss of 0.14956831027079492\n",
      "The 36003 th iteration gives loss of 0.1495667772170508\n",
      "The 36004 th iteration gives loss of 0.14956524421734926\n",
      "The 36005 th iteration gives loss of 0.14956371127170737\n",
      "The 36006 th iteration gives loss of 0.14956217838010075\n",
      "The 36007 th iteration gives loss of 0.14956064554253468\n",
      "The 36008 th iteration gives loss of 0.14955911275899064\n",
      "The 36009 th iteration gives loss of 0.14955758002946998\n",
      "The 36010 th iteration gives loss of 0.14955604735395966\n",
      "The 36011 th iteration gives loss of 0.14955451473245898\n",
      "The 36012 th iteration gives loss of 0.14955298216495222\n",
      "The 36013 th iteration gives loss of 0.14955144965144201\n",
      "The 36014 th iteration gives loss of 0.14954991719191985\n",
      "The 36015 th iteration gives loss of 0.14954838478637716\n",
      "The 36016 th iteration gives loss of 0.14954685243480217\n",
      "The 36017 th iteration gives loss of 0.14954532013719343\n",
      "The 36018 th iteration gives loss of 0.14954378789354972\n",
      "The 36019 th iteration gives loss of 0.14954225570385152\n",
      "The 36020 th iteration gives loss of 0.14954072356809017\n",
      "The 36021 th iteration gives loss of 0.14953919148628694\n",
      "The 36022 th iteration gives loss of 0.14953765945839115\n",
      "The 36023 th iteration gives loss of 0.14953612748442613\n",
      "The 36024 th iteration gives loss of 0.1495345955643821\n",
      "The 36025 th iteration gives loss of 0.14953306369825622\n",
      "The 36026 th iteration gives loss of 0.1495315318860224\n",
      "The 36027 th iteration gives loss of 0.14953000012768636\n",
      "The 36028 th iteration gives loss of 0.1495284684232424\n",
      "The 36029 th iteration gives loss of 0.14952693677268125\n",
      "The 36030 th iteration gives loss of 0.14952540517598692\n",
      "The 36031 th iteration gives loss of 0.14952387363316666\n",
      "The 36032 th iteration gives loss of 0.1495223421442052\n",
      "The 36033 th iteration gives loss of 0.14952081070909987\n",
      "The 36034 th iteration gives loss of 0.1495192793278447\n",
      "The 36035 th iteration gives loss of 0.14951774800042156\n",
      "The 36036 th iteration gives loss of 0.14951621672684284\n",
      "The 36037 th iteration gives loss of 0.14951468550709418\n",
      "The 36038 th iteration gives loss of 0.1495131543411515\n",
      "The 36039 th iteration gives loss of 0.14951162322903544\n",
      "The 36040 th iteration gives loss of 0.14951009217072325\n",
      "The 36041 th iteration gives loss of 0.14950856116621358\n",
      "The 36042 th iteration gives loss of 0.14950703021548709\n",
      "The 36043 th iteration gives loss of 0.149505499318553\n",
      "The 36044 th iteration gives loss of 0.149503968475395\n",
      "The 36045 th iteration gives loss of 0.1495024376860123\n",
      "The 36046 th iteration gives loss of 0.14950090695039062\n",
      "The 36047 th iteration gives loss of 0.14949937626852933\n",
      "The 36048 th iteration gives loss of 0.1494978456404231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 36049 th iteration gives loss of 0.14949631506606065\n",
      "The 36050 th iteration gives loss of 0.14949478454543677\n",
      "The 36051 th iteration gives loss of 0.14949325407854058\n",
      "The 36052 th iteration gives loss of 0.14949172366536392\n",
      "The 36053 th iteration gives loss of 0.14949019330590968\n",
      "The 36054 th iteration gives loss of 0.14948866300017058\n",
      "The 36055 th iteration gives loss of 0.14948713274813222\n",
      "The 36056 th iteration gives loss of 0.14948560254978974\n",
      "The 36057 th iteration gives loss of 0.14948407240513067\n",
      "The 36058 th iteration gives loss of 0.1494825423141559\n",
      "The 36059 th iteration gives loss of 0.14948101227686447\n",
      "The 36060 th iteration gives loss of 0.14947948229323774\n",
      "The 36061 th iteration gives loss of 0.1494779523632683\n",
      "The 36062 th iteration gives loss of 0.1494764224869686\n",
      "The 36063 th iteration gives loss of 0.14947489266429886\n",
      "The 36064 th iteration gives loss of 0.1494733628952816\n",
      "The 36065 th iteration gives loss of 0.14947183317990162\n",
      "The 36066 th iteration gives loss of 0.14947030351814553\n",
      "The 36067 th iteration gives loss of 0.14946877391000685\n",
      "The 36068 th iteration gives loss of 0.14946724435549016\n",
      "The 36069 th iteration gives loss of 0.149465714854579\n",
      "The 36070 th iteration gives loss of 0.1494641854072626\n",
      "The 36071 th iteration gives loss of 0.14946265601354825\n",
      "The 36072 th iteration gives loss of 0.1494611266734148\n",
      "The 36073 th iteration gives loss of 0.14945959738686412\n",
      "The 36074 th iteration gives loss of 0.1494580681538835\n",
      "The 36075 th iteration gives loss of 0.14945653897447125\n",
      "The 36076 th iteration gives loss of 0.14945500984861682\n",
      "The 36077 th iteration gives loss of 0.14945348077632217\n",
      "The 36078 th iteration gives loss of 0.14945195175756945\n",
      "The 36079 th iteration gives loss of 0.1494504227923563\n",
      "The 36080 th iteration gives loss of 0.14944889388067012\n",
      "The 36081 th iteration gives loss of 0.1494473650225153\n",
      "The 36082 th iteration gives loss of 0.14944583621787416\n",
      "The 36083 th iteration gives loss of 0.1494443074667424\n",
      "The 36084 th iteration gives loss of 0.14944277876911977\n",
      "The 36085 th iteration gives loss of 0.1494412501249978\n",
      "The 36086 th iteration gives loss of 0.1494397215343585\n",
      "The 36087 th iteration gives loss of 0.14943819299721317\n",
      "The 36088 th iteration gives loss of 0.14943666451353194\n",
      "The 36089 th iteration gives loss of 0.14943513608333628\n",
      "The 36090 th iteration gives loss of 0.14943360770659403\n",
      "The 36091 th iteration gives loss of 0.14943207938331582\n",
      "The 36092 th iteration gives loss of 0.14943055111348824\n",
      "The 36093 th iteration gives loss of 0.149429022897097\n",
      "The 36094 th iteration gives loss of 0.1494274947341474\n",
      "The 36095 th iteration gives loss of 0.1494259666246258\n",
      "The 36096 th iteration gives loss of 0.14942443856853127\n",
      "The 36097 th iteration gives loss of 0.1494229105658465\n",
      "The 36098 th iteration gives loss of 0.14942138261657167\n",
      "The 36099 th iteration gives loss of 0.14941985472070082\n",
      "The 36100 th iteration gives loss of 0.14941832687822643\n",
      "The 36101 th iteration gives loss of 0.14941679908913688\n",
      "The 36102 th iteration gives loss of 0.14941527135343546\n",
      "The 36103 th iteration gives loss of 0.14941374367110322\n",
      "The 36104 th iteration gives loss of 0.1494122160421483\n",
      "The 36105 th iteration gives loss of 0.14941068846654945\n",
      "The 36106 th iteration gives loss of 0.1494091609442986\n",
      "The 36107 th iteration gives loss of 0.14940763347539962\n",
      "The 36108 th iteration gives loss of 0.14940610605984228\n",
      "The 36109 th iteration gives loss of 0.14940457869762536\n",
      "The 36110 th iteration gives loss of 0.1494030513887262\n",
      "The 36111 th iteration gives loss of 0.14940152413315272\n",
      "The 36112 th iteration gives loss of 0.14939999693089223\n",
      "The 36113 th iteration gives loss of 0.14939846978194504\n",
      "The 36114 th iteration gives loss of 0.14939694268628798\n",
      "The 36115 th iteration gives loss of 0.1493954156439307\n",
      "The 36116 th iteration gives loss of 0.14939388865485698\n",
      "The 36117 th iteration gives loss of 0.149392361719065\n",
      "The 36118 th iteration gives loss of 0.14939083483654964\n",
      "The 36119 th iteration gives loss of 0.1493893080072894\n",
      "The 36120 th iteration gives loss of 0.14938778123129476\n",
      "The 36121 th iteration gives loss of 0.1493862545085579\n",
      "The 36122 th iteration gives loss of 0.1493847278390651\n",
      "The 36123 th iteration gives loss of 0.1493832012228107\n",
      "The 36124 th iteration gives loss of 0.14938167465978378\n",
      "The 36125 th iteration gives loss of 0.149380148149981\n",
      "The 36126 th iteration gives loss of 0.14937862169339755\n",
      "The 36127 th iteration gives loss of 0.1493770952900331\n",
      "The 36128 th iteration gives loss of 0.14937556893986326\n",
      "The 36129 th iteration gives loss of 0.1493740426429098\n",
      "The 36130 th iteration gives loss of 0.1493725163991305\n",
      "The 36131 th iteration gives loss of 0.14937099020854086\n",
      "The 36132 th iteration gives loss of 0.14936946407113139\n",
      "The 36133 th iteration gives loss of 0.14936793798689327\n",
      "The 36134 th iteration gives loss of 0.14936641195581393\n",
      "The 36135 th iteration gives loss of 0.14936488597789532\n",
      "The 36136 th iteration gives loss of 0.1493633600531312\n",
      "The 36137 th iteration gives loss of 0.14936183418150759\n",
      "The 36138 th iteration gives loss of 0.1493603083630225\n",
      "The 36139 th iteration gives loss of 0.14935878259766888\n",
      "The 36140 th iteration gives loss of 0.14935725688543242\n",
      "The 36141 th iteration gives loss of 0.14935573122632367\n",
      "The 36142 th iteration gives loss of 0.14935420562032597\n",
      "The 36143 th iteration gives loss of 0.14935268006741753\n",
      "The 36144 th iteration gives loss of 0.14935115456761425\n",
      "The 36145 th iteration gives loss of 0.14934962912090022\n",
      "The 36146 th iteration gives loss of 0.14934810372726698\n",
      "The 36147 th iteration gives loss of 0.1493465783867196\n",
      "The 36148 th iteration gives loss of 0.14934505309922866\n",
      "The 36149 th iteration gives loss of 0.14934352786480745\n",
      "The 36150 th iteration gives loss of 0.14934200268344172\n",
      "The 36151 th iteration gives loss of 0.14934047755512028\n",
      "The 36152 th iteration gives loss of 0.14933895247985313\n",
      "The 36153 th iteration gives loss of 0.14933742745761072\n",
      "The 36154 th iteration gives loss of 0.14933590248840345\n",
      "The 36155 th iteration gives loss of 0.14933437757221404\n",
      "The 36156 th iteration gives loss of 0.14933285270904956\n",
      "The 36157 th iteration gives loss of 0.14933132789888143\n",
      "The 36158 th iteration gives loss of 0.14932980314171918\n",
      "The 36159 th iteration gives loss of 0.1493282784375535\n",
      "The 36160 th iteration gives loss of 0.14932675378638066\n",
      "The 36161 th iteration gives loss of 0.14932522918818625\n",
      "The 36162 th iteration gives loss of 0.14932370464296613\n",
      "The 36163 th iteration gives loss of 0.1493221801507063\n",
      "The 36164 th iteration gives loss of 0.14932065571141942\n",
      "The 36165 th iteration gives loss of 0.14931913132508173\n",
      "The 36166 th iteration gives loss of 0.14931760699169716\n",
      "The 36167 th iteration gives loss of 0.14931608271124297\n",
      "The 36168 th iteration gives loss of 0.14931455848373473\n",
      "The 36169 th iteration gives loss of 0.1493130343091514\n",
      "The 36170 th iteration gives loss of 0.14931151018748406\n",
      "The 36171 th iteration gives loss of 0.14930998611873644\n",
      "The 36172 th iteration gives loss of 0.149308462102899\n",
      "The 36173 th iteration gives loss of 0.14930693813995166\n",
      "The 36174 th iteration gives loss of 0.14930541422990293\n",
      "The 36175 th iteration gives loss of 0.14930389037274627\n",
      "The 36176 th iteration gives loss of 0.14930236656846468\n",
      "The 36177 th iteration gives loss of 0.14930084281705738\n",
      "The 36178 th iteration gives loss of 0.14929931911851888\n",
      "The 36179 th iteration gives loss of 0.14929779547284475\n",
      "The 36180 th iteration gives loss of 0.14929627188002395\n",
      "The 36181 th iteration gives loss of 0.14929474834004314\n",
      "The 36182 th iteration gives loss of 0.14929322485290694\n",
      "The 36183 th iteration gives loss of 0.14929170141860218\n",
      "The 36184 th iteration gives loss of 0.14929017803711342\n",
      "The 36185 th iteration gives loss of 0.14928865470846592\n",
      "The 36186 th iteration gives loss of 0.14928713143262476\n",
      "The 36187 th iteration gives loss of 0.14928560820959233\n",
      "The 36188 th iteration gives loss of 0.14928408503935586\n",
      "The 36189 th iteration gives loss of 0.1492825619219079\n",
      "The 36190 th iteration gives loss of 0.14928103885725016\n",
      "The 36191 th iteration gives loss of 0.14927951584536964\n",
      "The 36192 th iteration gives loss of 0.1492779928862687\n",
      "The 36193 th iteration gives loss of 0.1492764699799263\n",
      "The 36194 th iteration gives loss of 0.14927494712635248\n",
      "The 36195 th iteration gives loss of 0.14927342432552576\n",
      "The 36196 th iteration gives loss of 0.1492719015774426\n",
      "The 36197 th iteration gives loss of 0.14927037888210518\n",
      "The 36198 th iteration gives loss of 0.1492688562394963\n",
      "The 36199 th iteration gives loss of 0.14926733364961084\n",
      "The 36200 th iteration gives loss of 0.14926581111244808\n",
      "The 36201 th iteration gives loss of 0.14926428862799485\n",
      "The 36202 th iteration gives loss of 0.14926276619625273\n",
      "The 36203 th iteration gives loss of 0.1492612438172039\n",
      "The 36204 th iteration gives loss of 0.14925972149085243\n",
      "The 36205 th iteration gives loss of 0.14925819921718633\n",
      "The 36206 th iteration gives loss of 0.1492566769961929\n",
      "The 36207 th iteration gives loss of 0.1492551548278828\n",
      "The 36208 th iteration gives loss of 0.14925363271222528\n",
      "The 36209 th iteration gives loss of 0.14925211064923388\n",
      "The 36210 th iteration gives loss of 0.14925058863890023\n",
      "The 36211 th iteration gives loss of 0.14924906668119803\n",
      "The 36212 th iteration gives loss of 0.149247544776146\n",
      "The 36213 th iteration gives loss of 0.14924602292372557\n",
      "The 36214 th iteration gives loss of 0.14924450112392834\n",
      "The 36215 th iteration gives loss of 0.14924297937674127\n",
      "The 36216 th iteration gives loss of 0.14924145768218153\n",
      "The 36217 th iteration gives loss of 0.14923993604021862\n",
      "The 36218 th iteration gives loss of 0.14923841445085897\n",
      "The 36219 th iteration gives loss of 0.1492368929140857\n",
      "The 36220 th iteration gives loss of 0.14923537142990254\n",
      "The 36221 th iteration gives loss of 0.14923384999830105\n",
      "The 36222 th iteration gives loss of 0.14923232861925378\n",
      "The 36223 th iteration gives loss of 0.1492308072927858\n",
      "The 36224 th iteration gives loss of 0.14922928601887306\n",
      "The 36225 th iteration gives loss of 0.14922776479751917\n",
      "The 36226 th iteration gives loss of 0.1492262436287025\n",
      "The 36227 th iteration gives loss of 0.1492247225124259\n",
      "The 36228 th iteration gives loss of 0.14922320144867784\n",
      "The 36229 th iteration gives loss of 0.14922168043746395\n",
      "The 36230 th iteration gives loss of 0.14922015947875855\n",
      "The 36231 th iteration gives loss of 0.14921863857257864\n",
      "The 36232 th iteration gives loss of 0.14921711771888835\n",
      "The 36233 th iteration gives loss of 0.14921559691770037\n",
      "The 36234 th iteration gives loss of 0.14921407616901006\n",
      "The 36235 th iteration gives loss of 0.14921255547279544\n",
      "The 36236 th iteration gives loss of 0.1492110348290673\n",
      "The 36237 th iteration gives loss of 0.1492095142378106\n",
      "The 36238 th iteration gives loss of 0.14920799369901597\n",
      "The 36239 th iteration gives loss of 0.14920647321268093\n",
      "The 36240 th iteration gives loss of 0.14920495277879683\n",
      "The 36241 th iteration gives loss of 0.1492034323973587\n",
      "The 36242 th iteration gives loss of 0.149201912068354\n",
      "The 36243 th iteration gives loss of 0.14920039179179204\n",
      "The 36244 th iteration gives loss of 0.14919887156763995\n",
      "The 36245 th iteration gives loss of 0.1491973513959216\n",
      "The 36246 th iteration gives loss of 0.1491958312766138\n",
      "The 36247 th iteration gives loss of 0.14919431120970011\n",
      "The 36248 th iteration gives loss of 0.14919279119519402\n",
      "The 36249 th iteration gives loss of 0.14919127123306697\n",
      "The 36250 th iteration gives loss of 0.1491897513233335\n",
      "The 36251 th iteration gives loss of 0.1491882314659814\n",
      "The 36252 th iteration gives loss of 0.1491867116609936\n",
      "The 36253 th iteration gives loss of 0.14918519190837962\n",
      "The 36254 th iteration gives loss of 0.14918367220812154\n",
      "The 36255 th iteration gives loss of 0.14918215256021888\n",
      "The 36256 th iteration gives loss of 0.1491806329646548\n",
      "The 36257 th iteration gives loss of 0.1491791134214285\n",
      "The 36258 th iteration gives loss of 0.14917759393053387\n",
      "The 36259 th iteration gives loss of 0.1491760744919666\n",
      "The 36260 th iteration gives loss of 0.1491745551057215\n",
      "The 36261 th iteration gives loss of 0.14917303577178387\n",
      "The 36262 th iteration gives loss of 0.1491715164901546\n",
      "The 36263 th iteration gives loss of 0.1491699972608317\n",
      "The 36264 th iteration gives loss of 0.14916847808378436\n",
      "The 36265 th iteration gives loss of 0.14916695895903245\n",
      "The 36266 th iteration gives loss of 0.1491654398865552\n",
      "The 36267 th iteration gives loss of 0.14916392086634792\n",
      "The 36268 th iteration gives loss of 0.14916240189840785\n",
      "The 36269 th iteration gives loss of 0.14916088298273783\n",
      "The 36270 th iteration gives loss of 0.14915936411930408\n",
      "The 36271 th iteration gives loss of 0.14915784530811874\n",
      "The 36272 th iteration gives loss of 0.14915632654917765\n",
      "The 36273 th iteration gives loss of 0.14915480784246762\n",
      "The 36274 th iteration gives loss of 0.14915328918798668\n",
      "The 36275 th iteration gives loss of 0.14915177058571955\n",
      "The 36276 th iteration gives loss of 0.14915025203567073\n",
      "The 36277 th iteration gives loss of 0.14914873353781488\n",
      "The 36278 th iteration gives loss of 0.149147215092165\n",
      "The 36279 th iteration gives loss of 0.14914569669871727\n",
      "The 36280 th iteration gives loss of 0.1491441783574406\n",
      "The 36281 th iteration gives loss of 0.1491426600683466\n",
      "The 36282 th iteration gives loss of 0.14914114183143296\n",
      "The 36283 th iteration gives loss of 0.1491396236466811\n",
      "The 36284 th iteration gives loss of 0.14913810551409082\n",
      "The 36285 th iteration gives loss of 0.1491365874336477\n",
      "The 36286 th iteration gives loss of 0.14913506940535007\n",
      "The 36287 th iteration gives loss of 0.14913355142920037\n",
      "The 36288 th iteration gives loss of 0.149132033505173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 36289 th iteration gives loss of 0.149130515633282\n",
      "The 36290 th iteration gives loss of 0.14912899781350203\n",
      "The 36291 th iteration gives loss of 0.14912748004584062\n",
      "The 36292 th iteration gives loss of 0.1491259623302846\n",
      "The 36293 th iteration gives loss of 0.1491244446668301\n",
      "The 36294 th iteration gives loss of 0.1491229270554646\n",
      "The 36295 th iteration gives loss of 0.14912140949619201\n",
      "The 36296 th iteration gives loss of 0.14911989198899706\n",
      "The 36297 th iteration gives loss of 0.1491183745338752\n",
      "The 36298 th iteration gives loss of 0.14911685713081457\n",
      "The 36299 th iteration gives loss of 0.14911533977981956\n",
      "The 36300 th iteration gives loss of 0.14911382248087693\n",
      "The 36301 th iteration gives loss of 0.149112305233983\n",
      "The 36302 th iteration gives loss of 0.14911078803912592\n",
      "The 36303 th iteration gives loss of 0.14910927089631248\n",
      "The 36304 th iteration gives loss of 0.1491077538055206\n",
      "The 36305 th iteration gives loss of 0.14910623676674553\n",
      "The 36306 th iteration gives loss of 0.14910471977998088\n",
      "The 36307 th iteration gives loss of 0.14910320284522868\n",
      "The 36308 th iteration gives loss of 0.14910168596248077\n",
      "The 36309 th iteration gives loss of 0.1491001691317209\n",
      "The 36310 th iteration gives loss of 0.14909865235295094\n",
      "The 36311 th iteration gives loss of 0.14909713562616644\n",
      "The 36312 th iteration gives loss of 0.14909561895134824\n",
      "The 36313 th iteration gives loss of 0.1490941023285009\n",
      "The 36314 th iteration gives loss of 0.14909258575761342\n",
      "The 36315 th iteration gives loss of 0.1490910692386884\n",
      "The 36316 th iteration gives loss of 0.14908955277170172\n",
      "The 36317 th iteration gives loss of 0.14908803635666032\n",
      "The 36318 th iteration gives loss of 0.1490865199935631\n",
      "The 36319 th iteration gives loss of 0.14908500368238403\n",
      "The 36320 th iteration gives loss of 0.1490834874231276\n",
      "The 36321 th iteration gives loss of 0.14908197121579322\n",
      "The 36322 th iteration gives loss of 0.14908045506035844\n",
      "The 36323 th iteration gives loss of 0.14907893895682608\n",
      "The 36324 th iteration gives loss of 0.14907742290518958\n",
      "The 36325 th iteration gives loss of 0.14907590690543682\n",
      "The 36326 th iteration gives loss of 0.1490743909575773\n",
      "The 36327 th iteration gives loss of 0.14907287506159103\n",
      "The 36328 th iteration gives loss of 0.1490713592174661\n",
      "The 36329 th iteration gives loss of 0.14906984342522095\n",
      "The 36330 th iteration gives loss of 0.1490683276848181\n",
      "The 36331 th iteration gives loss of 0.1490668119962671\n",
      "The 36332 th iteration gives loss of 0.1490652963595645\n",
      "The 36333 th iteration gives loss of 0.14906378077469667\n",
      "The 36334 th iteration gives loss of 0.14906226524165084\n",
      "The 36335 th iteration gives loss of 0.14906074976042713\n",
      "The 36336 th iteration gives loss of 0.14905923433102086\n",
      "The 36337 th iteration gives loss of 0.1490577189534293\n",
      "The 36338 th iteration gives loss of 0.1490562036276444\n",
      "The 36339 th iteration gives loss of 0.14905468835365512\n",
      "The 36340 th iteration gives loss of 0.14905317313145608\n",
      "The 36341 th iteration gives loss of 0.14905165796103315\n",
      "The 36342 th iteration gives loss of 0.14905014284239518\n",
      "The 36343 th iteration gives loss of 0.14904862777552746\n",
      "The 36344 th iteration gives loss of 0.14904711276041976\n",
      "The 36345 th iteration gives loss of 0.1490455977970726\n",
      "The 36346 th iteration gives loss of 0.14904408288547413\n",
      "The 36347 th iteration gives loss of 0.14904256802562013\n",
      "The 36348 th iteration gives loss of 0.14904105321750472\n",
      "The 36349 th iteration gives loss of 0.1490395384611169\n",
      "The 36350 th iteration gives loss of 0.14903802375646252\n",
      "The 36351 th iteration gives loss of 0.14903650910351657\n",
      "The 36352 th iteration gives loss of 0.1490349945022849\n",
      "The 36353 th iteration gives loss of 0.1490334799527633\n",
      "The 36354 th iteration gives loss of 0.14903196545493774\n",
      "The 36355 th iteration gives loss of 0.14903045100880588\n",
      "The 36356 th iteration gives loss of 0.1490289366143604\n",
      "The 36357 th iteration gives loss of 0.1490274222715884\n",
      "The 36358 th iteration gives loss of 0.14902590798048748\n",
      "The 36359 th iteration gives loss of 0.14902439374105758\n",
      "The 36360 th iteration gives loss of 0.14902287955328333\n",
      "The 36361 th iteration gives loss of 0.14902136541716907\n",
      "The 36362 th iteration gives loss of 0.14901985133269424\n",
      "The 36363 th iteration gives loss of 0.14901833729986277\n",
      "The 36364 th iteration gives loss of 0.1490168233186624\n",
      "The 36365 th iteration gives loss of 0.1490153093890938\n",
      "The 36366 th iteration gives loss of 0.14901379551113583\n",
      "The 36367 th iteration gives loss of 0.1490122816847881\n",
      "The 36368 th iteration gives loss of 0.1490107679100612\n",
      "The 36369 th iteration gives loss of 0.14900925418692115\n",
      "The 36370 th iteration gives loss of 0.1490077405153893\n",
      "The 36371 th iteration gives loss of 0.14900622689543913\n",
      "The 36372 th iteration gives loss of 0.14900471332706672\n",
      "The 36373 th iteration gives loss of 0.14900319981027044\n",
      "The 36374 th iteration gives loss of 0.14900168634504732\n",
      "The 36375 th iteration gives loss of 0.14900017293137358\n",
      "The 36376 th iteration gives loss of 0.148998659569258\n",
      "The 36377 th iteration gives loss of 0.14899714625869617\n",
      "The 36378 th iteration gives loss of 0.14899563299967125\n",
      "The 36379 th iteration gives loss of 0.14899411979219476\n",
      "The 36380 th iteration gives loss of 0.14899260663622843\n",
      "The 36381 th iteration gives loss of 0.1489910935317954\n",
      "The 36382 th iteration gives loss of 0.1489895804788755\n",
      "The 36383 th iteration gives loss of 0.14898806747746607\n",
      "The 36384 th iteration gives loss of 0.14898655452755746\n",
      "The 36385 th iteration gives loss of 0.14898504162913984\n",
      "The 36386 th iteration gives loss of 0.1489835287822166\n",
      "The 36387 th iteration gives loss of 0.14898201598677552\n",
      "The 36388 th iteration gives loss of 0.1489805032428126\n",
      "The 36389 th iteration gives loss of 0.14897899055031744\n",
      "The 36390 th iteration gives loss of 0.1489774779092847\n",
      "The 36391 th iteration gives loss of 0.14897596531971818\n",
      "The 36392 th iteration gives loss of 0.14897445278159085\n",
      "The 36393 th iteration gives loss of 0.14897294029491137\n",
      "The 36394 th iteration gives loss of 0.14897142785967613\n",
      "The 36395 th iteration gives loss of 0.1489699154758639\n",
      "The 36396 th iteration gives loss of 0.14896840314347562\n",
      "The 36397 th iteration gives loss of 0.14896689086250678\n",
      "The 36398 th iteration gives loss of 0.14896537863295276\n",
      "The 36399 th iteration gives loss of 0.14896386645479648\n",
      "The 36400 th iteration gives loss of 0.14896235432804625\n",
      "The 36401 th iteration gives loss of 0.14896084225268216\n",
      "The 36402 th iteration gives loss of 0.1489593302287056\n",
      "The 36403 th iteration gives loss of 0.1489578182561123\n",
      "The 36404 th iteration gives loss of 0.1489563063348905\n",
      "The 36405 th iteration gives loss of 0.14895479446502985\n",
      "The 36406 th iteration gives loss of 0.148953282646532\n",
      "The 36407 th iteration gives loss of 0.14895177087938546\n",
      "The 36408 th iteration gives loss of 0.14895025916358548\n",
      "The 36409 th iteration gives loss of 0.14894874749912584\n",
      "The 36410 th iteration gives loss of 0.14894723588600306\n",
      "The 36411 th iteration gives loss of 0.14894572432420372\n",
      "The 36412 th iteration gives loss of 0.14894421281372272\n",
      "The 36413 th iteration gives loss of 0.14894270135456497\n",
      "The 36414 th iteration gives loss of 0.14894118994670397\n",
      "The 36415 th iteration gives loss of 0.14893967859014215\n",
      "The 36416 th iteration gives loss of 0.14893816728488804\n",
      "The 36417 th iteration gives loss of 0.1489366560309223\n",
      "The 36418 th iteration gives loss of 0.14893514482822934\n",
      "The 36419 th iteration gives loss of 0.1489336336768125\n",
      "The 36420 th iteration gives loss of 0.14893212257666388\n",
      "The 36421 th iteration gives loss of 0.14893061152777634\n",
      "The 36422 th iteration gives loss of 0.1489291005301526\n",
      "The 36423 th iteration gives loss of 0.14892758958377544\n",
      "The 36424 th iteration gives loss of 0.14892607868863297\n",
      "The 36425 th iteration gives loss of 0.1489245678447396\n",
      "The 36426 th iteration gives loss of 0.14892305705206865\n",
      "The 36427 th iteration gives loss of 0.14892154631061766\n",
      "The 36428 th iteration gives loss of 0.1489200356203808\n",
      "The 36429 th iteration gives loss of 0.1489185249813607\n",
      "The 36430 th iteration gives loss of 0.14891701439354182\n",
      "The 36431 th iteration gives loss of 0.14891550385692418\n",
      "The 36432 th iteration gives loss of 0.14891399337150124\n",
      "The 36433 th iteration gives loss of 0.14891248293725895\n",
      "The 36434 th iteration gives loss of 0.1489109725541961\n",
      "The 36435 th iteration gives loss of 0.14890946222229753\n",
      "The 36436 th iteration gives loss of 0.14890795194157389\n",
      "The 36437 th iteration gives loss of 0.14890644171200473\n",
      "The 36438 th iteration gives loss of 0.14890493153358655\n",
      "The 36439 th iteration gives loss of 0.1489034214063189\n",
      "The 36440 th iteration gives loss of 0.14890191133017877\n",
      "The 36441 th iteration gives loss of 0.14890040130518267\n",
      "The 36442 th iteration gives loss of 0.14889889133131\n",
      "The 36443 th iteration gives loss of 0.14889738140855763\n",
      "The 36444 th iteration gives loss of 0.148895871536925\n",
      "The 36445 th iteration gives loss of 0.14889436171638676\n",
      "The 36446 th iteration gives loss of 0.14889285194695245\n",
      "The 36447 th iteration gives loss of 0.1488913422286125\n",
      "The 36448 th iteration gives loss of 0.1488898325613698\n",
      "The 36449 th iteration gives loss of 0.1488883229451999\n",
      "The 36450 th iteration gives loss of 0.14888681338010443\n",
      "The 36451 th iteration gives loss of 0.1488853038660824\n",
      "The 36452 th iteration gives loss of 0.14888379440311603\n",
      "The 36453 th iteration gives loss of 0.14888228499121578\n",
      "The 36454 th iteration gives loss of 0.1488807756303537\n",
      "The 36455 th iteration gives loss of 0.1488792663205353\n",
      "The 36456 th iteration gives loss of 0.14887775706175607\n",
      "The 36457 th iteration gives loss of 0.14887624785400175\n",
      "The 36458 th iteration gives loss of 0.14887473869727363\n",
      "The 36459 th iteration gives loss of 0.14887322959156563\n",
      "The 36460 th iteration gives loss of 0.14887172053686457\n",
      "The 36461 th iteration gives loss of 0.14887021153316912\n",
      "The 36462 th iteration gives loss of 0.14886870258047463\n",
      "The 36463 th iteration gives loss of 0.14886719367876483\n",
      "The 36464 th iteration gives loss of 0.14886568482803836\n",
      "The 36465 th iteration gives loss of 0.1488641760282987\n",
      "The 36466 th iteration gives loss of 0.14886266727952402\n",
      "The 36467 th iteration gives loss of 0.14886115858171917\n",
      "The 36468 th iteration gives loss of 0.1488596499348701\n",
      "The 36469 th iteration gives loss of 0.14885814133897177\n",
      "The 36470 th iteration gives loss of 0.1488566327940167\n",
      "The 36471 th iteration gives loss of 0.14885512430001213\n",
      "The 36472 th iteration gives loss of 0.14885361585693718\n",
      "The 36473 th iteration gives loss of 0.1488521074647911\n",
      "The 36474 th iteration gives loss of 0.14885059912355922\n",
      "The 36475 th iteration gives loss of 0.14884909083324005\n",
      "The 36476 th iteration gives loss of 0.14884758259382846\n",
      "The 36477 th iteration gives loss of 0.14884607440531925\n",
      "The 36478 th iteration gives loss of 0.14884456626771114\n",
      "The 36479 th iteration gives loss of 0.14884305818098537\n",
      "The 36480 th iteration gives loss of 0.14884155014514014\n",
      "The 36481 th iteration gives loss of 0.1488400421601761\n",
      "The 36482 th iteration gives loss of 0.14883853422608176\n",
      "The 36483 th iteration gives loss of 0.1488370263428384\n",
      "The 36484 th iteration gives loss of 0.14883551851045884\n",
      "The 36485 th iteration gives loss of 0.14883401072893065\n",
      "The 36486 th iteration gives loss of 0.14883250299824138\n",
      "The 36487 th iteration gives loss of 0.14883099531839275\n",
      "The 36488 th iteration gives loss of 0.14882948768936902\n",
      "The 36489 th iteration gives loss of 0.1488279801111737\n",
      "The 36490 th iteration gives loss of 0.14882647258379977\n",
      "The 36491 th iteration gives loss of 0.14882496510723917\n",
      "The 36492 th iteration gives loss of 0.14882345768147776\n",
      "The 36493 th iteration gives loss of 0.14882195030651246\n",
      "The 36494 th iteration gives loss of 0.14882044298234381\n",
      "The 36495 th iteration gives loss of 0.14881893570895907\n",
      "The 36496 th iteration gives loss of 0.14881742848635487\n",
      "The 36497 th iteration gives loss of 0.14881592131452118\n",
      "The 36498 th iteration gives loss of 0.14881441419345948\n",
      "The 36499 th iteration gives loss of 0.14881290712315196\n",
      "The 36500 th iteration gives loss of 0.14881140010359242\n",
      "The 36501 th iteration gives loss of 0.1488098931347892\n",
      "The 36502 th iteration gives loss of 0.14880838621672685\n",
      "The 36503 th iteration gives loss of 0.14880687934939957\n",
      "The 36504 th iteration gives loss of 0.1488053725328014\n",
      "The 36505 th iteration gives loss of 0.1488038657669152\n",
      "The 36506 th iteration gives loss of 0.14880235905175376\n",
      "The 36507 th iteration gives loss of 0.14880085238729662\n",
      "The 36508 th iteration gives loss of 0.14879934577354248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 36509 th iteration gives loss of 0.1487978392104906\n",
      "The 36510 th iteration gives loss of 0.1487963326981209\n",
      "The 36511 th iteration gives loss of 0.1487948262364339\n",
      "The 36512 th iteration gives loss of 0.1487933198254306\n",
      "The 36513 th iteration gives loss of 0.14879181346509282\n",
      "The 36514 th iteration gives loss of 0.14879030715542263\n",
      "The 36515 th iteration gives loss of 0.14878880089640356\n",
      "The 36516 th iteration gives loss of 0.148787294688047\n",
      "The 36517 th iteration gives loss of 0.14878578853032795\n",
      "The 36518 th iteration gives loss of 0.1487842824232467\n",
      "The 36519 th iteration gives loss of 0.14878277636680234\n",
      "The 36520 th iteration gives loss of 0.1487812703609827\n",
      "The 36521 th iteration gives loss of 0.14877976440578544\n",
      "The 36522 th iteration gives loss of 0.14877825850119428\n",
      "The 36523 th iteration gives loss of 0.14877675264721507\n",
      "The 36524 th iteration gives loss of 0.14877524684383364\n",
      "The 36525 th iteration gives loss of 0.14877374109104594\n",
      "The 36526 th iteration gives loss of 0.1487722353888485\n",
      "The 36527 th iteration gives loss of 0.1487707297372337\n",
      "The 36528 th iteration gives loss of 0.14876922413619156\n",
      "The 36529 th iteration gives loss of 0.14876771858571738\n",
      "The 36530 th iteration gives loss of 0.1487662130858079\n",
      "The 36531 th iteration gives loss of 0.14876470763645128\n",
      "The 36532 th iteration gives loss of 0.1487632022376466\n",
      "The 36533 th iteration gives loss of 0.148761696889384\n",
      "The 36534 th iteration gives loss of 0.14876019159165504\n",
      "The 36535 th iteration gives loss of 0.1487586863444645\n",
      "The 36536 th iteration gives loss of 0.14875718114779277\n",
      "The 36537 th iteration gives loss of 0.1487556760016355\n",
      "The 36538 th iteration gives loss of 0.14875417090599546\n",
      "The 36539 th iteration gives loss of 0.14875266586085534\n",
      "The 36540 th iteration gives loss of 0.14875116086622497\n",
      "The 36541 th iteration gives loss of 0.14874965592207298\n",
      "The 36542 th iteration gives loss of 0.14874815102841554\n",
      "The 36543 th iteration gives loss of 0.14874664618523312\n",
      "The 36544 th iteration gives loss of 0.1487451413925254\n",
      "The 36545 th iteration gives loss of 0.1487436366502878\n",
      "The 36546 th iteration gives loss of 0.14874213195850516\n",
      "The 36547 th iteration gives loss of 0.14874062731717988\n",
      "The 36548 th iteration gives loss of 0.14873912272630227\n",
      "The 36549 th iteration gives loss of 0.14873761818586612\n",
      "The 36550 th iteration gives loss of 0.14873611369586165\n",
      "The 36551 th iteration gives loss of 0.14873460925628704\n",
      "The 36552 th iteration gives loss of 0.14873310486713767\n",
      "The 36553 th iteration gives loss of 0.1487316005284081\n",
      "The 36554 th iteration gives loss of 0.14873009624008507\n",
      "The 36555 th iteration gives loss of 0.14872859200215946\n",
      "The 36556 th iteration gives loss of 0.14872708781464025\n",
      "The 36557 th iteration gives loss of 0.14872558367750924\n",
      "The 36558 th iteration gives loss of 0.1487240795907567\n",
      "The 36559 th iteration gives loss of 0.14872257555438323\n",
      "The 36560 th iteration gives loss of 0.1487210715683822\n",
      "The 36561 th iteration gives loss of 0.1487195676327551\n",
      "The 36562 th iteration gives loss of 0.1487180637474816\n",
      "The 36563 th iteration gives loss of 0.14871655991256205\n",
      "The 36564 th iteration gives loss of 0.14871505612798666\n",
      "The 36565 th iteration gives loss of 0.1487135523937467\n",
      "The 36566 th iteration gives loss of 0.14871204870985208\n",
      "The 36567 th iteration gives loss of 0.1487105450762724\n",
      "The 36568 th iteration gives loss of 0.1487090414930235\n",
      "The 36569 th iteration gives loss of 0.14870753796008704\n",
      "The 36570 th iteration gives loss of 0.14870603447745895\n",
      "The 36571 th iteration gives loss of 0.14870453104512518\n",
      "The 36572 th iteration gives loss of 0.14870302766310114\n",
      "The 36573 th iteration gives loss of 0.14870152433135367\n",
      "The 36574 th iteration gives loss of 0.14870002104989422\n",
      "The 36575 th iteration gives loss of 0.1486985178187089\n",
      "The 36576 th iteration gives loss of 0.14869701463779872\n",
      "The 36577 th iteration gives loss of 0.14869551150715044\n",
      "The 36578 th iteration gives loss of 0.14869400842675087\n",
      "The 36579 th iteration gives loss of 0.14869250539662685\n",
      "The 36580 th iteration gives loss of 0.14869100241672742\n",
      "The 36581 th iteration gives loss of 0.14868949948707202\n",
      "The 36582 th iteration gives loss of 0.14868799660765214\n",
      "The 36583 th iteration gives loss of 0.1486864937784579\n",
      "The 36584 th iteration gives loss of 0.14868499099948582\n",
      "The 36585 th iteration gives loss of 0.1486834882707266\n",
      "The 36586 th iteration gives loss of 0.1486819855921722\n",
      "The 36587 th iteration gives loss of 0.14868048296381903\n",
      "The 36588 th iteration gives loss of 0.1486789803856532\n",
      "The 36589 th iteration gives loss of 0.14867747785768431\n",
      "The 36590 th iteration gives loss of 0.14867597537989788\n",
      "The 36591 th iteration gives loss of 0.1486744729522821\n",
      "The 36592 th iteration gives loss of 0.14867297057484377\n",
      "The 36593 th iteration gives loss of 0.14867146824756658\n",
      "The 36594 th iteration gives loss of 0.14866996597043672\n",
      "The 36595 th iteration gives loss of 0.14866846374346954\n",
      "The 36596 th iteration gives loss of 0.14866696156663975\n",
      "The 36597 th iteration gives loss of 0.148665459439956\n",
      "The 36598 th iteration gives loss of 0.14866395736339286\n",
      "The 36599 th iteration gives loss of 0.1486624553369545\n",
      "The 36600 th iteration gives loss of 0.14866095336064072\n",
      "The 36601 th iteration gives loss of 0.1486594514344449\n",
      "The 36602 th iteration gives loss of 0.14865794955834777\n",
      "The 36603 th iteration gives loss of 0.14865644773235082\n",
      "The 36604 th iteration gives loss of 0.14865494595645362\n",
      "The 36605 th iteration gives loss of 0.14865344423063842\n",
      "The 36606 th iteration gives loss of 0.14865194255490763\n",
      "The 36607 th iteration gives loss of 0.14865044092925514\n",
      "The 36608 th iteration gives loss of 0.14864893935366683\n",
      "The 36609 th iteration gives loss of 0.14864743782813947\n",
      "The 36610 th iteration gives loss of 0.14864593635267048\n",
      "The 36611 th iteration gives loss of 0.1486444349272491\n",
      "The 36612 th iteration gives loss of 0.14864293355187816\n",
      "The 36613 th iteration gives loss of 0.148641432226543\n",
      "The 36614 th iteration gives loss of 0.14863993095123323\n",
      "The 36615 th iteration gives loss of 0.1486384297259543\n",
      "The 36616 th iteration gives loss of 0.14863692855069016\n",
      "The 36617 th iteration gives loss of 0.14863542742544086\n",
      "The 36618 th iteration gives loss of 0.14863392635019226\n",
      "The 36619 th iteration gives loss of 0.14863242532494592\n",
      "The 36620 th iteration gives loss of 0.14863092434968883\n",
      "The 36621 th iteration gives loss of 0.14862942342442828\n",
      "The 36622 th iteration gives loss of 0.1486279225491425\n",
      "The 36623 th iteration gives loss of 0.14862642172383964\n",
      "The 36624 th iteration gives loss of 0.1486249209484893\n",
      "The 36625 th iteration gives loss of 0.14862342022310737\n",
      "The 36626 th iteration gives loss of 0.14862191954768708\n",
      "The 36627 th iteration gives loss of 0.14862041892220892\n",
      "The 36628 th iteration gives loss of 0.14861891834668545\n",
      "The 36629 th iteration gives loss of 0.1486174178210919\n",
      "The 36630 th iteration gives loss of 0.14861591734543309\n",
      "The 36631 th iteration gives loss of 0.14861441691968402\n",
      "The 36632 th iteration gives loss of 0.14861291654387127\n",
      "The 36633 th iteration gives loss of 0.148611416217959\n",
      "The 36634 th iteration gives loss of 0.14860991594195685\n",
      "The 36635 th iteration gives loss of 0.14860841571585473\n",
      "The 36636 th iteration gives loss of 0.1486069155396443\n",
      "The 36637 th iteration gives loss of 0.14860541541331093\n",
      "The 36638 th iteration gives loss of 0.1486039153368708\n",
      "The 36639 th iteration gives loss of 0.14860241531030322\n",
      "The 36640 th iteration gives loss of 0.14860091533360392\n",
      "The 36641 th iteration gives loss of 0.14859941540676186\n",
      "The 36642 th iteration gives loss of 0.14859791552977958\n",
      "The 36643 th iteration gives loss of 0.1485964157026445\n",
      "The 36644 th iteration gives loss of 0.14859491592535382\n",
      "The 36645 th iteration gives loss of 0.14859341619790142\n",
      "The 36646 th iteration gives loss of 0.14859191652028025\n",
      "The 36647 th iteration gives loss of 0.14859041689248262\n",
      "The 36648 th iteration gives loss of 0.1485889173144995\n",
      "The 36649 th iteration gives loss of 0.14858741778632595\n",
      "The 36650 th iteration gives loss of 0.14858591830796375\n",
      "The 36651 th iteration gives loss of 0.1485844188793996\n",
      "The 36652 th iteration gives loss of 0.14858291950063002\n",
      "The 36653 th iteration gives loss of 0.14858142017164447\n",
      "The 36654 th iteration gives loss of 0.14857992089243713\n",
      "The 36655 th iteration gives loss of 0.14857842166300908\n",
      "The 36656 th iteration gives loss of 0.14857692248335178\n",
      "The 36657 th iteration gives loss of 0.14857542335345036\n",
      "The 36658 th iteration gives loss of 0.14857392427331403\n",
      "The 36659 th iteration gives loss of 0.1485724252429101\n",
      "The 36660 th iteration gives loss of 0.14857092626226232\n",
      "The 36661 th iteration gives loss of 0.14856942733134784\n",
      "The 36662 th iteration gives loss of 0.14856792845016406\n",
      "The 36663 th iteration gives loss of 0.14856642961870742\n",
      "The 36664 th iteration gives loss of 0.14856493083696287\n",
      "The 36665 th iteration gives loss of 0.14856343210493592\n",
      "The 36666 th iteration gives loss of 0.1485619334226087\n",
      "The 36667 th iteration gives loss of 0.1485604347899838\n",
      "The 36668 th iteration gives loss of 0.14855893620705232\n",
      "The 36669 th iteration gives loss of 0.14855743767381113\n",
      "The 36670 th iteration gives loss of 0.14855593919025364\n",
      "The 36671 th iteration gives loss of 0.14855444075636637\n",
      "The 36672 th iteration gives loss of 0.14855294237213987\n",
      "The 36673 th iteration gives loss of 0.1485514440375793\n",
      "The 36674 th iteration gives loss of 0.14854994575268513\n",
      "The 36675 th iteration gives loss of 0.14854844751743165\n",
      "The 36676 th iteration gives loss of 0.14854694933182358\n",
      "The 36677 th iteration gives loss of 0.1485454511958548\n",
      "The 36678 th iteration gives loss of 0.14854395310951746\n",
      "The 36679 th iteration gives loss of 0.14854245507279945\n",
      "The 36680 th iteration gives loss of 0.14854095708569756\n",
      "The 36681 th iteration gives loss of 0.14853945914821837\n",
      "The 36682 th iteration gives loss of 0.14853796126034047\n",
      "The 36683 th iteration gives loss of 0.14853646342206625\n",
      "The 36684 th iteration gives loss of 0.14853496563337934\n",
      "The 36685 th iteration gives loss of 0.14853346789427824\n",
      "The 36686 th iteration gives loss of 0.14853197020477277\n",
      "The 36687 th iteration gives loss of 0.1485304725648346\n",
      "The 36688 th iteration gives loss of 0.1485289749744591\n",
      "The 36689 th iteration gives loss of 0.14852747743365094\n",
      "The 36690 th iteration gives loss of 0.14852597994239478\n",
      "The 36691 th iteration gives loss of 0.14852448250070172\n",
      "The 36692 th iteration gives loss of 0.1485229851085508\n",
      "The 36693 th iteration gives loss of 0.14852148776592294\n",
      "The 36694 th iteration gives loss of 0.14851999047283052\n",
      "The 36695 th iteration gives loss of 0.14851849322927285\n",
      "The 36696 th iteration gives loss of 0.14851699603523458\n",
      "The 36697 th iteration gives loss of 0.14851549889070634\n",
      "The 36698 th iteration gives loss of 0.14851400179568475\n",
      "The 36699 th iteration gives loss of 0.14851250475015865\n",
      "The 36700 th iteration gives loss of 0.14851100775412882\n",
      "The 36701 th iteration gives loss of 0.14850951080759017\n",
      "The 36702 th iteration gives loss of 0.14850801391054186\n",
      "The 36703 th iteration gives loss of 0.14850651706295767\n",
      "The 36704 th iteration gives loss of 0.14850502026485016\n",
      "The 36705 th iteration gives loss of 0.14850352351619964\n",
      "The 36706 th iteration gives loss of 0.14850202681701283\n",
      "The 36707 th iteration gives loss of 0.14850053016726866\n",
      "The 36708 th iteration gives loss of 0.14849903356697636\n",
      "The 36709 th iteration gives loss of 0.14849753701611904\n",
      "The 36710 th iteration gives loss of 0.148496040514699\n",
      "The 36711 th iteration gives loss of 0.14849454406270446\n",
      "The 36712 th iteration gives loss of 0.14849304766012858\n",
      "The 36713 th iteration gives loss of 0.14849155130696332\n",
      "The 36714 th iteration gives loss of 0.14849005500321\n",
      "The 36715 th iteration gives loss of 0.1484885587488611\n",
      "The 36716 th iteration gives loss of 0.14848706254390995\n",
      "The 36717 th iteration gives loss of 0.14848556638833738\n",
      "The 36718 th iteration gives loss of 0.14848407028215516\n",
      "The 36719 th iteration gives loss of 0.14848257422534603\n",
      "The 36720 th iteration gives loss of 0.1484810782179107\n",
      "The 36721 th iteration gives loss of 0.1484795822598412\n",
      "The 36722 th iteration gives loss of 0.14847808635112292\n",
      "The 36723 th iteration gives loss of 0.14847659049176806\n",
      "The 36724 th iteration gives loss of 0.1484750946817583\n",
      "The 36725 th iteration gives loss of 0.14847359892107811\n",
      "The 36726 th iteration gives loss of 0.14847210320973755\n",
      "The 36727 th iteration gives loss of 0.1484706075477223\n",
      "The 36728 th iteration gives loss of 0.1484691119350287\n",
      "The 36729 th iteration gives loss of 0.14846761637165265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 36730 th iteration gives loss of 0.1484661208575787\n",
      "The 36731 th iteration gives loss of 0.14846462539282157\n",
      "The 36732 th iteration gives loss of 0.14846312997735023\n",
      "The 36733 th iteration gives loss of 0.14846163461117576\n",
      "The 36734 th iteration gives loss of 0.1484601392942814\n",
      "The 36735 th iteration gives loss of 0.14845864402666994\n",
      "The 36736 th iteration gives loss of 0.14845714880833058\n",
      "The 36737 th iteration gives loss of 0.14845565363924948\n",
      "The 36738 th iteration gives loss of 0.14845415851943405\n",
      "The 36739 th iteration gives loss of 0.14845266344886887\n",
      "The 36740 th iteration gives loss of 0.14845116842755723\n",
      "The 36741 th iteration gives loss of 0.14844967345548718\n",
      "The 36742 th iteration gives loss of 0.1484481785326416\n",
      "The 36743 th iteration gives loss of 0.14844668365903252\n",
      "The 36744 th iteration gives loss of 0.14844518883464664\n",
      "The 36745 th iteration gives loss of 0.14844369405948307\n",
      "The 36746 th iteration gives loss of 0.1484421993335167\n",
      "The 36747 th iteration gives loss of 0.14844070465676365\n",
      "The 36748 th iteration gives loss of 0.1484392100292044\n",
      "The 36749 th iteration gives loss of 0.14843771545083642\n",
      "The 36750 th iteration gives loss of 0.14843622092165873\n",
      "The 36751 th iteration gives loss of 0.14843472644166214\n",
      "The 36752 th iteration gives loss of 0.14843323201083738\n",
      "The 36753 th iteration gives loss of 0.14843173762917491\n",
      "The 36754 th iteration gives loss of 0.14843024329668023\n",
      "The 36755 th iteration gives loss of 0.1484287490133405\n",
      "The 36756 th iteration gives loss of 0.14842725477915272\n",
      "The 36757 th iteration gives loss of 0.1484257605941015\n",
      "The 36758 th iteration gives loss of 0.14842426645818482\n",
      "The 36759 th iteration gives loss of 0.14842277237140133\n",
      "The 36760 th iteration gives loss of 0.14842127833374288\n",
      "The 36761 th iteration gives loss of 0.14841978434520017\n",
      "The 36762 th iteration gives loss of 0.14841829040578142\n",
      "The 36763 th iteration gives loss of 0.14841679651545772\n",
      "The 36764 th iteration gives loss of 0.14841530267423245\n",
      "The 36765 th iteration gives loss of 0.1484138088821055\n",
      "The 36766 th iteration gives loss of 0.14841231513906927\n",
      "The 36767 th iteration gives loss of 0.1484108214451096\n",
      "The 36768 th iteration gives loss of 0.14840932780022847\n",
      "The 36769 th iteration gives loss of 0.1484078342044151\n",
      "The 36770 th iteration gives loss of 0.14840634065767042\n",
      "The 36771 th iteration gives loss of 0.14840484715997304\n",
      "The 36772 th iteration gives loss of 0.14840335371132757\n",
      "The 36773 th iteration gives loss of 0.1484018603117383\n",
      "The 36774 th iteration gives loss of 0.14840036696118\n",
      "The 36775 th iteration gives loss of 0.1483988736596583\n",
      "The 36776 th iteration gives loss of 0.1483973804071597\n",
      "The 36777 th iteration gives loss of 0.1483958872036797\n",
      "The 36778 th iteration gives loss of 0.14839439404921334\n",
      "The 36779 th iteration gives loss of 0.14839290094375174\n",
      "The 36780 th iteration gives loss of 0.14839140788730457\n",
      "The 36781 th iteration gives loss of 0.14838991487984216\n",
      "The 36782 th iteration gives loss of 0.14838842192137391\n",
      "The 36783 th iteration gives loss of 0.14838692901188577\n",
      "The 36784 th iteration gives loss of 0.14838543615138475\n",
      "The 36785 th iteration gives loss of 0.14838394333984842\n",
      "The 36786 th iteration gives loss of 0.14838245057728044\n",
      "The 36787 th iteration gives loss of 0.14838095786366426\n",
      "The 36788 th iteration gives loss of 0.14837946519900297\n",
      "The 36789 th iteration gives loss of 0.14837797258329238\n",
      "The 36790 th iteration gives loss of 0.14837648001652337\n",
      "The 36791 th iteration gives loss of 0.14837498749868755\n",
      "The 36792 th iteration gives loss of 0.14837349502977792\n",
      "The 36793 th iteration gives loss of 0.14837200260979458\n",
      "The 36794 th iteration gives loss of 0.1483705102387241\n",
      "The 36795 th iteration gives loss of 0.14836901791656942\n",
      "The 36796 th iteration gives loss of 0.14836752564331493\n",
      "The 36797 th iteration gives loss of 0.14836603341896248\n",
      "The 36798 th iteration gives loss of 0.14836454124349924\n",
      "The 36799 th iteration gives loss of 0.14836304911692508\n",
      "The 36800 th iteration gives loss of 0.14836155703921913\n",
      "The 36801 th iteration gives loss of 0.14836006501040144\n",
      "The 36802 th iteration gives loss of 0.14835857303043998\n",
      "The 36803 th iteration gives loss of 0.14835708109934945\n",
      "The 36804 th iteration gives loss of 0.14835558921710892\n",
      "The 36805 th iteration gives loss of 0.1483540973837193\n",
      "The 36806 th iteration gives loss of 0.1483526055991716\n",
      "The 36807 th iteration gives loss of 0.14835111386346267\n",
      "The 36808 th iteration gives loss of 0.14834962217658457\n",
      "The 36809 th iteration gives loss of 0.1483481305385306\n",
      "The 36810 th iteration gives loss of 0.14834663894929506\n",
      "The 36811 th iteration gives loss of 0.1483451474088728\n",
      "The 36812 th iteration gives loss of 0.14834365591725807\n",
      "The 36813 th iteration gives loss of 0.14834216447444676\n",
      "The 36814 th iteration gives loss of 0.14834067308042392\n",
      "The 36815 th iteration gives loss of 0.14833918173519395\n",
      "The 36816 th iteration gives loss of 0.1483376904387483\n",
      "The 36817 th iteration gives loss of 0.1483361991910745\n",
      "The 36818 th iteration gives loss of 0.14833470799216827\n",
      "The 36819 th iteration gives loss of 0.1483332168420337\n",
      "The 36820 th iteration gives loss of 0.14833172574065176\n",
      "The 36821 th iteration gives loss of 0.1483302346880212\n",
      "The 36822 th iteration gives loss of 0.1483287436841381\n",
      "The 36823 th iteration gives loss of 0.14832725272899291\n",
      "The 36824 th iteration gives loss of 0.1483257618225835\n",
      "The 36825 th iteration gives loss of 0.1483242709649084\n",
      "The 36826 th iteration gives loss of 0.14832278015594685\n",
      "The 36827 th iteration gives loss of 0.148321289395699\n",
      "The 36828 th iteration gives loss of 0.14831979868416537\n",
      "The 36829 th iteration gives loss of 0.14831830802133494\n",
      "The 36830 th iteration gives loss of 0.14831681740720318\n",
      "The 36831 th iteration gives loss of 0.1483153268417589\n",
      "The 36832 th iteration gives loss of 0.14831383632500303\n",
      "The 36833 th iteration gives loss of 0.14831234585692324\n",
      "The 36834 th iteration gives loss of 0.1483108554375193\n",
      "The 36835 th iteration gives loss of 0.14830936506678086\n",
      "The 36836 th iteration gives loss of 0.14830787474470175\n",
      "The 36837 th iteration gives loss of 0.14830638447127764\n",
      "The 36838 th iteration gives loss of 0.1483048942465058\n",
      "The 36839 th iteration gives loss of 0.14830340407037135\n",
      "The 36840 th iteration gives loss of 0.14830191394288653\n",
      "The 36841 th iteration gives loss of 0.14830042386401612\n",
      "The 36842 th iteration gives loss of 0.14829893383377227\n",
      "The 36843 th iteration gives loss of 0.14829744385215543\n",
      "The 36844 th iteration gives loss of 0.14829595391914976\n",
      "The 36845 th iteration gives loss of 0.14829446403474403\n",
      "The 36846 th iteration gives loss of 0.1482929741989418\n",
      "The 36847 th iteration gives loss of 0.14829148441173604\n",
      "The 36848 th iteration gives loss of 0.1482899946731177\n",
      "The 36849 th iteration gives loss of 0.14828850498307863\n",
      "The 36850 th iteration gives loss of 0.1482870153416211\n",
      "The 36851 th iteration gives loss of 0.14828552574873088\n",
      "The 36852 th iteration gives loss of 0.14828403620440608\n",
      "The 36853 th iteration gives loss of 0.14828254670863228\n",
      "The 36854 th iteration gives loss of 0.14828105726141833\n",
      "The 36855 th iteration gives loss of 0.1482795678627491\n",
      "The 36856 th iteration gives loss of 0.1482780785126179\n",
      "The 36857 th iteration gives loss of 0.14827658921101639\n",
      "The 36858 th iteration gives loss of 0.1482750999579479\n",
      "The 36859 th iteration gives loss of 0.1482736107534007\n",
      "The 36860 th iteration gives loss of 0.1482721215973722\n",
      "The 36861 th iteration gives loss of 0.14827063248984507\n",
      "The 36862 th iteration gives loss of 0.14826914343082753\n",
      "The 36863 th iteration gives loss of 0.14826765442031167\n",
      "The 36864 th iteration gives loss of 0.14826616545827595\n",
      "The 36865 th iteration gives loss of 0.14826467654473138\n",
      "The 36866 th iteration gives loss of 0.1482631876796704\n",
      "The 36867 th iteration gives loss of 0.1482616988630747\n",
      "The 36868 th iteration gives loss of 0.14826021009494963\n",
      "The 36869 th iteration gives loss of 0.14825872137529375\n",
      "The 36870 th iteration gives loss of 0.14825723270408434\n",
      "The 36871 th iteration gives loss of 0.14825574408132586\n",
      "The 36872 th iteration gives loss of 0.14825425550700522\n",
      "The 36873 th iteration gives loss of 0.14825276698113266\n",
      "The 36874 th iteration gives loss of 0.1482512785036827\n",
      "The 36875 th iteration gives loss of 0.14824979007466718\n",
      "The 36876 th iteration gives loss of 0.14824830169405667\n",
      "The 36877 th iteration gives loss of 0.1482468133618703\n",
      "The 36878 th iteration gives loss of 0.14824532507809185\n",
      "The 36879 th iteration gives loss of 0.14824383684271183\n",
      "The 36880 th iteration gives loss of 0.1482423486557323\n",
      "The 36881 th iteration gives loss of 0.14824086051713214\n",
      "The 36882 th iteration gives loss of 0.14823937242691565\n",
      "The 36883 th iteration gives loss of 0.1482378843850821\n",
      "The 36884 th iteration gives loss of 0.14823639639161063\n",
      "The 36885 th iteration gives loss of 0.1482349084465154\n",
      "The 36886 th iteration gives loss of 0.14823342054976582\n",
      "The 36887 th iteration gives loss of 0.14823193270137444\n",
      "The 36888 th iteration gives loss of 0.14823044490134096\n",
      "The 36889 th iteration gives loss of 0.1482289571496395\n",
      "The 36890 th iteration gives loss of 0.14822746944626733\n",
      "The 36891 th iteration gives loss of 0.14822598179123003\n",
      "The 36892 th iteration gives loss of 0.14822449418451514\n",
      "The 36893 th iteration gives loss of 0.14822300662612145\n",
      "The 36894 th iteration gives loss of 0.14822151911603787\n",
      "The 36895 th iteration gives loss of 0.14822003165425643\n",
      "The 36896 th iteration gives loss of 0.14821854424077913\n",
      "The 36897 th iteration gives loss of 0.14821705687558376\n",
      "The 36898 th iteration gives loss of 0.1482155695586804\n",
      "The 36899 th iteration gives loss of 0.14821408229005356\n",
      "The 36900 th iteration gives loss of 0.14821259506970438\n",
      "The 36901 th iteration gives loss of 0.1482111078976319\n",
      "The 36902 th iteration gives loss of 0.14820962077381553\n",
      "The 36903 th iteration gives loss of 0.14820813369825767\n",
      "The 36904 th iteration gives loss of 0.14820664667095118\n",
      "The 36905 th iteration gives loss of 0.14820515969189202\n",
      "The 36906 th iteration gives loss of 0.1482036727610639\n",
      "The 36907 th iteration gives loss of 0.14820218587847064\n",
      "The 36908 th iteration gives loss of 0.14820069904410424\n",
      "The 36909 th iteration gives loss of 0.1481992122579622\n",
      "The 36910 th iteration gives loss of 0.14819772552003244\n",
      "The 36911 th iteration gives loss of 0.14819623883031582\n",
      "The 36912 th iteration gives loss of 0.14819475218879016\n",
      "The 36913 th iteration gives loss of 0.1481932655954731\n",
      "The 36914 th iteration gives loss of 0.14819177905034808\n",
      "The 36915 th iteration gives loss of 0.1481902925534006\n",
      "The 36916 th iteration gives loss of 0.14818880610463767\n",
      "The 36917 th iteration gives loss of 0.14818731970403484\n",
      "The 36918 th iteration gives loss of 0.1481858333516073\n",
      "The 36919 th iteration gives loss of 0.14818434704734676\n",
      "The 36920 th iteration gives loss of 0.1481828607912386\n",
      "The 36921 th iteration gives loss of 0.14818137458327418\n",
      "The 36922 th iteration gives loss of 0.14817988842346214\n",
      "The 36923 th iteration gives loss of 0.14817840231177518\n",
      "The 36924 th iteration gives loss of 0.14817691624822749\n",
      "The 36925 th iteration gives loss of 0.14817543023280738\n",
      "The 36926 th iteration gives loss of 0.14817394426550304\n",
      "The 36927 th iteration gives loss of 0.1481724583463067\n",
      "The 36928 th iteration gives loss of 0.14817097247522146\n",
      "The 36929 th iteration gives loss of 0.14816948665223673\n",
      "The 36930 th iteration gives loss of 0.14816800087734888\n",
      "The 36931 th iteration gives loss of 0.14816651515055182\n",
      "The 36932 th iteration gives loss of 0.14816502947183602\n",
      "The 36933 th iteration gives loss of 0.14816354384119595\n",
      "The 36934 th iteration gives loss of 0.14816205825863094\n",
      "The 36935 th iteration gives loss of 0.14816057272412272\n",
      "The 36936 th iteration gives loss of 0.14815908723768328\n",
      "The 36937 th iteration gives loss of 0.14815760179928944\n",
      "The 36938 th iteration gives loss of 0.14815611640894827\n",
      "The 36939 th iteration gives loss of 0.1481546310666526\n",
      "The 36940 th iteration gives loss of 0.14815314577238853\n",
      "The 36941 th iteration gives loss of 0.14815166052614737\n",
      "The 36942 th iteration gives loss of 0.14815017532794145\n",
      "The 36943 th iteration gives loss of 0.14814869017774057\n",
      "The 36944 th iteration gives loss of 0.14814720507555978\n",
      "The 36945 th iteration gives loss of 0.148145720021382\n",
      "The 36946 th iteration gives loss of 0.14814423501520652\n",
      "The 36947 th iteration gives loss of 0.1481427500570238\n",
      "The 36948 th iteration gives loss of 0.14814126514682283\n",
      "The 36949 th iteration gives loss of 0.14813978028461527\n",
      "The 36950 th iteration gives loss of 0.1481382954703719\n",
      "The 36951 th iteration gives loss of 0.14813681070410595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 36952 th iteration gives loss of 0.148135325985805\n",
      "The 36953 th iteration gives loss of 0.14813384131545973\n",
      "The 36954 th iteration gives loss of 0.14813235669306302\n",
      "The 36955 th iteration gives loss of 0.14813087211861845\n",
      "The 36956 th iteration gives loss of 0.14812938759211033\n",
      "The 36957 th iteration gives loss of 0.14812790311353907\n",
      "The 36958 th iteration gives loss of 0.1481264186828975\n",
      "The 36959 th iteration gives loss of 0.14812493430017382\n",
      "The 36960 th iteration gives loss of 0.14812344996536608\n",
      "The 36961 th iteration gives loss of 0.14812196567847155\n",
      "The 36962 th iteration gives loss of 0.14812048143947495\n",
      "The 36963 th iteration gives loss of 0.14811899724839042\n",
      "The 36964 th iteration gives loss of 0.14811751310518487\n",
      "The 36965 th iteration gives loss of 0.14811602900986987\n",
      "The 36966 th iteration gives loss of 0.14811454496243734\n",
      "The 36967 th iteration gives loss of 0.14811306096287752\n",
      "The 36968 th iteration gives loss of 0.14811157701119515\n",
      "The 36969 th iteration gives loss of 0.14811009310736648\n",
      "The 36970 th iteration gives loss of 0.14810860925140185\n",
      "The 36971 th iteration gives loss of 0.1481071254432837\n",
      "The 36972 th iteration gives loss of 0.14810564168300994\n",
      "The 36973 th iteration gives loss of 0.14810415797057766\n",
      "The 36974 th iteration gives loss of 0.14810267430597507\n",
      "The 36975 th iteration gives loss of 0.14810119068921224\n",
      "The 36976 th iteration gives loss of 0.14809970712025475\n",
      "The 36977 th iteration gives loss of 0.14809822359911992\n",
      "The 36978 th iteration gives loss of 0.14809674012579208\n",
      "The 36979 th iteration gives loss of 0.1480952567002716\n",
      "The 36980 th iteration gives loss of 0.14809377332254642\n",
      "The 36981 th iteration gives loss of 0.14809228999261537\n",
      "The 36982 th iteration gives loss of 0.14809080671046265\n",
      "The 36983 th iteration gives loss of 0.14808932347609738\n",
      "The 36984 th iteration gives loss of 0.1480878402895074\n",
      "The 36985 th iteration gives loss of 0.14808635715068674\n",
      "The 36986 th iteration gives loss of 0.1480848740596196\n",
      "The 36987 th iteration gives loss of 0.1480833910163124\n",
      "The 36988 th iteration gives loss of 0.14808190802075974\n",
      "The 36989 th iteration gives loss of 0.14808042507294875\n",
      "The 36990 th iteration gives loss of 0.14807894217287745\n",
      "The 36991 th iteration gives loss of 0.14807745932054317\n",
      "The 36992 th iteration gives loss of 0.14807597651592286\n",
      "The 36993 th iteration gives loss of 0.14807449375903\n",
      "The 36994 th iteration gives loss of 0.14807301104985035\n",
      "The 36995 th iteration gives loss of 0.14807152838838256\n",
      "The 36996 th iteration gives loss of 0.14807004577461697\n",
      "The 36997 th iteration gives loss of 0.14806856320854844\n",
      "The 36998 th iteration gives loss of 0.14806708069017038\n",
      "The 36999 th iteration gives loss of 0.14806559821948176\n",
      "The 37000 th iteration gives loss of 0.14806411579647122\n",
      "The 37001 th iteration gives loss of 0.1480626334211313\n",
      "The 37002 th iteration gives loss of 0.14806115109345713\n",
      "The 37003 th iteration gives loss of 0.14805966881344768\n",
      "The 37004 th iteration gives loss of 0.14805818658109315\n",
      "The 37005 th iteration gives loss of 0.14805670439638177\n",
      "The 37006 th iteration gives loss of 0.1480552222593257\n",
      "The 37007 th iteration gives loss of 0.14805374016990258\n",
      "The 37008 th iteration gives loss of 0.14805225812811623\n",
      "The 37009 th iteration gives loss of 0.14805077613395048\n",
      "The 37010 th iteration gives loss of 0.1480492941874073\n",
      "The 37011 th iteration gives loss of 0.1480478122884706\n",
      "The 37012 th iteration gives loss of 0.14804633043715137\n",
      "The 37013 th iteration gives loss of 0.148044848633433\n",
      "The 37014 th iteration gives loss of 0.14804336687731418\n",
      "The 37015 th iteration gives loss of 0.1480418851687864\n",
      "The 37016 th iteration gives loss of 0.14804040350783917\n",
      "The 37017 th iteration gives loss of 0.14803892189447945\n",
      "The 37018 th iteration gives loss of 0.1480374403286756\n",
      "The 37019 th iteration gives loss of 0.14803595881045137\n",
      "The 37020 th iteration gives loss of 0.1480344773397883\n",
      "The 37021 th iteration gives loss of 0.1480329959166738\n",
      "The 37022 th iteration gives loss of 0.14803151454112387\n",
      "The 37023 th iteration gives loss of 0.14803003321311087\n",
      "The 37024 th iteration gives loss of 0.14802855193262976\n",
      "The 37025 th iteration gives loss of 0.14802707069968055\n",
      "The 37026 th iteration gives loss of 0.1480255895142646\n",
      "The 37027 th iteration gives loss of 0.14802410837636665\n",
      "The 37028 th iteration gives loss of 0.1480226272859855\n",
      "The 37029 th iteration gives loss of 0.14802114624310478\n",
      "The 37030 th iteration gives loss of 0.1480196652477293\n",
      "The 37031 th iteration gives loss of 0.1480181842998533\n",
      "The 37032 th iteration gives loss of 0.14801670339945996\n",
      "The 37033 th iteration gives loss of 0.14801522254656854\n",
      "The 37034 th iteration gives loss of 0.14801374174114362\n",
      "The 37035 th iteration gives loss of 0.14801226098319606\n",
      "The 37036 th iteration gives loss of 0.14801078027271444\n",
      "The 37037 th iteration gives loss of 0.14800929960969322\n",
      "The 37038 th iteration gives loss of 0.14800781899413062\n",
      "The 37039 th iteration gives loss of 0.1480063384260168\n",
      "The 37040 th iteration gives loss of 0.1480048579053457\n",
      "The 37041 th iteration gives loss of 0.1480033774321081\n",
      "The 37042 th iteration gives loss of 0.1480018970063072\n",
      "The 37043 th iteration gives loss of 0.1480004166279352\n",
      "The 37044 th iteration gives loss of 0.14799893629697727\n",
      "The 37045 th iteration gives loss of 0.14799745601343822\n",
      "The 37046 th iteration gives loss of 0.14799597577731302\n",
      "The 37047 th iteration gives loss of 0.14799449558858083\n",
      "The 37048 th iteration gives loss of 0.14799301544725157\n",
      "The 37049 th iteration gives loss of 0.1479915353533112\n",
      "The 37050 th iteration gives loss of 0.1479900553067498\n",
      "The 37051 th iteration gives loss of 0.14798857530758225\n",
      "The 37052 th iteration gives loss of 0.14798709535577834\n",
      "The 37053 th iteration gives loss of 0.14798561545134467\n",
      "The 37054 th iteration gives loss of 0.1479841355942666\n",
      "The 37055 th iteration gives loss of 0.14798265578455166\n",
      "The 37056 th iteration gives loss of 0.14798117602218808\n",
      "The 37057 th iteration gives loss of 0.14797969630716085\n",
      "The 37058 th iteration gives loss of 0.14797821663947952\n",
      "The 37059 th iteration gives loss of 0.14797673701913208\n",
      "The 37060 th iteration gives loss of 0.14797525744610934\n",
      "The 37061 th iteration gives loss of 0.14797377792040342\n",
      "The 37062 th iteration gives loss of 0.14797229844201493\n",
      "The 37063 th iteration gives loss of 0.14797081901093515\n",
      "The 37064 th iteration gives loss of 0.14796933962715503\n",
      "The 37065 th iteration gives loss of 0.14796786029067588\n",
      "The 37066 th iteration gives loss of 0.14796638100148535\n",
      "The 37067 th iteration gives loss of 0.14796490175958335\n",
      "The 37068 th iteration gives loss of 0.14796342256496153\n",
      "The 37069 th iteration gives loss of 0.14796194341761457\n",
      "The 37070 th iteration gives loss of 0.14796046431753546\n",
      "The 37071 th iteration gives loss of 0.14795898526471551\n",
      "The 37072 th iteration gives loss of 0.1479575062591535\n",
      "The 37073 th iteration gives loss of 0.14795602730084645\n",
      "The 37074 th iteration gives loss of 0.14795454838978236\n",
      "The 37075 th iteration gives loss of 0.14795306952595805\n",
      "The 37076 th iteration gives loss of 0.1479515907093558\n",
      "The 37077 th iteration gives loss of 0.14795011193999094\n",
      "The 37078 th iteration gives loss of 0.14794863321784713\n",
      "The 37079 th iteration gives loss of 0.14794715454291824\n",
      "The 37080 th iteration gives loss of 0.14794567591520302\n",
      "The 37081 th iteration gives loss of 0.14794419733467762\n",
      "The 37082 th iteration gives loss of 0.14794271880136645\n",
      "The 37083 th iteration gives loss of 0.14794124031524308\n",
      "The 37084 th iteration gives loss of 0.14793976187630245\n",
      "The 37085 th iteration gives loss of 0.1479382834845478\n",
      "The 37086 th iteration gives loss of 0.14793680513996618\n",
      "The 37087 th iteration gives loss of 0.14793532684255203\n",
      "The 37088 th iteration gives loss of 0.14793384859230438\n",
      "The 37089 th iteration gives loss of 0.1479323703892104\n",
      "The 37090 th iteration gives loss of 0.14793089223327147\n",
      "The 37091 th iteration gives loss of 0.14792941412446925\n",
      "The 37092 th iteration gives loss of 0.1479279360628104\n",
      "The 37093 th iteration gives loss of 0.14792645804828614\n",
      "The 37094 th iteration gives loss of 0.14792498008089758\n",
      "The 37095 th iteration gives loss of 0.14792350216062952\n",
      "The 37096 th iteration gives loss of 0.1479220242874736\n",
      "The 37097 th iteration gives loss of 0.14792054646143335\n",
      "The 37098 th iteration gives loss of 0.1479190686824943\n",
      "The 37099 th iteration gives loss of 0.14791759095064477\n",
      "The 37100 th iteration gives loss of 0.14791611326590107\n",
      "The 37101 th iteration gives loss of 0.14791463562824114\n",
      "The 37102 th iteration gives loss of 0.14791315803766913\n",
      "The 37103 th iteration gives loss of 0.14791168049416328\n",
      "The 37104 th iteration gives loss of 0.14791020299773133\n",
      "The 37105 th iteration gives loss of 0.14790872554836573\n",
      "The 37106 th iteration gives loss of 0.14790724814605744\n",
      "The 37107 th iteration gives loss of 0.14790577079079967\n",
      "The 37108 th iteration gives loss of 0.14790429348259782\n",
      "The 37109 th iteration gives loss of 0.14790281622142656\n",
      "The 37110 th iteration gives loss of 0.14790133900728794\n",
      "The 37111 th iteration gives loss of 0.14789986184018683\n",
      "The 37112 th iteration gives loss of 0.14789838472010766\n",
      "The 37113 th iteration gives loss of 0.14789690764705218\n",
      "The 37114 th iteration gives loss of 0.14789543062100194\n",
      "The 37115 th iteration gives loss of 0.14789395364195804\n",
      "The 37116 th iteration gives loss of 0.14789247670991246\n",
      "The 37117 th iteration gives loss of 0.14789099982485918\n",
      "The 37118 th iteration gives loss of 0.1478895229868083\n",
      "The 37119 th iteration gives loss of 0.147888046195733\n",
      "The 37120 th iteration gives loss of 0.14788656945162892\n",
      "The 37121 th iteration gives loss of 0.1478850927545097\n",
      "The 37122 th iteration gives loss of 0.14788361610434478\n",
      "The 37123 th iteration gives loss of 0.14788213950114285\n",
      "The 37124 th iteration gives loss of 0.14788066294490163\n",
      "The 37125 th iteration gives loss of 0.1478791864355987\n",
      "The 37126 th iteration gives loss of 0.14787770997324282\n",
      "The 37127 th iteration gives loss of 0.14787623355781648\n",
      "The 37128 th iteration gives loss of 0.14787475718932655\n",
      "The 37129 th iteration gives loss of 0.14787328086776477\n",
      "The 37130 th iteration gives loss of 0.14787180459312063\n",
      "The 37131 th iteration gives loss of 0.14787032836538772\n",
      "The 37132 th iteration gives loss of 0.14786885218455967\n",
      "The 37133 th iteration gives loss of 0.14786737605064348\n",
      "The 37134 th iteration gives loss of 0.14786589996361768\n",
      "The 37135 th iteration gives loss of 0.14786442392348162\n",
      "The 37136 th iteration gives loss of 0.14786294793023166\n",
      "The 37137 th iteration gives loss of 0.14786147198385993\n",
      "The 37138 th iteration gives loss of 0.1478599960843647\n",
      "The 37139 th iteration gives loss of 0.14785852023173188\n",
      "The 37140 th iteration gives loss of 0.14785704442596212\n",
      "The 37141 th iteration gives loss of 0.14785556866704827\n",
      "The 37142 th iteration gives loss of 0.14785409295498295\n",
      "The 37143 th iteration gives loss of 0.1478526172897638\n",
      "The 37144 th iteration gives loss of 0.14785114167138744\n",
      "The 37145 th iteration gives loss of 0.1478496660998381\n",
      "The 37146 th iteration gives loss of 0.1478481905751155\n",
      "The 37147 th iteration gives loss of 0.14784671509721847\n",
      "The 37148 th iteration gives loss of 0.14784523966613142\n",
      "The 37149 th iteration gives loss of 0.14784376428185075\n",
      "The 37150 th iteration gives loss of 0.14784228894437884\n",
      "The 37151 th iteration gives loss of 0.14784081365371013\n",
      "The 37152 th iteration gives loss of 0.14783933840982483\n",
      "The 37153 th iteration gives loss of 0.14783786321273326\n",
      "The 37154 th iteration gives loss of 0.1478363880624155\n",
      "The 37155 th iteration gives loss of 0.1478349129588743\n",
      "The 37156 th iteration gives loss of 0.14783343790211076\n",
      "The 37157 th iteration gives loss of 0.1478319628921022\n",
      "The 37158 th iteration gives loss of 0.14783048792885423\n",
      "The 37159 th iteration gives loss of 0.14782901301234913\n",
      "The 37160 th iteration gives loss of 0.14782753814259936\n",
      "The 37161 th iteration gives loss of 0.14782606331959333\n",
      "The 37162 th iteration gives loss of 0.14782458854331543\n",
      "The 37163 th iteration gives loss of 0.14782311381377133\n",
      "The 37164 th iteration gives loss of 0.14782163913094573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 37165 th iteration gives loss of 0.14782016449484445\n",
      "The 37166 th iteration gives loss of 0.14781868990545244\n",
      "The 37167 th iteration gives loss of 0.14781721536275855\n",
      "The 37168 th iteration gives loss of 0.14781574086676755\n",
      "The 37169 th iteration gives loss of 0.14781426641747705\n",
      "The 37170 th iteration gives loss of 0.14781279201487554\n",
      "The 37171 th iteration gives loss of 0.14781131765895203\n",
      "The 37172 th iteration gives loss of 0.14780984334971226\n",
      "The 37173 th iteration gives loss of 0.1478083690871331\n",
      "The 37174 th iteration gives loss of 0.1478068948712254\n",
      "The 37175 th iteration gives loss of 0.14780542070197258\n",
      "The 37176 th iteration gives loss of 0.14780394657938128\n",
      "The 37177 th iteration gives loss of 0.14780247250343978\n",
      "The 37178 th iteration gives loss of 0.14780099847413813\n",
      "The 37179 th iteration gives loss of 0.14779952449147132\n",
      "The 37180 th iteration gives loss of 0.14779805055543754\n",
      "The 37181 th iteration gives loss of 0.147796576666025\n",
      "The 37182 th iteration gives loss of 0.14779510282323594\n",
      "The 37183 th iteration gives loss of 0.14779362902706106\n",
      "The 37184 th iteration gives loss of 0.14779215527749273\n",
      "The 37185 th iteration gives loss of 0.1477906815745287\n",
      "The 37186 th iteration gives loss of 0.14778920791815991\n",
      "The 37187 th iteration gives loss of 0.14778773430838515\n",
      "The 37188 th iteration gives loss of 0.14778626074518866\n",
      "The 37189 th iteration gives loss of 0.14778478722857713\n",
      "The 37190 th iteration gives loss of 0.1477833137585438\n",
      "The 37191 th iteration gives loss of 0.14778184033506825\n",
      "The 37192 th iteration gives loss of 0.14778036695815894\n",
      "The 37193 th iteration gives loss of 0.1477788936278121\n",
      "The 37194 th iteration gives loss of 0.14777742034400543\n",
      "The 37195 th iteration gives loss of 0.1477759471067551\n",
      "The 37196 th iteration gives loss of 0.14777447391603943\n",
      "The 37197 th iteration gives loss of 0.14777300077185776\n",
      "The 37198 th iteration gives loss of 0.14777152767420537\n",
      "The 37199 th iteration gives loss of 0.14777005462307127\n",
      "The 37200 th iteration gives loss of 0.14776858161846168\n",
      "The 37201 th iteration gives loss of 0.14776710866035242\n",
      "The 37202 th iteration gives loss of 0.1477656357487639\n",
      "The 37203 th iteration gives loss of 0.1477641628836576\n",
      "The 37204 th iteration gives loss of 0.14776269006504997\n",
      "The 37205 th iteration gives loss of 0.1477612172929325\n",
      "The 37206 th iteration gives loss of 0.14775974456729235\n",
      "The 37207 th iteration gives loss of 0.14775827188813612\n",
      "The 37208 th iteration gives loss of 0.14775679925545016\n",
      "The 37209 th iteration gives loss of 0.14775532666922894\n",
      "The 37210 th iteration gives loss of 0.14775385412946568\n",
      "The 37211 th iteration gives loss of 0.147752381636151\n",
      "The 37212 th iteration gives loss of 0.14775090918929204\n",
      "The 37213 th iteration gives loss of 0.14774943678887004\n",
      "The 37214 th iteration gives loss of 0.14774796443489024\n",
      "The 37215 th iteration gives loss of 0.1477464921273336\n",
      "The 37216 th iteration gives loss of 0.14774501986621272\n",
      "The 37217 th iteration gives loss of 0.14774354765149908\n",
      "The 37218 th iteration gives loss of 0.1477420754832053\n",
      "The 37219 th iteration gives loss of 0.14774060336132167\n",
      "The 37220 th iteration gives loss of 0.14773913128583652\n",
      "The 37221 th iteration gives loss of 0.14773765925675164\n",
      "The 37222 th iteration gives loss of 0.1477361872740532\n",
      "The 37223 th iteration gives loss of 0.14773471533774427\n",
      "The 37224 th iteration gives loss of 0.14773324344780578\n",
      "The 37225 th iteration gives loss of 0.14773177160425105\n",
      "The 37226 th iteration gives loss of 0.14773029980706176\n",
      "The 37227 th iteration gives loss of 0.1477288280562376\n",
      "The 37228 th iteration gives loss of 0.14772735635176373\n",
      "The 37229 th iteration gives loss of 0.14772588469364836\n",
      "The 37230 th iteration gives loss of 0.14772441308187342\n",
      "The 37231 th iteration gives loss of 0.1477229415164358\n",
      "The 37232 th iteration gives loss of 0.14772146999733277\n",
      "The 37233 th iteration gives loss of 0.14771999852456721\n",
      "The 37234 th iteration gives loss of 0.1477185270981071\n",
      "The 37235 th iteration gives loss of 0.14771705571798346\n",
      "The 37236 th iteration gives loss of 0.14771558438416443\n",
      "The 37237 th iteration gives loss of 0.1477141130966485\n",
      "The 37238 th iteration gives loss of 0.14771264185542912\n",
      "The 37239 th iteration gives loss of 0.1477111706604999\n",
      "The 37240 th iteration gives loss of 0.14770969951187146\n",
      "The 37241 th iteration gives loss of 0.14770822840952164\n",
      "The 37242 th iteration gives loss of 0.14770675735345037\n",
      "The 37243 th iteration gives loss of 0.14770528634364047\n",
      "The 37244 th iteration gives loss of 0.1477038153801065\n",
      "The 37245 th iteration gives loss of 0.14770234446283037\n",
      "The 37246 th iteration gives loss of 0.14770087359180453\n",
      "The 37247 th iteration gives loss of 0.1476994027670314\n",
      "The 37248 th iteration gives loss of 0.14769793198850362\n",
      "The 37249 th iteration gives loss of 0.1476964612562102\n",
      "The 37250 th iteration gives loss of 0.14769499057014515\n",
      "The 37251 th iteration gives loss of 0.1476935199303107\n",
      "The 37252 th iteration gives loss of 0.14769204933669036\n",
      "The 37253 th iteration gives loss of 0.14769057878929043\n",
      "The 37254 th iteration gives loss of 0.14768910828809786\n",
      "The 37255 th iteration gives loss of 0.14768763783310962\n",
      "The 37256 th iteration gives loss of 0.14768616742431243\n",
      "The 37257 th iteration gives loss of 0.14768469706171436\n",
      "The 37258 th iteration gives loss of 0.14768322674529782\n",
      "The 37259 th iteration gives loss of 0.14768175647507698\n",
      "The 37260 th iteration gives loss of 0.14768028625101273\n",
      "The 37261 th iteration gives loss of 0.14767881607313163\n",
      "The 37262 th iteration gives loss of 0.14767734594140186\n",
      "The 37263 th iteration gives loss of 0.14767587585583225\n",
      "The 37264 th iteration gives loss of 0.14767440581641536\n",
      "The 37265 th iteration gives loss of 0.14767293582314364\n",
      "The 37266 th iteration gives loss of 0.1476714658760164\n",
      "The 37267 th iteration gives loss of 0.14766999597502556\n",
      "The 37268 th iteration gives loss of 0.1476685261201606\n",
      "The 37269 th iteration gives loss of 0.14766705631142427\n",
      "The 37270 th iteration gives loss of 0.14766558654879838\n",
      "The 37271 th iteration gives loss of 0.14766411683229375\n",
      "The 37272 th iteration gives loss of 0.14766264716189245\n",
      "The 37273 th iteration gives loss of 0.14766117753759544\n",
      "The 37274 th iteration gives loss of 0.14765970795938402\n",
      "The 37275 th iteration gives loss of 0.14765823842727424\n",
      "The 37276 th iteration gives loss of 0.14765676894124383\n",
      "The 37277 th iteration gives loss of 0.14765529950129652\n",
      "The 37278 th iteration gives loss of 0.14765383010740968\n",
      "The 37279 th iteration gives loss of 0.14765236075960117\n",
      "The 37280 th iteration gives loss of 0.14765089145785137\n",
      "The 37281 th iteration gives loss of 0.14764942220216057\n",
      "The 37282 th iteration gives loss of 0.1476479529925148\n",
      "The 37283 th iteration gives loss of 0.14764648382891551\n",
      "The 37284 th iteration gives loss of 0.14764501471135552\n",
      "The 37285 th iteration gives loss of 0.14764354563982898\n",
      "The 37286 th iteration gives loss of 0.1476420766143364\n",
      "The 37287 th iteration gives loss of 0.14764060763486167\n",
      "The 37288 th iteration gives loss of 0.1476391387013982\n",
      "The 37289 th iteration gives loss of 0.14763766981395232\n",
      "The 37290 th iteration gives loss of 0.14763620097250985\n",
      "The 37291 th iteration gives loss of 0.14763473217707074\n",
      "The 37292 th iteration gives loss of 0.14763326342761782\n",
      "The 37293 th iteration gives loss of 0.1476317947241495\n",
      "The 37294 th iteration gives loss of 0.14763032606667337\n",
      "The 37295 th iteration gives loss of 0.14762885745517626\n",
      "The 37296 th iteration gives loss of 0.14762738888964536\n",
      "The 37297 th iteration gives loss of 0.14762592037008443\n",
      "The 37298 th iteration gives loss of 0.14762445189647985\n",
      "The 37299 th iteration gives loss of 0.1476229834688342\n",
      "The 37300 th iteration gives loss of 0.14762151508712404\n",
      "The 37301 th iteration gives loss of 0.14762004675136556\n",
      "The 37302 th iteration gives loss of 0.1476185784615517\n",
      "The 37303 th iteration gives loss of 0.14761711021766277\n",
      "The 37304 th iteration gives loss of 0.14761564201969968\n",
      "The 37305 th iteration gives loss of 0.1476141738676595\n",
      "The 37306 th iteration gives loss of 0.14761270576153304\n",
      "The 37307 th iteration gives loss of 0.14761123770131465\n",
      "The 37308 th iteration gives loss of 0.1476097696870077\n",
      "The 37309 th iteration gives loss of 0.14760830171859982\n",
      "The 37310 th iteration gives loss of 0.1476068337960748\n",
      "The 37311 th iteration gives loss of 0.1476053659194394\n",
      "The 37312 th iteration gives loss of 0.14760389808868923\n",
      "The 37313 th iteration gives loss of 0.14760243030381134\n",
      "The 37314 th iteration gives loss of 0.14760096256480862\n",
      "The 37315 th iteration gives loss of 0.14759949487166113\n",
      "The 37316 th iteration gives loss of 0.14759802722438134\n",
      "The 37317 th iteration gives loss of 0.14759655962294924\n",
      "The 37318 th iteration gives loss of 0.14759509206736499\n",
      "The 37319 th iteration gives loss of 0.14759362455762653\n",
      "The 37320 th iteration gives loss of 0.14759215709372017\n",
      "The 37321 th iteration gives loss of 0.14759068967565586\n",
      "The 37322 th iteration gives loss of 0.1475892223034035\n",
      "The 37323 th iteration gives loss of 0.147587754976978\n",
      "The 37324 th iteration gives loss of 0.14758628769636703\n",
      "The 37325 th iteration gives loss of 0.14758482046155721\n",
      "The 37326 th iteration gives loss of 0.14758335327255168\n",
      "The 37327 th iteration gives loss of 0.1475818861293485\n",
      "The 37328 th iteration gives loss of 0.1475804190319382\n",
      "The 37329 th iteration gives loss of 0.14757895198030185\n",
      "The 37330 th iteration gives loss of 0.14757748497445689\n",
      "The 37331 th iteration gives loss of 0.14757601801438158\n",
      "The 37332 th iteration gives loss of 0.14757455110008919\n",
      "The 37333 th iteration gives loss of 0.1475730842315473\n",
      "The 37334 th iteration gives loss of 0.14757161740876837\n",
      "The 37335 th iteration gives loss of 0.1475701506317347\n",
      "The 37336 th iteration gives loss of 0.14756868390044825\n",
      "The 37337 th iteration gives loss of 0.14756721721490293\n",
      "The 37338 th iteration gives loss of 0.1475657505750986\n",
      "The 37339 th iteration gives loss of 0.14756428398101723\n",
      "The 37340 th iteration gives loss of 0.14756281743266783\n",
      "The 37341 th iteration gives loss of 0.14756135093003697\n",
      "The 37342 th iteration gives loss of 0.14755988447311622\n",
      "The 37343 th iteration gives loss of 0.14755841806190867\n",
      "The 37344 th iteration gives loss of 0.14755695169639602\n",
      "The 37345 th iteration gives loss of 0.14755548537657573\n",
      "The 37346 th iteration gives loss of 0.14755401910244967\n",
      "The 37347 th iteration gives loss of 0.1475525528740155\n",
      "The 37348 th iteration gives loss of 0.14755108669125497\n",
      "The 37349 th iteration gives loss of 0.14754962055417073\n",
      "The 37350 th iteration gives loss of 0.14754815446275724\n",
      "The 37351 th iteration gives loss of 0.14754668841700044\n",
      "The 37352 th iteration gives loss of 0.14754522241690832\n",
      "The 37353 th iteration gives loss of 0.14754375646245776\n",
      "The 37354 th iteration gives loss of 0.14754229055367324\n",
      "The 37355 th iteration gives loss of 0.14754082469051313\n",
      "The 37356 th iteration gives loss of 0.14753935887299216\n",
      "The 37357 th iteration gives loss of 0.14753789310109022\n",
      "The 37358 th iteration gives loss of 0.14753642737482453\n",
      "The 37359 th iteration gives loss of 0.14753496169417532\n",
      "The 37360 th iteration gives loss of 0.14753349605913457\n",
      "The 37361 th iteration gives loss of 0.14753203046970456\n",
      "The 37362 th iteration gives loss of 0.14753056492586697\n",
      "The 37363 th iteration gives loss of 0.14752909942763565\n",
      "The 37364 th iteration gives loss of 0.14752763397498675\n",
      "The 37365 th iteration gives loss of 0.14752616856793088\n",
      "The 37366 th iteration gives loss of 0.14752470320645117\n",
      "The 37367 th iteration gives loss of 0.14752323789054214\n",
      "The 37368 th iteration gives loss of 0.14752177262020222\n",
      "The 37369 th iteration gives loss of 0.1475203073954287\n",
      "The 37370 th iteration gives loss of 0.14751884221621234\n",
      "The 37371 th iteration gives loss of 0.14751737708254242\n",
      "The 37372 th iteration gives loss of 0.14751591199441508\n",
      "The 37373 th iteration gives loss of 0.1475144469518347\n",
      "The 37374 th iteration gives loss of 0.14751298195479057\n",
      "The 37375 th iteration gives loss of 0.14751151700327608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 37376 th iteration gives loss of 0.14751005209726906\n",
      "The 37377 th iteration gives loss of 0.1475085872367975\n",
      "The 37378 th iteration gives loss of 0.14750712242183253\n",
      "The 37379 th iteration gives loss of 0.1475056576523698\n",
      "The 37380 th iteration gives loss of 0.1475041929284145\n",
      "The 37381 th iteration gives loss of 0.14750272824995578\n",
      "The 37382 th iteration gives loss of 0.14750126361698113\n",
      "The 37383 th iteration gives loss of 0.14749979902949653\n",
      "The 37384 th iteration gives loss of 0.14749833448749283\n",
      "The 37385 th iteration gives loss of 0.14749686999096312\n",
      "The 37386 th iteration gives loss of 0.1474954055398891\n",
      "The 37387 th iteration gives loss of 0.14749394113429032\n",
      "The 37388 th iteration gives loss of 0.1474924767741434\n",
      "The 37389 th iteration gives loss of 0.14749101245944796\n",
      "The 37390 th iteration gives loss of 0.1474895481901982\n",
      "The 37391 th iteration gives loss of 0.14748808396638458\n",
      "The 37392 th iteration gives loss of 0.14748661978801195\n",
      "The 37393 th iteration gives loss of 0.14748515565506803\n",
      "The 37394 th iteration gives loss of 0.1474836915675425\n",
      "The 37395 th iteration gives loss of 0.147482227525433\n",
      "The 37396 th iteration gives loss of 0.14748076352873588\n",
      "The 37397 th iteration gives loss of 0.14747929957745104\n",
      "The 37398 th iteration gives loss of 0.14747783567156278\n",
      "The 37399 th iteration gives loss of 0.14747637181107548\n",
      "The 37400 th iteration gives loss of 0.14747490799597449\n",
      "The 37401 th iteration gives loss of 0.14747344422626354\n",
      "The 37402 th iteration gives loss of 0.14747198050192803\n",
      "The 37403 th iteration gives loss of 0.1474705168229663\n",
      "The 37404 th iteration gives loss of 0.14746905318937586\n",
      "The 37405 th iteration gives loss of 0.14746758960114795\n",
      "The 37406 th iteration gives loss of 0.14746612605827136\n",
      "The 37407 th iteration gives loss of 0.14746466256074894\n",
      "The 37408 th iteration gives loss of 0.1474631991085745\n",
      "The 37409 th iteration gives loss of 0.14746173570173562\n",
      "The 37410 th iteration gives loss of 0.14746027234023537\n",
      "The 37411 th iteration gives loss of 0.1474588090240671\n",
      "The 37412 th iteration gives loss of 0.14745734575321123\n",
      "The 37413 th iteration gives loss of 0.147455882527688\n",
      "The 37414 th iteration gives loss of 0.14745441934746278\n",
      "The 37415 th iteration gives loss of 0.14745295621255378\n",
      "The 37416 th iteration gives loss of 0.14745149312294284\n",
      "The 37417 th iteration gives loss of 0.14745003007863294\n",
      "The 37418 th iteration gives loss of 0.14744856707961226\n",
      "The 37419 th iteration gives loss of 0.14744710412587517\n",
      "The 37420 th iteration gives loss of 0.14744564121741566\n",
      "The 37421 th iteration gives loss of 0.14744417835423607\n",
      "The 37422 th iteration gives loss of 0.14744271553631474\n",
      "The 37423 th iteration gives loss of 0.14744125276366857\n",
      "The 37424 th iteration gives loss of 0.14743979003627466\n",
      "The 37425 th iteration gives loss of 0.14743832735413764\n",
      "The 37426 th iteration gives loss of 0.14743686471723355\n",
      "The 37427 th iteration gives loss of 0.14743540212557615\n",
      "The 37428 th iteration gives loss of 0.14743393957915676\n",
      "The 37429 th iteration gives loss of 0.14743247707796214\n",
      "The 37430 th iteration gives loss of 0.14743101462199917\n",
      "The 37431 th iteration gives loss of 0.14742955221125073\n",
      "The 37432 th iteration gives loss of 0.14742808984571323\n",
      "The 37433 th iteration gives loss of 0.14742662752538765\n",
      "The 37434 th iteration gives loss of 0.14742516525026267\n",
      "The 37435 th iteration gives loss of 0.14742370302033617\n",
      "The 37436 th iteration gives loss of 0.14742224083560376\n",
      "The 37437 th iteration gives loss of 0.14742077869605905\n",
      "The 37438 th iteration gives loss of 0.14741931660168833\n",
      "The 37439 th iteration gives loss of 0.1474178545524858\n",
      "The 37440 th iteration gives loss of 0.1474163925484617\n",
      "The 37441 th iteration gives loss of 0.14741493058959648\n",
      "The 37442 th iteration gives loss of 0.14741346867589436\n",
      "The 37443 th iteration gives loss of 0.14741200680734473\n",
      "The 37444 th iteration gives loss of 0.1474105449839434\n",
      "The 37445 th iteration gives loss of 0.14740908320567334\n",
      "The 37446 th iteration gives loss of 0.14740762147254882\n",
      "The 37447 th iteration gives loss of 0.14740615978454996\n",
      "The 37448 th iteration gives loss of 0.14740469814167909\n",
      "The 37449 th iteration gives loss of 0.14740323654392964\n",
      "The 37450 th iteration gives loss of 0.14740177499129162\n",
      "The 37451 th iteration gives loss of 0.1474003134837582\n",
      "The 37452 th iteration gives loss of 0.14739885202133732\n",
      "The 37453 th iteration gives loss of 0.14739739060400964\n",
      "The 37454 th iteration gives loss of 0.14739592923177755\n",
      "The 37455 th iteration gives loss of 0.14739446790462726\n",
      "The 37456 th iteration gives loss of 0.14739300662256077\n",
      "The 37457 th iteration gives loss of 0.1473915453855649\n",
      "The 37458 th iteration gives loss of 0.14739008419364588\n",
      "The 37459 th iteration gives loss of 0.14738862304679082\n",
      "The 37460 th iteration gives loss of 0.14738716194499493\n",
      "The 37461 th iteration gives loss of 0.14738570088825304\n",
      "The 37462 th iteration gives loss of 0.14738423987655497\n",
      "The 37463 th iteration gives loss of 0.14738277890990284\n",
      "The 37464 th iteration gives loss of 0.14738131798828424\n",
      "The 37465 th iteration gives loss of 0.147379857111701\n",
      "The 37466 th iteration gives loss of 0.1473783962801412\n",
      "The 37467 th iteration gives loss of 0.1473769354936132\n",
      "The 37468 th iteration gives loss of 0.1473754747520903\n",
      "The 37469 th iteration gives loss of 0.1473740140555776\n",
      "The 37470 th iteration gives loss of 0.14737255340407007\n",
      "The 37471 th iteration gives loss of 0.1473710927975547\n",
      "The 37472 th iteration gives loss of 0.1473696322360443\n",
      "The 37473 th iteration gives loss of 0.14736817171951766\n",
      "The 37474 th iteration gives loss of 0.14736671124797668\n",
      "The 37475 th iteration gives loss of 0.14736525082140087\n",
      "The 37476 th iteration gives loss of 0.1473637904398104\n",
      "The 37477 th iteration gives loss of 0.14736233010318475\n",
      "The 37478 th iteration gives loss of 0.14736086981150934\n",
      "The 37479 th iteration gives loss of 0.14735940956479265\n",
      "The 37480 th iteration gives loss of 0.1473579493630261\n",
      "The 37481 th iteration gives loss of 0.14735648920620537\n",
      "The 37482 th iteration gives loss of 0.14735502909432216\n",
      "The 37483 th iteration gives loss of 0.14735356902737362\n",
      "The 37484 th iteration gives loss of 0.14735210900535223\n",
      "The 37485 th iteration gives loss of 0.14735064902825412\n",
      "The 37486 th iteration gives loss of 0.147349189096073\n",
      "The 37487 th iteration gives loss of 0.14734772920879965\n",
      "The 37488 th iteration gives loss of 0.14734626936643905\n",
      "The 37489 th iteration gives loss of 0.14734480956897517\n",
      "The 37490 th iteration gives loss of 0.14734334981640101\n",
      "The 37491 th iteration gives loss of 0.14734189010872356\n",
      "The 37492 th iteration gives loss of 0.14734043044592968\n",
      "The 37493 th iteration gives loss of 0.14733897082800768\n",
      "The 37494 th iteration gives loss of 0.14733751125496494\n",
      "The 37495 th iteration gives loss of 0.14733605172678518\n",
      "The 37496 th iteration gives loss of 0.14733459224347542\n",
      "The 37497 th iteration gives loss of 0.14733313280501092\n",
      "The 37498 th iteration gives loss of 0.14733167341140088\n",
      "The 37499 th iteration gives loss of 0.1473302140626473\n",
      "The 37500 th iteration gives loss of 0.14732875475872023\n",
      "The 37501 th iteration gives loss of 0.14732729549963527\n",
      "The 37502 th iteration gives loss of 0.14732583628538057\n",
      "The 37503 th iteration gives loss of 0.14732437711595076\n",
      "The 37504 th iteration gives loss of 0.14732291799133385\n",
      "The 37505 th iteration gives loss of 0.1473214589115366\n",
      "The 37506 th iteration gives loss of 0.1473199998765418\n",
      "The 37507 th iteration gives loss of 0.1473185408863474\n",
      "The 37508 th iteration gives loss of 0.14731708194095292\n",
      "The 37509 th iteration gives loss of 0.14731562304034493\n",
      "The 37510 th iteration gives loss of 0.14731416418452944\n",
      "The 37511 th iteration gives loss of 0.14731270537349314\n",
      "The 37512 th iteration gives loss of 0.1473112466072343\n",
      "The 37513 th iteration gives loss of 0.1473097878857349\n",
      "The 37514 th iteration gives loss of 0.14730832920900372\n",
      "The 37515 th iteration gives loss of 0.14730687057703448\n",
      "The 37516 th iteration gives loss of 0.1473054119898229\n",
      "The 37517 th iteration gives loss of 0.14730395344734956\n",
      "The 37518 th iteration gives loss of 0.1473024949496224\n",
      "The 37519 th iteration gives loss of 0.14730103649663237\n",
      "The 37520 th iteration gives loss of 0.14729957808837812\n",
      "The 37521 th iteration gives loss of 0.1472981197248483\n",
      "The 37522 th iteration gives loss of 0.14729666140602715\n",
      "The 37523 th iteration gives loss of 0.1472952031319302\n",
      "The 37524 th iteration gives loss of 0.14729374490254393\n",
      "The 37525 th iteration gives loss of 0.14729228671785466\n",
      "The 37526 th iteration gives loss of 0.14729082857786843\n",
      "The 37527 th iteration gives loss of 0.14728937048257543\n",
      "The 37528 th iteration gives loss of 0.14728791243197287\n",
      "The 37529 th iteration gives loss of 0.1472864544260553\n",
      "The 37530 th iteration gives loss of 0.14728499646480941\n",
      "The 37531 th iteration gives loss of 0.14728353854823903\n",
      "The 37532 th iteration gives loss of 0.147282080676334\n",
      "The 37533 th iteration gives loss of 0.14728062284908863\n",
      "The 37534 th iteration gives loss of 0.14727916506649302\n",
      "The 37535 th iteration gives loss of 0.14727770732855414\n",
      "The 37536 th iteration gives loss of 0.14727624963525965\n",
      "The 37537 th iteration gives loss of 0.1472747919865979\n",
      "The 37538 th iteration gives loss of 0.1472733343825802\n",
      "The 37539 th iteration gives loss of 0.1472718768231809\n",
      "The 37540 th iteration gives loss of 0.14727041930840903\n",
      "The 37541 th iteration gives loss of 0.14726896183826335\n",
      "The 37542 th iteration gives loss of 0.14726750441271824\n",
      "The 37543 th iteration gives loss of 0.14726604703177615\n",
      "The 37544 th iteration gives loss of 0.1472645896954436\n",
      "The 37545 th iteration gives loss of 0.14726313240370284\n",
      "The 37546 th iteration gives loss of 0.14726167515655822\n",
      "The 37547 th iteration gives loss of 0.14726021795399316\n",
      "The 37548 th iteration gives loss of 0.14725876079600522\n",
      "The 37549 th iteration gives loss of 0.1472573036825955\n",
      "The 37550 th iteration gives loss of 0.147255846613754\n",
      "The 37551 th iteration gives loss of 0.1472543895894736\n",
      "The 37552 th iteration gives loss of 0.1472529326097526\n",
      "The 37553 th iteration gives loss of 0.14725147567458913\n",
      "The 37554 th iteration gives loss of 0.14725001878396313\n",
      "The 37555 th iteration gives loss of 0.1472485619378845\n",
      "The 37556 th iteration gives loss of 0.14724710513633565\n",
      "The 37557 th iteration gives loss of 0.14724564837932902\n",
      "The 37558 th iteration gives loss of 0.14724419166684272\n",
      "The 37559 th iteration gives loss of 0.1472427349988732\n",
      "The 37560 th iteration gives loss of 0.14724127837541645\n",
      "The 37561 th iteration gives loss of 0.14723982179647302\n",
      "The 37562 th iteration gives loss of 0.14723836526203535\n",
      "The 37563 th iteration gives loss of 0.1472369087720886\n",
      "The 37564 th iteration gives loss of 0.1472354523266437\n",
      "The 37565 th iteration gives loss of 0.14723399592568154\n",
      "The 37566 th iteration gives loss of 0.14723253956920035\n",
      "The 37567 th iteration gives loss of 0.14723108325719994\n",
      "The 37568 th iteration gives loss of 0.147229626989658\n",
      "The 37569 th iteration gives loss of 0.1472281707666008\n",
      "The 37570 th iteration gives loss of 0.14722671458799655\n",
      "The 37571 th iteration gives loss of 0.1472252584538495\n",
      "The 37572 th iteration gives loss of 0.1472238023641533\n",
      "The 37573 th iteration gives loss of 0.14722234631889944\n",
      "The 37574 th iteration gives loss of 0.14722089031807964\n",
      "The 37575 th iteration gives loss of 0.1472194343617001\n",
      "The 37576 th iteration gives loss of 0.1472179784497405\n",
      "The 37577 th iteration gives loss of 0.1472165225822072\n",
      "The 37578 th iteration gives loss of 0.14721506675908785\n",
      "The 37579 th iteration gives loss of 0.14721361098039015\n",
      "The 37580 th iteration gives loss of 0.14721215524610148\n",
      "The 37581 th iteration gives loss of 0.14721069955620178\n",
      "The 37582 th iteration gives loss of 0.14720924391070192\n",
      "The 37583 th iteration gives loss of 0.1472077883095899\n",
      "The 37584 th iteration gives loss of 0.14720633275287243\n",
      "The 37585 th iteration gives loss of 0.14720487724052694\n",
      "The 37586 th iteration gives loss of 0.1472034217725592\n",
      "The 37587 th iteration gives loss of 0.14720196634896868\n",
      "The 37588 th iteration gives loss of 0.14720051096972633\n",
      "The 37589 th iteration gives loss of 0.14719905563484603\n",
      "The 37590 th iteration gives loss of 0.14719760034432233\n",
      "The 37591 th iteration gives loss of 0.14719614509814632\n",
      "The 37592 th iteration gives loss of 0.1471946898963112\n",
      "The 37593 th iteration gives loss of 0.14719323473881732\n",
      "The 37594 th iteration gives loss of 0.14719177962564015\n",
      "The 37595 th iteration gives loss of 0.14719032455680364\n",
      "The 37596 th iteration gives loss of 0.14718886953228713\n",
      "The 37597 th iteration gives loss of 0.14718741455207457\n",
      "The 37598 th iteration gives loss of 0.14718595961618428\n",
      "The 37599 th iteration gives loss of 0.14718450472459232\n",
      "The 37600 th iteration gives loss of 0.14718304987730016\n",
      "The 37601 th iteration gives loss of 0.14718159507430154\n",
      "The 37602 th iteration gives loss of 0.14718014031559518\n",
      "The 37603 th iteration gives loss of 0.14717868560116096\n",
      "The 37604 th iteration gives loss of 0.14717723093101562\n",
      "The 37605 th iteration gives loss of 0.14717577630513765\n",
      "The 37606 th iteration gives loss of 0.1471743217235327\n",
      "The 37607 th iteration gives loss of 0.14717286718618394\n",
      "The 37608 th iteration gives loss of 0.1471714126930823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 37609 th iteration gives loss of 0.14716995824424156\n",
      "The 37610 th iteration gives loss of 0.14716850383964783\n",
      "The 37611 th iteration gives loss of 0.14716704947929352\n",
      "The 37612 th iteration gives loss of 0.14716559516317199\n",
      "The 37613 th iteration gives loss of 0.14716414089128427\n",
      "The 37614 th iteration gives loss of 0.1471626866636112\n",
      "The 37615 th iteration gives loss of 0.14716123248016938\n",
      "The 37616 th iteration gives loss of 0.14715977834092614\n",
      "The 37617 th iteration gives loss of 0.14715832424589725\n",
      "The 37618 th iteration gives loss of 0.14715687019507542\n",
      "The 37619 th iteration gives loss of 0.1471554161884474\n",
      "The 37620 th iteration gives loss of 0.14715396222601373\n",
      "The 37621 th iteration gives loss of 0.1471525083077614\n",
      "The 37622 th iteration gives loss of 0.1471510544336978\n",
      "The 37623 th iteration gives loss of 0.14714960060379878\n",
      "The 37624 th iteration gives loss of 0.14714814681808944\n",
      "The 37625 th iteration gives loss of 0.1471466930765304\n",
      "The 37626 th iteration gives loss of 0.14714523937912652\n",
      "The 37627 th iteration gives loss of 0.14714378572588752\n",
      "The 37628 th iteration gives loss of 0.14714233211679276\n",
      "The 37629 th iteration gives loss of 0.14714087855184674\n",
      "The 37630 th iteration gives loss of 0.14713942503103894\n",
      "The 37631 th iteration gives loss of 0.14713797155435673\n",
      "The 37632 th iteration gives loss of 0.14713651812180675\n",
      "The 37633 th iteration gives loss of 0.14713506473337756\n",
      "The 37634 th iteration gives loss of 0.1471336113890693\n",
      "The 37635 th iteration gives loss of 0.1471321580888717\n",
      "The 37636 th iteration gives loss of 0.14713070483277801\n",
      "The 37637 th iteration gives loss of 0.14712925162078516\n",
      "The 37638 th iteration gives loss of 0.14712779845289076\n",
      "The 37639 th iteration gives loss of 0.14712634532907962\n",
      "The 37640 th iteration gives loss of 0.14712489224936084\n",
      "The 37641 th iteration gives loss of 0.1471234392137217\n",
      "The 37642 th iteration gives loss of 0.14712198622215739\n",
      "The 37643 th iteration gives loss of 0.14712053327465516\n",
      "The 37644 th iteration gives loss of 0.14711908037122579\n",
      "The 37645 th iteration gives loss of 0.14711762751185195\n",
      "The 37646 th iteration gives loss of 0.14711617469652316\n",
      "The 37647 th iteration gives loss of 0.1471147219252551\n",
      "The 37648 th iteration gives loss of 0.14711326919801956\n",
      "The 37649 th iteration gives loss of 0.14711181651482316\n",
      "The 37650 th iteration gives loss of 0.1471103638756604\n",
      "The 37651 th iteration gives loss of 0.1471089112805264\n",
      "The 37652 th iteration gives loss of 0.1471074587294107\n",
      "The 37653 th iteration gives loss of 0.1471060062223113\n",
      "The 37654 th iteration gives loss of 0.14710455375922182\n",
      "The 37655 th iteration gives loss of 0.14710310134012924\n",
      "The 37656 th iteration gives loss of 0.14710164896504932\n",
      "The 37657 th iteration gives loss of 0.1471001966339615\n",
      "The 37658 th iteration gives loss of 0.14709874434686065\n",
      "The 37659 th iteration gives loss of 0.14709729210374067\n",
      "The 37660 th iteration gives loss of 0.14709583990460146\n",
      "The 37661 th iteration gives loss of 0.14709438774944228\n",
      "The 37662 th iteration gives loss of 0.14709293563823903\n",
      "The 37663 th iteration gives loss of 0.1470914835710026\n",
      "The 37664 th iteration gives loss of 0.14709003154772826\n",
      "The 37665 th iteration gives loss of 0.14708857956839597\n",
      "The 37666 th iteration gives loss of 0.14708712763301673\n",
      "The 37667 th iteration gives loss of 0.14708567574157969\n",
      "The 37668 th iteration gives loss of 0.1470842238940814\n",
      "The 37669 th iteration gives loss of 0.14708277209051057\n",
      "The 37670 th iteration gives loss of 0.14708132033085733\n",
      "The 37671 th iteration gives loss of 0.14707986861513886\n",
      "The 37672 th iteration gives loss of 0.1470784169433202\n",
      "The 37673 th iteration gives loss of 0.147076965315423\n",
      "The 37674 th iteration gives loss of 0.1470755137314249\n",
      "The 37675 th iteration gives loss of 0.147074062191321\n",
      "The 37676 th iteration gives loss of 0.14707261069511646\n",
      "The 37677 th iteration gives loss of 0.14707115924279968\n",
      "The 37678 th iteration gives loss of 0.1470697078343674\n",
      "The 37679 th iteration gives loss of 0.14706825646980837\n",
      "The 37680 th iteration gives loss of 0.14706680514912607\n",
      "The 37681 th iteration gives loss of 0.14706535387231717\n",
      "The 37682 th iteration gives loss of 0.1470639026393582\n",
      "The 37683 th iteration gives loss of 0.14706245145025926\n",
      "The 37684 th iteration gives loss of 0.14706100030500677\n",
      "The 37685 th iteration gives loss of 0.14705954920361083\n",
      "The 37686 th iteration gives loss of 0.1470580981460496\n",
      "The 37687 th iteration gives loss of 0.14705664713231836\n",
      "The 37688 th iteration gives loss of 0.14705519616242518\n",
      "The 37689 th iteration gives loss of 0.14705374523635395\n",
      "The 37690 th iteration gives loss of 0.14705229435409886\n",
      "The 37691 th iteration gives loss of 0.14705084351565612\n",
      "The 37692 th iteration gives loss of 0.14704939272103332\n",
      "The 37693 th iteration gives loss of 0.14704794197020443\n",
      "The 37694 th iteration gives loss of 0.14704649126318245\n",
      "The 37695 th iteration gives loss of 0.14704504059993834\n",
      "The 37696 th iteration gives loss of 0.14704358998049924\n",
      "The 37697 th iteration gives loss of 0.1470421394048296\n",
      "The 37698 th iteration gives loss of 0.14704068887293958\n",
      "The 37699 th iteration gives loss of 0.14703923838482436\n",
      "The 37700 th iteration gives loss of 0.14703778794047834\n",
      "The 37701 th iteration gives loss of 0.14703633753988937\n",
      "The 37702 th iteration gives loss of 0.14703488718305238\n",
      "The 37703 th iteration gives loss of 0.1470334368699677\n",
      "The 37704 th iteration gives loss of 0.14703198660063407\n",
      "The 37705 th iteration gives loss of 0.14703053637503538\n",
      "The 37706 th iteration gives loss of 0.14702908619317223\n",
      "The 37707 th iteration gives loss of 0.14702763605505015\n",
      "The 37708 th iteration gives loss of 0.14702618596063363\n",
      "The 37709 th iteration gives loss of 0.14702473590994133\n",
      "The 37710 th iteration gives loss of 0.1470232859029666\n",
      "The 37711 th iteration gives loss of 0.1470218359397003\n",
      "The 37712 th iteration gives loss of 0.1470203860201357\n",
      "The 37713 th iteration gives loss of 0.14701893614427222\n",
      "The 37714 th iteration gives loss of 0.14701748631210101\n",
      "The 37715 th iteration gives loss of 0.14701603652360973\n",
      "The 37716 th iteration gives loss of 0.14701458677880647\n",
      "The 37717 th iteration gives loss of 0.14701313707768396\n",
      "The 37718 th iteration gives loss of 0.1470116874202291\n",
      "The 37719 th iteration gives loss of 0.14701023780644004\n",
      "The 37720 th iteration gives loss of 0.1470087882363115\n",
      "The 37721 th iteration gives loss of 0.14700733870983157\n",
      "The 37722 th iteration gives loss of 0.14700588922700555\n",
      "The 37723 th iteration gives loss of 0.14700443978783154\n",
      "The 37724 th iteration gives loss of 0.14700299039229256\n",
      "The 37725 th iteration gives loss of 0.14700154104038335\n",
      "The 37726 th iteration gives loss of 0.14700009173211284\n",
      "The 37727 th iteration gives loss of 0.14699864246745928\n",
      "The 37728 th iteration gives loss of 0.14699719324643112\n",
      "The 37729 th iteration gives loss of 0.14699574406901517\n",
      "The 37730 th iteration gives loss of 0.14699429493520882\n",
      "The 37731 th iteration gives loss of 0.14699284584499628\n",
      "The 37732 th iteration gives loss of 0.14699139679838674\n",
      "The 37733 th iteration gives loss of 0.1469899477953745\n",
      "The 37734 th iteration gives loss of 0.14698849883594825\n",
      "The 37735 th iteration gives loss of 0.1469870499200953\n",
      "The 37736 th iteration gives loss of 0.1469856010478286\n",
      "The 37737 th iteration gives loss of 0.14698415221912875\n",
      "The 37738 th iteration gives loss of 0.14698270343400097\n",
      "The 37739 th iteration gives loss of 0.1469812546924357\n",
      "The 37740 th iteration gives loss of 0.14697980599441166\n",
      "The 37741 th iteration gives loss of 0.14697835733995654\n",
      "The 37742 th iteration gives loss of 0.14697690872903416\n",
      "The 37743 th iteration gives loss of 0.1469754601616544\n",
      "The 37744 th iteration gives loss of 0.1469740116378066\n",
      "The 37745 th iteration gives loss of 0.14697256315749618\n",
      "The 37746 th iteration gives loss of 0.14697111472070742\n",
      "The 37747 th iteration gives loss of 0.14696966632742792\n",
      "The 37748 th iteration gives loss of 0.14696821797767168\n",
      "The 37749 th iteration gives loss of 0.1469667696714264\n",
      "The 37750 th iteration gives loss of 0.14696532140867874\n",
      "The 37751 th iteration gives loss of 0.14696387318942356\n",
      "The 37752 th iteration gives loss of 0.14696242501367365\n",
      "The 37753 th iteration gives loss of 0.1469609768814133\n",
      "The 37754 th iteration gives loss of 0.1469595287926283\n",
      "The 37755 th iteration gives loss of 0.1469580807473141\n",
      "The 37756 th iteration gives loss of 0.146956632745476\n",
      "The 37757 th iteration gives loss of 0.14695518478710873\n",
      "The 37758 th iteration gives loss of 0.14695373687219815\n",
      "The 37759 th iteration gives loss of 0.14695228900074633\n",
      "The 37760 th iteration gives loss of 0.14695084117274523\n",
      "The 37761 th iteration gives loss of 0.14694939338819968\n",
      "The 37762 th iteration gives loss of 0.14694794564707742\n",
      "The 37763 th iteration gives loss of 0.14694649794939685\n",
      "The 37764 th iteration gives loss of 0.14694505029515062\n",
      "The 37765 th iteration gives loss of 0.14694360268432013\n",
      "The 37766 th iteration gives loss of 0.14694215511691508\n",
      "The 37767 th iteration gives loss of 0.14694070759293199\n",
      "The 37768 th iteration gives loss of 0.14693926011234204\n",
      "The 37769 th iteration gives loss of 0.14693781267516218\n",
      "The 37770 th iteration gives loss of 0.14693636528138196\n",
      "The 37771 th iteration gives loss of 0.14693491793100327\n",
      "The 37772 th iteration gives loss of 0.146933470624003\n",
      "The 37773 th iteration gives loss of 0.14693202336038466\n",
      "The 37774 th iteration gives loss of 0.1469305761401487\n",
      "The 37775 th iteration gives loss of 0.14692912896327806\n",
      "The 37776 th iteration gives loss of 0.14692768182978364\n",
      "The 37777 th iteration gives loss of 0.14692623473965252\n",
      "The 37778 th iteration gives loss of 0.146924787692871\n",
      "The 37779 th iteration gives loss of 0.14692334068944507\n",
      "The 37780 th iteration gives loss of 0.14692189372936199\n",
      "The 37781 th iteration gives loss of 0.14692044681262334\n",
      "The 37782 th iteration gives loss of 0.14691899993922142\n",
      "The 37783 th iteration gives loss of 0.14691755310914645\n",
      "The 37784 th iteration gives loss of 0.14691610632240287\n",
      "The 37785 th iteration gives loss of 0.14691465957897235\n",
      "The 37786 th iteration gives loss of 0.14691321287885778\n",
      "The 37787 th iteration gives loss of 0.1469117662220577\n",
      "The 37788 th iteration gives loss of 0.14691031960855683\n",
      "The 37789 th iteration gives loss of 0.14690887303835995\n",
      "The 37790 th iteration gives loss of 0.14690742651146113\n",
      "The 37791 th iteration gives loss of 0.14690598002784513\n",
      "The 37792 th iteration gives loss of 0.14690453358751757\n",
      "The 37793 th iteration gives loss of 0.14690308719046521\n",
      "The 37794 th iteration gives loss of 0.14690164083667923\n",
      "The 37795 th iteration gives loss of 0.14690019452617556\n",
      "The 37796 th iteration gives loss of 0.14689874825893057\n",
      "The 37797 th iteration gives loss of 0.14689730203494017\n",
      "The 37798 th iteration gives loss of 0.14689585585420167\n",
      "The 37799 th iteration gives loss of 0.1468944097167127\n",
      "The 37800 th iteration gives loss of 0.14689296362246396\n",
      "The 37801 th iteration gives loss of 0.14689151757145505\n",
      "The 37802 th iteration gives loss of 0.1468900715636792\n",
      "The 37803 th iteration gives loss of 0.14688862559912785\n",
      "The 37804 th iteration gives loss of 0.14688717967779466\n",
      "The 37805 th iteration gives loss of 0.1468857337996797\n",
      "The 37806 th iteration gives loss of 0.14688428796477426\n",
      "The 37807 th iteration gives loss of 0.14688284217308314\n",
      "The 37808 th iteration gives loss of 0.14688139642458325\n",
      "The 37809 th iteration gives loss of 0.14687995071928592\n",
      "The 37810 th iteration gives loss of 0.14687850505717107\n",
      "The 37811 th iteration gives loss of 0.14687705943824358\n",
      "The 37812 th iteration gives loss of 0.14687561386250117\n",
      "The 37813 th iteration gives loss of 0.1468741683299313\n",
      "The 37814 th iteration gives loss of 0.14687272284053043\n",
      "The 37815 th iteration gives loss of 0.14687127739429598\n",
      "The 37816 th iteration gives loss of 0.14686983199121095\n",
      "The 37817 th iteration gives loss of 0.1468683866312864\n",
      "The 37818 th iteration gives loss of 0.1468669413145182\n",
      "The 37819 th iteration gives loss of 0.14686549604088578\n",
      "The 37820 th iteration gives loss of 0.1468640508103912\n",
      "The 37821 th iteration gives loss of 0.14686260562302988\n",
      "The 37822 th iteration gives loss of 0.1468611604787957\n",
      "The 37823 th iteration gives loss of 0.14685971537768241\n",
      "The 37824 th iteration gives loss of 0.1468582703196975\n",
      "The 37825 th iteration gives loss of 0.1468568253048215\n",
      "The 37826 th iteration gives loss of 0.14685538033304935\n",
      "The 37827 th iteration gives loss of 0.1468539354043759\n",
      "The 37828 th iteration gives loss of 0.1468524905187983\n",
      "The 37829 th iteration gives loss of 0.14685104567631757\n",
      "The 37830 th iteration gives loss of 0.14684960087692175\n",
      "The 37831 th iteration gives loss of 0.1468481561206151\n",
      "The 37832 th iteration gives loss of 0.1468467114073742\n",
      "The 37833 th iteration gives loss of 0.14684526673720216\n",
      "The 37834 th iteration gives loss of 0.1468438221101061\n",
      "The 37835 th iteration gives loss of 0.14684237752606738\n",
      "The 37836 th iteration gives loss of 0.1468409329850869\n",
      "The 37837 th iteration gives loss of 0.14683948848714617\n",
      "The 37838 th iteration gives loss of 0.14683804403225917\n",
      "The 37839 th iteration gives loss of 0.1468365996204175\n",
      "The 37840 th iteration gives loss of 0.1468351552515975\n",
      "The 37841 th iteration gives loss of 0.1468337109258089\n",
      "The 37842 th iteration gives loss of 0.14683226664305524\n",
      "The 37843 th iteration gives loss of 0.14683082240331619\n",
      "The 37844 th iteration gives loss of 0.1468293782065905\n",
      "The 37845 th iteration gives loss of 0.146827934052872\n",
      "The 37846 th iteration gives loss of 0.14682648994216285\n",
      "The 37847 th iteration gives loss of 0.14682504587444534\n",
      "The 37848 th iteration gives loss of 0.1468236018497255\n",
      "The 37849 th iteration gives loss of 0.14682215786799388\n",
      "The 37850 th iteration gives loss of 0.14682071392924856\n",
      "The 37851 th iteration gives loss of 0.14681927003347003\n",
      "The 37852 th iteration gives loss of 0.14681782618067857\n",
      "The 37853 th iteration gives loss of 0.14681638237084513\n",
      "The 37854 th iteration gives loss of 0.14681493860397773\n",
      "The 37855 th iteration gives loss of 0.1468134948800656\n",
      "The 37856 th iteration gives loss of 0.14681205119910873\n",
      "The 37857 th iteration gives loss of 0.1468106075611007\n",
      "The 37858 th iteration gives loss of 0.14680916396603014\n",
      "The 37859 th iteration gives loss of 0.1468077204138996\n",
      "The 37860 th iteration gives loss of 0.14680627690469583\n",
      "The 37861 th iteration gives loss of 0.1468048334384226\n",
      "The 37862 th iteration gives loss of 0.14680339001506798\n",
      "The 37863 th iteration gives loss of 0.14680194663463286\n",
      "The 37864 th iteration gives loss of 0.14680050329710237\n",
      "The 37865 th iteration gives loss of 0.14679906000249116\n",
      "The 37866 th iteration gives loss of 0.1467976167507774\n",
      "The 37867 th iteration gives loss of 0.1467961735419503\n",
      "The 37868 th iteration gives loss of 0.1467947303760127\n",
      "The 37869 th iteration gives loss of 0.14679328725296667\n",
      "The 37870 th iteration gives loss of 0.14679184417279792\n",
      "The 37871 th iteration gives loss of 0.1467904011355049\n",
      "The 37872 th iteration gives loss of 0.1467889581410789\n",
      "The 37873 th iteration gives loss of 0.14678751518952252\n",
      "The 37874 th iteration gives loss of 0.14678607228082552\n",
      "The 37875 th iteration gives loss of 0.14678462941498213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 37876 th iteration gives loss of 0.14678318659199316\n",
      "The 37877 th iteration gives loss of 0.14678174381183998\n",
      "The 37878 th iteration gives loss of 0.14678030107453424\n",
      "The 37879 th iteration gives loss of 0.14677885838005195\n",
      "The 37880 th iteration gives loss of 0.1467774157284109\n",
      "The 37881 th iteration gives loss of 0.14677597311958085\n",
      "The 37882 th iteration gives loss of 0.14677453055357748\n",
      "The 37883 th iteration gives loss of 0.14677308803038072\n",
      "The 37884 th iteration gives loss of 0.1467716455500028\n",
      "The 37885 th iteration gives loss of 0.14677020311242178\n",
      "The 37886 th iteration gives loss of 0.1467687607176374\n",
      "The 37887 th iteration gives loss of 0.1467673183656441\n",
      "The 37888 th iteration gives loss of 0.14676587605644098\n",
      "The 37889 th iteration gives loss of 0.1467644337900192\n",
      "The 37890 th iteration gives loss of 0.1467629915663752\n",
      "The 37891 th iteration gives loss of 0.14676154938551636\n",
      "The 37892 th iteration gives loss of 0.14676010724741387\n",
      "The 37893 th iteration gives loss of 0.14675866515207445\n",
      "The 37894 th iteration gives loss of 0.14675722309948352\n",
      "The 37895 th iteration gives loss of 0.14675578108965168\n",
      "The 37896 th iteration gives loss of 0.14675433912256614\n",
      "The 37897 th iteration gives loss of 0.14675289719822474\n",
      "The 37898 th iteration gives loss of 0.14675145531662123\n",
      "The 37899 th iteration gives loss of 0.1467500134777512\n",
      "The 37900 th iteration gives loss of 0.1467485716816005\n",
      "The 37901 th iteration gives loss of 0.14674712992816782\n",
      "The 37902 th iteration gives loss of 0.14674568821745598\n",
      "The 37903 th iteration gives loss of 0.14674424654945337\n",
      "The 37904 th iteration gives loss of 0.14674280492416428\n",
      "The 37905 th iteration gives loss of 0.14674136334156662\n",
      "The 37906 th iteration gives loss of 0.14673992180166645\n",
      "The 37907 th iteration gives loss of 0.14673848030446326\n",
      "The 37908 th iteration gives loss of 0.14673703884993208\n",
      "The 37909 th iteration gives loss of 0.14673559743809758\n",
      "The 37910 th iteration gives loss of 0.14673415606892243\n",
      "The 37911 th iteration gives loss of 0.1467327147424272\n",
      "The 37912 th iteration gives loss of 0.14673127345859685\n",
      "The 37913 th iteration gives loss of 0.146729832217425\n",
      "The 37914 th iteration gives loss of 0.14672839101890386\n",
      "The 37915 th iteration gives loss of 0.1467269498630395\n",
      "The 37916 th iteration gives loss of 0.1467255087498159\n",
      "The 37917 th iteration gives loss of 0.14672406767922494\n",
      "The 37918 th iteration gives loss of 0.14672262665127173\n",
      "The 37919 th iteration gives loss of 0.14672118566595116\n",
      "The 37920 th iteration gives loss of 0.14671974472325275\n",
      "The 37921 th iteration gives loss of 0.14671830382317508\n",
      "The 37922 th iteration gives loss of 0.14671686296571051\n",
      "The 37923 th iteration gives loss of 0.146715422150859\n",
      "The 37924 th iteration gives loss of 0.1467139813786035\n",
      "The 37925 th iteration gives loss of 0.14671254064894762\n",
      "The 37926 th iteration gives loss of 0.1467110999618868\n",
      "The 37927 th iteration gives loss of 0.14670965931741523\n",
      "The 37928 th iteration gives loss of 0.1467082187155305\n",
      "The 37929 th iteration gives loss of 0.146706778156219\n",
      "The 37930 th iteration gives loss of 0.1467053376394833\n",
      "The 37931 th iteration gives loss of 0.14670389716530938\n",
      "The 37932 th iteration gives loss of 0.14670245673370405\n",
      "The 37933 th iteration gives loss of 0.14670101634465565\n",
      "The 37934 th iteration gives loss of 0.1466995759981583\n",
      "The 37935 th iteration gives loss of 0.1466981356942082\n",
      "The 37936 th iteration gives loss of 0.14669669543279662\n",
      "The 37937 th iteration gives loss of 0.14669525521393204\n",
      "The 37938 th iteration gives loss of 0.14669381503760145\n",
      "The 37939 th iteration gives loss of 0.1466923749037924\n",
      "The 37940 th iteration gives loss of 0.14669093481250106\n",
      "The 37941 th iteration gives loss of 0.14668949476373186\n",
      "The 37942 th iteration gives loss of 0.14668805475747626\n",
      "The 37943 th iteration gives loss of 0.14668661479372536\n",
      "The 37944 th iteration gives loss of 0.14668517487247687\n",
      "The 37945 th iteration gives loss of 0.14668373499372378\n",
      "The 37946 th iteration gives loss of 0.14668229515746384\n",
      "The 37947 th iteration gives loss of 0.14668085536369083\n",
      "The 37948 th iteration gives loss of 0.14667941561239337\n",
      "The 37949 th iteration gives loss of 0.14667797590357962\n",
      "The 37950 th iteration gives loss of 0.1466765362372374\n",
      "The 37951 th iteration gives loss of 0.1466750966133616\n",
      "The 37952 th iteration gives loss of 0.1466736570319408\n",
      "The 37953 th iteration gives loss of 0.1466722174929797\n",
      "The 37954 th iteration gives loss of 0.14667077799646963\n",
      "The 37955 th iteration gives loss of 0.14666933854241354\n",
      "The 37956 th iteration gives loss of 0.1466678991307842\n",
      "The 37957 th iteration gives loss of 0.14666645976159745\n",
      "The 37958 th iteration gives loss of 0.1466650204348352\n",
      "The 37959 th iteration gives loss of 0.14666358115050213\n",
      "The 37960 th iteration gives loss of 0.14666214190859556\n",
      "The 37961 th iteration gives loss of 0.14666070270910403\n",
      "The 37962 th iteration gives loss of 0.14665926355201533\n",
      "The 37963 th iteration gives loss of 0.14665782443733685\n",
      "The 37964 th iteration gives loss of 0.146656385365061\n",
      "The 37965 th iteration gives loss of 0.14665494633517398\n",
      "The 37966 th iteration gives loss of 0.1466535073476835\n",
      "The 37967 th iteration gives loss of 0.14665206840257156\n",
      "The 37968 th iteration gives loss of 0.1466506294998442\n",
      "The 37969 th iteration gives loss of 0.146649190639498\n",
      "The 37970 th iteration gives loss of 0.1466477518215111\n",
      "The 37971 th iteration gives loss of 0.14664631304589154\n",
      "The 37972 th iteration gives loss of 0.14664487431263604\n",
      "The 37973 th iteration gives loss of 0.1466434356217337\n",
      "The 37974 th iteration gives loss of 0.14664199697317942\n",
      "The 37975 th iteration gives loss of 0.14664055836697226\n",
      "The 37976 th iteration gives loss of 0.14663911980310015\n",
      "The 37977 th iteration gives loss of 0.1466376812815659\n",
      "The 37978 th iteration gives loss of 0.1466362428023596\n",
      "The 37979 th iteration gives loss of 0.14663480436547502\n",
      "The 37980 th iteration gives loss of 0.1466333659709193\n",
      "The 37981 th iteration gives loss of 0.14663192761866978\n",
      "The 37982 th iteration gives loss of 0.1466304893087335\n",
      "The 37983 th iteration gives loss of 0.14662905104109547\n",
      "The 37984 th iteration gives loss of 0.14662761281575876\n",
      "The 37985 th iteration gives loss of 0.14662617463271707\n",
      "The 37986 th iteration gives loss of 0.1466247364919676\n",
      "The 37987 th iteration gives loss of 0.1466232983934976\n",
      "The 37988 th iteration gives loss of 0.14662186033730815\n",
      "The 37989 th iteration gives loss of 0.14662042232339487\n",
      "The 37990 th iteration gives loss of 0.146618984351749\n",
      "The 37991 th iteration gives loss of 0.146617546422371\n",
      "The 37992 th iteration gives loss of 0.14661610853524754\n",
      "The 37993 th iteration gives loss of 0.14661467069037548\n",
      "The 37994 th iteration gives loss of 0.14661323288774958\n",
      "The 37995 th iteration gives loss of 0.14661179512738046\n",
      "The 37996 th iteration gives loss of 0.14661035740923964\n",
      "The 37997 th iteration gives loss of 0.14660891973333504\n",
      "The 37998 th iteration gives loss of 0.14660748209966112\n",
      "The 37999 th iteration gives loss of 0.1466060445082029\n",
      "The 38000 th iteration gives loss of 0.14660460695896962\n",
      "The 38001 th iteration gives loss of 0.14660316945194687\n",
      "The 38002 th iteration gives loss of 0.146601731987137\n",
      "The 38003 th iteration gives loss of 0.14660029456452808\n",
      "The 38004 th iteration gives loss of 0.1465988571841156\n",
      "The 38005 th iteration gives loss of 0.14659741984589533\n",
      "The 38006 th iteration gives loss of 0.1465959825498643\n",
      "The 38007 th iteration gives loss of 0.1465945452960214\n",
      "The 38008 th iteration gives loss of 0.14659310808435344\n",
      "The 38009 th iteration gives loss of 0.14659167091485745\n",
      "The 38010 th iteration gives loss of 0.14659023378752772\n",
      "The 38011 th iteration gives loss of 0.14658879670236766\n",
      "The 38012 th iteration gives loss of 0.14658735965935918\n",
      "The 38013 th iteration gives loss of 0.14658592265850848\n",
      "The 38014 th iteration gives loss of 0.146584485699806\n",
      "The 38015 th iteration gives loss of 0.14658304878323947\n",
      "The 38016 th iteration gives loss of 0.14658161190881033\n",
      "The 38017 th iteration gives loss of 0.14658017507652313\n",
      "The 38018 th iteration gives loss of 0.14657873828636445\n",
      "The 38019 th iteration gives loss of 0.1465773015383223\n",
      "The 38020 th iteration gives loss of 0.14657586483239665\n",
      "The 38021 th iteration gives loss of 0.146574428168592\n",
      "The 38022 th iteration gives loss of 0.14657299154688763\n",
      "The 38023 th iteration gives loss of 0.14657155496728638\n",
      "The 38024 th iteration gives loss of 0.14657011842978726\n",
      "The 38025 th iteration gives loss of 0.14656868193437847\n",
      "The 38026 th iteration gives loss of 0.14656724548105252\n",
      "The 38027 th iteration gives loss of 0.14656580906981634\n",
      "The 38028 th iteration gives loss of 0.14656437270065908\n",
      "The 38029 th iteration gives loss of 0.14656293637357914\n",
      "The 38030 th iteration gives loss of 0.14656150008855046\n",
      "The 38031 th iteration gives loss of 0.1465600638455897\n",
      "The 38032 th iteration gives loss of 0.14655862764469002\n",
      "The 38033 th iteration gives loss of 0.14655719148583973\n",
      "The 38034 th iteration gives loss of 0.1465557553690484\n",
      "The 38035 th iteration gives loss of 0.14655431929428528\n",
      "The 38036 th iteration gives loss of 0.1465528832615699\n",
      "The 38037 th iteration gives loss of 0.1465514472708849\n",
      "The 38038 th iteration gives loss of 0.14655001132222803\n",
      "The 38039 th iteration gives loss of 0.1465485754155914\n",
      "The 38040 th iteration gives loss of 0.14654713955097676\n",
      "The 38041 th iteration gives loss of 0.14654570372836767\n",
      "The 38042 th iteration gives loss of 0.14654426794777078\n",
      "The 38043 th iteration gives loss of 0.14654283220916983\n",
      "The 38044 th iteration gives loss of 0.1465413965125784\n",
      "The 38045 th iteration gives loss of 0.14653996085797155\n",
      "The 38046 th iteration gives loss of 0.1465385252453439\n",
      "The 38047 th iteration gives loss of 0.14653708967470958\n",
      "The 38048 th iteration gives loss of 0.14653565414605427\n",
      "The 38049 th iteration gives loss of 0.14653421865936614\n",
      "The 38050 th iteration gives loss of 0.14653278321464622\n",
      "The 38051 th iteration gives loss of 0.1465313478118928\n",
      "The 38052 th iteration gives loss of 0.14652991245109637\n",
      "The 38053 th iteration gives loss of 0.1465284771322509\n",
      "The 38054 th iteration gives loss of 0.14652704185534796\n",
      "The 38055 th iteration gives loss of 0.14652560662039518\n",
      "The 38056 th iteration gives loss of 0.14652417142737867\n",
      "The 38057 th iteration gives loss of 0.14652273627629045\n",
      "The 38058 th iteration gives loss of 0.146521301167134\n",
      "The 38059 th iteration gives loss of 0.14651986609990178\n",
      "The 38060 th iteration gives loss of 0.1465184310745823\n",
      "The 38061 th iteration gives loss of 0.14651699609117952\n",
      "The 38062 th iteration gives loss of 0.14651556114967382\n",
      "The 38063 th iteration gives loss of 0.14651412625008345\n",
      "The 38064 th iteration gives loss of 0.1465126913923909\n",
      "The 38065 th iteration gives loss of 0.146511256576586\n",
      "The 38066 th iteration gives loss of 0.1465098218026653\n",
      "The 38067 th iteration gives loss of 0.14650838707063726\n",
      "The 38068 th iteration gives loss of 0.14650695238047848\n",
      "The 38069 th iteration gives loss of 0.14650551773219783\n",
      "The 38070 th iteration gives loss of 0.14650408312578225\n",
      "The 38071 th iteration gives loss of 0.14650264856122985\n",
      "The 38072 th iteration gives loss of 0.1465012140385328\n",
      "The 38073 th iteration gives loss of 0.14649977955768584\n",
      "The 38074 th iteration gives loss of 0.14649834511869483\n",
      "The 38075 th iteration gives loss of 0.146496910721546\n",
      "The 38076 th iteration gives loss of 0.14649547636622987\n",
      "The 38077 th iteration gives loss of 0.1464940420527491\n",
      "The 38078 th iteration gives loss of 0.14649260778109505\n",
      "The 38079 th iteration gives loss of 0.14649117355126565\n",
      "The 38080 th iteration gives loss of 0.14648973936325815\n",
      "The 38081 th iteration gives loss of 0.14648830521705702\n",
      "The 38082 th iteration gives loss of 0.14648687111266745\n",
      "The 38083 th iteration gives loss of 0.14648543705007838\n",
      "The 38084 th iteration gives loss of 0.1464840030292891\n",
      "The 38085 th iteration gives loss of 0.14648256905029378\n",
      "The 38086 th iteration gives loss of 0.146481135113089\n",
      "The 38087 th iteration gives loss of 0.1464797012176655\n",
      "The 38088 th iteration gives loss of 0.1464782673640161\n",
      "The 38089 th iteration gives loss of 0.1464768335521413\n",
      "The 38090 th iteration gives loss of 0.14647539978203816\n",
      "The 38091 th iteration gives loss of 0.14647396605368973\n",
      "The 38092 th iteration gives loss of 0.1464725323671135\n",
      "The 38093 th iteration gives loss of 0.14647109872227596\n",
      "The 38094 th iteration gives loss of 0.146469665119197\n",
      "The 38095 th iteration gives loss of 0.14646823155785763\n",
      "The 38096 th iteration gives loss of 0.14646679803826085\n",
      "The 38097 th iteration gives loss of 0.146465364560396\n",
      "The 38098 th iteration gives loss of 0.1464639311242556\n",
      "The 38099 th iteration gives loss of 0.1464624977298469\n",
      "The 38100 th iteration gives loss of 0.1464610643771476\n",
      "The 38101 th iteration gives loss of 0.14645963106616675\n",
      "The 38102 th iteration gives loss of 0.14645819779689387\n",
      "The 38103 th iteration gives loss of 0.14645676456931941\n",
      "The 38104 th iteration gives loss of 0.14645533138344852\n",
      "The 38105 th iteration gives loss of 0.1464538982392782\n",
      "The 38106 th iteration gives loss of 0.14645246513678534\n",
      "The 38107 th iteration gives loss of 0.14645103207598573\n",
      "The 38108 th iteration gives loss of 0.146449599056853\n",
      "The 38109 th iteration gives loss of 0.14644816607940125\n",
      "The 38110 th iteration gives loss of 0.14644673314362172\n",
      "The 38111 th iteration gives loss of 0.14644530024950073\n",
      "The 38112 th iteration gives loss of 0.14644386739704723\n",
      "The 38113 th iteration gives loss of 0.1464424345862465\n",
      "The 38114 th iteration gives loss of 0.14644100181708372\n",
      "The 38115 th iteration gives loss of 0.146439569089583\n",
      "The 38116 th iteration gives loss of 0.14643813640370826\n",
      "The 38117 th iteration gives loss of 0.1464367037594704\n",
      "The 38118 th iteration gives loss of 0.14643527115686097\n",
      "The 38119 th iteration gives loss of 0.14643383859587852\n",
      "The 38120 th iteration gives loss of 0.1464324060765213\n",
      "The 38121 th iteration gives loss of 0.14643097359877147\n",
      "The 38122 th iteration gives loss of 0.14642954116263585\n",
      "The 38123 th iteration gives loss of 0.1464281087681019\n",
      "The 38124 th iteration gives loss of 0.14642667641516527\n",
      "The 38125 th iteration gives loss of 0.14642524410382735\n",
      "The 38126 th iteration gives loss of 0.14642381183408124\n",
      "The 38127 th iteration gives loss of 0.14642237960591742\n",
      "The 38128 th iteration gives loss of 0.14642094741933712\n",
      "The 38129 th iteration gives loss of 0.14641951527432553\n",
      "The 38130 th iteration gives loss of 0.14641808317088936\n",
      "The 38131 th iteration gives loss of 0.14641665110901572\n",
      "The 38132 th iteration gives loss of 0.14641521908870725\n",
      "The 38133 th iteration gives loss of 0.1464137871099424\n",
      "The 38134 th iteration gives loss of 0.14641235517274037\n",
      "The 38135 th iteration gives loss of 0.14641092327708446\n",
      "The 38136 th iteration gives loss of 0.14640949142296206\n",
      "The 38137 th iteration gives loss of 0.1464080596103814\n",
      "The 38138 th iteration gives loss of 0.1464066278393287\n",
      "The 38139 th iteration gives loss of 0.1464051961097954\n",
      "The 38140 th iteration gives loss of 0.14640376442179026\n",
      "The 38141 th iteration gives loss of 0.14640233277529813\n",
      "The 38142 th iteration gives loss of 0.14640090117031532\n",
      "The 38143 th iteration gives loss of 0.14639946960684658\n",
      "The 38144 th iteration gives loss of 0.14639803808487734\n",
      "The 38145 th iteration gives loss of 0.1463966066043959\n",
      "The 38146 th iteration gives loss of 0.14639517516541703\n",
      "The 38147 th iteration gives loss of 0.1463937437679227\n",
      "The 38148 th iteration gives loss of 0.1463923124119056\n",
      "The 38149 th iteration gives loss of 0.14639088109736165\n",
      "The 38150 th iteration gives loss of 0.14638944982428645\n",
      "The 38151 th iteration gives loss of 0.14638801859268852\n",
      "The 38152 th iteration gives loss of 0.1463865874025554\n",
      "The 38153 th iteration gives loss of 0.14638515625387616\n",
      "The 38154 th iteration gives loss of 0.14638372514663878\n",
      "The 38155 th iteration gives loss of 0.14638229408085882\n",
      "The 38156 th iteration gives loss of 0.14638086305651957\n",
      "The 38157 th iteration gives loss of 0.14637943207361345\n",
      "The 38158 th iteration gives loss of 0.14637800113214447\n",
      "The 38159 th iteration gives loss of 0.1463765702320962\n",
      "The 38160 th iteration gives loss of 0.14637513937348132\n",
      "The 38161 th iteration gives loss of 0.14637370855627463\n",
      "The 38162 th iteration gives loss of 0.14637227778048753\n",
      "The 38163 th iteration gives loss of 0.14637084704609912\n",
      "The 38164 th iteration gives loss of 0.14636941635312412\n",
      "The 38165 th iteration gives loss of 0.1463679857015395\n",
      "The 38166 th iteration gives loss of 0.14636655509134808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 38167 th iteration gives loss of 0.1463651245225487\n",
      "The 38168 th iteration gives loss of 0.14636369399512608\n",
      "The 38169 th iteration gives loss of 0.1463622635090932\n",
      "The 38170 th iteration gives loss of 0.14636083306442943\n",
      "The 38171 th iteration gives loss of 0.14635940266112732\n",
      "The 38172 th iteration gives loss of 0.14635797229920025\n",
      "The 38173 th iteration gives loss of 0.146356541978622\n",
      "The 38174 th iteration gives loss of 0.14635511169940407\n",
      "The 38175 th iteration gives loss of 0.14635368146154057\n",
      "The 38176 th iteration gives loss of 0.14635225126501034\n",
      "The 38177 th iteration gives loss of 0.1463508211098268\n",
      "The 38178 th iteration gives loss of 0.14634939099596964\n",
      "The 38179 th iteration gives loss of 0.14634796092343844\n",
      "The 38180 th iteration gives loss of 0.14634653089224242\n",
      "The 38181 th iteration gives loss of 0.14634510090236186\n",
      "The 38182 th iteration gives loss of 0.14634367095379663\n",
      "The 38183 th iteration gives loss of 0.14634224104653942\n",
      "The 38184 th iteration gives loss of 0.14634081118058725\n",
      "The 38185 th iteration gives loss of 0.146339381355938\n",
      "The 38186 th iteration gives loss of 0.14633795157257834\n",
      "The 38187 th iteration gives loss of 0.14633652183051885\n",
      "The 38188 th iteration gives loss of 0.14633509212973742\n",
      "The 38189 th iteration gives loss of 0.14633366247024301\n",
      "The 38190 th iteration gives loss of 0.1463322328520215\n",
      "The 38191 th iteration gives loss of 0.146330803275061\n",
      "The 38192 th iteration gives loss of 0.14632937373937024\n",
      "The 38193 th iteration gives loss of 0.14632794424494722\n",
      "The 38194 th iteration gives loss of 0.1463265147917812\n",
      "The 38195 th iteration gives loss of 0.14632508537986022\n",
      "The 38196 th iteration gives loss of 0.1463236560091889\n",
      "The 38197 th iteration gives loss of 0.1463222266797521\n",
      "The 38198 th iteration gives loss of 0.14632079739155884\n",
      "The 38199 th iteration gives loss of 0.146319368144598\n",
      "The 38200 th iteration gives loss of 0.1463179389388575\n",
      "The 38201 th iteration gives loss of 0.14631650977434468\n",
      "The 38202 th iteration gives loss of 0.14631508065104806\n",
      "The 38203 th iteration gives loss of 0.14631365156896287\n",
      "The 38204 th iteration gives loss of 0.14631222252808368\n",
      "The 38205 th iteration gives loss of 0.14631079352841275\n",
      "The 38206 th iteration gives loss of 0.14630936456993543\n",
      "The 38207 th iteration gives loss of 0.14630793565264644\n",
      "The 38208 th iteration gives loss of 0.14630650677654206\n",
      "The 38209 th iteration gives loss of 0.14630507794163214\n",
      "The 38210 th iteration gives loss of 0.14630364914789393\n",
      "The 38211 th iteration gives loss of 0.14630222039533342\n",
      "The 38212 th iteration gives loss of 0.14630079168393662\n",
      "The 38213 th iteration gives loss of 0.14629936301370217\n",
      "The 38214 th iteration gives loss of 0.14629793438462843\n",
      "The 38215 th iteration gives loss of 0.14629650579671194\n",
      "The 38216 th iteration gives loss of 0.1462950772499407\n",
      "The 38217 th iteration gives loss of 0.14629364874431186\n",
      "The 38218 th iteration gives loss of 0.14629222027982133\n",
      "The 38219 th iteration gives loss of 0.14629079185646499\n",
      "The 38220 th iteration gives loss of 0.14628936347423802\n",
      "The 38221 th iteration gives loss of 0.146287935133141\n",
      "The 38222 th iteration gives loss of 0.14628650683315386\n",
      "The 38223 th iteration gives loss of 0.14628507857429898\n",
      "The 38224 th iteration gives loss of 0.14628365035653335\n",
      "The 38225 th iteration gives loss of 0.14628222217988654\n",
      "The 38226 th iteration gives loss of 0.14628079404433952\n",
      "The 38227 th iteration gives loss of 0.14627936594987698\n",
      "The 38228 th iteration gives loss of 0.14627793789651183\n",
      "The 38229 th iteration gives loss of 0.14627650988423013\n",
      "The 38230 th iteration gives loss of 0.14627508191303337\n",
      "The 38231 th iteration gives loss of 0.1462736539829094\n",
      "The 38232 th iteration gives loss of 0.14627222609386015\n",
      "The 38233 th iteration gives loss of 0.14627079824587447\n",
      "The 38234 th iteration gives loss of 0.1462693704389501\n",
      "The 38235 th iteration gives loss of 0.1462679426730776\n",
      "The 38236 th iteration gives loss of 0.1462665149482613\n",
      "The 38237 th iteration gives loss of 0.14626508726448884\n",
      "The 38238 th iteration gives loss of 0.1462636596217551\n",
      "The 38239 th iteration gives loss of 0.14626223202006697\n",
      "The 38240 th iteration gives loss of 0.14626080445940454\n",
      "The 38241 th iteration gives loss of 0.14625937693977561\n",
      "The 38242 th iteration gives loss of 0.14625794946116324\n",
      "The 38243 th iteration gives loss of 0.14625652202357273\n",
      "The 38244 th iteration gives loss of 0.14625509462699976\n",
      "The 38245 th iteration gives loss of 0.1462536672714239\n",
      "The 38246 th iteration gives loss of 0.14625223995685474\n",
      "The 38247 th iteration gives loss of 0.14625081268328333\n",
      "The 38248 th iteration gives loss of 0.14624938545070923\n",
      "The 38249 th iteration gives loss of 0.14624795825911935\n",
      "The 38250 th iteration gives loss of 0.14624653110851166\n",
      "The 38251 th iteration gives loss of 0.14624510399889318\n",
      "The 38252 th iteration gives loss of 0.14624367693023865\n",
      "The 38253 th iteration gives loss of 0.14624224990256118\n",
      "The 38254 th iteration gives loss of 0.1462408229158428\n",
      "The 38255 th iteration gives loss of 0.14623939597008515\n",
      "The 38256 th iteration gives loss of 0.14623796906527475\n",
      "The 38257 th iteration gives loss of 0.1462365422014236\n",
      "The 38258 th iteration gives loss of 0.14623511537851205\n",
      "The 38259 th iteration gives loss of 0.14623368859654692\n",
      "The 38260 th iteration gives loss of 0.1462322618555145\n",
      "The 38261 th iteration gives loss of 0.14623083515541446\n",
      "The 38262 th iteration gives loss of 0.14622940849623622\n",
      "The 38263 th iteration gives loss of 0.1462279818779807\n",
      "The 38264 th iteration gives loss of 0.14622655530063364\n",
      "The 38265 th iteration gives loss of 0.14622512876421312\n",
      "The 38266 th iteration gives loss of 0.1462237022686887\n",
      "The 38267 th iteration gives loss of 0.1462222758140628\n",
      "The 38268 th iteration gives loss of 0.1462208494003375\n",
      "The 38269 th iteration gives loss of 0.1462194230275021\n",
      "The 38270 th iteration gives loss of 0.14621799669555996\n",
      "The 38271 th iteration gives loss of 0.14621657040449815\n",
      "The 38272 th iteration gives loss of 0.14621514415431308\n",
      "The 38273 th iteration gives loss of 0.14621371794500113\n",
      "The 38274 th iteration gives loss of 0.1462122917765567\n",
      "The 38275 th iteration gives loss of 0.14621086564897484\n",
      "The 38276 th iteration gives loss of 0.14620943956223947\n",
      "The 38277 th iteration gives loss of 0.1462080135163706\n",
      "The 38278 th iteration gives loss of 0.14620658751135512\n",
      "The 38279 th iteration gives loss of 0.1462051615471692\n",
      "The 38280 th iteration gives loss of 0.146203735623831\n",
      "The 38281 th iteration gives loss of 0.1462023097413245\n",
      "The 38282 th iteration gives loss of 0.14620088389964886\n",
      "The 38283 th iteration gives loss of 0.14619945809879947\n",
      "The 38284 th iteration gives loss of 0.14619803233876755\n",
      "The 38285 th iteration gives loss of 0.14619660661954836\n",
      "The 38286 th iteration gives loss of 0.14619518094113823\n",
      "The 38287 th iteration gives loss of 0.14619375530353879\n",
      "The 38288 th iteration gives loss of 0.14619232970673524\n",
      "The 38289 th iteration gives loss of 0.1461909041507294\n",
      "The 38290 th iteration gives loss of 0.14618947863550935\n",
      "The 38291 th iteration gives loss of 0.14618805316107492\n",
      "The 38292 th iteration gives loss of 0.1461866277274211\n",
      "The 38293 th iteration gives loss of 0.146185202334555\n",
      "The 38294 th iteration gives loss of 0.14618377698245577\n",
      "The 38295 th iteration gives loss of 0.14618235167111635\n",
      "The 38296 th iteration gives loss of 0.14618092640054106\n",
      "The 38297 th iteration gives loss of 0.14617950117073014\n",
      "The 38298 th iteration gives loss of 0.14617807598165994\n",
      "The 38299 th iteration gives loss of 0.14617665083334203\n",
      "The 38300 th iteration gives loss of 0.14617522572577332\n",
      "The 38301 th iteration gives loss of 0.1461738006589354\n",
      "The 38302 th iteration gives loss of 0.1461723756328351\n",
      "The 38303 th iteration gives loss of 0.14617095064745567\n",
      "The 38304 th iteration gives loss of 0.14616952570280448\n",
      "The 38305 th iteration gives loss of 0.1461681007988664\n",
      "The 38306 th iteration gives loss of 0.14616667593565227\n",
      "The 38307 th iteration gives loss of 0.14616525111314232\n",
      "The 38308 th iteration gives loss of 0.14616382633133\n",
      "The 38309 th iteration gives loss of 0.14616240159023\n",
      "The 38310 th iteration gives loss of 0.1461609768898159\n",
      "The 38311 th iteration gives loss of 0.14615955223009183\n",
      "The 38312 th iteration gives loss of 0.14615812761105446\n",
      "The 38313 th iteration gives loss of 0.14615670303269473\n",
      "The 38314 th iteration gives loss of 0.14615527849501628\n",
      "The 38315 th iteration gives loss of 0.14615385399800923\n",
      "The 38316 th iteration gives loss of 0.1461524295416643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 38317 th iteration gives loss of 0.1461510051259766\n",
      "The 38318 th iteration gives loss of 0.1461495807509524\n",
      "The 38319 th iteration gives loss of 0.14614815641657056\n",
      "The 38320 th iteration gives loss of 0.14614673212284\n",
      "The 38321 th iteration gives loss of 0.14614530786975966\n",
      "The 38322 th iteration gives loss of 0.14614388365730613\n",
      "The 38323 th iteration gives loss of 0.14614245948548515\n",
      "The 38324 th iteration gives loss of 0.14614103535429313\n",
      "The 38325 th iteration gives loss of 0.14613961126372688\n",
      "The 38326 th iteration gives loss of 0.14613818721377586\n",
      "The 38327 th iteration gives loss of 0.14613676320443875\n",
      "The 38328 th iteration gives loss of 0.1461353392357133\n",
      "The 38329 th iteration gives loss of 0.14613391530759037\n",
      "The 38330 th iteration gives loss of 0.14613249142006193\n",
      "The 38331 th iteration gives loss of 0.14613106757312844\n",
      "The 38332 th iteration gives loss of 0.14612964376678853\n",
      "The 38333 th iteration gives loss of 0.1461282200010283\n",
      "The 38334 th iteration gives loss of 0.14612679627585526\n",
      "The 38335 th iteration gives loss of 0.1461253725912527\n",
      "The 38336 th iteration gives loss of 0.1461239489472149\n",
      "The 38337 th iteration gives loss of 0.14612252534375025\n",
      "The 38338 th iteration gives loss of 0.14612110178084833\n",
      "The 38339 th iteration gives loss of 0.14611967825849528\n",
      "The 38340 th iteration gives loss of 0.1461182547766957\n",
      "The 38341 th iteration gives loss of 0.14611683133544046\n",
      "The 38342 th iteration gives loss of 0.1461154079347218\n",
      "The 38343 th iteration gives loss of 0.14611398457455038\n",
      "The 38344 th iteration gives loss of 0.14611256125490565\n",
      "The 38345 th iteration gives loss of 0.14611113797579123\n",
      "The 38346 th iteration gives loss of 0.14610971473719245\n",
      "The 38347 th iteration gives loss of 0.14610829153911506\n",
      "The 38348 th iteration gives loss of 0.14610686838155462\n",
      "The 38349 th iteration gives loss of 0.1461054452645052\n",
      "The 38350 th iteration gives loss of 0.14610402218794638\n",
      "The 38351 th iteration gives loss of 0.14610259915189835\n",
      "The 38352 th iteration gives loss of 0.14610117615634147\n",
      "The 38353 th iteration gives loss of 0.1460997532012677\n",
      "The 38354 th iteration gives loss of 0.1460983302866844\n",
      "The 38355 th iteration gives loss of 0.1460969074125792\n",
      "The 38356 th iteration gives loss of 0.14609548457895374\n",
      "The 38357 th iteration gives loss of 0.14609406178579126\n",
      "The 38358 th iteration gives loss of 0.14609263903309685\n",
      "The 38359 th iteration gives loss of 0.14609121632085773\n",
      "The 38360 th iteration gives loss of 0.14608979364908123\n",
      "The 38361 th iteration gives loss of 0.14608837101775127\n",
      "The 38362 th iteration gives loss of 0.14608694842686856\n",
      "The 38363 th iteration gives loss of 0.14608552587643162\n",
      "The 38364 th iteration gives loss of 0.14608410336642835\n",
      "The 38365 th iteration gives loss of 0.14608268089685866\n",
      "The 38366 th iteration gives loss of 0.14608125846772027\n",
      "The 38367 th iteration gives loss of 0.14607983607899658\n",
      "The 38368 th iteration gives loss of 0.1460784137306933\n",
      "The 38369 th iteration gives loss of 0.14607699142279792\n",
      "The 38370 th iteration gives loss of 0.1460755691553152\n",
      "The 38371 th iteration gives loss of 0.1460741469282343\n",
      "The 38372 th iteration gives loss of 0.14607272474156296\n",
      "The 38373 th iteration gives loss of 0.14607130259526738\n",
      "The 38374 th iteration gives loss of 0.14606988048937394\n",
      "The 38375 th iteration gives loss of 0.14606845842386343\n",
      "The 38376 th iteration gives loss of 0.1460670363987325\n",
      "The 38377 th iteration gives loss of 0.146065614413978\n",
      "The 38378 th iteration gives loss of 0.14606419246958724\n",
      "The 38379 th iteration gives loss of 0.14606277056556166\n",
      "The 38380 th iteration gives loss of 0.14606134870190193\n",
      "The 38381 th iteration gives loss of 0.14605992687860164\n",
      "The 38382 th iteration gives loss of 0.14605850509565071\n",
      "The 38383 th iteration gives loss of 0.14605708335303846\n",
      "The 38384 th iteration gives loss of 0.14605566165077621\n",
      "The 38385 th iteration gives loss of 0.14605423998885184\n",
      "The 38386 th iteration gives loss of 0.1460528183672569\n",
      "The 38387 th iteration gives loss of 0.1460513967859886\n",
      "The 38388 th iteration gives loss of 0.14604997524503832\n",
      "The 38389 th iteration gives loss of 0.14604855374442058\n",
      "The 38390 th iteration gives loss of 0.14604713228410476\n",
      "The 38391 th iteration gives loss of 0.14604571086409907\n",
      "The 38392 th iteration gives loss of 0.14604428948439402\n",
      "The 38393 th iteration gives loss of 0.14604286814499848\n",
      "The 38394 th iteration gives loss of 0.14604144684589043\n",
      "The 38395 th iteration gives loss of 0.1460400255870748\n",
      "The 38396 th iteration gives loss of 0.14603860436854355\n",
      "The 38397 th iteration gives loss of 0.14603718319029066\n",
      "The 38398 th iteration gives loss of 0.14603576205231075\n",
      "The 38399 th iteration gives loss of 0.14603434095460444\n",
      "The 38400 th iteration gives loss of 0.1460329198971689\n",
      "The 38401 th iteration gives loss of 0.1460314988799882\n",
      "The 38402 th iteration gives loss of 0.1460300779030699\n",
      "The 38403 th iteration gives loss of 0.1460286569663987\n",
      "The 38404 th iteration gives loss of 0.14602723606998053\n",
      "The 38405 th iteration gives loss of 0.14602581521379981\n",
      "The 38406 th iteration gives loss of 0.14602439439786036\n",
      "The 38407 th iteration gives loss of 0.14602297362215366\n",
      "The 38408 th iteration gives loss of 0.14602155288667332\n",
      "The 38409 th iteration gives loss of 0.14602013219141352\n",
      "The 38410 th iteration gives loss of 0.14601871153637888\n",
      "The 38411 th iteration gives loss of 0.14601729092155655\n",
      "The 38412 th iteration gives loss of 0.14601587034694\n",
      "The 38413 th iteration gives loss of 0.14601444981253106\n",
      "The 38414 th iteration gives loss of 0.1460130293183182\n",
      "The 38415 th iteration gives loss of 0.1460116088643078\n",
      "The 38416 th iteration gives loss of 0.14601018845048483\n",
      "The 38417 th iteration gives loss of 0.14600876807684549\n",
      "The 38418 th iteration gives loss of 0.14600734774339122\n",
      "The 38419 th iteration gives loss of 0.14600592745010496\n",
      "The 38420 th iteration gives loss of 0.1460045071969978\n",
      "The 38421 th iteration gives loss of 0.14600308698405298\n",
      "The 38422 th iteration gives loss of 0.14600166681128043\n",
      "The 38423 th iteration gives loss of 0.14600024667865455\n",
      "The 38424 th iteration gives loss of 0.1459988265861882\n",
      "The 38425 th iteration gives loss of 0.14599740653386575\n",
      "The 38426 th iteration gives loss of 0.1459959865216849\n",
      "The 38427 th iteration gives loss of 0.1459945665496548\n",
      "The 38428 th iteration gives loss of 0.14599314661775398\n",
      "The 38429 th iteration gives loss of 0.14599172672597613\n",
      "The 38430 th iteration gives loss of 0.14599030687432707\n",
      "The 38431 th iteration gives loss of 0.14598888706280078\n",
      "The 38432 th iteration gives loss of 0.1459874672913845\n",
      "The 38433 th iteration gives loss of 0.145986047560077\n",
      "The 38434 th iteration gives loss of 0.14598462786888133\n",
      "The 38435 th iteration gives loss of 0.1459832082177812\n",
      "The 38436 th iteration gives loss of 0.14598178860677805\n",
      "The 38437 th iteration gives loss of 0.14598036903586825\n",
      "The 38438 th iteration gives loss of 0.1459789495050549\n",
      "The 38439 th iteration gives loss of 0.14597753001430902\n",
      "The 38440 th iteration gives loss of 0.1459761105636454\n",
      "The 38441 th iteration gives loss of 0.14597469115306153\n",
      "The 38442 th iteration gives loss of 0.14597327178253794\n",
      "The 38443 th iteration gives loss of 0.1459718524520756\n",
      "The 38444 th iteration gives loss of 0.1459704331616849\n",
      "The 38445 th iteration gives loss of 0.14596901391133957\n",
      "The 38446 th iteration gives loss of 0.14596759470105022\n",
      "The 38447 th iteration gives loss of 0.14596617553079808\n",
      "The 38448 th iteration gives loss of 0.14596475640058762\n",
      "The 38449 th iteration gives loss of 0.1459633373104182\n",
      "The 38450 th iteration gives loss of 0.1459619182602688\n",
      "The 38451 th iteration gives loss of 0.1459604992501445\n",
      "The 38452 th iteration gives loss of 0.14595908028004778\n",
      "The 38453 th iteration gives loss of 0.14595766134997912\n",
      "The 38454 th iteration gives loss of 0.14595624245990224\n",
      "The 38455 th iteration gives loss of 0.14595482360984377\n",
      "The 38456 th iteration gives loss of 0.14595340479978727\n",
      "The 38457 th iteration gives loss of 0.14595198602972534\n",
      "The 38458 th iteration gives loss of 0.1459505672996569\n",
      "The 38459 th iteration gives loss of 0.14594914860957944\n",
      "The 38460 th iteration gives loss of 0.14594772995948702\n",
      "The 38461 th iteration gives loss of 0.14594631134936514\n",
      "The 38462 th iteration gives loss of 0.14594489277923248\n",
      "The 38463 th iteration gives loss of 0.14594347424906007\n",
      "The 38464 th iteration gives loss of 0.14594205575885444\n",
      "The 38465 th iteration gives loss of 0.1459406373086156\n",
      "The 38466 th iteration gives loss of 0.14593921889831718\n",
      "The 38467 th iteration gives loss of 0.1459378005279827\n",
      "The 38468 th iteration gives loss of 0.14593638219758853\n",
      "The 38469 th iteration gives loss of 0.14593496390713814\n",
      "The 38470 th iteration gives loss of 0.1459335456566279\n",
      "The 38471 th iteration gives loss of 0.14593212744604592\n",
      "The 38472 th iteration gives loss of 0.14593070927539534\n",
      "The 38473 th iteration gives loss of 0.14592929114466316\n",
      "The 38474 th iteration gives loss of 0.14592787305385185\n",
      "The 38475 th iteration gives loss of 0.1459264550029461\n",
      "The 38476 th iteration gives loss of 0.14592503699195686\n",
      "The 38477 th iteration gives loss of 0.1459236190208806\n",
      "The 38478 th iteration gives loss of 0.14592220108969353\n",
      "The 38479 th iteration gives loss of 0.14592078319840004\n",
      "The 38480 th iteration gives loss of 0.14591936534700287\n",
      "The 38481 th iteration gives loss of 0.14591794753548595\n",
      "The 38482 th iteration gives loss of 0.14591652976385336\n",
      "The 38483 th iteration gives loss of 0.14591511203209898\n",
      "The 38484 th iteration gives loss of 0.14591369434020787\n",
      "The 38485 th iteration gives loss of 0.14591227668819065\n",
      "The 38486 th iteration gives loss of 0.14591085907603918\n",
      "The 38487 th iteration gives loss of 0.1459094415037399\n",
      "The 38488 th iteration gives loss of 0.14590802397129982\n",
      "The 38489 th iteration gives loss of 0.14590660647870196\n",
      "The 38490 th iteration gives loss of 0.14590518902594868\n",
      "The 38491 th iteration gives loss of 0.1459037716130395\n",
      "The 38492 th iteration gives loss of 0.14590235423995873\n",
      "The 38493 th iteration gives loss of 0.14590093690670836\n",
      "The 38494 th iteration gives loss of 0.14589951961328376\n",
      "The 38495 th iteration gives loss of 0.14589810235967088\n",
      "The 38496 th iteration gives loss of 0.14589668514588294\n",
      "The 38497 th iteration gives loss of 0.1458952679719045\n",
      "The 38498 th iteration gives loss of 0.14589385083773315\n",
      "The 38499 th iteration gives loss of 0.145892433743365\n",
      "The 38500 th iteration gives loss of 0.14589101668879928\n",
      "The 38501 th iteration gives loss of 0.14588959967401124\n",
      "The 38502 th iteration gives loss of 0.1458881826990223\n",
      "The 38503 th iteration gives loss of 0.14588676576381623\n",
      "The 38504 th iteration gives loss of 0.14588534886839183\n",
      "The 38505 th iteration gives loss of 0.14588393201272873\n",
      "The 38506 th iteration gives loss of 0.14588251519685094\n",
      "The 38507 th iteration gives loss of 0.14588109842072405\n",
      "The 38508 th iteration gives loss of 0.1458796816843642\n",
      "The 38509 th iteration gives loss of 0.14587826498775686\n",
      "The 38510 th iteration gives loss of 0.14587684833089776\n",
      "The 38511 th iteration gives loss of 0.14587543171378925\n",
      "The 38512 th iteration gives loss of 0.14587401513641787\n",
      "The 38513 th iteration gives loss of 0.14587259859878482\n",
      "The 38514 th iteration gives loss of 0.14587118210088604\n",
      "The 38515 th iteration gives loss of 0.14586976564271525\n",
      "The 38516 th iteration gives loss of 0.1458683492242708\n",
      "The 38517 th iteration gives loss of 0.145866932845537\n",
      "The 38518 th iteration gives loss of 0.1458655165065168\n",
      "The 38519 th iteration gives loss of 0.14586410020720966\n",
      "The 38520 th iteration gives loss of 0.14586268394760316\n",
      "The 38521 th iteration gives loss of 0.14586126772770264\n",
      "The 38522 th iteration gives loss of 0.14585985154748807\n",
      "The 38523 th iteration gives loss of 0.1458584354069696\n",
      "The 38524 th iteration gives loss of 0.14585701930613668\n",
      "The 38525 th iteration gives loss of 0.14585560324497832\n",
      "The 38526 th iteration gives loss of 0.1458541872235057\n",
      "The 38527 th iteration gives loss of 0.14585277124169652\n",
      "The 38528 th iteration gives loss of 0.14585135529956492\n",
      "The 38529 th iteration gives loss of 0.14584993939708335\n",
      "The 38530 th iteration gives loss of 0.14584852353427116\n",
      "The 38531 th iteration gives loss of 0.14584710771110396\n",
      "The 38532 th iteration gives loss of 0.1458456919275936\n",
      "The 38533 th iteration gives loss of 0.14584427618372126\n",
      "The 38534 th iteration gives loss of 0.14584286047948433\n",
      "The 38535 th iteration gives loss of 0.14584144481489053\n",
      "The 38536 th iteration gives loss of 0.14584002918991584\n",
      "The 38537 th iteration gives loss of 0.1458386136045811\n",
      "The 38538 th iteration gives loss of 0.1458371980588609\n",
      "The 38539 th iteration gives loss of 0.14583578255275698\n",
      "The 38540 th iteration gives loss of 0.14583436708626019\n",
      "The 38541 th iteration gives loss of 0.1458329516593766\n",
      "The 38542 th iteration gives loss of 0.14583153627209658\n",
      "The 38543 th iteration gives loss of 0.14583012092440595\n",
      "The 38544 th iteration gives loss of 0.14582870561631647\n",
      "The 38545 th iteration gives loss of 0.14582729034780859\n",
      "The 38546 th iteration gives loss of 0.14582587511889034\n",
      "The 38547 th iteration gives loss of 0.14582445992954698\n",
      "The 38548 th iteration gives loss of 0.14582304477978242\n",
      "The 38549 th iteration gives loss of 0.14582162966958936\n",
      "The 38550 th iteration gives loss of 0.1458202145989554\n",
      "The 38551 th iteration gives loss of 0.14581879956788626\n",
      "The 38552 th iteration gives loss of 0.14581738457637697\n",
      "The 38553 th iteration gives loss of 0.14581596962441312\n",
      "The 38554 th iteration gives loss of 0.14581455471199783\n",
      "The 38555 th iteration gives loss of 0.14581313983912914\n",
      "The 38556 th iteration gives loss of 0.1458117250057869\n",
      "The 38557 th iteration gives loss of 0.14581031021199256\n",
      "The 38558 th iteration gives loss of 0.1458088954577219\n",
      "The 38559 th iteration gives loss of 0.14580748074297242\n",
      "The 38560 th iteration gives loss of 0.14580606606774021\n",
      "The 38561 th iteration gives loss of 0.145804651432025\n",
      "The 38562 th iteration gives loss of 0.14580323683582563\n",
      "The 38563 th iteration gives loss of 0.1458018222791228\n",
      "The 38564 th iteration gives loss of 0.14580040776192815\n",
      "The 38565 th iteration gives loss of 0.14579899328422175\n",
      "The 38566 th iteration gives loss of 0.14579757884601385\n",
      "The 38567 th iteration gives loss of 0.14579616444729004\n",
      "The 38568 th iteration gives loss of 0.1457947500880487\n",
      "The 38569 th iteration gives loss of 0.1457933357682831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 38570 th iteration gives loss of 0.14579192148799044\n",
      "The 38571 th iteration gives loss of 0.14579050724717557\n",
      "The 38572 th iteration gives loss of 0.14578909304581608\n",
      "The 38573 th iteration gives loss of 0.14578767888392044\n",
      "The 38574 th iteration gives loss of 0.14578626476147918\n",
      "The 38575 th iteration gives loss of 0.14578485067848707\n",
      "The 38576 th iteration gives loss of 0.14578343663493584\n",
      "The 38577 th iteration gives loss of 0.14578202263083664\n",
      "The 38578 th iteration gives loss of 0.14578060866617104\n",
      "The 38579 th iteration gives loss of 0.14577919474092968\n",
      "The 38580 th iteration gives loss of 0.14577778085512152\n",
      "The 38581 th iteration gives loss of 0.1457763670087346\n",
      "The 38582 th iteration gives loss of 0.14577495320176392\n",
      "The 38583 th iteration gives loss of 0.14577353943420973\n",
      "The 38584 th iteration gives loss of 0.14577212570606204\n",
      "The 38585 th iteration gives loss of 0.14577071201731942\n",
      "The 38586 th iteration gives loss of 0.14576929836797828\n",
      "The 38587 th iteration gives loss of 0.14576788475803232\n",
      "The 38588 th iteration gives loss of 0.14576647118747665\n",
      "The 38589 th iteration gives loss of 0.1457650576563094\n",
      "The 38590 th iteration gives loss of 0.14576364416452176\n",
      "The 38591 th iteration gives loss of 0.145762230712106\n",
      "The 38592 th iteration gives loss of 0.14576081729906246\n",
      "The 38593 th iteration gives loss of 0.145759403925396\n",
      "The 38594 th iteration gives loss of 0.14575799059108804\n",
      "The 38595 th iteration gives loss of 0.14575657729613672\n",
      "The 38596 th iteration gives loss of 0.14575516404053376\n",
      "The 38597 th iteration gives loss of 0.14575375082428804\n",
      "The 38598 th iteration gives loss of 0.1457523376473863\n",
      "The 38599 th iteration gives loss of 0.14575092450982366\n",
      "The 38600 th iteration gives loss of 0.14574951141160009\n",
      "The 38601 th iteration gives loss of 0.1457480983527002\n",
      "The 38602 th iteration gives loss of 0.14574668533313556\n",
      "The 38603 th iteration gives loss of 0.14574527235288653\n",
      "The 38604 th iteration gives loss of 0.14574385941195353\n",
      "The 38605 th iteration gives loss of 0.14574244651033544\n",
      "The 38606 th iteration gives loss of 0.1457410336480235\n",
      "The 38607 th iteration gives loss of 0.14573962082502068\n",
      "The 38608 th iteration gives loss of 0.14573820804131132\n",
      "The 38609 th iteration gives loss of 0.14573679529690137\n",
      "The 38610 th iteration gives loss of 0.14573538259177612\n",
      "The 38611 th iteration gives loss of 0.1457339699259415\n",
      "The 38612 th iteration gives loss of 0.14573255729938844\n",
      "The 38613 th iteration gives loss of 0.1457311447121071\n",
      "The 38614 th iteration gives loss of 0.14572973216409207\n",
      "The 38615 th iteration gives loss of 0.1457283196553476\n",
      "The 38616 th iteration gives loss of 0.14572690718587522\n",
      "The 38617 th iteration gives loss of 0.14572549475565352\n",
      "The 38618 th iteration gives loss of 0.14572408236468443\n",
      "The 38619 th iteration gives loss of 0.14572267001296688\n",
      "The 38620 th iteration gives loss of 0.1457212577004929\n",
      "The 38621 th iteration gives loss of 0.14571984542725339\n",
      "The 38622 th iteration gives loss of 0.1457184331932529\n",
      "The 38623 th iteration gives loss of 0.1457170209984821\n",
      "The 38624 th iteration gives loss of 0.14571560884293483\n",
      "The 38625 th iteration gives loss of 0.145714196726611\n",
      "The 38626 th iteration gives loss of 0.14571278464950976\n",
      "The 38627 th iteration gives loss of 0.14571137261161274\n",
      "The 38628 th iteration gives loss of 0.14570996061292776\n",
      "The 38629 th iteration gives loss of 0.14570854865344263\n",
      "The 38630 th iteration gives loss of 0.14570713673315774\n",
      "The 38631 th iteration gives loss of 0.1457057248520721\n",
      "The 38632 th iteration gives loss of 0.1457043130101595\n",
      "The 38633 th iteration gives loss of 0.1457029012074478\n",
      "The 38634 th iteration gives loss of 0.14570148944391706\n",
      "The 38635 th iteration gives loss of 0.1457000777195511\n",
      "The 38636 th iteration gives loss of 0.14569866603436257\n",
      "The 38637 th iteration gives loss of 0.14569725438833944\n",
      "The 38638 th iteration gives loss of 0.1456958427814783\n",
      "The 38639 th iteration gives loss of 0.14569443121377582\n",
      "The 38640 th iteration gives loss of 0.14569301968522638\n",
      "The 38641 th iteration gives loss of 0.14569160819582763\n",
      "The 38642 th iteration gives loss of 0.14569019674556452\n",
      "The 38643 th iteration gives loss of 0.1456887853344504\n",
      "The 38644 th iteration gives loss of 0.1456873739624646\n",
      "The 38645 th iteration gives loss of 0.1456859626296087\n",
      "The 38646 th iteration gives loss of 0.1456845513358861\n",
      "The 38647 th iteration gives loss of 0.14568314008127647\n",
      "The 38648 th iteration gives loss of 0.14568172886578576\n",
      "The 38649 th iteration gives loss of 0.14568031768941\n",
      "The 38650 th iteration gives loss of 0.14567890655213891\n",
      "The 38651 th iteration gives loss of 0.14567749545397615\n",
      "The 38652 th iteration gives loss of 0.14567608439490687\n",
      "The 38653 th iteration gives loss of 0.145674673374935\n",
      "The 38654 th iteration gives loss of 0.145673262394047\n",
      "The 38655 th iteration gives loss of 0.14567185145224856\n",
      "The 38656 th iteration gives loss of 0.145670440549527\n",
      "The 38657 th iteration gives loss of 0.1456690296858746\n",
      "The 38658 th iteration gives loss of 0.1456676188613016\n",
      "The 38659 th iteration gives loss of 0.14566620807580025\n",
      "The 38660 th iteration gives loss of 0.1456647973293509\n",
      "The 38661 th iteration gives loss of 0.14566338662196632\n",
      "The 38662 th iteration gives loss of 0.14566197595363065\n",
      "The 38663 th iteration gives loss of 0.1456605653243485\n",
      "The 38664 th iteration gives loss of 0.1456591547341111\n",
      "The 38665 th iteration gives loss of 0.14565774418290853\n",
      "The 38666 th iteration gives loss of 0.14565633367073552\n",
      "The 38667 th iteration gives loss of 0.14565492319760337\n",
      "The 38668 th iteration gives loss of 0.14565351276349073\n",
      "The 38669 th iteration gives loss of 0.1456521023684027\n",
      "The 38670 th iteration gives loss of 0.1456506920123306\n",
      "The 38671 th iteration gives loss of 0.14564928169526575\n",
      "The 38672 th iteration gives loss of 0.1456478714172171\n",
      "The 38673 th iteration gives loss of 0.14564646117816513\n",
      "The 38674 th iteration gives loss of 0.14564505097811908\n",
      "The 38675 th iteration gives loss of 0.14564364081706058\n",
      "The 38676 th iteration gives loss of 0.14564223069499918\n",
      "The 38677 th iteration gives loss of 0.145640820611922\n",
      "The 38678 th iteration gives loss of 0.14563941056782093\n",
      "The 38679 th iteration gives loss of 0.14563800056269272\n",
      "The 38680 th iteration gives loss of 0.14563659059654172\n",
      "The 38681 th iteration gives loss of 0.1456351806693583\n",
      "The 38682 th iteration gives loss of 0.14563377078112927\n",
      "The 38683 th iteration gives loss of 0.14563236093186369\n",
      "The 38684 th iteration gives loss of 0.1456309511215554\n",
      "The 38685 th iteration gives loss of 0.1456295413501957\n",
      "The 38686 th iteration gives loss of 0.14562813161778038\n",
      "The 38687 th iteration gives loss of 0.1456267219243073\n",
      "The 38688 th iteration gives loss of 0.14562531226975792\n",
      "The 38689 th iteration gives loss of 0.1456239026541396\n",
      "The 38690 th iteration gives loss of 0.14562249307746022\n",
      "The 38691 th iteration gives loss of 0.14562108353969644\n",
      "The 38692 th iteration gives loss of 0.14561967404085613\n",
      "The 38693 th iteration gives loss of 0.14561826458091764\n",
      "The 38694 th iteration gives loss of 0.14561685515989153\n",
      "The 38695 th iteration gives loss of 0.14561544577777089\n",
      "The 38696 th iteration gives loss of 0.1456140364345438\n",
      "The 38697 th iteration gives loss of 0.1456126271302172\n",
      "The 38698 th iteration gives loss of 0.14561121786478484\n",
      "The 38699 th iteration gives loss of 0.14560980863823167\n",
      "The 38700 th iteration gives loss of 0.14560839945056225\n",
      "The 38701 th iteration gives loss of 0.14560699030177043\n",
      "The 38702 th iteration gives loss of 0.14560558119185302\n",
      "The 38703 th iteration gives loss of 0.14560417212079596\n",
      "The 38704 th iteration gives loss of 0.14560276308861073\n",
      "The 38705 th iteration gives loss of 0.1456013540952726\n",
      "The 38706 th iteration gives loss of 0.1455999451408077\n",
      "The 38707 th iteration gives loss of 0.14559853622517802\n",
      "The 38708 th iteration gives loss of 0.14559712734839608\n",
      "The 38709 th iteration gives loss of 0.14559571851045977\n",
      "The 38710 th iteration gives loss of 0.14559430971134726\n",
      "The 38711 th iteration gives loss of 0.1455929009510804\n",
      "The 38712 th iteration gives loss of 0.14559149222963513\n",
      "The 38713 th iteration gives loss of 0.1455900835470123\n",
      "The 38714 th iteration gives loss of 0.1455886749032127\n",
      "The 38715 th iteration gives loss of 0.14558726629822002\n",
      "The 38716 th iteration gives loss of 0.1455858577320333\n",
      "The 38717 th iteration gives loss of 0.14558444920466132\n",
      "The 38718 th iteration gives loss of 0.14558304071609376\n",
      "The 38719 th iteration gives loss of 0.14558163226629955\n",
      "The 38720 th iteration gives loss of 0.14558022385531713\n",
      "The 38721 th iteration gives loss of 0.14557881548311472\n",
      "The 38722 th iteration gives loss of 0.14557740714969453\n",
      "The 38723 th iteration gives loss of 0.14557599885505543\n",
      "The 38724 th iteration gives loss of 0.14557459059918026\n",
      "The 38725 th iteration gives loss of 0.14557318238208836\n",
      "The 38726 th iteration gives loss of 0.1455717742037502\n",
      "The 38727 th iteration gives loss of 0.14557036606417573\n",
      "The 38728 th iteration gives loss of 0.14556895796335462\n",
      "The 38729 th iteration gives loss of 0.14556754990129228\n",
      "The 38730 th iteration gives loss of 0.14556614187796624\n",
      "The 38731 th iteration gives loss of 0.14556473389338653\n",
      "The 38732 th iteration gives loss of 0.1455633259475477\n",
      "The 38733 th iteration gives loss of 0.1455619180404345\n",
      "The 38734 th iteration gives loss of 0.14556051017205193\n",
      "The 38735 th iteration gives loss of 0.14555910234239125\n",
      "The 38736 th iteration gives loss of 0.14555769455145381\n",
      "The 38737 th iteration gives loss of 0.14555628679923205\n",
      "The 38738 th iteration gives loss of 0.14555487908571849\n",
      "The 38739 th iteration gives loss of 0.14555347141091282\n",
      "The 38740 th iteration gives loss of 0.14555206377480462\n",
      "The 38741 th iteration gives loss of 0.14555065617739846\n",
      "The 38742 th iteration gives loss of 0.1455492486186836\n",
      "The 38743 th iteration gives loss of 0.14554784109866034\n",
      "The 38744 th iteration gives loss of 0.14554643361732095\n",
      "The 38745 th iteration gives loss of 0.1455450261746586\n",
      "The 38746 th iteration gives loss of 0.14554361877067565\n",
      "The 38747 th iteration gives loss of 0.14554221140535392\n",
      "The 38748 th iteration gives loss of 0.14554080407869568\n",
      "The 38749 th iteration gives loss of 0.14553939679070288\n",
      "The 38750 th iteration gives loss of 0.14553798954136857\n",
      "The 38751 th iteration gives loss of 0.145536582330682\n",
      "The 38752 th iteration gives loss of 0.14553517515864686\n",
      "The 38753 th iteration gives loss of 0.14553376802525708\n",
      "The 38754 th iteration gives loss of 0.1455323609304997\n",
      "The 38755 th iteration gives loss of 0.14553095387439025\n",
      "The 38756 th iteration gives loss of 0.14552954685690322\n",
      "The 38757 th iteration gives loss of 0.14552813987804264\n",
      "The 38758 th iteration gives loss of 0.1455267329378006\n",
      "The 38759 th iteration gives loss of 0.1455253260361756\n",
      "The 38760 th iteration gives loss of 0.1455239191731654\n",
      "The 38761 th iteration gives loss of 0.1455225123487618\n",
      "The 38762 th iteration gives loss of 0.14552110556295877\n",
      "The 38763 th iteration gives loss of 0.14551969881575869\n",
      "The 38764 th iteration gives loss of 0.1455182921071531\n",
      "The 38765 th iteration gives loss of 0.14551688543713207\n",
      "The 38766 th iteration gives loss of 0.14551547880570354\n",
      "The 38767 th iteration gives loss of 0.14551407221284932\n",
      "The 38768 th iteration gives loss of 0.14551266565857657\n",
      "The 38769 th iteration gives loss of 0.14551125914287447\n",
      "The 38770 th iteration gives loss of 0.14550985266574162\n",
      "The 38771 th iteration gives loss of 0.1455084462271663\n",
      "The 38772 th iteration gives loss of 0.14550703982715263\n",
      "The 38773 th iteration gives loss of 0.14550563346569606\n",
      "The 38774 th iteration gives loss of 0.1455042271427893\n",
      "The 38775 th iteration gives loss of 0.14550282085842603\n",
      "The 38776 th iteration gives loss of 0.14550141461259625\n",
      "The 38777 th iteration gives loss of 0.14550000840530722\n",
      "The 38778 th iteration gives loss of 0.1454986022365557\n",
      "The 38779 th iteration gives loss of 0.14549719610633058\n",
      "The 38780 th iteration gives loss of 0.14549579001462964\n",
      "The 38781 th iteration gives loss of 0.14549438396143838\n",
      "The 38782 th iteration gives loss of 0.14549297794676383\n",
      "The 38783 th iteration gives loss of 0.1454915719705993\n",
      "The 38784 th iteration gives loss of 0.14549016603294496\n",
      "The 38785 th iteration gives loss of 0.145488760133789\n",
      "The 38786 th iteration gives loss of 0.14548735427313136\n",
      "The 38787 th iteration gives loss of 0.14548594845096668\n",
      "The 38788 th iteration gives loss of 0.14548454266728508\n",
      "The 38789 th iteration gives loss of 0.14548313692209014\n",
      "The 38790 th iteration gives loss of 0.14548173121537097\n",
      "The 38791 th iteration gives loss of 0.14548032554712467\n",
      "The 38792 th iteration gives loss of 0.14547891991735212\n",
      "The 38793 th iteration gives loss of 0.14547751432603942\n",
      "The 38794 th iteration gives loss of 0.1454761087731899\n",
      "The 38795 th iteration gives loss of 0.14547470325879644\n",
      "The 38796 th iteration gives loss of 0.14547329778286514\n",
      "The 38797 th iteration gives loss of 0.14547189234537197\n",
      "The 38798 th iteration gives loss of 0.14547048694631873\n",
      "The 38799 th iteration gives loss of 0.1454690815857046\n",
      "The 38800 th iteration gives loss of 0.14546767626353238\n",
      "The 38801 th iteration gives loss of 0.14546627097978376\n",
      "The 38802 th iteration gives loss of 0.14546486573446246\n",
      "The 38803 th iteration gives loss of 0.1454634605275582\n",
      "The 38804 th iteration gives loss of 0.1454620553590729\n",
      "The 38805 th iteration gives loss of 0.14546065022900517\n",
      "The 38806 th iteration gives loss of 0.1454592451373381\n",
      "The 38807 th iteration gives loss of 0.1454578400840761\n",
      "The 38808 th iteration gives loss of 0.14545643506921258\n",
      "The 38809 th iteration gives loss of 0.14545503009274663\n",
      "The 38810 th iteration gives loss of 0.1454536251546655\n",
      "The 38811 th iteration gives loss of 0.14545222025497948\n",
      "The 38812 th iteration gives loss of 0.14545081539366747\n",
      "The 38813 th iteration gives loss of 0.14544941057073027\n",
      "The 38814 th iteration gives loss of 0.14544800578616088\n",
      "The 38815 th iteration gives loss of 0.14544660103997115\n",
      "The 38816 th iteration gives loss of 0.14544519633213976\n",
      "The 38817 th iteration gives loss of 0.14544379166266555\n",
      "The 38818 th iteration gives loss of 0.14544238703154588\n",
      "The 38819 th iteration gives loss of 0.1454409824387825\n",
      "The 38820 th iteration gives loss of 0.14543957788435555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 38821 th iteration gives loss of 0.14543817336827583\n",
      "The 38822 th iteration gives loss of 0.14543676889052634\n",
      "The 38823 th iteration gives loss of 0.14543536445111627\n",
      "The 38824 th iteration gives loss of 0.14543396005003073\n",
      "The 38825 th iteration gives loss of 0.14543255568727054\n",
      "The 38826 th iteration gives loss of 0.14543115136282977\n",
      "The 38827 th iteration gives loss of 0.14542974707669648\n",
      "The 38828 th iteration gives loss of 0.14542834282888056\n",
      "The 38829 th iteration gives loss of 0.1454269386193722\n",
      "The 38830 th iteration gives loss of 0.14542553444815617\n",
      "The 38831 th iteration gives loss of 0.14542413031524445\n",
      "The 38832 th iteration gives loss of 0.1454227262206243\n",
      "The 38833 th iteration gives loss of 0.14542132216429263\n",
      "The 38834 th iteration gives loss of 0.14541991814624153\n",
      "The 38835 th iteration gives loss of 0.14541851416647922\n",
      "The 38836 th iteration gives loss of 0.1454171102249867\n",
      "The 38837 th iteration gives loss of 0.14541570632175993\n",
      "The 38838 th iteration gives loss of 0.14541430245680192\n",
      "The 38839 th iteration gives loss of 0.14541289863010937\n",
      "The 38840 th iteration gives loss of 0.14541149484167304\n",
      "The 38841 th iteration gives loss of 0.1454100910914838\n",
      "The 38842 th iteration gives loss of 0.1454086873795496\n",
      "The 38843 th iteration gives loss of 0.1454072837058559\n",
      "The 38844 th iteration gives loss of 0.145405880070404\n",
      "The 38845 th iteration gives loss of 0.14540447647318835\n",
      "The 38846 th iteration gives loss of 0.1454030729142054\n",
      "The 38847 th iteration gives loss of 0.14540166939344293\n",
      "The 38848 th iteration gives loss of 0.14540026591090566\n",
      "The 38849 th iteration gives loss of 0.1453988624665831\n",
      "The 38850 th iteration gives loss of 0.14539745906047769\n",
      "The 38851 th iteration gives loss of 0.14539605569258107\n",
      "The 38852 th iteration gives loss of 0.1453946523628856\n",
      "The 38853 th iteration gives loss of 0.14539324907139012\n",
      "The 38854 th iteration gives loss of 0.14539184581809825\n",
      "The 38855 th iteration gives loss of 0.14539044260298667\n",
      "The 38856 th iteration gives loss of 0.14538903942606754\n",
      "The 38857 th iteration gives loss of 0.14538763628732923\n",
      "The 38858 th iteration gives loss of 0.14538623318676788\n",
      "The 38859 th iteration gives loss of 0.1453848301243912\n",
      "The 38860 th iteration gives loss of 0.14538342710016855\n",
      "The 38861 th iteration gives loss of 0.14538202411412596\n",
      "The 38862 th iteration gives loss of 0.14538062116623476\n",
      "The 38863 th iteration gives loss of 0.1453792182565012\n",
      "The 38864 th iteration gives loss of 0.1453778153849128\n",
      "The 38865 th iteration gives loss of 0.1453764125514772\n",
      "The 38866 th iteration gives loss of 0.14537500975618622\n",
      "The 38867 th iteration gives loss of 0.1453736069990363\n",
      "The 38868 th iteration gives loss of 0.14537220428002037\n",
      "The 38869 th iteration gives loss of 0.1453708015991331\n",
      "The 38870 th iteration gives loss of 0.14536939895636639\n",
      "The 38871 th iteration gives loss of 0.14536799635172443\n",
      "The 38872 th iteration gives loss of 0.14536659378519257\n",
      "The 38873 th iteration gives loss of 0.14536519125677877\n",
      "The 38874 th iteration gives loss of 0.1453637887664694\n",
      "The 38875 th iteration gives loss of 0.1453623863142638\n",
      "The 38876 th iteration gives loss of 0.1453609839001659\n",
      "The 38877 th iteration gives loss of 0.1453595815241578\n",
      "The 38878 th iteration gives loss of 0.14535817918623894\n",
      "The 38879 th iteration gives loss of 0.14535677688640553\n",
      "The 38880 th iteration gives loss of 0.1453553746246571\n",
      "The 38881 th iteration gives loss of 0.14535397240098158\n",
      "The 38882 th iteration gives loss of 0.14535257021538392\n",
      "The 38883 th iteration gives loss of 0.14535116806785092\n",
      "The 38884 th iteration gives loss of 0.14534976595837829\n",
      "The 38885 th iteration gives loss of 0.14534836388697212\n",
      "The 38886 th iteration gives loss of 0.14534696185362275\n",
      "The 38887 th iteration gives loss of 0.14534555985832143\n",
      "The 38888 th iteration gives loss of 0.14534415790106694\n",
      "The 38889 th iteration gives loss of 0.1453427559818494\n",
      "The 38890 th iteration gives loss of 0.145341354100674\n",
      "The 38891 th iteration gives loss of 0.1453399522575338\n",
      "The 38892 th iteration gives loss of 0.14533855045242175\n",
      "The 38893 th iteration gives loss of 0.14533714868533226\n",
      "The 38894 th iteration gives loss of 0.14533574695626203\n",
      "The 38895 th iteration gives loss of 0.14533434526521546\n",
      "The 38896 th iteration gives loss of 0.14533294361217522\n",
      "The 38897 th iteration gives loss of 0.14533154199714124\n",
      "The 38898 th iteration gives loss of 0.14533014042010897\n",
      "The 38899 th iteration gives loss of 0.1453287388810821\n",
      "The 38900 th iteration gives loss of 0.14532733738004633\n",
      "The 38901 th iteration gives loss of 0.14532593591699816\n",
      "The 38902 th iteration gives loss of 0.14532453449193286\n",
      "The 38903 th iteration gives loss of 0.14532313310484837\n",
      "The 38904 th iteration gives loss of 0.14532173175574326\n",
      "The 38905 th iteration gives loss of 0.14532033044461126\n",
      "The 38906 th iteration gives loss of 0.1453189291714458\n",
      "The 38907 th iteration gives loss of 0.14531752793625058\n",
      "The 38908 th iteration gives loss of 0.14531612673900474\n",
      "The 38909 th iteration gives loss of 0.1453147255797191\n",
      "The 38910 th iteration gives loss of 0.14531332445837694\n",
      "The 38911 th iteration gives loss of 0.14531192337498727\n",
      "The 38912 th iteration gives loss of 0.14531052232954203\n",
      "The 38913 th iteration gives loss of 0.14530912132203155\n",
      "The 38914 th iteration gives loss of 0.14530772035245573\n",
      "The 38915 th iteration gives loss of 0.14530631942080674\n",
      "The 38916 th iteration gives loss of 0.14530491852708974\n",
      "The 38917 th iteration gives loss of 0.14530351767128102\n",
      "The 38918 th iteration gives loss of 0.1453021168533925\n",
      "The 38919 th iteration gives loss of 0.145300716073416\n",
      "The 38920 th iteration gives loss of 0.14529931533135015\n",
      "The 38921 th iteration gives loss of 0.14529791462717967\n",
      "The 38922 th iteration gives loss of 0.14529651396090945\n",
      "The 38923 th iteration gives loss of 0.14529511333253317\n",
      "The 38924 th iteration gives loss of 0.14529371274204705\n",
      "The 38925 th iteration gives loss of 0.14529231218944713\n",
      "The 38926 th iteration gives loss of 0.14529091167472877\n",
      "The 38927 th iteration gives loss of 0.14528951119788944\n",
      "The 38928 th iteration gives loss of 0.14528811075891543\n",
      "The 38929 th iteration gives loss of 0.14528671035781948\n",
      "The 38930 th iteration gives loss of 0.1452853099945738\n",
      "The 38931 th iteration gives loss of 0.14528390966919308\n",
      "The 38932 th iteration gives loss of 0.14528250938166848\n",
      "The 38933 th iteration gives loss of 0.14528110913199344\n",
      "The 38934 th iteration gives loss of 0.14527970892016667\n",
      "The 38935 th iteration gives loss of 0.14527830874617914\n",
      "The 38936 th iteration gives loss of 0.14527690861002243\n",
      "The 38937 th iteration gives loss of 0.14527550851170778\n",
      "The 38938 th iteration gives loss of 0.14527410845122357\n",
      "The 38939 th iteration gives loss of 0.14527270842856158\n",
      "The 38940 th iteration gives loss of 0.1452713084437148\n",
      "The 38941 th iteration gives loss of 0.14526990849668844\n",
      "The 38942 th iteration gives loss of 0.14526850858747126\n",
      "The 38943 th iteration gives loss of 0.1452671087160629\n",
      "The 38944 th iteration gives loss of 0.14526570888245402\n",
      "The 38945 th iteration gives loss of 0.1452643090866445\n",
      "The 38946 th iteration gives loss of 0.14526290932863054\n",
      "The 38947 th iteration gives loss of 0.1452615096084048\n",
      "The 38948 th iteration gives loss of 0.1452601099259644\n",
      "The 38949 th iteration gives loss of 0.14525871028130416\n",
      "The 38950 th iteration gives loss of 0.1452573106744308\n",
      "The 38951 th iteration gives loss of 0.145255911105327\n",
      "The 38952 th iteration gives loss of 0.14525451157398211\n",
      "The 38953 th iteration gives loss of 0.14525311208040118\n",
      "The 38954 th iteration gives loss of 0.14525171262458778\n",
      "The 38955 th iteration gives loss of 0.14525031320652473\n",
      "The 38956 th iteration gives loss of 0.14524891382621674\n",
      "The 38957 th iteration gives loss of 0.14524751448364717\n",
      "The 38958 th iteration gives loss of 0.1452461151788233\n",
      "The 38959 th iteration gives loss of 0.14524471591173957\n",
      "The 38960 th iteration gives loss of 0.14524331668238613\n",
      "The 38961 th iteration gives loss of 0.1452419174907635\n",
      "The 38962 th iteration gives loss of 0.14524051833686683\n",
      "The 38963 th iteration gives loss of 0.14523911922069213\n",
      "The 38964 th iteration gives loss of 0.14523772014223246\n",
      "The 38965 th iteration gives loss of 0.1452363211014817\n",
      "The 38966 th iteration gives loss of 0.1452349220984401\n",
      "The 38967 th iteration gives loss of 0.14523352313310203\n",
      "The 38968 th iteration gives loss of 0.14523212420546258\n",
      "The 38969 th iteration gives loss of 0.14523072531551778\n",
      "The 38970 th iteration gives loss of 0.14522932646326447\n",
      "The 38971 th iteration gives loss of 0.14522792764869324\n",
      "The 38972 th iteration gives loss of 0.14522652887180246\n",
      "The 38973 th iteration gives loss of 0.1452251301325956\n",
      "The 38974 th iteration gives loss of 0.14522373143105718\n",
      "The 38975 th iteration gives loss of 0.14522233276718557\n",
      "The 38976 th iteration gives loss of 0.1452209341409853\n",
      "The 38977 th iteration gives loss of 0.14521953555244416\n",
      "The 38978 th iteration gives loss of 0.1452181370015606\n",
      "The 38979 th iteration gives loss of 0.14521673848832176\n",
      "The 38980 th iteration gives loss of 0.14521534001273215\n",
      "The 38981 th iteration gives loss of 0.14521394157478199\n",
      "The 38982 th iteration gives loss of 0.1452125431744719\n",
      "The 38983 th iteration gives loss of 0.14521114481179828\n",
      "The 38984 th iteration gives loss of 0.1452097464867537\n",
      "The 38985 th iteration gives loss of 0.14520834819933617\n",
      "The 38986 th iteration gives loss of 0.14520694994953467\n",
      "The 38987 th iteration gives loss of 0.1452055517373563\n",
      "The 38988 th iteration gives loss of 0.1452041535627914\n",
      "The 38989 th iteration gives loss of 0.14520275542582473\n",
      "The 38990 th iteration gives loss of 0.14520135732647663\n",
      "The 38991 th iteration gives loss of 0.14519995926472276\n",
      "The 38992 th iteration gives loss of 0.1451985612405599\n",
      "The 38993 th iteration gives loss of 0.14519716325399834\n",
      "The 38994 th iteration gives loss of 0.14519576530501063\n",
      "The 38995 th iteration gives loss of 0.1451943673936143\n",
      "The 38996 th iteration gives loss of 0.1451929695197899\n",
      "The 38997 th iteration gives loss of 0.1451915716835444\n",
      "The 38998 th iteration gives loss of 0.14519017388486388\n",
      "The 38999 th iteration gives loss of 0.1451887761237538\n",
      "The 39000 th iteration gives loss of 0.14518737840020762\n",
      "The 39001 th iteration gives loss of 0.1451859807142081\n",
      "The 39002 th iteration gives loss of 0.14518458306576418\n",
      "The 39003 th iteration gives loss of 0.14518318545487036\n",
      "The 39004 th iteration gives loss of 0.1451817878815217\n",
      "The 39005 th iteration gives loss of 0.14518039034571387\n",
      "The 39006 th iteration gives loss of 0.14517899284744057\n",
      "The 39007 th iteration gives loss of 0.14517759538669106\n",
      "The 39008 th iteration gives loss of 0.1451761979634699\n",
      "The 39009 th iteration gives loss of 0.14517480057777074\n",
      "The 39010 th iteration gives loss of 0.14517340322959643\n",
      "The 39011 th iteration gives loss of 0.14517200591893822\n",
      "The 39012 th iteration gives loss of 0.14517060864578704\n",
      "The 39013 th iteration gives loss of 0.14516921141013794\n",
      "The 39014 th iteration gives loss of 0.1451678142119925\n",
      "The 39015 th iteration gives loss of 0.14516641705133393\n",
      "The 39016 th iteration gives loss of 0.14516501992818096\n",
      "The 39017 th iteration gives loss of 0.14516362284251605\n",
      "The 39018 th iteration gives loss of 0.14516222579432464\n",
      "The 39019 th iteration gives loss of 0.1451608287836163\n",
      "The 39020 th iteration gives loss of 0.14515943181038563\n",
      "The 39021 th iteration gives loss of 0.14515803487462006\n",
      "The 39022 th iteration gives loss of 0.14515663797632986\n",
      "The 39023 th iteration gives loss of 0.1451552411154988\n",
      "The 39024 th iteration gives loss of 0.1451538442921266\n",
      "The 39025 th iteration gives loss of 0.14515244750620934\n",
      "The 39026 th iteration gives loss of 0.14515105075774187\n",
      "The 39027 th iteration gives loss of 0.14514965404671196\n",
      "The 39028 th iteration gives loss of 0.1451482573731232\n",
      "The 39029 th iteration gives loss of 0.14514686073697622\n",
      "The 39030 th iteration gives loss of 0.14514546413826057\n",
      "The 39031 th iteration gives loss of 0.14514406757697437\n",
      "The 39032 th iteration gives loss of 0.1451426710531235\n",
      "The 39033 th iteration gives loss of 0.14514127456667716\n",
      "The 39034 th iteration gives loss of 0.14513987811764584\n",
      "The 39035 th iteration gives loss of 0.1451384817060316\n",
      "The 39036 th iteration gives loss of 0.14513708533182254\n",
      "The 39037 th iteration gives loss of 0.14513568899501808\n",
      "The 39038 th iteration gives loss of 0.14513429269560627\n",
      "The 39039 th iteration gives loss of 0.1451328964335876\n",
      "The 39040 th iteration gives loss of 0.14513150020896198\n",
      "The 39041 th iteration gives loss of 0.14513010402173032\n",
      "The 39042 th iteration gives loss of 0.1451287078718646\n",
      "The 39043 th iteration gives loss of 0.14512731175938895\n",
      "The 39044 th iteration gives loss of 0.14512591568427624\n",
      "The 39045 th iteration gives loss of 0.14512451964653914\n",
      "The 39046 th iteration gives loss of 0.14512312364616164\n",
      "The 39047 th iteration gives loss of 0.1451217276831407\n",
      "The 39048 th iteration gives loss of 0.1451203317574812\n",
      "The 39049 th iteration gives loss of 0.1451189358691758\n",
      "The 39050 th iteration gives loss of 0.14511754001821967\n",
      "The 39051 th iteration gives loss of 0.14511614420458993\n",
      "The 39052 th iteration gives loss of 0.14511474842830932\n",
      "The 39053 th iteration gives loss of 0.14511335268935907\n",
      "The 39054 th iteration gives loss of 0.14511195698773982\n",
      "The 39055 th iteration gives loss of 0.14511056132344458\n",
      "The 39056 th iteration gives loss of 0.14510916569647922\n",
      "The 39057 th iteration gives loss of 0.14510777010682602\n",
      "The 39058 th iteration gives loss of 0.14510637455449396\n",
      "The 39059 th iteration gives loss of 0.1451049790394616\n",
      "The 39060 th iteration gives loss of 0.14510358356173556\n",
      "The 39061 th iteration gives loss of 0.1451021881213075\n",
      "The 39062 th iteration gives loss of 0.14510079271818108\n",
      "The 39063 th iteration gives loss of 0.14509939735234254\n",
      "The 39064 th iteration gives loss of 0.14509800202379042\n",
      "The 39065 th iteration gives loss of 0.14509660673252348\n",
      "The 39066 th iteration gives loss of 0.14509521147853519\n",
      "The 39067 th iteration gives loss of 0.1450938162618184\n",
      "The 39068 th iteration gives loss of 0.14509242108237333\n",
      "The 39069 th iteration gives loss of 0.1450910259402025\n",
      "The 39070 th iteration gives loss of 0.14508963083528922\n",
      "The 39071 th iteration gives loss of 0.14508823576763\n",
      "The 39072 th iteration gives loss of 0.1450868407372278\n",
      "The 39073 th iteration gives loss of 0.14508544574407034\n",
      "The 39074 th iteration gives loss of 0.1450840507881599\n",
      "The 39075 th iteration gives loss of 0.14508265586949054\n",
      "The 39076 th iteration gives loss of 0.14508126098805857\n",
      "The 39077 th iteration gives loss of 0.14507986614385152\n",
      "The 39078 th iteration gives loss of 0.1450784713368814\n",
      "The 39079 th iteration gives loss of 0.14507707656713986\n",
      "The 39080 th iteration gives loss of 0.14507568183460662\n",
      "The 39081 th iteration gives loss of 0.14507428713929368\n",
      "The 39082 th iteration gives loss of 0.145072892481188\n",
      "The 39083 th iteration gives loss of 0.14507149786029425\n",
      "The 39084 th iteration gives loss of 0.14507010327660008\n",
      "The 39085 th iteration gives loss of 0.14506870873010258\n",
      "The 39086 th iteration gives loss of 0.14506731422080468\n",
      "The 39087 th iteration gives loss of 0.14506591974869307\n",
      "The 39088 th iteration gives loss of 0.14506452531376302\n",
      "The 39089 th iteration gives loss of 0.14506313091601808\n",
      "The 39090 th iteration gives loss of 0.14506173655544694\n",
      "The 39091 th iteration gives loss of 0.14506034223205735\n",
      "The 39092 th iteration gives loss of 0.14505894794583232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 39093 th iteration gives loss of 0.14505755369676818\n",
      "The 39094 th iteration gives loss of 0.14505615948486794\n",
      "The 39095 th iteration gives loss of 0.14505476531012532\n",
      "The 39096 th iteration gives loss of 0.14505337117252473\n",
      "The 39097 th iteration gives loss of 0.14505197707208092\n",
      "The 39098 th iteration gives loss of 0.14505058300877008\n",
      "The 39099 th iteration gives loss of 0.14504918898261474\n",
      "The 39100 th iteration gives loss of 0.14504779499358061\n",
      "The 39101 th iteration gives loss of 0.1450464010416823\n",
      "The 39102 th iteration gives loss of 0.14504500712691348\n",
      "The 39103 th iteration gives loss of 0.14504361324926635\n",
      "The 39104 th iteration gives loss of 0.1450422194087288\n",
      "The 39105 th iteration gives loss of 0.14504082560530743\n",
      "The 39106 th iteration gives loss of 0.1450394318390004\n",
      "The 39107 th iteration gives loss of 0.14503803810979532\n",
      "The 39108 th iteration gives loss of 0.1450366444176926\n",
      "The 39109 th iteration gives loss of 0.14503525076268547\n",
      "The 39110 th iteration gives loss of 0.14503385714477438\n",
      "The 39111 th iteration gives loss of 0.14503246356394509\n",
      "The 39112 th iteration gives loss of 0.14503107002020996\n",
      "The 39113 th iteration gives loss of 0.14502967651354376\n",
      "The 39114 th iteration gives loss of 0.145028283043958\n",
      "The 39115 th iteration gives loss of 0.14502688961144491\n",
      "The 39116 th iteration gives loss of 0.1450254962159957\n",
      "The 39117 th iteration gives loss of 0.14502410285761383\n",
      "The 39118 th iteration gives loss of 0.14502270953628668\n",
      "The 39119 th iteration gives loss of 0.1450213162520168\n",
      "The 39120 th iteration gives loss of 0.14501992300479782\n",
      "The 39121 th iteration gives loss of 0.14501852979461669\n",
      "The 39122 th iteration gives loss of 0.14501713662149046\n",
      "The 39123 th iteration gives loss of 0.1450157434853945\n",
      "The 39124 th iteration gives loss of 0.14501435038633503\n",
      "The 39125 th iteration gives loss of 0.14501295732429464\n",
      "The 39126 th iteration gives loss of 0.14501156429928874\n",
      "The 39127 th iteration gives loss of 0.1450101713113065\n",
      "The 39128 th iteration gives loss of 0.14500877836032985\n",
      "The 39129 th iteration gives loss of 0.14500738544637737\n",
      "The 39130 th iteration gives loss of 0.14500599256942773\n",
      "The 39131 th iteration gives loss of 0.1450045997294869\n",
      "The 39132 th iteration gives loss of 0.1450032069265382\n",
      "The 39133 th iteration gives loss of 0.14500181416058422\n",
      "The 39134 th iteration gives loss of 0.1450004214316227\n",
      "The 39135 th iteration gives loss of 0.14499902873965562\n",
      "The 39136 th iteration gives loss of 0.1449976360846667\n",
      "The 39137 th iteration gives loss of 0.1449962434666583\n",
      "The 39138 th iteration gives loss of 0.1449948508856259\n",
      "The 39139 th iteration gives loss of 0.14499345834156616\n",
      "The 39140 th iteration gives loss of 0.14499206583445884\n",
      "The 39141 th iteration gives loss of 0.14499067336432228\n",
      "The 39142 th iteration gives loss of 0.1449892809311498\n",
      "The 39143 th iteration gives loss of 0.14498788853491879\n",
      "The 39144 th iteration gives loss of 0.1449864961756448\n",
      "The 39145 th iteration gives loss of 0.1449851038533106\n",
      "The 39146 th iteration gives loss of 0.1449837115679207\n",
      "The 39147 th iteration gives loss of 0.14498231931947153\n",
      "The 39148 th iteration gives loss of 0.1449809271079514\n",
      "The 39149 th iteration gives loss of 0.14497953493335822\n",
      "The 39150 th iteration gives loss of 0.14497814279568788\n",
      "The 39151 th iteration gives loss of 0.1449767506949372\n",
      "The 39152 th iteration gives loss of 0.1449753586311083\n",
      "The 39153 th iteration gives loss of 0.14497396660418557\n",
      "The 39154 th iteration gives loss of 0.14497257461417934\n",
      "The 39155 th iteration gives loss of 0.1449711826610689\n",
      "The 39156 th iteration gives loss of 0.14496979074486246\n",
      "The 39157 th iteration gives loss of 0.14496839886553975\n",
      "The 39158 th iteration gives loss of 0.14496700702312337\n",
      "The 39159 th iteration gives loss of 0.14496561521757906\n",
      "The 39160 th iteration gives loss of 0.14496422344891988\n",
      "The 39161 th iteration gives loss of 0.14496283171714242\n",
      "The 39162 th iteration gives loss of 0.1449614400222401\n",
      "The 39163 th iteration gives loss of 0.14496004836420387\n",
      "The 39164 th iteration gives loss of 0.1449586567430296\n",
      "The 39165 th iteration gives loss of 0.14495726515872592\n",
      "The 39166 th iteration gives loss of 0.14495587361127485\n",
      "The 39167 th iteration gives loss of 0.14495448210067324\n",
      "The 39168 th iteration gives loss of 0.1449530906269229\n",
      "The 39169 th iteration gives loss of 0.14495169919001993\n",
      "The 39170 th iteration gives loss of 0.14495030778995405\n",
      "The 39171 th iteration gives loss of 0.1449489164267286\n",
      "The 39172 th iteration gives loss of 0.14494752510033218\n",
      "The 39173 th iteration gives loss of 0.14494613381076515\n",
      "The 39174 th iteration gives loss of 0.14494474255801645\n",
      "The 39175 th iteration gives loss of 0.14494335134209763\n",
      "The 39176 th iteration gives loss of 0.14494196016298344\n",
      "The 39177 th iteration gives loss of 0.144940569020692\n",
      "The 39178 th iteration gives loss of 0.14493917791519714\n",
      "The 39179 th iteration gives loss of 0.14493778684651346\n",
      "The 39180 th iteration gives loss of 0.14493639581462028\n",
      "The 39181 th iteration gives loss of 0.14493500481952004\n",
      "The 39182 th iteration gives loss of 0.14493361386121612\n",
      "The 39183 th iteration gives loss of 0.1449322229396956\n",
      "The 39184 th iteration gives loss of 0.14493083205495744\n",
      "The 39185 th iteration gives loss of 0.1449294412070002\n",
      "The 39186 th iteration gives loss of 0.14492805039581164\n",
      "The 39187 th iteration gives loss of 0.14492665962140083\n",
      "The 39188 th iteration gives loss of 0.14492526888374663\n",
      "The 39189 th iteration gives loss of 0.1449238781828517\n",
      "The 39190 th iteration gives loss of 0.14492248751871947\n",
      "The 39191 th iteration gives loss of 0.14492109689133942\n",
      "The 39192 th iteration gives loss of 0.14491970630071233\n",
      "The 39193 th iteration gives loss of 0.14491831574682826\n",
      "The 39194 th iteration gives loss of 0.1449169252296789\n",
      "The 39195 th iteration gives loss of 0.1449155347492637\n",
      "The 39196 th iteration gives loss of 0.1449141443055911\n",
      "The 39197 th iteration gives loss of 0.14491275389863184\n",
      "The 39198 th iteration gives loss of 0.14491136352840409\n",
      "The 39199 th iteration gives loss of 0.14490997319489457\n",
      "The 39200 th iteration gives loss of 0.14490858289809666\n",
      "The 39201 th iteration gives loss of 0.1449071926380209\n",
      "The 39202 th iteration gives loss of 0.144905802414647\n",
      "The 39203 th iteration gives loss of 0.14490441222796832\n",
      "The 39204 th iteration gives loss of 0.144903022077994\n",
      "The 39205 th iteration gives loss of 0.14490163196471337\n",
      "The 39206 th iteration gives loss of 0.14490024188812392\n",
      "The 39207 th iteration gives loss of 0.14489885184822188\n",
      "The 39208 th iteration gives loss of 0.14489746184500005\n",
      "The 39209 th iteration gives loss of 0.14489607187845888\n",
      "The 39210 th iteration gives loss of 0.14489468194858932\n",
      "The 39211 th iteration gives loss of 0.14489329205538842\n",
      "The 39212 th iteration gives loss of 0.14489190219885065\n",
      "The 39213 th iteration gives loss of 0.14489051237897685\n",
      "The 39214 th iteration gives loss of 0.14488912259575898\n",
      "The 39215 th iteration gives loss of 0.1448877328491915\n",
      "The 39216 th iteration gives loss of 0.14488634313927784\n",
      "The 39217 th iteration gives loss of 0.1448849534660075\n",
      "The 39218 th iteration gives loss of 0.1448835638293837\n",
      "The 39219 th iteration gives loss of 0.1448821742293872\n",
      "The 39220 th iteration gives loss of 0.14488078466602777\n",
      "The 39221 th iteration gives loss of 0.14487939513928846\n",
      "The 39222 th iteration gives loss of 0.14487800564918374\n",
      "The 39223 th iteration gives loss of 0.1448766161956974\n",
      "The 39224 th iteration gives loss of 0.14487522677881842\n",
      "The 39225 th iteration gives loss of 0.14487383739855522\n",
      "The 39226 th iteration gives loss of 0.14487244805489774\n",
      "The 39227 th iteration gives loss of 0.14487105874784537\n",
      "The 39228 th iteration gives loss of 0.14486966947738852\n",
      "The 39229 th iteration gives loss of 0.1448682802435297\n",
      "The 39230 th iteration gives loss of 0.1448668910462569\n",
      "The 39231 th iteration gives loss of 0.1448655018855814\n",
      "The 39232 th iteration gives loss of 0.14486411276148356\n",
      "The 39233 th iteration gives loss of 0.14486272367395445\n",
      "The 39234 th iteration gives loss of 0.14486133462301137\n",
      "The 39235 th iteration gives loss of 0.14485994560863152\n",
      "The 39236 th iteration gives loss of 0.1448585566308225\n",
      "The 39237 th iteration gives loss of 0.14485716768957269\n",
      "The 39238 th iteration gives loss of 0.14485577878488043\n",
      "The 39239 th iteration gives loss of 0.14485438991674932\n",
      "The 39240 th iteration gives loss of 0.1448530010851518\n",
      "The 39241 th iteration gives loss of 0.14485161229010848\n",
      "The 39242 th iteration gives loss of 0.14485022353161342\n",
      "The 39243 th iteration gives loss of 0.14484883480964525\n",
      "The 39244 th iteration gives loss of 0.14484744612420813\n",
      "The 39245 th iteration gives loss of 0.14484605747530174\n",
      "The 39246 th iteration gives loss of 0.14484466886291805\n",
      "The 39247 th iteration gives loss of 0.14484328028706056\n",
      "The 39248 th iteration gives loss of 0.144841891747719\n",
      "The 39249 th iteration gives loss of 0.1448405032448833\n",
      "The 39250 th iteration gives loss of 0.14483911477855746\n",
      "The 39251 th iteration gives loss of 0.14483772634873876\n",
      "The 39252 th iteration gives loss of 0.1448363379554176\n",
      "The 39253 th iteration gives loss of 0.14483494959859097\n",
      "The 39254 th iteration gives loss of 0.14483356127825836\n",
      "The 39255 th iteration gives loss of 0.14483217299440948\n",
      "The 39256 th iteration gives loss of 0.14483078474704733\n",
      "The 39257 th iteration gives loss of 0.14482939653616514\n",
      "The 39258 th iteration gives loss of 0.1448280083617608\n",
      "The 39259 th iteration gives loss of 0.14482662022381418\n",
      "The 39260 th iteration gives loss of 0.1448252321223415\n",
      "The 39261 th iteration gives loss of 0.14482384405733345\n",
      "The 39262 th iteration gives loss of 0.1448224560287907\n",
      "The 39263 th iteration gives loss of 0.1448210680366896\n",
      "The 39264 th iteration gives loss of 0.14481968008104135\n",
      "The 39265 th iteration gives loss of 0.14481829216184694\n",
      "The 39266 th iteration gives loss of 0.14481690427908067\n",
      "The 39267 th iteration gives loss of 0.14481551643276563\n",
      "The 39268 th iteration gives loss of 0.14481412862287948\n",
      "The 39269 th iteration gives loss of 0.1448127408494233\n",
      "The 39270 th iteration gives loss of 0.14481135311238885\n",
      "The 39271 th iteration gives loss of 0.14480996541177807\n",
      "The 39272 th iteration gives loss of 0.144808577747586\n",
      "The 39273 th iteration gives loss of 0.1448071901198112\n",
      "The 39274 th iteration gives loss of 0.144805802528446\n",
      "The 39275 th iteration gives loss of 0.14480441497348234\n",
      "The 39276 th iteration gives loss of 0.14480302745491894\n",
      "The 39277 th iteration gives loss of 0.14480163997274453\n",
      "The 39278 th iteration gives loss of 0.14480025252697262\n",
      "The 39279 th iteration gives loss of 0.14479886511758244\n",
      "The 39280 th iteration gives loss of 0.14479747774457766\n",
      "The 39281 th iteration gives loss of 0.14479609040795802\n",
      "The 39282 th iteration gives loss of 0.14479470310771386\n",
      "The 39283 th iteration gives loss of 0.14479331584384367\n",
      "The 39284 th iteration gives loss of 0.144791928616335\n",
      "The 39285 th iteration gives loss of 0.14479054142519307\n",
      "The 39286 th iteration gives loss of 0.1447891542704165\n",
      "The 39287 th iteration gives loss of 0.14478776715198224\n",
      "The 39288 th iteration gives loss of 0.14478638006990535\n",
      "The 39289 th iteration gives loss of 0.14478499302417475\n",
      "The 39290 th iteration gives loss of 0.14478360601479634\n",
      "The 39291 th iteration gives loss of 0.14478221904174857\n",
      "The 39292 th iteration gives loss of 0.14478083210503814\n",
      "The 39293 th iteration gives loss of 0.14477944520466193\n",
      "The 39294 th iteration gives loss of 0.14477805834060417\n",
      "The 39295 th iteration gives loss of 0.14477667151288387\n",
      "The 39296 th iteration gives loss of 0.14477528472147197\n",
      "The 39297 th iteration gives loss of 0.14477389796637333\n",
      "The 39298 th iteration gives loss of 0.14477251124759372\n",
      "The 39299 th iteration gives loss of 0.14477112456510968\n",
      "The 39300 th iteration gives loss of 0.14476973791892866\n",
      "The 39301 th iteration gives loss of 0.14476835130905397\n",
      "The 39302 th iteration gives loss of 0.14476696473547543\n",
      "The 39303 th iteration gives loss of 0.14476557819817612\n",
      "The 39304 th iteration gives loss of 0.14476419169716725\n",
      "The 39305 th iteration gives loss of 0.1447628052324418\n",
      "The 39306 th iteration gives loss of 0.14476141880399285\n",
      "The 39307 th iteration gives loss of 0.14476003241182245\n",
      "The 39308 th iteration gives loss of 0.14475864605591277\n",
      "The 39309 th iteration gives loss of 0.14475725973627168\n",
      "The 39310 th iteration gives loss of 0.14475587345289537\n",
      "The 39311 th iteration gives loss of 0.14475448720577386\n",
      "The 39312 th iteration gives loss of 0.1447531009949052\n",
      "The 39313 th iteration gives loss of 0.1447517148202862\n",
      "The 39314 th iteration gives loss of 0.1447503286819122\n",
      "The 39315 th iteration gives loss of 0.14474894257977763\n",
      "The 39316 th iteration gives loss of 0.14474755651388183\n",
      "The 39317 th iteration gives loss of 0.1447461704842187\n",
      "The 39318 th iteration gives loss of 0.14474478449078748\n",
      "The 39319 th iteration gives loss of 0.14474339853357607\n",
      "The 39320 th iteration gives loss of 0.14474201261258604\n",
      "The 39321 th iteration gives loss of 0.14474062672781438\n",
      "The 39322 th iteration gives loss of 0.1447392408792609\n",
      "The 39323 th iteration gives loss of 0.14473785506690293\n",
      "The 39324 th iteration gives loss of 0.1447364692907518\n",
      "The 39325 th iteration gives loss of 0.14473508355080458\n",
      "The 39326 th iteration gives loss of 0.144733697847051\n",
      "The 39327 th iteration gives loss of 0.14473231217948646\n",
      "The 39328 th iteration gives loss of 0.1447309265481173\n",
      "The 39329 th iteration gives loss of 0.14472954095292728\n",
      "The 39330 th iteration gives loss of 0.14472815539391856\n",
      "The 39331 th iteration gives loss of 0.14472676987108948\n",
      "The 39332 th iteration gives loss of 0.14472538438442914\n",
      "The 39333 th iteration gives loss of 0.14472399893393742\n",
      "The 39334 th iteration gives loss of 0.14472261351960616\n",
      "The 39335 th iteration gives loss of 0.1447212281414325\n",
      "The 39336 th iteration gives loss of 0.14471984279941494\n",
      "The 39337 th iteration gives loss of 0.1447184574935613\n",
      "The 39338 th iteration gives loss of 0.14471707222383023\n",
      "The 39339 th iteration gives loss of 0.14471568699025625\n",
      "The 39340 th iteration gives loss of 0.14471430179282246\n",
      "The 39341 th iteration gives loss of 0.1447129166315222\n",
      "The 39342 th iteration gives loss of 0.1447115315063545\n",
      "The 39343 th iteration gives loss of 0.14471014641730265\n",
      "The 39344 th iteration gives loss of 0.14470876136438826\n",
      "The 39345 th iteration gives loss of 0.14470737634758582\n",
      "The 39346 th iteration gives loss of 0.1447059913668978\n",
      "The 39347 th iteration gives loss of 0.14470460642232277\n",
      "The 39348 th iteration gives loss of 0.1447032215138542\n",
      "The 39349 th iteration gives loss of 0.14470183664148914\n",
      "The 39350 th iteration gives loss of 0.14470045180521662\n",
      "The 39351 th iteration gives loss of 0.1446990670050373\n",
      "The 39352 th iteration gives loss of 0.14469768224095295\n",
      "The 39353 th iteration gives loss of 0.14469629751296315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 39354 th iteration gives loss of 0.14469491282104494\n",
      "The 39355 th iteration gives loss of 0.1446935281652085\n",
      "The 39356 th iteration gives loss of 0.14469214354543877\n",
      "The 39357 th iteration gives loss of 0.14469075896174588\n",
      "The 39358 th iteration gives loss of 0.14468937441411484\n",
      "The 39359 th iteration gives loss of 0.1446879899025431\n",
      "The 39360 th iteration gives loss of 0.14468660542703823\n",
      "The 39361 th iteration gives loss of 0.1446852209875737\n",
      "The 39362 th iteration gives loss of 0.14468383658417075\n",
      "The 39363 th iteration gives loss of 0.14468245221681095\n",
      "The 39364 th iteration gives loss of 0.14468106788549376\n",
      "The 39365 th iteration gives loss of 0.1446796835902137\n",
      "The 39366 th iteration gives loss of 0.14467829933096074\n",
      "The 39367 th iteration gives loss of 0.14467691510774086\n",
      "The 39368 th iteration gives loss of 0.14467553092054625\n",
      "The 39369 th iteration gives loss of 0.14467414676937365\n",
      "The 39370 th iteration gives loss of 0.14467276265422027\n",
      "The 39371 th iteration gives loss of 0.14467137857508144\n",
      "The 39372 th iteration gives loss of 0.14466999453194618\n",
      "The 39373 th iteration gives loss of 0.14466861052482136\n",
      "The 39374 th iteration gives loss of 0.14466722655369002\n",
      "The 39375 th iteration gives loss of 0.1446658426185563\n",
      "The 39376 th iteration gives loss of 0.14466445871941358\n",
      "The 39377 th iteration gives loss of 0.14466307485626553\n",
      "The 39378 th iteration gives loss of 0.144661691029105\n",
      "The 39379 th iteration gives loss of 0.14466030723791565\n",
      "The 39380 th iteration gives loss of 0.1446589234827168\n",
      "The 39381 th iteration gives loss of 0.14465753976348553\n",
      "The 39382 th iteration gives loss of 0.1446561560802191\n",
      "The 39383 th iteration gives loss of 0.14465477243291305\n",
      "The 39384 th iteration gives loss of 0.1446533888215742\n",
      "The 39385 th iteration gives loss of 0.14465200524619454\n",
      "The 39386 th iteration gives loss of 0.14465062170676204\n",
      "The 39387 th iteration gives loss of 0.14464923820327968\n",
      "The 39388 th iteration gives loss of 0.1446478547357375\n",
      "The 39389 th iteration gives loss of 0.1446464713041395\n",
      "The 39390 th iteration gives loss of 0.14464508790848105\n",
      "The 39391 th iteration gives loss of 0.1446437045487486\n",
      "The 39392 th iteration gives loss of 0.14464232122494428\n",
      "The 39393 th iteration gives loss of 0.14464093793707244\n",
      "The 39394 th iteration gives loss of 0.14463955468510956\n",
      "The 39395 th iteration gives loss of 0.14463817146906502\n",
      "The 39396 th iteration gives loss of 0.14463678828893736\n",
      "The 39397 th iteration gives loss of 0.14463540514472104\n",
      "The 39398 th iteration gives loss of 0.14463402203639802\n",
      "The 39399 th iteration gives loss of 0.14463263896398929\n",
      "The 39400 th iteration gives loss of 0.14463125592746393\n",
      "The 39401 th iteration gives loss of 0.14462987292683763\n",
      "The 39402 th iteration gives loss of 0.14462848996208993\n",
      "The 39403 th iteration gives loss of 0.1446271070332248\n",
      "The 39404 th iteration gives loss of 0.1446257241402549\n",
      "The 39405 th iteration gives loss of 0.14462434128315035\n",
      "The 39406 th iteration gives loss of 0.1446229584619219\n",
      "The 39407 th iteration gives loss of 0.14462157567656278\n",
      "The 39408 th iteration gives loss of 0.14462019292706543\n",
      "The 39409 th iteration gives loss of 0.1446188102134233\n",
      "The 39410 th iteration gives loss of 0.14461742753563886\n",
      "The 39411 th iteration gives loss of 0.14461604489370528\n",
      "The 39412 th iteration gives loss of 0.14461466228762046\n",
      "The 39413 th iteration gives loss of 0.1446132797173877\n",
      "The 39414 th iteration gives loss of 0.14461189718298123\n",
      "The 39415 th iteration gives loss of 0.14461051468441635\n",
      "The 39416 th iteration gives loss of 0.1446091322216856\n",
      "The 39417 th iteration gives loss of 0.14460774979477412\n",
      "The 39418 th iteration gives loss of 0.1446063674036911\n",
      "The 39419 th iteration gives loss of 0.14460498504842062\n",
      "The 39420 th iteration gives loss of 0.14460360272897102\n",
      "The 39421 th iteration gives loss of 0.1446022204453392\n",
      "The 39422 th iteration gives loss of 0.14460083819751057\n",
      "The 39423 th iteration gives loss of 0.1445994559854813\n",
      "The 39424 th iteration gives loss of 0.14459807380925757\n",
      "The 39425 th iteration gives loss of 0.14459669166882705\n",
      "The 39426 th iteration gives loss of 0.14459530956418598\n",
      "The 39427 th iteration gives loss of 0.14459392749533562\n",
      "The 39428 th iteration gives loss of 0.14459254546226286\n",
      "The 39429 th iteration gives loss of 0.14459116346496917\n",
      "The 39430 th iteration gives loss of 0.1445897815034513\n",
      "The 39431 th iteration gives loss of 0.14458839957770747\n",
      "The 39432 th iteration gives loss of 0.1445870176877289\n",
      "The 39433 th iteration gives loss of 0.14458563583352055\n",
      "The 39434 th iteration gives loss of 0.14458425401506575\n",
      "The 39435 th iteration gives loss of 0.14458287223237318\n",
      "The 39436 th iteration gives loss of 0.14458149048542077\n",
      "The 39437 th iteration gives loss of 0.1445801087742167\n",
      "The 39438 th iteration gives loss of 0.14457872709875824\n",
      "The 39439 th iteration gives loss of 0.14457734545903894\n",
      "The 39440 th iteration gives loss of 0.14457596385505048\n",
      "The 39441 th iteration gives loss of 0.14457458228679823\n",
      "The 39442 th iteration gives loss of 0.14457320075427366\n",
      "The 39443 th iteration gives loss of 0.14457181925747237\n",
      "The 39444 th iteration gives loss of 0.14457043779639023\n",
      "The 39445 th iteration gives loss of 0.14456905637102796\n",
      "The 39446 th iteration gives loss of 0.1445676749813698\n",
      "The 39447 th iteration gives loss of 0.14456629362741577\n",
      "The 39448 th iteration gives loss of 0.14456491230917362\n",
      "The 39449 th iteration gives loss of 0.14456353102662403\n",
      "The 39450 th iteration gives loss of 0.1445621497797761\n",
      "The 39451 th iteration gives loss of 0.14456076856861055\n",
      "The 39452 th iteration gives loss of 0.14455938739313898\n",
      "The 39453 th iteration gives loss of 0.14455800625334406\n",
      "The 39454 th iteration gives loss of 0.14455662514923107\n",
      "The 39455 th iteration gives loss of 0.14455524408079745\n",
      "The 39456 th iteration gives loss of 0.14455386304803275\n",
      "The 39457 th iteration gives loss of 0.1445524820509358\n",
      "The 39458 th iteration gives loss of 0.14455110108950506\n",
      "The 39459 th iteration gives loss of 0.14454972016372575\n",
      "The 39460 th iteration gives loss of 0.14454833927360233\n",
      "The 39461 th iteration gives loss of 0.14454695841914153\n",
      "The 39462 th iteration gives loss of 0.14454557760031495\n",
      "The 39463 th iteration gives loss of 0.1445441968171337\n",
      "The 39464 th iteration gives loss of 0.14454281606959266\n",
      "The 39465 th iteration gives loss of 0.14454143535768374\n",
      "The 39466 th iteration gives loss of 0.14454005468141246\n",
      "The 39467 th iteration gives loss of 0.14453867404076678\n",
      "The 39468 th iteration gives loss of 0.14453729343574248\n",
      "The 39469 th iteration gives loss of 0.14453591286633716\n",
      "The 39470 th iteration gives loss of 0.1445345323325524\n",
      "The 39471 th iteration gives loss of 0.14453315183437865\n",
      "The 39472 th iteration gives loss of 0.14453177137180415\n",
      "The 39473 th iteration gives loss of 0.14453039094484288\n",
      "The 39474 th iteration gives loss of 0.14452901055347683\n",
      "The 39475 th iteration gives loss of 0.1445276301977013\n",
      "The 39476 th iteration gives loss of 0.14452624987752247\n",
      "The 39477 th iteration gives loss of 0.14452486959292785\n",
      "The 39478 th iteration gives loss of 0.14452348934391746\n",
      "The 39479 th iteration gives loss of 0.1445221091304867\n",
      "The 39480 th iteration gives loss of 0.144520728952632\n",
      "The 39481 th iteration gives loss of 0.14451934881034875\n",
      "The 39482 th iteration gives loss of 0.14451796870363331\n",
      "The 39483 th iteration gives loss of 0.14451658863247852\n",
      "The 39484 th iteration gives loss of 0.14451520859688785\n",
      "The 39485 th iteration gives loss of 0.14451382859684747\n",
      "The 39486 th iteration gives loss of 0.14451244863236273\n",
      "The 39487 th iteration gives loss of 0.14451106870342456\n",
      "The 39488 th iteration gives loss of 0.14450968881002788\n",
      "The 39489 th iteration gives loss of 0.14450830895217204\n",
      "The 39490 th iteration gives loss of 0.1445069291298485\n",
      "The 39491 th iteration gives loss of 0.14450554934306306\n",
      "The 39492 th iteration gives loss of 0.14450416959180576\n",
      "The 39493 th iteration gives loss of 0.144502789876072\n",
      "The 39494 th iteration gives loss of 0.1445014101958513\n",
      "The 39495 th iteration gives loss of 0.14450003055114996\n",
      "The 39496 th iteration gives loss of 0.1444986509419573\n",
      "The 39497 th iteration gives loss of 0.1444972713682788\n",
      "The 39498 th iteration gives loss of 0.14449589183009473\n",
      "The 39499 th iteration gives loss of 0.14449451232742505\n",
      "The 39500 th iteration gives loss of 0.14449313286024254\n",
      "The 39501 th iteration gives loss of 0.14449175342855175\n",
      "The 39502 th iteration gives loss of 0.14449037403235052\n",
      "The 39503 th iteration gives loss of 0.14448899467163118\n",
      "The 39504 th iteration gives loss of 0.14448761534639998\n",
      "The 39505 th iteration gives loss of 0.14448623605664018\n",
      "The 39506 th iteration gives loss of 0.14448485680234313\n",
      "The 39507 th iteration gives loss of 0.14448347758352265\n",
      "The 39508 th iteration gives loss of 0.14448209840016318\n",
      "The 39509 th iteration gives loss of 0.14448071925227404\n",
      "The 39510 th iteration gives loss of 0.14447934013982403\n",
      "The 39511 th iteration gives loss of 0.14447796106284064\n",
      "The 39512 th iteration gives loss of 0.14447658202130048\n",
      "The 39513 th iteration gives loss of 0.14447520301520486\n",
      "The 39514 th iteration gives loss of 0.14447382404455064\n",
      "The 39515 th iteration gives loss of 0.14447244510933413\n",
      "The 39516 th iteration gives loss of 0.1444710662095452\n",
      "The 39517 th iteration gives loss of 0.1444696873451942\n",
      "The 39518 th iteration gives loss of 0.14446830851625245\n",
      "The 39519 th iteration gives loss of 0.14446692972274008\n",
      "The 39520 th iteration gives loss of 0.14446555096464714\n",
      "The 39521 th iteration gives loss of 0.14446417224196326\n",
      "The 39522 th iteration gives loss of 0.1444627935546876\n",
      "The 39523 th iteration gives loss of 0.14446141490282358\n",
      "The 39524 th iteration gives loss of 0.14446003628635495\n",
      "The 39525 th iteration gives loss of 0.1444586577052852\n",
      "The 39526 th iteration gives loss of 0.14445727915960344\n",
      "The 39527 th iteration gives loss of 0.1444559006493154\n",
      "The 39528 th iteration gives loss of 0.14445452217441157\n",
      "The 39529 th iteration gives loss of 0.14445314373488838\n",
      "The 39530 th iteration gives loss of 0.14445176533074866\n",
      "The 39531 th iteration gives loss of 0.14445038696197002\n",
      "The 39532 th iteration gives loss of 0.14444900862857094\n",
      "The 39533 th iteration gives loss of 0.14444763033052707\n",
      "The 39534 th iteration gives loss of 0.14444625206785439\n",
      "The 39535 th iteration gives loss of 0.14444487384053248\n",
      "The 39536 th iteration gives loss of 0.14444349564856868\n",
      "The 39537 th iteration gives loss of 0.14444211749194985\n",
      "The 39538 th iteration gives loss of 0.14444073937067964\n",
      "The 39539 th iteration gives loss of 0.144439361284745\n",
      "The 39540 th iteration gives loss of 0.14443798323415838\n",
      "The 39541 th iteration gives loss of 0.14443660521890073\n",
      "The 39542 th iteration gives loss of 0.14443522723897242\n",
      "The 39543 th iteration gives loss of 0.14443384929436884\n",
      "The 39544 th iteration gives loss of 0.1444324713850905\n",
      "The 39545 th iteration gives loss of 0.14443109351113045\n",
      "The 39546 th iteration gives loss of 0.14442971567247867\n",
      "The 39547 th iteration gives loss of 0.144428337869137\n",
      "The 39548 th iteration gives loss of 0.14442696010111353\n",
      "The 39549 th iteration gives loss of 0.1444255823683801\n",
      "The 39550 th iteration gives loss of 0.14442420467095088\n",
      "The 39551 th iteration gives loss of 0.14442282700881273\n",
      "The 39552 th iteration gives loss of 0.1444214493819685\n",
      "The 39553 th iteration gives loss of 0.14442007179041422\n",
      "The 39554 th iteration gives loss of 0.1444186942341367\n",
      "The 39555 th iteration gives loss of 0.14441731671313432\n",
      "The 39556 th iteration gives loss of 0.1444159392274106\n",
      "The 39557 th iteration gives loss of 0.144414561776959\n",
      "The 39558 th iteration gives loss of 0.14441318436177064\n",
      "The 39559 th iteration gives loss of 0.14441180698184797\n",
      "The 39560 th iteration gives loss of 0.14441042963718126\n",
      "The 39561 th iteration gives loss of 0.14440905232777348\n",
      "The 39562 th iteration gives loss of 0.1444076750536133\n",
      "The 39563 th iteration gives loss of 0.144406297814707\n",
      "The 39564 th iteration gives loss of 0.14440492061104065\n",
      "The 39565 th iteration gives loss of 0.1444035434426096\n",
      "The 39566 th iteration gives loss of 0.1444021663094188\n",
      "The 39567 th iteration gives loss of 0.14440078921145968\n",
      "The 39568 th iteration gives loss of 0.1443994121487249\n",
      "The 39569 th iteration gives loss of 0.1443980351212126\n",
      "The 39570 th iteration gives loss of 0.1443966581289211\n",
      "The 39571 th iteration gives loss of 0.14439528117184178\n",
      "The 39572 th iteration gives loss of 0.14439390424998258\n",
      "The 39573 th iteration gives loss of 0.14439252736332664\n",
      "The 39574 th iteration gives loss of 0.14439115051187493\n",
      "The 39575 th iteration gives loss of 0.14438977369562592\n",
      "The 39576 th iteration gives loss of 0.14438839691457028\n",
      "The 39577 th iteration gives loss of 0.14438702016870986\n",
      "The 39578 th iteration gives loss of 0.14438564345803515\n",
      "The 39579 th iteration gives loss of 0.14438426678254196\n",
      "The 39580 th iteration gives loss of 0.14438289014223776\n",
      "The 39581 th iteration gives loss of 0.1443815135371042\n",
      "The 39582 th iteration gives loss of 0.14438013696714694\n",
      "The 39583 th iteration gives loss of 0.14437876043235157\n",
      "The 39584 th iteration gives loss of 0.14437738393272345\n",
      "The 39585 th iteration gives loss of 0.1443760074682617\n",
      "The 39586 th iteration gives loss of 0.14437463103895365\n",
      "The 39587 th iteration gives loss of 0.1443732546447941\n",
      "The 39588 th iteration gives loss of 0.144371878285788\n",
      "The 39589 th iteration gives loss of 0.14437050196192286\n",
      "The 39590 th iteration gives loss of 0.1443691256732076\n",
      "The 39591 th iteration gives loss of 0.1443677494196184\n",
      "The 39592 th iteration gives loss of 0.14436637320117596\n",
      "The 39593 th iteration gives loss of 0.1443649970178622\n",
      "The 39594 th iteration gives loss of 0.1443636208696605\n",
      "The 39595 th iteration gives loss of 0.14436224475659037\n",
      "The 39596 th iteration gives loss of 0.14436086867863937\n",
      "The 39597 th iteration gives loss of 0.14435949263579867\n",
      "The 39598 th iteration gives loss of 0.14435811662807121\n",
      "The 39599 th iteration gives loss of 0.14435674065544615\n",
      "The 39600 th iteration gives loss of 0.14435536471792293\n",
      "The 39601 th iteration gives loss of 0.14435398881550554\n",
      "The 39602 th iteration gives loss of 0.14435261294817195\n",
      "The 39603 th iteration gives loss of 0.14435123711593217\n",
      "The 39604 th iteration gives loss of 0.14434986131878305\n",
      "The 39605 th iteration gives loss of 0.14434848555671104\n",
      "The 39606 th iteration gives loss of 0.14434710982972626\n",
      "The 39607 th iteration gives loss of 0.1443457341378101\n",
      "The 39608 th iteration gives loss of 0.14434435848096633\n",
      "The 39609 th iteration gives loss of 0.14434298285918717\n",
      "The 39610 th iteration gives loss of 0.1443416072724741\n",
      "The 39611 th iteration gives loss of 0.14434023172082008\n",
      "The 39612 th iteration gives loss of 0.14433885620421796\n",
      "The 39613 th iteration gives loss of 0.14433748072267333\n",
      "The 39614 th iteration gives loss of 0.14433610527617347\n",
      "The 39615 th iteration gives loss of 0.1443347298647143\n",
      "The 39616 th iteration gives loss of 0.14433335448830495\n",
      "The 39617 th iteration gives loss of 0.14433197914692902\n",
      "The 39618 th iteration gives loss of 0.1443306038405795\n",
      "The 39619 th iteration gives loss of 0.14432922856926045\n",
      "The 39620 th iteration gives loss of 0.14432785333295656\n",
      "The 39621 th iteration gives loss of 0.14432647813168067\n",
      "The 39622 th iteration gives loss of 0.14432510296542864\n",
      "The 39623 th iteration gives loss of 0.14432372783418265\n",
      "The 39624 th iteration gives loss of 0.1443223527379392\n",
      "The 39625 th iteration gives loss of 0.14432097767671617\n",
      "The 39626 th iteration gives loss of 0.1443196026504805\n",
      "The 39627 th iteration gives loss of 0.14431822765924296\n",
      "The 39628 th iteration gives loss of 0.14431685270300268\n",
      "The 39629 th iteration gives loss of 0.14431547778175186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 39630 th iteration gives loss of 0.14431410289548105\n",
      "The 39631 th iteration gives loss of 0.14431272804419776\n",
      "The 39632 th iteration gives loss of 0.1443113532278882\n",
      "The 39633 th iteration gives loss of 0.14430997844655524\n",
      "The 39634 th iteration gives loss of 0.14430860370019338\n",
      "The 39635 th iteration gives loss of 0.14430722898879492\n",
      "The 39636 th iteration gives loss of 0.14430585431236156\n",
      "The 39637 th iteration gives loss of 0.1443044796708737\n",
      "The 39638 th iteration gives loss of 0.14430310506435395\n",
      "The 39639 th iteration gives loss of 0.14430173049278328\n",
      "The 39640 th iteration gives loss of 0.14430035595615348\n",
      "The 39641 th iteration gives loss of 0.14429898145447137\n",
      "The 39642 th iteration gives loss of 0.14429760698772065\n",
      "The 39643 th iteration gives loss of 0.14429623255590915\n",
      "The 39644 th iteration gives loss of 0.14429485815903803\n",
      "The 39645 th iteration gives loss of 0.14429348379708626\n",
      "The 39646 th iteration gives loss of 0.14429210947005536\n",
      "The 39647 th iteration gives loss of 0.1442907351779416\n",
      "The 39648 th iteration gives loss of 0.14428936092074982\n",
      "The 39649 th iteration gives loss of 0.14428798669846551\n",
      "The 39650 th iteration gives loss of 0.14428661251109232\n",
      "The 39651 th iteration gives loss of 0.14428523835862045\n",
      "The 39652 th iteration gives loss of 0.14428386424105002\n",
      "The 39653 th iteration gives loss of 0.14428249015837635\n",
      "The 39654 th iteration gives loss of 0.14428111611059374\n",
      "The 39655 th iteration gives loss of 0.14427974209770253\n",
      "The 39656 th iteration gives loss of 0.14427836811968928\n",
      "The 39657 th iteration gives loss of 0.14427699417655826\n",
      "The 39658 th iteration gives loss of 0.14427562026830587\n",
      "The 39659 th iteration gives loss of 0.14427424639493083\n",
      "The 39660 th iteration gives loss of 0.14427287255641832\n",
      "The 39661 th iteration gives loss of 0.14427149875277342\n",
      "The 39662 th iteration gives loss of 0.1442701249839841\n",
      "The 39663 th iteration gives loss of 0.144268751250061\n",
      "The 39664 th iteration gives loss of 0.14426737755099175\n",
      "The 39665 th iteration gives loss of 0.14426600388677233\n",
      "The 39666 th iteration gives loss of 0.1442646302573905\n",
      "The 39667 th iteration gives loss of 0.14426325666285572\n",
      "The 39668 th iteration gives loss of 0.1442618831031546\n",
      "The 39669 th iteration gives loss of 0.14426050957829437\n",
      "The 39670 th iteration gives loss of 0.14425913608826313\n",
      "The 39671 th iteration gives loss of 0.1442577626330555\n",
      "The 39672 th iteration gives loss of 0.14425638921267325\n",
      "The 39673 th iteration gives loss of 0.14425501582711067\n",
      "The 39674 th iteration gives loss of 0.14425364247635616\n",
      "The 39675 th iteration gives loss of 0.14425226916041023\n",
      "The 39676 th iteration gives loss of 0.14425089587927772\n",
      "The 39677 th iteration gives loss of 0.14424952263295343\n",
      "The 39678 th iteration gives loss of 0.1442481494214294\n",
      "The 39679 th iteration gives loss of 0.14424677624469268\n",
      "The 39680 th iteration gives loss of 0.1442454031027567\n",
      "The 39681 th iteration gives loss of 0.1442440299956055\n",
      "The 39682 th iteration gives loss of 0.1442426569232319\n",
      "The 39683 th iteration gives loss of 0.14424128388564153\n",
      "The 39684 th iteration gives loss of 0.14423991088283344\n",
      "The 39685 th iteration gives loss of 0.14423853791479496\n",
      "The 39686 th iteration gives loss of 0.14423716498151987\n",
      "The 39687 th iteration gives loss of 0.14423579208301324\n",
      "The 39688 th iteration gives loss of 0.14423441921926777\n",
      "The 39689 th iteration gives loss of 0.14423304639028095\n",
      "The 39690 th iteration gives loss of 0.14423167359605016\n",
      "The 39691 th iteration gives loss of 0.14423030083656654\n",
      "The 39692 th iteration gives loss of 0.14422892811182061\n",
      "The 39693 th iteration gives loss of 0.14422755542182783\n",
      "The 39694 th iteration gives loss of 0.14422618276656246\n",
      "The 39695 th iteration gives loss of 0.1442248101460326\n",
      "The 39696 th iteration gives loss of 0.1442234375602365\n",
      "The 39697 th iteration gives loss of 0.14422206500916976\n",
      "The 39698 th iteration gives loss of 0.14422069249281344\n",
      "The 39699 th iteration gives loss of 0.14421932001119042\n",
      "The 39700 th iteration gives loss of 0.14421794756427536\n",
      "The 39701 th iteration gives loss of 0.14421657515207673\n",
      "The 39702 th iteration gives loss of 0.14421520277458305\n",
      "The 39703 th iteration gives loss of 0.1442138304317865\n",
      "The 39704 th iteration gives loss of 0.14421245812369407\n",
      "The 39705 th iteration gives loss of 0.14421108585029374\n",
      "The 39706 th iteration gives loss of 0.14420971361158727\n",
      "The 39707 th iteration gives loss of 0.14420834140757066\n",
      "The 39708 th iteration gives loss of 0.144206969238236\n",
      "The 39709 th iteration gives loss of 0.14420559710357894\n",
      "The 39710 th iteration gives loss of 0.14420422500360455\n",
      "The 39711 th iteration gives loss of 0.14420285293829954\n",
      "The 39712 th iteration gives loss of 0.14420148090765977\n",
      "The 39713 th iteration gives loss of 0.14420010891169124\n",
      "The 39714 th iteration gives loss of 0.14419873695038313\n",
      "The 39715 th iteration gives loss of 0.14419736502372804\n",
      "The 39716 th iteration gives loss of 0.14419599313172873\n",
      "The 39717 th iteration gives loss of 0.14419462127438062\n",
      "The 39718 th iteration gives loss of 0.14419324945167458\n",
      "The 39719 th iteration gives loss of 0.14419187766360578\n",
      "The 39720 th iteration gives loss of 0.14419050591018362\n",
      "The 39721 th iteration gives loss of 0.14418913419139476\n",
      "The 39722 th iteration gives loss of 0.14418776250723186\n",
      "The 39723 th iteration gives loss of 0.14418639085770033\n",
      "The 39724 th iteration gives loss of 0.14418501924278848\n",
      "The 39725 th iteration gives loss of 0.1441836476624908\n",
      "The 39726 th iteration gives loss of 0.14418227611681844\n",
      "The 39727 th iteration gives loss of 0.14418090460575467\n",
      "The 39728 th iteration gives loss of 0.14417953312929696\n",
      "The 39729 th iteration gives loss of 0.14417816168743644\n",
      "The 39730 th iteration gives loss of 0.14417679028018923\n",
      "The 39731 th iteration gives loss of 0.14417541890752597\n",
      "The 39732 th iteration gives loss of 0.14417404756945887\n",
      "The 39733 th iteration gives loss of 0.14417267626598018\n",
      "The 39734 th iteration gives loss of 0.14417130499708752\n",
      "The 39735 th iteration gives loss of 0.14416993376277173\n",
      "The 39736 th iteration gives loss of 0.144168562563035\n",
      "The 39737 th iteration gives loss of 0.1441671913978679\n",
      "The 39738 th iteration gives loss of 0.14416582026727826\n",
      "The 39739 th iteration gives loss of 0.14416444917124563\n",
      "The 39740 th iteration gives loss of 0.14416307810978002\n",
      "The 39741 th iteration gives loss of 0.14416170708286768\n",
      "The 39742 th iteration gives loss of 0.14416033609051232\n",
      "The 39743 th iteration gives loss of 0.14415896513270784\n",
      "The 39744 th iteration gives loss of 0.14415759420944607\n",
      "The 39745 th iteration gives loss of 0.14415622332072928\n",
      "The 39746 th iteration gives loss of 0.14415485246655485\n",
      "The 39747 th iteration gives loss of 0.14415348164690245\n",
      "The 39748 th iteration gives loss of 0.1441521108617886\n",
      "The 39749 th iteration gives loss of 0.14415074011120996\n",
      "The 39750 th iteration gives loss of 0.1441493693951472\n",
      "The 39751 th iteration gives loss of 0.14414799871360026\n",
      "The 39752 th iteration gives loss of 0.14414662806657377\n",
      "The 39753 th iteration gives loss of 0.144145257454058\n",
      "The 39754 th iteration gives loss of 0.14414388687605106\n",
      "The 39755 th iteration gives loss of 0.1441425163325457\n",
      "The 39756 th iteration gives loss of 0.14414114582354362\n",
      "The 39757 th iteration gives loss of 0.1441397753490361\n",
      "The 39758 th iteration gives loss of 0.14413840490901936\n",
      "The 39759 th iteration gives loss of 0.14413703450350263\n",
      "The 39760 th iteration gives loss of 0.14413566413246026\n",
      "The 39761 th iteration gives loss of 0.1441342937959085\n",
      "The 39762 th iteration gives loss of 0.14413292349382756\n",
      "The 39763 th iteration gives loss of 0.1441315532262227\n",
      "The 39764 th iteration gives loss of 0.1441301829930851\n",
      "The 39765 th iteration gives loss of 0.14412881279441486\n",
      "The 39766 th iteration gives loss of 0.14412744263021052\n",
      "The 39767 th iteration gives loss of 0.1441260725004627\n",
      "The 39768 th iteration gives loss of 0.14412470240516428\n",
      "The 39769 th iteration gives loss of 0.14412333234431965\n",
      "The 39770 th iteration gives loss of 0.144121962317926\n",
      "The 39771 th iteration gives loss of 0.1441205923259755\n",
      "The 39772 th iteration gives loss of 0.14411922236846372\n",
      "The 39773 th iteration gives loss of 0.1441178524453887\n",
      "The 39774 th iteration gives loss of 0.14411648255674217\n",
      "The 39775 th iteration gives loss of 0.14411511270252605\n",
      "The 39776 th iteration gives loss of 0.14411374288273038\n",
      "The 39777 th iteration gives loss of 0.14411237309736152\n",
      "The 39778 th iteration gives loss of 0.14411100334640858\n",
      "The 39779 th iteration gives loss of 0.14410963362986742\n",
      "The 39780 th iteration gives loss of 0.14410826394773205\n",
      "The 39781 th iteration gives loss of 0.14410689430000623\n",
      "The 39782 th iteration gives loss of 0.14410552468667948\n",
      "The 39783 th iteration gives loss of 0.1441041551077553\n",
      "The 39784 th iteration gives loss of 0.14410278556322503\n",
      "The 39785 th iteration gives loss of 0.14410141605308255\n",
      "The 39786 th iteration gives loss of 0.144100046577322\n",
      "The 39787 th iteration gives loss of 0.14409867713595065\n",
      "The 39788 th iteration gives loss of 0.14409730772894774\n",
      "The 39789 th iteration gives loss of 0.14409593835633128\n",
      "The 39790 th iteration gives loss of 0.14409456901808151\n",
      "The 39791 th iteration gives loss of 0.1440931997141948\n",
      "The 39792 th iteration gives loss of 0.14409183044468554\n",
      "The 39793 th iteration gives loss of 0.14409046120952343\n",
      "The 39794 th iteration gives loss of 0.14408909200872883\n",
      "The 39795 th iteration gives loss of 0.1440877228422758\n",
      "The 39796 th iteration gives loss of 0.14408635371017608\n",
      "The 39797 th iteration gives loss of 0.1440849846124227\n",
      "The 39798 th iteration gives loss of 0.14408361554900628\n",
      "The 39799 th iteration gives loss of 0.14408224651993087\n",
      "The 39800 th iteration gives loss of 0.14408087752518517\n",
      "The 39801 th iteration gives loss of 0.144079508564771\n",
      "The 39802 th iteration gives loss of 0.14407813963868119\n",
      "The 39803 th iteration gives loss of 0.1440767707469195\n",
      "The 39804 th iteration gives loss of 0.14407540188947102\n",
      "The 39805 th iteration gives loss of 0.14407403306633537\n",
      "The 39806 th iteration gives loss of 0.14407266427750748\n",
      "The 39807 th iteration gives loss of 0.14407129552299433\n",
      "The 39808 th iteration gives loss of 0.14406992680278005\n",
      "The 39809 th iteration gives loss of 0.14406855811686317\n",
      "The 39810 th iteration gives loss of 0.14406718946524746\n",
      "The 39811 th iteration gives loss of 0.1440658208479239\n",
      "The 39812 th iteration gives loss of 0.1440644522648831\n",
      "The 39813 th iteration gives loss of 0.14406308371613588\n",
      "The 39814 th iteration gives loss of 0.1440617152016617\n",
      "The 39815 th iteration gives loss of 0.14406034672145834\n",
      "The 39816 th iteration gives loss of 0.14405897827553724\n",
      "The 39817 th iteration gives loss of 0.14405760986388588\n",
      "The 39818 th iteration gives loss of 0.14405624148649687\n",
      "The 39819 th iteration gives loss of 0.1440548731433676\n",
      "The 39820 th iteration gives loss of 0.1440535048345024\n",
      "The 39821 th iteration gives loss of 0.14405213655988505\n",
      "The 39822 th iteration gives loss of 0.14405076831952182\n",
      "The 39823 th iteration gives loss of 0.14404940011340286\n",
      "The 39824 th iteration gives loss of 0.14404803194152632\n",
      "The 39825 th iteration gives loss of 0.14404666380388934\n",
      "The 39826 th iteration gives loss of 0.14404529570049124\n",
      "The 39827 th iteration gives loss of 0.14404392763132684\n",
      "The 39828 th iteration gives loss of 0.14404255959637557\n",
      "The 39829 th iteration gives loss of 0.14404119159566406\n",
      "The 39830 th iteration gives loss of 0.14403982362916354\n",
      "The 39831 th iteration gives loss of 0.1440384556968843\n",
      "The 39832 th iteration gives loss of 0.1440370877988225\n",
      "The 39833 th iteration gives loss of 0.14403571993496384\n",
      "The 39834 th iteration gives loss of 0.14403435210530652\n",
      "The 39835 th iteration gives loss of 0.14403298430985267\n",
      "The 39836 th iteration gives loss of 0.14403161654859814\n",
      "The 39837 th iteration gives loss of 0.14403024882153617\n",
      "The 39838 th iteration gives loss of 0.14402888112866244\n",
      "The 39839 th iteration gives loss of 0.14402751346997678\n",
      "The 39840 th iteration gives loss of 0.14402614584547474\n",
      "The 39841 th iteration gives loss of 0.14402477825514695\n",
      "The 39842 th iteration gives loss of 0.1440234106990016\n",
      "The 39843 th iteration gives loss of 0.14402204317703068\n",
      "The 39844 th iteration gives loss of 0.1440206756892201\n",
      "The 39845 th iteration gives loss of 0.14401930823557638\n",
      "The 39846 th iteration gives loss of 0.14401794081608701\n",
      "The 39847 th iteration gives loss of 0.14401657343075672\n",
      "The 39848 th iteration gives loss of 0.14401520607957674\n",
      "The 39849 th iteration gives loss of 0.14401383876255058\n",
      "The 39850 th iteration gives loss of 0.1440124714796715\n",
      "The 39851 th iteration gives loss of 0.14401110423092497\n",
      "The 39852 th iteration gives loss of 0.1440097370163248\n",
      "The 39853 th iteration gives loss of 0.1440083698358557\n",
      "The 39854 th iteration gives loss of 0.14400700268951427\n",
      "The 39855 th iteration gives loss of 0.14400563557729787\n",
      "The 39856 th iteration gives loss of 0.1440042684992078\n",
      "The 39857 th iteration gives loss of 0.14400290145523298\n",
      "The 39858 th iteration gives loss of 0.14400153444537545\n",
      "The 39859 th iteration gives loss of 0.14400016746962618\n",
      "The 39860 th iteration gives loss of 0.14399880052798147\n",
      "The 39861 th iteration gives loss of 0.14399743362045384\n",
      "The 39862 th iteration gives loss of 0.1439960667470198\n",
      "The 39863 th iteration gives loss of 0.14399469990767927\n",
      "The 39864 th iteration gives loss of 0.1439933331024415\n",
      "The 39865 th iteration gives loss of 0.1439919663312762\n",
      "The 39866 th iteration gives loss of 0.1439905995942042\n",
      "The 39867 th iteration gives loss of 0.14398923289120774\n",
      "The 39868 th iteration gives loss of 0.14398786622229753\n",
      "The 39869 th iteration gives loss of 0.14398649958745927\n",
      "The 39870 th iteration gives loss of 0.14398513298668472\n",
      "The 39871 th iteration gives loss of 0.14398376641998129\n",
      "The 39872 th iteration gives loss of 0.14398239988734116\n",
      "The 39873 th iteration gives loss of 0.14398103338875812\n",
      "The 39874 th iteration gives loss of 0.1439796669242307\n",
      "The 39875 th iteration gives loss of 0.14397830049375607\n",
      "The 39876 th iteration gives loss of 0.1439769340973284\n",
      "The 39877 th iteration gives loss of 0.14397556773495165\n",
      "The 39878 th iteration gives loss of 0.1439742014066039\n",
      "The 39879 th iteration gives loss of 0.1439728351122936\n",
      "The 39880 th iteration gives loss of 0.14397146885201698\n",
      "The 39881 th iteration gives loss of 0.14397010262577542\n",
      "The 39882 th iteration gives loss of 0.1439687364335492\n",
      "The 39883 th iteration gives loss of 0.14396737027534923\n",
      "The 39884 th iteration gives loss of 0.14396600415116442\n",
      "The 39885 th iteration gives loss of 0.14396463806100215\n",
      "The 39886 th iteration gives loss of 0.14396327200484837\n",
      "The 39887 th iteration gives loss of 0.14396190598269998\n",
      "The 39888 th iteration gives loss of 0.14396053999454397\n",
      "The 39889 th iteration gives loss of 0.14395917404040096\n",
      "The 39890 th iteration gives loss of 0.14395780812024947\n",
      "The 39891 th iteration gives loss of 0.14395644223408302\n",
      "The 39892 th iteration gives loss of 0.143955076381913\n",
      "The 39893 th iteration gives loss of 0.14395371056371847\n",
      "The 39894 th iteration gives loss of 0.14395234477951424\n",
      "The 39895 th iteration gives loss of 0.14395097902928292\n",
      "The 39896 th iteration gives loss of 0.14394961331301936\n",
      "The 39897 th iteration gives loss of 0.14394824763072606\n",
      "The 39898 th iteration gives loss of 0.14394688198240055\n",
      "The 39899 th iteration gives loss of 0.1439455163680448\n",
      "The 39900 th iteration gives loss of 0.14394415078764256\n",
      "The 39901 th iteration gives loss of 0.14394278524118923\n",
      "The 39902 th iteration gives loss of 0.14394141972869076\n",
      "The 39903 th iteration gives loss of 0.14394005425013462\n",
      "The 39904 th iteration gives loss of 0.14393868880552788\n",
      "The 39905 th iteration gives loss of 0.14393732339485782\n",
      "The 39906 th iteration gives loss of 0.14393595801812112\n",
      "The 39907 th iteration gives loss of 0.14393459267532097\n",
      "The 39908 th iteration gives loss of 0.14393322736644887\n",
      "The 39909 th iteration gives loss of 0.1439318620914952\n",
      "The 39910 th iteration gives loss of 0.1439304968504707\n",
      "The 39911 th iteration gives loss of 0.14392913164335497\n",
      "The 39912 th iteration gives loss of 0.1439277664701573\n",
      "The 39913 th iteration gives loss of 0.14392640133087317\n",
      "The 39914 th iteration gives loss of 0.14392503622548738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 39915 th iteration gives loss of 0.14392367115400725\n",
      "The 39916 th iteration gives loss of 0.14392230611642548\n",
      "The 39917 th iteration gives loss of 0.14392094111273426\n",
      "The 39918 th iteration gives loss of 0.14391957614294798\n",
      "The 39919 th iteration gives loss of 0.1439182112070395\n",
      "The 39920 th iteration gives loss of 0.14391684630501086\n",
      "The 39921 th iteration gives loss of 0.1439154814368711\n",
      "The 39922 th iteration gives loss of 0.14391411660260256\n",
      "The 39923 th iteration gives loss of 0.14391275180220395\n",
      "The 39924 th iteration gives loss of 0.14391138703568102\n",
      "The 39925 th iteration gives loss of 0.1439100223030217\n",
      "The 39926 th iteration gives loss of 0.14390865760421995\n",
      "The 39927 th iteration gives loss of 0.14390729293928167\n",
      "The 39928 th iteration gives loss of 0.14390592830819093\n",
      "The 39929 th iteration gives loss of 0.14390456371095825\n",
      "The 39930 th iteration gives loss of 0.143903199147561\n",
      "The 39931 th iteration gives loss of 0.14390183461802009\n",
      "The 39932 th iteration gives loss of 0.1439004701223168\n",
      "The 39933 th iteration gives loss of 0.14389910566044029\n",
      "The 39934 th iteration gives loss of 0.14389774123239288\n",
      "The 39935 th iteration gives loss of 0.14389637683818698\n",
      "The 39936 th iteration gives loss of 0.1438950124777978\n",
      "The 39937 th iteration gives loss of 0.1438936481512288\n",
      "The 39938 th iteration gives loss of 0.1438922838584828\n",
      "The 39939 th iteration gives loss of 0.14389091959953998\n",
      "The 39940 th iteration gives loss of 0.14388955537440903\n",
      "The 39941 th iteration gives loss of 0.1438881911830867\n",
      "The 39942 th iteration gives loss of 0.14388682702557623\n",
      "The 39943 th iteration gives loss of 0.14388546290184953\n",
      "The 39944 th iteration gives loss of 0.1438840988119233\n",
      "The 39945 th iteration gives loss of 0.1438827347557943\n",
      "The 39946 th iteration gives loss of 0.14388137073344548\n",
      "The 39947 th iteration gives loss of 0.14388000674488116\n",
      "The 39948 th iteration gives loss of 0.14387864279009624\n",
      "The 39949 th iteration gives loss of 0.1438772788690918\n",
      "The 39950 th iteration gives loss of 0.14387591498185495\n",
      "The 39951 th iteration gives loss of 0.1438745511283916\n",
      "The 39952 th iteration gives loss of 0.14387318730869053\n",
      "The 39953 th iteration gives loss of 0.14387182352275654\n",
      "The 39954 th iteration gives loss of 0.14387045977057175\n",
      "The 39955 th iteration gives loss of 0.14386909605214426\n",
      "The 39956 th iteration gives loss of 0.14386773236747066\n",
      "The 39957 th iteration gives loss of 0.14386636871653505\n",
      "The 39958 th iteration gives loss of 0.14386500509934108\n",
      "The 39959 th iteration gives loss of 0.1438636415158998\n",
      "The 39960 th iteration gives loss of 0.1438622779661903\n",
      "The 39961 th iteration gives loss of 0.1438609144502152\n",
      "The 39962 th iteration gives loss of 0.14385955096795908\n",
      "The 39963 th iteration gives loss of 0.14385818751943974\n",
      "The 39964 th iteration gives loss of 0.14385682410463696\n",
      "The 39965 th iteration gives loss of 0.1438554607235489\n",
      "The 39966 th iteration gives loss of 0.14385409737617566\n",
      "The 39967 th iteration gives loss of 0.1438527340625086\n",
      "The 39968 th iteration gives loss of 0.14385137078255214\n",
      "The 39969 th iteration gives loss of 0.1438500075362976\n",
      "The 39970 th iteration gives loss of 0.14384864432374028\n",
      "The 39971 th iteration gives loss of 0.14384728114488382\n",
      "The 39972 th iteration gives loss of 0.1438459179997035\n",
      "The 39973 th iteration gives loss of 0.14384455488822448\n",
      "The 39974 th iteration gives loss of 0.1438431918104241\n",
      "The 39975 th iteration gives loss of 0.14384182876631582\n",
      "The 39976 th iteration gives loss of 0.14384046575587134\n",
      "The 39977 th iteration gives loss of 0.1438391027791103\n",
      "The 39978 th iteration gives loss of 0.14383773983600598\n",
      "The 39979 th iteration gives loss of 0.14383637692657342\n",
      "The 39980 th iteration gives loss of 0.14383501405080323\n",
      "The 39981 th iteration gives loss of 0.1438336512086947\n",
      "The 39982 th iteration gives loss of 0.14383228840023438\n",
      "The 39983 th iteration gives loss of 0.14383092562542696\n",
      "The 39984 th iteration gives loss of 0.14382956288427085\n",
      "The 39985 th iteration gives loss of 0.14382820017674985\n",
      "The 39986 th iteration gives loss of 0.1438268375028793\n",
      "The 39987 th iteration gives loss of 0.14382547486264063\n",
      "The 39988 th iteration gives loss of 0.1438241122560403\n",
      "The 39989 th iteration gives loss of 0.14382274968306336\n",
      "The 39990 th iteration gives loss of 0.14382138714371118\n",
      "The 39991 th iteration gives loss of 0.1438200246379793\n",
      "The 39992 th iteration gives loss of 0.14381866216586753\n",
      "The 39993 th iteration gives loss of 0.1438172997273698\n",
      "The 39994 th iteration gives loss of 0.14381593732248665\n",
      "The 39995 th iteration gives loss of 0.14381457495120817\n",
      "The 39996 th iteration gives loss of 0.1438132126135367\n",
      "The 39997 th iteration gives loss of 0.14381185030945598\n",
      "The 39998 th iteration gives loss of 0.143810488038981\n",
      "The 39999 th iteration gives loss of 0.14380912580208827\n",
      "The 40000 th iteration gives loss of 0.14380776359879532\n",
      "The 40001 th iteration gives loss of 0.14380640142907242\n",
      "The 40002 th iteration gives loss of 0.14380503929294114\n",
      "The 40003 th iteration gives loss of 0.1438036771903933\n",
      "The 40004 th iteration gives loss of 0.1438023151214075\n",
      "The 40005 th iteration gives loss of 0.14380095308599286\n",
      "The 40006 th iteration gives loss of 0.1437995910841505\n",
      "The 40007 th iteration gives loss of 0.14379822911587112\n",
      "The 40008 th iteration gives loss of 0.14379686718114976\n",
      "The 40009 th iteration gives loss of 0.14379550527998172\n",
      "The 40010 th iteration gives loss of 0.14379414341237529\n",
      "The 40011 th iteration gives loss of 0.14379278157830747\n",
      "The 40012 th iteration gives loss of 0.14379141977778104\n",
      "The 40013 th iteration gives loss of 0.1437900580108024\n",
      "The 40014 th iteration gives loss of 0.1437886962773635\n",
      "The 40015 th iteration gives loss of 0.14378733457745185\n",
      "The 40016 th iteration gives loss of 0.1437859729110721\n",
      "The 40017 th iteration gives loss of 0.14378461127822353\n",
      "The 40018 th iteration gives loss of 0.14378324967889314\n",
      "The 40019 th iteration gives loss of 0.14378188811308168\n",
      "The 40020 th iteration gives loss of 0.14378052658078863\n",
      "The 40021 th iteration gives loss of 0.14377916508201566\n",
      "The 40022 th iteration gives loss of 0.14377780361673573\n",
      "The 40023 th iteration gives loss of 0.14377644218496477\n",
      "The 40024 th iteration gives loss of 0.14377508078669518\n",
      "The 40025 th iteration gives loss of 0.1437737194219246\n",
      "The 40026 th iteration gives loss of 0.14377235809064842\n",
      "The 40027 th iteration gives loss of 0.14377099679285857\n",
      "The 40028 th iteration gives loss of 0.14376963552855715\n",
      "The 40029 th iteration gives loss of 0.14376827429773806\n",
      "The 40030 th iteration gives loss of 0.14376691310039902\n",
      "The 40031 th iteration gives loss of 0.143765551936533\n",
      "The 40032 th iteration gives loss of 0.14376419080614342\n",
      "The 40033 th iteration gives loss of 0.14376282970921225\n",
      "The 40034 th iteration gives loss of 0.14376146864575473\n",
      "The 40035 th iteration gives loss of 0.14376010761575292\n",
      "The 40036 th iteration gives loss of 0.1437587466192061\n",
      "The 40037 th iteration gives loss of 0.1437573856561228\n",
      "The 40038 th iteration gives loss of 0.14375602472647722\n",
      "The 40039 th iteration gives loss of 0.143754663830294\n",
      "The 40040 th iteration gives loss of 0.14375330296753966\n",
      "The 40041 th iteration gives loss of 0.14375194213822848\n",
      "The 40042 th iteration gives loss of 0.14375058134235164\n",
      "The 40043 th iteration gives loss of 0.14374922057990627\n",
      "The 40044 th iteration gives loss of 0.14374785985089086\n",
      "The 40045 th iteration gives loss of 0.14374649915529128\n",
      "The 40046 th iteration gives loss of 0.14374513849311976\n",
      "The 40047 th iteration gives loss of 0.14374377786437345\n",
      "The 40048 th iteration gives loss of 0.1437424172690315\n",
      "The 40049 th iteration gives loss of 0.1437410567071022\n",
      "The 40050 th iteration gives loss of 0.14373969617858015\n",
      "The 40051 th iteration gives loss of 0.143738335683458\n",
      "The 40052 th iteration gives loss of 0.1437369752217272\n",
      "The 40053 th iteration gives loss of 0.14373561479340705\n",
      "The 40054 th iteration gives loss of 0.1437342543984621\n",
      "The 40055 th iteration gives loss of 0.1437328940369154\n",
      "The 40056 th iteration gives loss of 0.14373153370874656\n",
      "The 40057 th iteration gives loss of 0.1437301734139688\n",
      "The 40058 th iteration gives loss of 0.14372881315255437\n",
      "The 40059 th iteration gives loss of 0.14372745292452654\n",
      "The 40060 th iteration gives loss of 0.14372609272985964\n",
      "The 40061 th iteration gives loss of 0.14372473256856452\n",
      "The 40062 th iteration gives loss of 0.14372337244063021\n",
      "The 40063 th iteration gives loss of 0.14372201234605586\n",
      "The 40064 th iteration gives loss of 0.14372065228483102\n",
      "The 40065 th iteration gives loss of 0.14371929225696203\n",
      "The 40066 th iteration gives loss of 0.14371793226244378\n",
      "The 40067 th iteration gives loss of 0.14371657230126803\n",
      "The 40068 th iteration gives loss of 0.14371521237343285\n",
      "The 40069 th iteration gives loss of 0.14371385247893678\n",
      "The 40070 th iteration gives loss of 0.14371249261777613\n",
      "The 40071 th iteration gives loss of 0.1437111327899362\n",
      "The 40072 th iteration gives loss of 0.14370977299542748\n",
      "The 40073 th iteration gives loss of 0.14370841323424127\n",
      "The 40074 th iteration gives loss of 0.14370705350637852\n",
      "The 40075 th iteration gives loss of 0.1437056938118245\n",
      "The 40076 th iteration gives loss of 0.14370433415058473\n",
      "The 40077 th iteration gives loss of 0.14370297452265007\n",
      "The 40078 th iteration gives loss of 0.14370161492802797\n",
      "The 40079 th iteration gives loss of 0.1437002553666996\n",
      "The 40080 th iteration gives loss of 0.14369889583867548\n",
      "The 40081 th iteration gives loss of 0.1436975363439435\n",
      "The 40082 th iteration gives loss of 0.14369617688249853\n",
      "The 40083 th iteration gives loss of 0.14369481745433843\n",
      "The 40084 th iteration gives loss of 0.14369345805945943\n",
      "The 40085 th iteration gives loss of 0.14369209869786773\n",
      "The 40086 th iteration gives loss of 0.14369073936954557\n",
      "The 40087 th iteration gives loss of 0.1436893800744957\n",
      "The 40088 th iteration gives loss of 0.1436880208127259\n",
      "The 40089 th iteration gives loss of 0.14368666158420917\n",
      "The 40090 th iteration gives loss of 0.1436853023889529\n",
      "The 40091 th iteration gives loss of 0.1436839432269555\n",
      "The 40092 th iteration gives loss of 0.14368258409821436\n",
      "The 40093 th iteration gives loss of 0.1436812250027262\n",
      "The 40094 th iteration gives loss of 0.1436798659404804\n",
      "The 40095 th iteration gives loss of 0.14367850691148168\n",
      "The 40096 th iteration gives loss of 0.14367714791571545\n",
      "The 40097 th iteration gives loss of 0.14367578895318275\n",
      "The 40098 th iteration gives loss of 0.14367443002389585\n",
      "The 40099 th iteration gives loss of 0.1436730711278285\n",
      "The 40100 th iteration gives loss of 0.14367171226498782\n",
      "The 40101 th iteration gives loss of 0.14367035343537377\n",
      "The 40102 th iteration gives loss of 0.143668994638968\n",
      "The 40103 th iteration gives loss of 0.14366763587578718\n",
      "The 40104 th iteration gives loss of 0.1436662771458096\n",
      "The 40105 th iteration gives loss of 0.14366491844904164\n",
      "The 40106 th iteration gives loss of 0.14366355978547735\n",
      "The 40107 th iteration gives loss of 0.14366220115510722\n",
      "The 40108 th iteration gives loss of 0.1436608425579415\n",
      "The 40109 th iteration gives loss of 0.14365948399396558\n",
      "The 40110 th iteration gives loss of 0.14365812546317813\n",
      "The 40111 th iteration gives loss of 0.14365676696557364\n",
      "The 40112 th iteration gives loss of 0.14365540850115263\n",
      "The 40113 th iteration gives loss of 0.14365405006990928\n",
      "The 40114 th iteration gives loss of 0.14365269167184253\n",
      "The 40115 th iteration gives loss of 0.14365133330695246\n",
      "The 40116 th iteration gives loss of 0.14364997497522308\n",
      "The 40117 th iteration gives loss of 0.14364861667665368\n",
      "The 40118 th iteration gives loss of 0.14364725841125156\n",
      "The 40119 th iteration gives loss of 0.1436459001790016\n",
      "The 40120 th iteration gives loss of 0.143644541979906\n",
      "The 40121 th iteration gives loss of 0.14364318381396193\n",
      "The 40122 th iteration gives loss of 0.14364182568116282\n",
      "The 40123 th iteration gives loss of 0.1436404675815003\n",
      "The 40124 th iteration gives loss of 0.14363910951498599\n",
      "The 40125 th iteration gives loss of 0.14363775148160277\n",
      "The 40126 th iteration gives loss of 0.14363639348135407\n",
      "The 40127 th iteration gives loss of 0.14363503551422993\n",
      "The 40128 th iteration gives loss of 0.14363367758022882\n",
      "The 40129 th iteration gives loss of 0.1436323196793539\n",
      "The 40130 th iteration gives loss of 0.14363096181159277\n",
      "The 40131 th iteration gives loss of 0.14362960397694471\n",
      "The 40132 th iteration gives loss of 0.1436282461754026\n",
      "The 40133 th iteration gives loss of 0.1436268884069688\n",
      "The 40134 th iteration gives loss of 0.14362553067163905\n",
      "The 40135 th iteration gives loss of 0.1436241729694144\n",
      "The 40136 th iteration gives loss of 0.14362281530027646\n",
      "The 40137 th iteration gives loss of 0.14362145766423795\n",
      "The 40138 th iteration gives loss of 0.14362010006128453\n",
      "The 40139 th iteration gives loss of 0.14361874249141218\n",
      "The 40140 th iteration gives loss of 0.14361738495462434\n",
      "The 40141 th iteration gives loss of 0.14361602745091376\n",
      "The 40142 th iteration gives loss of 0.14361466998027975\n",
      "The 40143 th iteration gives loss of 0.14361331254271464\n",
      "The 40144 th iteration gives loss of 0.14361195513822367\n",
      "The 40145 th iteration gives loss of 0.14361059776677954\n",
      "The 40146 th iteration gives loss of 0.14360924042840179\n",
      "The 40147 th iteration gives loss of 0.1436078831230848\n",
      "The 40148 th iteration gives loss of 0.14360652585081973\n",
      "The 40149 th iteration gives loss of 0.14360516861160133\n",
      "The 40150 th iteration gives loss of 0.14360381140542613\n",
      "The 40151 th iteration gives loss of 0.14360245423229864\n",
      "The 40152 th iteration gives loss of 0.1436010970922081\n",
      "The 40153 th iteration gives loss of 0.14359973998515135\n",
      "The 40154 th iteration gives loss of 0.1435983829111251\n",
      "The 40155 th iteration gives loss of 0.14359702587012446\n",
      "The 40156 th iteration gives loss of 0.1435956688621493\n",
      "The 40157 th iteration gives loss of 0.14359431188719854\n",
      "The 40158 th iteration gives loss of 0.14359295494525992\n",
      "The 40159 th iteration gives loss of 0.1435915980363353\n",
      "The 40160 th iteration gives loss of 0.1435902411604235\n",
      "The 40161 th iteration gives loss of 0.14358888431750857\n",
      "The 40162 th iteration gives loss of 0.14358752750760606\n",
      "The 40163 th iteration gives loss of 0.14358617073070482\n",
      "The 40164 th iteration gives loss of 0.14358481398679218\n",
      "The 40165 th iteration gives loss of 0.14358345727586916\n",
      "The 40166 th iteration gives loss of 0.14358210059794027\n",
      "The 40167 th iteration gives loss of 0.14358074395298698\n",
      "The 40168 th iteration gives loss of 0.1435793873410211\n",
      "The 40169 th iteration gives loss of 0.1435780307620448\n",
      "The 40170 th iteration gives loss of 0.1435766742160261\n",
      "The 40171 th iteration gives loss of 0.14357531770298398\n",
      "The 40172 th iteration gives loss of 0.143573961222905\n",
      "The 40173 th iteration gives loss of 0.14357260477579825\n",
      "The 40174 th iteration gives loss of 0.14357124836164675\n",
      "The 40175 th iteration gives loss of 0.14356989198043868\n",
      "The 40176 th iteration gives loss of 0.1435685356321971\n",
      "The 40177 th iteration gives loss of 0.14356717931690094\n",
      "The 40178 th iteration gives loss of 0.14356582303455007\n",
      "The 40179 th iteration gives loss of 0.14356446678513993\n",
      "The 40180 th iteration gives loss of 0.1435631105686727\n",
      "The 40181 th iteration gives loss of 0.1435617543851313\n",
      "The 40182 th iteration gives loss of 0.1435603982345353\n",
      "The 40183 th iteration gives loss of 0.14355904211685616\n",
      "The 40184 th iteration gives loss of 0.1435576860321069\n",
      "The 40185 th iteration gives loss of 0.1435563299802771\n",
      "The 40186 th iteration gives loss of 0.14355497396136396\n",
      "The 40187 th iteration gives loss of 0.143553617975364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 40188 th iteration gives loss of 0.14355226202227847\n",
      "The 40189 th iteration gives loss of 0.14355090610208532\n",
      "The 40190 th iteration gives loss of 0.143549550214805\n",
      "The 40191 th iteration gives loss of 0.14354819436042807\n",
      "The 40192 th iteration gives loss of 0.14354683853893543\n",
      "The 40193 th iteration gives loss of 0.1435454827503401\n",
      "The 40194 th iteration gives loss of 0.14354412699463454\n",
      "The 40195 th iteration gives loss of 0.1435427712718124\n",
      "The 40196 th iteration gives loss of 0.14354141558186856\n",
      "The 40197 th iteration gives loss of 0.14354005992481686\n",
      "The 40198 th iteration gives loss of 0.1435387043006221\n",
      "The 40199 th iteration gives loss of 0.1435373487093074\n",
      "The 40200 th iteration gives loss of 0.14353599315085866\n",
      "The 40201 th iteration gives loss of 0.14353463762527033\n",
      "The 40202 th iteration gives loss of 0.14353328213254601\n",
      "The 40203 th iteration gives loss of 0.14353192667268178\n",
      "The 40204 th iteration gives loss of 0.14353057124566096\n",
      "The 40205 th iteration gives loss of 0.14352921585149103\n",
      "The 40206 th iteration gives loss of 0.14352786049017496\n",
      "The 40207 th iteration gives loss of 0.14352650516169674\n",
      "The 40208 th iteration gives loss of 0.14352514986605203\n",
      "The 40209 th iteration gives loss of 0.1435237946032504\n",
      "The 40210 th iteration gives loss of 0.14352243937327158\n",
      "The 40211 th iteration gives loss of 0.1435210841761342\n",
      "The 40212 th iteration gives loss of 0.14351972901181448\n",
      "The 40213 th iteration gives loss of 0.14351837388031888\n",
      "The 40214 th iteration gives loss of 0.14351701878163253\n",
      "The 40215 th iteration gives loss of 0.14351566371577024\n",
      "The 40216 th iteration gives loss of 0.14351430868271262\n",
      "The 40217 th iteration gives loss of 0.1435129536824601\n",
      "The 40218 th iteration gives loss of 0.14351159871501792\n",
      "The 40219 th iteration gives loss of 0.14351024378036684\n",
      "The 40220 th iteration gives loss of 0.14350888887851634\n",
      "The 40221 th iteration gives loss of 0.14350753400945954\n",
      "The 40222 th iteration gives loss of 0.14350617917319433\n",
      "The 40223 th iteration gives loss of 0.14350482436971493\n",
      "The 40224 th iteration gives loss of 0.1435034695990123\n",
      "The 40225 th iteration gives loss of 0.1435021148610939\n",
      "The 40226 th iteration gives loss of 0.14350076015595076\n",
      "The 40227 th iteration gives loss of 0.14349940548357615\n",
      "The 40228 th iteration gives loss of 0.1434980508439666\n",
      "The 40229 th iteration gives loss of 0.14349669623712677\n",
      "The 40230 th iteration gives loss of 0.14349534166304329\n",
      "The 40231 th iteration gives loss of 0.14349398712172037\n",
      "The 40232 th iteration gives loss of 0.14349263261315537\n",
      "The 40233 th iteration gives loss of 0.1434912781373353\n",
      "The 40234 th iteration gives loss of 0.14348992369426478\n",
      "The 40235 th iteration gives loss of 0.14348856928393416\n",
      "The 40236 th iteration gives loss of 0.14348721490634747\n",
      "The 40237 th iteration gives loss of 0.1434858605614969\n",
      "The 40238 th iteration gives loss of 0.14348450624937167\n",
      "The 40239 th iteration gives loss of 0.14348315196998088\n",
      "The 40240 th iteration gives loss of 0.14348179772331401\n",
      "The 40241 th iteration gives loss of 0.1434804435093718\n",
      "The 40242 th iteration gives loss of 0.1434790893281459\n",
      "The 40243 th iteration gives loss of 0.14347773517964396\n",
      "The 40244 th iteration gives loss of 0.14347638106384222\n",
      "The 40245 th iteration gives loss of 0.14347502698075307\n",
      "The 40246 th iteration gives loss of 0.14347367293036853\n",
      "The 40247 th iteration gives loss of 0.1434723189126817\n",
      "The 40248 th iteration gives loss of 0.14347096492769668\n",
      "The 40249 th iteration gives loss of 0.14346961097540198\n",
      "The 40250 th iteration gives loss of 0.14346825705580418\n",
      "The 40251 th iteration gives loss of 0.1434669031688958\n",
      "The 40252 th iteration gives loss of 0.14346554931466332\n",
      "The 40253 th iteration gives loss of 0.14346419549310988\n",
      "The 40254 th iteration gives loss of 0.14346284170422985\n",
      "The 40255 th iteration gives loss of 0.1434614879480256\n",
      "The 40256 th iteration gives loss of 0.1434601342244945\n",
      "The 40257 th iteration gives loss of 0.1434587805336295\n",
      "The 40258 th iteration gives loss of 0.14345742687542148\n",
      "The 40259 th iteration gives loss of 0.14345607324987317\n",
      "The 40260 th iteration gives loss of 0.14345471965697842\n",
      "The 40261 th iteration gives loss of 0.1434533660967456\n",
      "The 40262 th iteration gives loss of 0.14345201256915735\n",
      "The 40263 th iteration gives loss of 0.14345065907420362\n",
      "The 40264 th iteration gives loss of 0.14344930561190308\n",
      "The 40265 th iteration gives loss of 0.14344795218223286\n",
      "The 40266 th iteration gives loss of 0.14344659878519445\n",
      "The 40267 th iteration gives loss of 0.1434452454207946\n",
      "The 40268 th iteration gives loss of 0.14344389208902122\n",
      "The 40269 th iteration gives loss of 0.14344253878986468\n",
      "The 40270 th iteration gives loss of 0.1434411855233434\n",
      "The 40271 th iteration gives loss of 0.14343983228942203\n",
      "The 40272 th iteration gives loss of 0.14343847908812016\n",
      "The 40273 th iteration gives loss of 0.14343712591942195\n",
      "The 40274 th iteration gives loss of 0.14343577278333772\n",
      "The 40275 th iteration gives loss of 0.14343441967985518\n",
      "The 40276 th iteration gives loss of 0.1434330666089677\n",
      "The 40277 th iteration gives loss of 0.14343171357067777\n",
      "The 40278 th iteration gives loss of 0.14343036056498454\n",
      "The 40279 th iteration gives loss of 0.14342900759186927\n",
      "The 40280 th iteration gives loss of 0.14342765465135196\n",
      "The 40281 th iteration gives loss of 0.1434263017434067\n",
      "The 40282 th iteration gives loss of 0.14342494886804316\n",
      "The 40283 th iteration gives loss of 0.14342359602525115\n",
      "The 40284 th iteration gives loss of 0.14342224321503505\n",
      "The 40285 th iteration gives loss of 0.1434208904373769\n",
      "The 40286 th iteration gives loss of 0.14341953769229374\n",
      "The 40287 th iteration gives loss of 0.14341818497977157\n",
      "The 40288 th iteration gives loss of 0.14341683229980154\n",
      "The 40289 th iteration gives loss of 0.1434154796523815\n",
      "The 40290 th iteration gives loss of 0.1434141270375149\n",
      "The 40291 th iteration gives loss of 0.1434127744551929\n",
      "The 40292 th iteration gives loss of 0.14341142190542086\n",
      "The 40293 th iteration gives loss of 0.14341006938817974\n",
      "The 40294 th iteration gives loss of 0.14340871690347567\n",
      "The 40295 th iteration gives loss of 0.14340736445130808\n",
      "The 40296 th iteration gives loss of 0.14340601203167094\n",
      "The 40297 th iteration gives loss of 0.14340465964455346\n",
      "The 40298 th iteration gives loss of 0.1434033072899615\n",
      "The 40299 th iteration gives loss of 0.14340195496789002\n",
      "The 40300 th iteration gives loss of 0.1434006026783301\n",
      "The 40301 th iteration gives loss of 0.14339925042128457\n",
      "The 40302 th iteration gives loss of 0.14339789819673737\n",
      "The 40303 th iteration gives loss of 0.14339654600470578\n",
      "The 40304 th iteration gives loss of 0.14339519384517302\n",
      "The 40305 th iteration gives loss of 0.14339384171813593\n",
      "The 40306 th iteration gives loss of 0.14339248962360357\n",
      "The 40307 th iteration gives loss of 0.1433911375615525\n",
      "The 40308 th iteration gives loss of 0.14338978553198659\n",
      "The 40309 th iteration gives loss of 0.14338843353490363\n",
      "The 40310 th iteration gives loss of 0.14338708157030036\n",
      "The 40311 th iteration gives loss of 0.14338572963817767\n",
      "The 40312 th iteration gives loss of 0.14338437773852244\n",
      "The 40313 th iteration gives loss of 0.14338302587134716\n",
      "The 40314 th iteration gives loss of 0.14338167403663676\n",
      "The 40315 th iteration gives loss of 0.14338032223438044\n",
      "The 40316 th iteration gives loss of 0.1433789704645861\n",
      "The 40317 th iteration gives loss of 0.1433776187272488\n",
      "The 40318 th iteration gives loss of 0.14337626702236073\n",
      "The 40319 th iteration gives loss of 0.14337491534992577\n",
      "The 40320 th iteration gives loss of 0.14337356370993437\n",
      "The 40321 th iteration gives loss of 0.14337221210238352\n",
      "The 40322 th iteration gives loss of 0.14337086052727416\n",
      "The 40323 th iteration gives loss of 0.143369508984589\n",
      "The 40324 th iteration gives loss of 0.14336815747434822\n",
      "The 40325 th iteration gives loss of 0.1433668059965346\n",
      "The 40326 th iteration gives loss of 0.14336545455114025\n",
      "The 40327 th iteration gives loss of 0.1433641031381687\n",
      "The 40328 th iteration gives loss of 0.1433627517576162\n",
      "The 40329 th iteration gives loss of 0.1433614004094738\n",
      "The 40330 th iteration gives loss of 0.143360049093745\n",
      "The 40331 th iteration gives loss of 0.14335869781042287\n",
      "The 40332 th iteration gives loss of 0.14335734655950036\n",
      "The 40333 th iteration gives loss of 0.1433559953409844\n",
      "The 40334 th iteration gives loss of 0.1433546441548619\n",
      "The 40335 th iteration gives loss of 0.14335329300112737\n",
      "The 40336 th iteration gives loss of 0.14335194187979106\n",
      "The 40337 th iteration gives loss of 0.1433505907908335\n",
      "The 40338 th iteration gives loss of 0.14334923973426048\n",
      "The 40339 th iteration gives loss of 0.14334788871006407\n",
      "The 40340 th iteration gives loss of 0.14334653771824846\n",
      "The 40341 th iteration gives loss of 0.14334518675880756\n",
      "The 40342 th iteration gives loss of 0.1433438358317348\n",
      "The 40343 th iteration gives loss of 0.14334248493702167\n",
      "The 40344 th iteration gives loss of 0.143341134074673\n",
      "The 40345 th iteration gives loss of 0.14333978324468247\n",
      "The 40346 th iteration gives loss of 0.14333843244704908\n",
      "The 40347 th iteration gives loss of 0.14333708168176126\n",
      "The 40348 th iteration gives loss of 0.14333573094883034\n",
      "The 40349 th iteration gives loss of 0.14333438024824358\n",
      "The 40350 th iteration gives loss of 0.14333302957998809\n",
      "The 40351 th iteration gives loss of 0.1433316789440737\n",
      "The 40352 th iteration gives loss of 0.1433303283404954\n",
      "The 40353 th iteration gives loss of 0.1433289777692491\n",
      "The 40354 th iteration gives loss of 0.14332762723032078\n",
      "The 40355 th iteration gives loss of 0.14332627672372628\n",
      "The 40356 th iteration gives loss of 0.14332492624944798\n",
      "The 40357 th iteration gives loss of 0.14332357580748126\n",
      "The 40358 th iteration gives loss of 0.14332222539784328\n",
      "The 40359 th iteration gives loss of 0.14332087502050425\n",
      "The 40360 th iteration gives loss of 0.14331952467547557\n",
      "The 40361 th iteration gives loss of 0.1433181743627463\n",
      "The 40362 th iteration gives loss of 0.1433168240823158\n",
      "The 40363 th iteration gives loss of 0.14331547383418733\n",
      "The 40364 th iteration gives loss of 0.14331412361834428\n",
      "The 40365 th iteration gives loss of 0.14331277343479282\n",
      "The 40366 th iteration gives loss of 0.14331142328352692\n",
      "The 40367 th iteration gives loss of 0.14331007316454078\n",
      "The 40368 th iteration gives loss of 0.14330872307783704\n",
      "The 40369 th iteration gives loss of 0.1433073730234087\n",
      "The 40370 th iteration gives loss of 0.14330602300125533\n",
      "The 40371 th iteration gives loss of 0.14330467301136748\n",
      "The 40372 th iteration gives loss of 0.14330332305373958\n",
      "The 40373 th iteration gives loss of 0.1433019731283775\n",
      "The 40374 th iteration gives loss of 0.1433006232352718\n",
      "The 40375 th iteration gives loss of 0.14329927337442536\n",
      "The 40376 th iteration gives loss of 0.14329792354582463\n",
      "The 40377 th iteration gives loss of 0.14329657374947374\n",
      "The 40378 th iteration gives loss of 0.1432952239853641\n",
      "The 40379 th iteration gives loss of 0.14329387425349524\n",
      "The 40380 th iteration gives loss of 0.14329252455387084\n",
      "The 40381 th iteration gives loss of 0.14329117488647303\n",
      "The 40382 th iteration gives loss of 0.14328982525131131\n",
      "The 40383 th iteration gives loss of 0.1432884756483704\n",
      "The 40384 th iteration gives loss of 0.14328712607765764\n",
      "The 40385 th iteration gives loss of 0.14328577653916214\n",
      "The 40386 th iteration gives loss of 0.14328442703288297\n",
      "The 40387 th iteration gives loss of 0.14328307755881295\n",
      "The 40388 th iteration gives loss of 0.14328172811695988\n",
      "The 40389 th iteration gives loss of 0.14328037870731367\n",
      "The 40390 th iteration gives loss of 0.14327902932986109\n",
      "The 40391 th iteration gives loss of 0.14327767998461796\n",
      "The 40392 th iteration gives loss of 0.14327633067156792\n",
      "The 40393 th iteration gives loss of 0.14327498139071262\n",
      "The 40394 th iteration gives loss of 0.14327363214203928\n",
      "The 40395 th iteration gives loss of 0.14327228292555327\n",
      "The 40396 th iteration gives loss of 0.14327093374125174\n",
      "The 40397 th iteration gives loss of 0.14326958458912514\n",
      "The 40398 th iteration gives loss of 0.1432682354691754\n",
      "The 40399 th iteration gives loss of 0.143266886381402\n",
      "The 40400 th iteration gives loss of 0.14326553732578995\n",
      "The 40401 th iteration gives loss of 0.14326418830234902\n",
      "The 40402 th iteration gives loss of 0.14326283931106398\n",
      "The 40403 th iteration gives loss of 0.1432614903519447\n",
      "The 40404 th iteration gives loss of 0.1432601414249769\n",
      "The 40405 th iteration gives loss of 0.14325879253016194\n",
      "The 40406 th iteration gives loss of 0.14325744366748863\n",
      "The 40407 th iteration gives loss of 0.14325609483696428\n",
      "The 40408 th iteration gives loss of 0.1432547460385798\n",
      "The 40409 th iteration gives loss of 0.143253397272331\n",
      "The 40410 th iteration gives loss of 0.14325204853821977\n",
      "The 40411 th iteration gives loss of 0.14325069983623387\n",
      "The 40412 th iteration gives loss of 0.1432493511663855\n",
      "The 40413 th iteration gives loss of 0.14324800252865152\n",
      "The 40414 th iteration gives loss of 0.14324665392303906\n",
      "The 40415 th iteration gives loss of 0.14324530534954003\n",
      "The 40416 th iteration gives loss of 0.1432439568081624\n",
      "The 40417 th iteration gives loss of 0.14324260829888807\n",
      "The 40418 th iteration gives loss of 0.14324125982172678\n",
      "The 40419 th iteration gives loss of 0.14323991137666017\n",
      "The 40420 th iteration gives loss of 0.14323856296370413\n",
      "The 40421 th iteration gives loss of 0.14323721458283523\n",
      "The 40422 th iteration gives loss of 0.14323586623406576\n",
      "The 40423 th iteration gives loss of 0.14323451791737896\n",
      "The 40424 th iteration gives loss of 0.14323316963278254\n",
      "The 40425 th iteration gives loss of 0.14323182138026797\n",
      "The 40426 th iteration gives loss of 0.14323047315982987\n",
      "The 40427 th iteration gives loss of 0.14322912497147183\n",
      "The 40428 th iteration gives loss of 0.14322777681518378\n",
      "The 40429 th iteration gives loss of 0.14322642869096985\n",
      "The 40430 th iteration gives loss of 0.14322508059880923\n",
      "The 40431 th iteration gives loss of 0.14322373253872442\n",
      "The 40432 th iteration gives loss of 0.14322238451069505\n",
      "The 40433 th iteration gives loss of 0.14322103651471363\n",
      "The 40434 th iteration gives loss of 0.14321968855078554\n",
      "The 40435 th iteration gives loss of 0.1432183406189076\n",
      "The 40436 th iteration gives loss of 0.14321699271907057\n",
      "The 40437 th iteration gives loss of 0.14321564485128058\n",
      "The 40438 th iteration gives loss of 0.14321429701553864\n",
      "The 40439 th iteration gives loss of 0.1432129492118168\n",
      "The 40440 th iteration gives loss of 0.14321160144012976\n",
      "The 40441 th iteration gives loss of 0.14321025370046972\n",
      "The 40442 th iteration gives loss of 0.14320890599283367\n",
      "The 40443 th iteration gives loss of 0.14320755831722146\n",
      "The 40444 th iteration gives loss of 0.14320621067362332\n",
      "The 40445 th iteration gives loss of 0.14320486306204133\n",
      "The 40446 th iteration gives loss of 0.14320351548247517\n",
      "The 40447 th iteration gives loss of 0.14320216793490773\n",
      "The 40448 th iteration gives loss of 0.1432008204193478\n",
      "The 40449 th iteration gives loss of 0.14319947293578367\n",
      "The 40450 th iteration gives loss of 0.1431981254842286\n",
      "The 40451 th iteration gives loss of 0.1431967780646631\n",
      "The 40452 th iteration gives loss of 0.14319543067707896\n",
      "The 40453 th iteration gives loss of 0.14319408332148442\n",
      "The 40454 th iteration gives loss of 0.1431927359978758\n",
      "The 40455 th iteration gives loss of 0.14319138870624498\n",
      "The 40456 th iteration gives loss of 0.14319004144659794\n",
      "The 40457 th iteration gives loss of 0.14318869421891772\n",
      "The 40458 th iteration gives loss of 0.14318734702321168\n",
      "The 40459 th iteration gives loss of 0.14318599985947625\n",
      "The 40460 th iteration gives loss of 0.14318465272769218\n",
      "The 40461 th iteration gives loss of 0.1431833056278716\n",
      "The 40462 th iteration gives loss of 0.1431819585600114\n",
      "The 40463 th iteration gives loss of 0.143180611524095\n",
      "The 40464 th iteration gives loss of 0.14317926452013316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 40465 th iteration gives loss of 0.14317791754811884\n",
      "The 40466 th iteration gives loss of 0.14317657060804984\n",
      "The 40467 th iteration gives loss of 0.1431752236999186\n",
      "The 40468 th iteration gives loss of 0.14317387682372057\n",
      "The 40469 th iteration gives loss of 0.14317252997945648\n",
      "The 40470 th iteration gives loss of 0.14317118316712243\n",
      "The 40471 th iteration gives loss of 0.14316983638670958\n",
      "The 40472 th iteration gives loss of 0.14316848963822112\n",
      "The 40473 th iteration gives loss of 0.14316714292165397\n",
      "The 40474 th iteration gives loss of 0.14316579623699577\n",
      "The 40475 th iteration gives loss of 0.1431644495842534\n",
      "The 40476 th iteration gives loss of 0.1431631029634258\n",
      "The 40477 th iteration gives loss of 0.14316175637449147\n",
      "The 40478 th iteration gives loss of 0.14316040981746714\n",
      "The 40479 th iteration gives loss of 0.1431590632923355\n",
      "The 40480 th iteration gives loss of 0.14315771679909556\n",
      "The 40481 th iteration gives loss of 0.14315637033776524\n",
      "The 40482 th iteration gives loss of 0.1431550239083143\n",
      "The 40483 th iteration gives loss of 0.14315367751074176\n",
      "The 40484 th iteration gives loss of 0.1431523311450515\n",
      "The 40485 th iteration gives loss of 0.14315098481124644\n",
      "The 40486 th iteration gives loss of 0.14314963850931053\n",
      "The 40487 th iteration gives loss of 0.14314829223924297\n",
      "The 40488 th iteration gives loss of 0.14314694600105046\n",
      "The 40489 th iteration gives loss of 0.14314559979472008\n",
      "The 40490 th iteration gives loss of 0.1431442536202436\n",
      "The 40491 th iteration gives loss of 0.14314290747763256\n",
      "The 40492 th iteration gives loss of 0.14314156136687997\n",
      "The 40493 th iteration gives loss of 0.14314021528796217\n",
      "The 40494 th iteration gives loss of 0.1431388692409101\n",
      "The 40495 th iteration gives loss of 0.143137523225698\n",
      "The 40496 th iteration gives loss of 0.14313617724232058\n",
      "The 40497 th iteration gives loss of 0.14313483129078303\n",
      "The 40498 th iteration gives loss of 0.14313348537107512\n",
      "The 40499 th iteration gives loss of 0.1431321394832075\n",
      "The 40500 th iteration gives loss of 0.14313079362716333\n",
      "The 40501 th iteration gives loss of 0.143129447802935\n",
      "The 40502 th iteration gives loss of 0.14312810201053028\n",
      "The 40503 th iteration gives loss of 0.14312675624995244\n",
      "The 40504 th iteration gives loss of 0.14312541052118358\n",
      "The 40505 th iteration gives loss of 0.14312406482422396\n",
      "The 40506 th iteration gives loss of 0.14312271915906274\n",
      "The 40507 th iteration gives loss of 0.1431213735257158\n",
      "The 40508 th iteration gives loss of 0.14312002792416492\n",
      "The 40509 th iteration gives loss of 0.14311868235441505\n",
      "The 40510 th iteration gives loss of 0.1431173368164509\n",
      "The 40511 th iteration gives loss of 0.14311599131028363\n",
      "The 40512 th iteration gives loss of 0.14311464583589847\n",
      "The 40513 th iteration gives loss of 0.1431133003932941\n",
      "The 40514 th iteration gives loss of 0.1431119549824738\n",
      "The 40515 th iteration gives loss of 0.14311060960343092\n",
      "The 40516 th iteration gives loss of 0.14310926425615828\n",
      "The 40517 th iteration gives loss of 0.14310791894065963\n",
      "The 40518 th iteration gives loss of 0.14310657365692991\n",
      "The 40519 th iteration gives loss of 0.14310522840495898\n",
      "The 40520 th iteration gives loss of 0.14310388318474343\n",
      "The 40521 th iteration gives loss of 0.14310253799629022\n",
      "The 40522 th iteration gives loss of 0.14310119283958356\n",
      "The 40523 th iteration gives loss of 0.1430998477146318\n",
      "The 40524 th iteration gives loss of 0.1430985026214176\n",
      "The 40525 th iteration gives loss of 0.14309715755995853\n",
      "The 40526 th iteration gives loss of 0.14309581253022668\n",
      "The 40527 th iteration gives loss of 0.14309446753224367\n",
      "The 40528 th iteration gives loss of 0.14309312256598258\n",
      "The 40529 th iteration gives loss of 0.14309177763145745\n",
      "The 40530 th iteration gives loss of 0.14309043272865354\n",
      "The 40531 th iteration gives loss of 0.14308908785757818\n",
      "The 40532 th iteration gives loss of 0.14308774301821106\n",
      "The 40533 th iteration gives loss of 0.14308639821057206\n",
      "The 40534 th iteration gives loss of 0.14308505343464104\n",
      "The 40535 th iteration gives loss of 0.14308370869041928\n",
      "The 40536 th iteration gives loss of 0.14308236397789698\n",
      "The 40537 th iteration gives loss of 0.14308101929708555\n",
      "The 40538 th iteration gives loss of 0.14307967464796945\n",
      "The 40539 th iteration gives loss of 0.14307833003054504\n",
      "The 40540 th iteration gives loss of 0.1430769854448215\n",
      "The 40541 th iteration gives loss of 0.1430756408907837\n",
      "The 40542 th iteration gives loss of 0.14307429636842842\n",
      "The 40543 th iteration gives loss of 0.14307295187775412\n",
      "The 40544 th iteration gives loss of 0.14307160741875502\n",
      "The 40545 th iteration gives loss of 0.14307026299144276\n",
      "The 40546 th iteration gives loss of 0.14306891859579796\n",
      "The 40547 th iteration gives loss of 0.14306757423182082\n",
      "The 40548 th iteration gives loss of 0.14306622989950712\n",
      "The 40549 th iteration gives loss of 0.1430648855988564\n",
      "The 40550 th iteration gives loss of 0.14306354132986776\n",
      "The 40551 th iteration gives loss of 0.1430621970925338\n",
      "The 40552 th iteration gives loss of 0.14306085288685302\n",
      "The 40553 th iteration gives loss of 0.1430595087128178\n",
      "The 40554 th iteration gives loss of 0.1430581645704318\n",
      "The 40555 th iteration gives loss of 0.14305682045968096\n",
      "The 40556 th iteration gives loss of 0.14305547638057112\n",
      "The 40557 th iteration gives loss of 0.14305413233309378\n",
      "The 40558 th iteration gives loss of 0.14305278831725385\n",
      "The 40559 th iteration gives loss of 0.14305144433303868\n",
      "The 40560 th iteration gives loss of 0.14305010038044522\n",
      "The 40561 th iteration gives loss of 0.14304875645948253\n",
      "The 40562 th iteration gives loss of 0.14304741257013293\n",
      "The 40563 th iteration gives loss of 0.14304606871239997\n",
      "The 40564 th iteration gives loss of 0.14304472488627648\n",
      "The 40565 th iteration gives loss of 0.14304338109175982\n",
      "The 40566 th iteration gives loss of 0.14304203732885518\n",
      "The 40567 th iteration gives loss of 0.14304069359754507\n",
      "The 40568 th iteration gives loss of 0.14303934989783174\n",
      "The 40569 th iteration gives loss of 0.1430380062297159\n",
      "The 40570 th iteration gives loss of 0.14303666259319409\n",
      "The 40571 th iteration gives loss of 0.14303531898825841\n",
      "The 40572 th iteration gives loss of 0.14303397541490645\n",
      "The 40573 th iteration gives loss of 0.1430326318731344\n",
      "The 40574 th iteration gives loss of 0.14303128836294493\n",
      "The 40575 th iteration gives loss of 0.14302994488433377\n",
      "The 40576 th iteration gives loss of 0.14302860143728346\n",
      "The 40577 th iteration gives loss of 0.14302725802180807\n",
      "The 40578 th iteration gives loss of 0.14302591463790032\n",
      "The 40579 th iteration gives loss of 0.14302457128554968\n",
      "The 40580 th iteration gives loss of 0.14302322796476247\n",
      "The 40581 th iteration gives loss of 0.14302188467552068\n",
      "The 40582 th iteration gives loss of 0.14302054141782777\n",
      "The 40583 th iteration gives loss of 0.14301919819170195\n",
      "The 40584 th iteration gives loss of 0.1430178549971109\n",
      "The 40585 th iteration gives loss of 0.14301651183405995\n",
      "The 40586 th iteration gives loss of 0.14301516870254424\n",
      "The 40587 th iteration gives loss of 0.14301382560256495\n",
      "The 40588 th iteration gives loss of 0.14301248253411125\n",
      "The 40589 th iteration gives loss of 0.14301113949719862\n",
      "The 40590 th iteration gives loss of 0.14300979649180368\n",
      "The 40591 th iteration gives loss of 0.143008453517932\n",
      "The 40592 th iteration gives loss of 0.14300711057557625\n",
      "The 40593 th iteration gives loss of 0.14300576766473705\n",
      "The 40594 th iteration gives loss of 0.14300442478540576\n",
      "The 40595 th iteration gives loss of 0.14300308193758865\n",
      "The 40596 th iteration gives loss of 0.1430017391212704\n",
      "The 40597 th iteration gives loss of 0.1430003963364566\n",
      "The 40598 th iteration gives loss of 0.14299905358313988\n",
      "The 40599 th iteration gives loss of 0.142997710861317\n",
      "The 40600 th iteration gives loss of 0.14299636817098557\n",
      "The 40601 th iteration gives loss of 0.14299502551215154\n",
      "The 40602 th iteration gives loss of 0.14299368288478803\n",
      "The 40603 th iteration gives loss of 0.14299234028891328\n",
      "The 40604 th iteration gives loss of 0.14299099772451226\n",
      "The 40605 th iteration gives loss of 0.14298965519158421\n",
      "The 40606 th iteration gives loss of 0.14298831269012974\n",
      "The 40607 th iteration gives loss of 0.14298697022015197\n",
      "The 40608 th iteration gives loss of 0.14298562778163157\n",
      "The 40609 th iteration gives loss of 0.1429842853745793\n",
      "The 40610 th iteration gives loss of 0.1429829429989767\n",
      "The 40611 th iteration gives loss of 0.14298160065483542\n",
      "The 40612 th iteration gives loss of 0.14298025834214315\n",
      "The 40613 th iteration gives loss of 0.1429789160608924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 40614 th iteration gives loss of 0.14297757381108953\n",
      "The 40615 th iteration gives loss of 0.14297623159273415\n",
      "The 40616 th iteration gives loss of 0.14297488940581232\n",
      "The 40617 th iteration gives loss of 0.14297354725032205\n",
      "The 40618 th iteration gives loss of 0.14297220512626607\n",
      "The 40619 th iteration gives loss of 0.14297086303364392\n",
      "The 40620 th iteration gives loss of 0.14296952097243815\n",
      "The 40621 th iteration gives loss of 0.14296817894266292\n",
      "The 40622 th iteration gives loss of 0.14296683694430243\n",
      "The 40623 th iteration gives loss of 0.14296549497734193\n",
      "The 40624 th iteration gives loss of 0.1429641530418136\n",
      "The 40625 th iteration gives loss of 0.1429628111376892\n",
      "The 40626 th iteration gives loss of 0.142961469264967\n",
      "The 40627 th iteration gives loss of 0.1429601274236388\n",
      "The 40628 th iteration gives loss of 0.14295878561371686\n",
      "The 40629 th iteration gives loss of 0.1429574438351915\n",
      "The 40630 th iteration gives loss of 0.1429561020880482\n",
      "The 40631 th iteration gives loss of 0.14295476037229907\n",
      "The 40632 th iteration gives loss of 0.14295341868794087\n",
      "The 40633 th iteration gives loss of 0.1429520770349605\n",
      "The 40634 th iteration gives loss of 0.14295073541336092\n",
      "The 40635 th iteration gives loss of 0.1429493938231349\n",
      "The 40636 th iteration gives loss of 0.14294805226428037\n",
      "The 40637 th iteration gives loss of 0.14294671073678908\n",
      "The 40638 th iteration gives loss of 0.14294536924066994\n",
      "The 40639 th iteration gives loss of 0.1429440277759131\n",
      "The 40640 th iteration gives loss of 0.1429426863425127\n",
      "The 40641 th iteration gives loss of 0.1429413449404589\n",
      "The 40642 th iteration gives loss of 0.1429400035697639\n",
      "The 40643 th iteration gives loss of 0.14293866223041735\n",
      "The 40644 th iteration gives loss of 0.14293732092241632\n",
      "The 40645 th iteration gives loss of 0.14293597964575896\n",
      "The 40646 th iteration gives loss of 0.14293463840043844\n",
      "The 40647 th iteration gives loss of 0.14293329718645395\n",
      "The 40648 th iteration gives loss of 0.14293195600380848\n",
      "The 40649 th iteration gives loss of 0.1429306148524825\n",
      "The 40650 th iteration gives loss of 0.1429292737324777\n",
      "The 40651 th iteration gives loss of 0.14292793264380474\n",
      "The 40652 th iteration gives loss of 0.14292659158644272\n",
      "The 40653 th iteration gives loss of 0.14292525056040553\n",
      "The 40654 th iteration gives loss of 0.14292390956567358\n",
      "The 40655 th iteration gives loss of 0.14292256860224883\n",
      "The 40656 th iteration gives loss of 0.1429212276701431\n",
      "The 40657 th iteration gives loss of 0.14291988676933198\n",
      "The 40658 th iteration gives loss of 0.142918545899818\n",
      "The 40659 th iteration gives loss of 0.14291720506159944\n",
      "The 40660 th iteration gives loss of 0.14291586425467032\n",
      "The 40661 th iteration gives loss of 0.14291452347903805\n",
      "The 40662 th iteration gives loss of 0.14291318273468742\n",
      "The 40663 th iteration gives loss of 0.14291184202161797\n",
      "The 40664 th iteration gives loss of 0.14291050133982885\n",
      "The 40665 th iteration gives loss of 0.14290916068932183\n",
      "The 40666 th iteration gives loss of 0.1429078200700745\n",
      "The 40667 th iteration gives loss of 0.14290647948210727\n",
      "The 40668 th iteration gives loss of 0.14290513892540227\n",
      "The 40669 th iteration gives loss of 0.1429037983999608\n",
      "The 40670 th iteration gives loss of 0.14290245790577677\n",
      "The 40671 th iteration gives loss of 0.14290111744285064\n",
      "The 40672 th iteration gives loss of 0.14289977701117643\n",
      "The 40673 th iteration gives loss of 0.14289843661075213\n",
      "The 40674 th iteration gives loss of 0.1428970962415726\n",
      "The 40675 th iteration gives loss of 0.14289575590364675\n",
      "The 40676 th iteration gives loss of 0.14289441559694893\n",
      "The 40677 th iteration gives loss of 0.1428930753214923\n",
      "The 40678 th iteration gives loss of 0.1428917350772635\n",
      "The 40679 th iteration gives loss of 0.14289039486426525\n",
      "The 40680 th iteration gives loss of 0.14288905468249669\n",
      "The 40681 th iteration gives loss of 0.14288771453195484\n",
      "The 40682 th iteration gives loss of 0.14288637441262625\n",
      "The 40683 th iteration gives loss of 0.1428850343245152\n",
      "The 40684 th iteration gives loss of 0.14288369426762088\n",
      "The 40685 th iteration gives loss of 0.14288235424193213\n",
      "The 40686 th iteration gives loss of 0.1428810142474525\n",
      "The 40687 th iteration gives loss of 0.1428796742841737\n",
      "The 40688 th iteration gives loss of 0.1428783343521046\n",
      "The 40689 th iteration gives loss of 0.1428769944512278\n",
      "The 40690 th iteration gives loss of 0.14287565458153959\n",
      "The 40691 th iteration gives loss of 0.14287431474304604\n",
      "The 40692 th iteration gives loss of 0.14287297493573822\n",
      "The 40693 th iteration gives loss of 0.14287163515961793\n",
      "The 40694 th iteration gives loss of 0.1428702954146856\n",
      "The 40695 th iteration gives loss of 0.14286895570091246\n",
      "The 40696 th iteration gives loss of 0.1428676160183148\n",
      "The 40697 th iteration gives loss of 0.14286627636689317\n",
      "The 40698 th iteration gives loss of 0.14286493674664139\n",
      "The 40699 th iteration gives loss of 0.1428635971575563\n",
      "The 40700 th iteration gives loss of 0.14286225759962154\n",
      "The 40701 th iteration gives loss of 0.1428609180728603\n",
      "The 40702 th iteration gives loss of 0.14285957857724144\n",
      "The 40703 th iteration gives loss of 0.14285823911278006\n",
      "The 40704 th iteration gives loss of 0.1428568996794613\n",
      "The 40705 th iteration gives loss of 0.1428555602772861\n",
      "The 40706 th iteration gives loss of 0.14285422090625444\n",
      "The 40707 th iteration gives loss of 0.14285288156635736\n",
      "The 40708 th iteration gives loss of 0.14285154225759703\n",
      "The 40709 th iteration gives loss of 0.1428502029799686\n",
      "The 40710 th iteration gives loss of 0.14284886373347155\n",
      "The 40711 th iteration gives loss of 0.1428475245180949\n",
      "The 40712 th iteration gives loss of 0.1428461853338449\n",
      "The 40713 th iteration gives loss of 0.14284484618070636\n",
      "The 40714 th iteration gives loss of 0.14284350705868665\n",
      "The 40715 th iteration gives loss of 0.14284216796777913\n",
      "The 40716 th iteration gives loss of 0.14284082890797742\n",
      "The 40717 th iteration gives loss of 0.14283948987928177\n",
      "The 40718 th iteration gives loss of 0.14283815088168877\n",
      "The 40719 th iteration gives loss of 0.14283681191519962\n",
      "The 40720 th iteration gives loss of 0.14283547297979882\n",
      "The 40721 th iteration gives loss of 0.14283413407548595\n",
      "The 40722 th iteration gives loss of 0.1428327952022703\n",
      "The 40723 th iteration gives loss of 0.14283145636013816\n",
      "The 40724 th iteration gives loss of 0.1428301175490919\n",
      "The 40725 th iteration gives loss of 0.14282877876911454\n",
      "The 40726 th iteration gives loss of 0.14282744002022482\n",
      "The 40727 th iteration gives loss of 0.14282610130239817\n",
      "The 40728 th iteration gives loss of 0.14282476261564636\n",
      "The 40729 th iteration gives loss of 0.14282342395995126\n",
      "The 40730 th iteration gives loss of 0.1428220853353286\n",
      "The 40731 th iteration gives loss of 0.1428207467417658\n",
      "The 40732 th iteration gives loss of 0.1428194081792616\n",
      "The 40733 th iteration gives loss of 0.1428180696478002\n",
      "The 40734 th iteration gives loss of 0.142816731147396\n",
      "The 40735 th iteration gives loss of 0.14281539267803717\n",
      "The 40736 th iteration gives loss of 0.14281405423972054\n",
      "The 40737 th iteration gives loss of 0.1428127158324426\n",
      "The 40738 th iteration gives loss of 0.1428113774561981\n",
      "The 40739 th iteration gives loss of 0.14281003911098675\n",
      "The 40740 th iteration gives loss of 0.14280870079681157\n",
      "The 40741 th iteration gives loss of 0.1428073625136616\n",
      "The 40742 th iteration gives loss of 0.14280602426154193\n",
      "The 40743 th iteration gives loss of 0.14280468604042906\n",
      "The 40744 th iteration gives loss of 0.14280334785034135\n",
      "The 40745 th iteration gives loss of 0.14280200969126\n",
      "The 40746 th iteration gives loss of 0.14280067156320106\n",
      "The 40747 th iteration gives loss of 0.1427993334661425\n",
      "The 40748 th iteration gives loss of 0.14279799540008592\n",
      "The 40749 th iteration gives loss of 0.14279665736503733\n",
      "The 40750 th iteration gives loss of 0.1427953193609786\n",
      "The 40751 th iteration gives loss of 0.14279398138791982\n",
      "The 40752 th iteration gives loss of 0.14279264344584988\n",
      "The 40753 th iteration gives loss of 0.14279130553476352\n",
      "The 40754 th iteration gives loss of 0.14278996765466584\n",
      "The 40755 th iteration gives loss of 0.14278862980555512\n",
      "The 40756 th iteration gives loss of 0.14278729198741819\n",
      "The 40757 th iteration gives loss of 0.14278595420025425\n",
      "The 40758 th iteration gives loss of 0.14278461644405624\n",
      "The 40759 th iteration gives loss of 0.14278327871883104\n",
      "The 40760 th iteration gives loss of 0.14278194102457661\n",
      "The 40761 th iteration gives loss of 0.1427806033612841\n",
      "The 40762 th iteration gives loss of 0.14277926572894692\n",
      "The 40763 th iteration gives loss of 0.1427779281275607\n",
      "The 40764 th iteration gives loss of 0.14277659055713118\n",
      "The 40765 th iteration gives loss of 0.14277525301765157\n",
      "The 40766 th iteration gives loss of 0.14277391550912008\n",
      "The 40767 th iteration gives loss of 0.14277257803152296\n",
      "The 40768 th iteration gives loss of 0.14277124058486745\n",
      "The 40769 th iteration gives loss of 0.14276990316914367\n",
      "The 40770 th iteration gives loss of 0.14276856578435837\n",
      "The 40771 th iteration gives loss of 0.14276722843050552\n",
      "The 40772 th iteration gives loss of 0.14276589110757415\n",
      "The 40773 th iteration gives loss of 0.14276455381556893\n",
      "The 40774 th iteration gives loss of 0.14276321655448046\n",
      "The 40775 th iteration gives loss of 0.1427618793243024\n",
      "The 40776 th iteration gives loss of 0.14276054212504066\n",
      "The 40777 th iteration gives loss of 0.1427592049566929\n",
      "The 40778 th iteration gives loss of 0.14275786781924635\n",
      "The 40779 th iteration gives loss of 0.14275653071271233\n",
      "The 40780 th iteration gives loss of 0.14275519363707326\n",
      "The 40781 th iteration gives loss of 0.1427538565923285\n",
      "The 40782 th iteration gives loss of 0.1427525195784823\n",
      "The 40783 th iteration gives loss of 0.14275118259553157\n",
      "The 40784 th iteration gives loss of 0.14274984564345947\n",
      "The 40785 th iteration gives loss of 0.14274850872227138\n",
      "The 40786 th iteration gives loss of 0.1427471718319611\n",
      "The 40787 th iteration gives loss of 0.1427458349725345\n",
      "The 40788 th iteration gives loss of 0.14274449814398713\n",
      "The 40789 th iteration gives loss of 0.14274316134630272\n",
      "The 40790 th iteration gives loss of 0.14274182457948822\n",
      "The 40791 th iteration gives loss of 0.14274048784353763\n",
      "The 40792 th iteration gives loss of 0.14273915113844604\n",
      "The 40793 th iteration gives loss of 0.14273781446422304\n",
      "The 40794 th iteration gives loss of 0.14273647782084392\n",
      "The 40795 th iteration gives loss of 0.1427351412083231\n",
      "The 40796 th iteration gives loss of 0.14273380462664137\n",
      "The 40797 th iteration gives loss of 0.14273246807581044\n",
      "The 40798 th iteration gives loss of 0.14273113155582257\n",
      "The 40799 th iteration gives loss of 0.14272979506667427\n",
      "The 40800 th iteration gives loss of 0.14272845860836073\n",
      "The 40801 th iteration gives loss of 0.14272712218087943\n",
      "The 40802 th iteration gives loss of 0.14272578578423198\n",
      "The 40803 th iteration gives loss of 0.1427244494184003\n",
      "The 40804 th iteration gives loss of 0.1427231130833958\n",
      "The 40805 th iteration gives loss of 0.14272177677920758\n",
      "The 40806 th iteration gives loss of 0.14272044050583826\n",
      "The 40807 th iteration gives loss of 0.14271910426328657\n",
      "The 40808 th iteration gives loss of 0.1427177680515387\n",
      "The 40809 th iteration gives loss of 0.14271643187060393\n",
      "The 40810 th iteration gives loss of 0.14271509572046603\n",
      "The 40811 th iteration gives loss of 0.14271375960112012\n",
      "The 40812 th iteration gives loss of 0.14271242351258828\n",
      "The 40813 th iteration gives loss of 0.142711087454839\n",
      "The 40814 th iteration gives loss of 0.1427097514278943\n",
      "The 40815 th iteration gives loss of 0.1427084154317207\n",
      "The 40816 th iteration gives loss of 0.14270707946633904\n",
      "The 40817 th iteration gives loss of 0.1427057435317405\n",
      "The 40818 th iteration gives loss of 0.1427044076279119\n",
      "The 40819 th iteration gives loss of 0.14270307175485678\n",
      "The 40820 th iteration gives loss of 0.14270173591258126\n",
      "The 40821 th iteration gives loss of 0.1427004001010685\n",
      "The 40822 th iteration gives loss of 0.14269906432031984\n",
      "The 40823 th iteration gives loss of 0.1426977285703382\n",
      "The 40824 th iteration gives loss of 0.142696392851113\n",
      "The 40825 th iteration gives loss of 0.142695057162637\n",
      "The 40826 th iteration gives loss of 0.14269372150490986\n",
      "The 40827 th iteration gives loss of 0.14269238587794314\n",
      "The 40828 th iteration gives loss of 0.14269105028171256\n",
      "The 40829 th iteration gives loss of 0.14268971471622754\n",
      "The 40830 th iteration gives loss of 0.14268837918148236\n",
      "The 40831 th iteration gives loss of 0.1426870436774733\n",
      "The 40832 th iteration gives loss of 0.1426857082041968\n",
      "The 40833 th iteration gives loss of 0.142684372761646\n",
      "The 40834 th iteration gives loss of 0.14268303734982185\n",
      "The 40835 th iteration gives loss of 0.14268170196872335\n",
      "The 40836 th iteration gives loss of 0.14268036661834718\n",
      "The 40837 th iteration gives loss of 0.14267903129868484\n",
      "The 40838 th iteration gives loss of 0.1426776960097285\n",
      "The 40839 th iteration gives loss of 0.1426763607514949\n",
      "The 40840 th iteration gives loss of 0.14267502552395941\n",
      "The 40841 th iteration gives loss of 0.14267369032713376\n",
      "The 40842 th iteration gives loss of 0.14267235516100113\n",
      "The 40843 th iteration gives loss of 0.14267102002557272\n",
      "The 40844 th iteration gives loss of 0.1426696849208368\n",
      "The 40845 th iteration gives loss of 0.1426683498467913\n",
      "The 40846 th iteration gives loss of 0.14266701480342622\n",
      "The 40847 th iteration gives loss of 0.14266567979074873\n",
      "The 40848 th iteration gives loss of 0.14266434480876075\n",
      "The 40849 th iteration gives loss of 0.14266300985744243\n",
      "The 40850 th iteration gives loss of 0.14266167493680326\n",
      "The 40851 th iteration gives loss of 0.1426603400468422\n",
      "The 40852 th iteration gives loss of 0.14265900518753682\n",
      "The 40853 th iteration gives loss of 0.14265767035890317\n",
      "The 40854 th iteration gives loss of 0.14265633556093418\n",
      "The 40855 th iteration gives loss of 0.14265500079361387\n",
      "The 40856 th iteration gives loss of 0.142653666056955\n",
      "The 40857 th iteration gives loss of 0.14265233135094946\n",
      "The 40858 th iteration gives loss of 0.142650996675596\n",
      "The 40859 th iteration gives loss of 0.14264966203088178\n",
      "The 40860 th iteration gives loss of 0.1426483274168124\n",
      "The 40861 th iteration gives loss of 0.142646992833384\n",
      "The 40862 th iteration gives loss of 0.1426456582805937\n",
      "The 40863 th iteration gives loss of 0.14264432375844002\n",
      "The 40864 th iteration gives loss of 0.14264298926690572\n",
      "The 40865 th iteration gives loss of 0.1426416548060017\n",
      "The 40866 th iteration gives loss of 0.1426403203757212\n",
      "The 40867 th iteration gives loss of 0.14263898597606509\n",
      "The 40868 th iteration gives loss of 0.14263765160702255\n",
      "The 40869 th iteration gives loss of 0.14263631726859985\n",
      "The 40870 th iteration gives loss of 0.14263498296077903\n",
      "The 40871 th iteration gives loss of 0.14263364868357564\n",
      "The 40872 th iteration gives loss of 0.14263231443697455\n",
      "The 40873 th iteration gives loss of 0.14263098022097204\n",
      "The 40874 th iteration gives loss of 0.14262964603557116\n",
      "The 40875 th iteration gives loss of 0.1426283118807533\n",
      "The 40876 th iteration gives loss of 0.14262697775654035\n",
      "The 40877 th iteration gives loss of 0.14262564366290903\n",
      "The 40878 th iteration gives loss of 0.14262430959986655\n",
      "The 40879 th iteration gives loss of 0.14262297556740813\n",
      "The 40880 th iteration gives loss of 0.14262164156552357\n",
      "The 40881 th iteration gives loss of 0.14262030759421096\n",
      "The 40882 th iteration gives loss of 0.14261897365348508\n",
      "The 40883 th iteration gives loss of 0.14261763974331915\n",
      "The 40884 th iteration gives loss of 0.14261630586372187\n",
      "The 40885 th iteration gives loss of 0.1426149720146872\n",
      "The 40886 th iteration gives loss of 0.14261363819621095\n",
      "The 40887 th iteration gives loss of 0.14261230440829525\n",
      "The 40888 th iteration gives loss of 0.142610970650931\n",
      "The 40889 th iteration gives loss of 0.14260963692411485\n",
      "The 40890 th iteration gives loss of 0.14260830322785575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 40891 th iteration gives loss of 0.142606969562128\n",
      "The 40892 th iteration gives loss of 0.14260563592694295\n",
      "The 40893 th iteration gives loss of 0.1426043023222937\n",
      "The 40894 th iteration gives loss of 0.14260296874818676\n",
      "The 40895 th iteration gives loss of 0.14260163520460833\n",
      "The 40896 th iteration gives loss of 0.14260030169155724\n",
      "The 40897 th iteration gives loss of 0.14259896820902843\n",
      "The 40898 th iteration gives loss of 0.14259763475702358\n",
      "The 40899 th iteration gives loss of 0.14259630133554485\n",
      "The 40900 th iteration gives loss of 0.1425949679445748\n",
      "The 40901 th iteration gives loss of 0.14259363458411117\n",
      "The 40902 th iteration gives loss of 0.1425923012541598\n",
      "The 40903 th iteration gives loss of 0.14259096795471402\n",
      "The 40904 th iteration gives loss of 0.14258963468577568\n",
      "The 40905 th iteration gives loss of 0.1425883014473299\n",
      "The 40906 th iteration gives loss of 0.14258696823938702\n",
      "The 40907 th iteration gives loss of 0.1425856350619338\n",
      "The 40908 th iteration gives loss of 0.14258430191497606\n",
      "The 40909 th iteration gives loss of 0.1425829687984963\n",
      "The 40910 th iteration gives loss of 0.14258163571251\n",
      "The 40911 th iteration gives loss of 0.14258030265699254\n",
      "The 40912 th iteration gives loss of 0.14257896963196529\n",
      "The 40913 th iteration gives loss of 0.1425776366374043\n",
      "The 40914 th iteration gives loss of 0.1425763036733192\n",
      "The 40915 th iteration gives loss of 0.14257497073969533\n",
      "The 40916 th iteration gives loss of 0.14257363783653412\n",
      "The 40917 th iteration gives loss of 0.14257230496383969\n",
      "The 40918 th iteration gives loss of 0.1425709721216015\n",
      "The 40919 th iteration gives loss of 0.1425696393098184\n",
      "The 40920 th iteration gives loss of 0.14256830652849303\n",
      "The 40921 th iteration gives loss of 0.14256697377761146\n",
      "The 40922 th iteration gives loss of 0.14256564105717684\n",
      "The 40923 th iteration gives loss of 0.14256430836718936\n",
      "The 40924 th iteration gives loss of 0.14256297570763196\n",
      "The 40925 th iteration gives loss of 0.1425616430785063\n",
      "The 40926 th iteration gives loss of 0.14256031047982226\n",
      "The 40927 th iteration gives loss of 0.14255897791156857\n",
      "The 40928 th iteration gives loss of 0.14255764537374574\n",
      "The 40929 th iteration gives loss of 0.1425563128663383\n",
      "The 40930 th iteration gives loss of 0.14255498038935338\n",
      "The 40931 th iteration gives loss of 0.14255364794278771\n",
      "The 40932 th iteration gives loss of 0.14255231552663383\n",
      "The 40933 th iteration gives loss of 0.14255098314088843\n",
      "The 40934 th iteration gives loss of 0.14254965078554918\n",
      "The 40935 th iteration gives loss of 0.14254831846062274\n",
      "The 40936 th iteration gives loss of 0.1425469861660947\n",
      "The 40937 th iteration gives loss of 0.14254565390196225\n",
      "The 40938 th iteration gives loss of 0.1425443216682293\n",
      "The 40939 th iteration gives loss of 0.14254298946488253\n",
      "The 40940 th iteration gives loss of 0.14254165729192506\n",
      "The 40941 th iteration gives loss of 0.14254032514935552\n",
      "The 40942 th iteration gives loss of 0.14253899303717482\n",
      "The 40943 th iteration gives loss of 0.14253766095536416\n",
      "The 40944 th iteration gives loss of 0.14253632890392845\n",
      "The 40945 th iteration gives loss of 0.14253499688287155\n",
      "The 40946 th iteration gives loss of 0.14253366489218622\n",
      "The 40947 th iteration gives loss of 0.14253233293185996\n",
      "The 40948 th iteration gives loss of 0.1425310010019028\n",
      "The 40949 th iteration gives loss of 0.14252966910230488\n",
      "The 40950 th iteration gives loss of 0.1425283372330616\n",
      "The 40951 th iteration gives loss of 0.14252700539418106\n",
      "The 40952 th iteration gives loss of 0.14252567358563759\n",
      "The 40953 th iteration gives loss of 0.1425243418074496\n",
      "The 40954 th iteration gives loss of 0.1425230100596013\n",
      "The 40955 th iteration gives loss of 0.1425216783421016\n",
      "The 40956 th iteration gives loss of 0.14252034665493715\n",
      "The 40957 th iteration gives loss of 0.14251901499811165\n",
      "The 40958 th iteration gives loss of 0.14251768337160406\n",
      "The 40959 th iteration gives loss of 0.14251635177543992\n",
      "The 40960 th iteration gives loss of 0.14251502020959278\n",
      "The 40961 th iteration gives loss of 0.14251368867407396\n",
      "The 40962 th iteration gives loss of 0.14251235716888228\n",
      "The 40963 th iteration gives loss of 0.14251102569399218\n",
      "The 40964 th iteration gives loss of 0.14250969424942414\n",
      "The 40965 th iteration gives loss of 0.14250836283516272\n",
      "The 40966 th iteration gives loss of 0.1425070314512016\n",
      "The 40967 th iteration gives loss of 0.1425057000975561\n",
      "The 40968 th iteration gives loss of 0.14250436877420158\n",
      "The 40969 th iteration gives loss of 0.14250303748114646\n",
      "The 40970 th iteration gives loss of 0.14250170621838826\n",
      "The 40971 th iteration gives loss of 0.14250037498592094\n",
      "The 40972 th iteration gives loss of 0.14249904378373474\n",
      "The 40973 th iteration gives loss of 0.14249771261184657\n",
      "The 40974 th iteration gives loss of 0.14249638147023158\n",
      "The 40975 th iteration gives loss of 0.1424950503588997\n",
      "The 40976 th iteration gives loss of 0.14249371927784016\n",
      "The 40977 th iteration gives loss of 0.142492388227048\n",
      "The 40978 th iteration gives loss of 0.14249105720653038\n",
      "The 40979 th iteration gives loss of 0.14248972621627873\n",
      "The 40980 th iteration gives loss of 0.1424883952562899\n",
      "The 40981 th iteration gives loss of 0.1424870643265576\n",
      "The 40982 th iteration gives loss of 0.14248573342708057\n",
      "The 40983 th iteration gives loss of 0.14248440255786876\n",
      "The 40984 th iteration gives loss of 0.1424830717188984\n",
      "The 40985 th iteration gives loss of 0.1424817409101785\n",
      "The 40986 th iteration gives loss of 0.14248041013169946\n",
      "The 40987 th iteration gives loss of 0.14247907938346369\n",
      "The 40988 th iteration gives loss of 0.14247774866546084\n",
      "The 40989 th iteration gives loss of 0.14247641797770047\n",
      "The 40990 th iteration gives loss of 0.14247508732017047\n",
      "The 40991 th iteration gives loss of 0.1424737566928651\n",
      "The 40992 th iteration gives loss of 0.14247242609578276\n",
      "The 40993 th iteration gives loss of 0.1424710955289257\n",
      "The 40994 th iteration gives loss of 0.14246976499228184\n",
      "The 40995 th iteration gives loss of 0.14246843448586066\n",
      "The 40996 th iteration gives loss of 0.14246710400964896\n",
      "The 40997 th iteration gives loss of 0.14246577356364998\n",
      "The 40998 th iteration gives loss of 0.1424644431478615\n",
      "The 40999 th iteration gives loss of 0.14246311276227075\n",
      "The 41000 th iteration gives loss of 0.14246178240687746\n",
      "The 41001 th iteration gives loss of 0.1424604520816802\n",
      "The 41002 th iteration gives loss of 0.14245912178667955\n",
      "The 41003 th iteration gives loss of 0.14245779152187568\n",
      "The 41004 th iteration gives loss of 0.14245646128725445\n",
      "The 41005 th iteration gives loss of 0.14245513108282035\n",
      "The 41006 th iteration gives loss of 0.14245380090856388\n",
      "The 41007 th iteration gives loss of 0.14245247076448792\n",
      "The 41008 th iteration gives loss of 0.14245114065058578\n",
      "The 41009 th iteration gives loss of 0.14244981056686312\n",
      "The 41010 th iteration gives loss of 0.14244848051330256\n",
      "The 41011 th iteration gives loss of 0.14244715048991077\n",
      "The 41012 th iteration gives loss of 0.14244582049667903\n",
      "The 41013 th iteration gives loss of 0.1424444905336049\n",
      "The 41014 th iteration gives loss of 0.1424431606006872\n",
      "The 41015 th iteration gives loss of 0.14244183069793273\n",
      "The 41016 th iteration gives loss of 0.14244050082532073\n",
      "The 41017 th iteration gives loss of 0.14243917098285774\n",
      "The 41018 th iteration gives loss of 0.14243784117053754\n",
      "The 41019 th iteration gives loss of 0.14243651138835456\n",
      "The 41020 th iteration gives loss of 0.14243518163631336\n",
      "The 41021 th iteration gives loss of 0.14243385191441058\n",
      "The 41022 th iteration gives loss of 0.14243252222263686\n",
      "The 41023 th iteration gives loss of 0.14243119256098435\n",
      "The 41024 th iteration gives loss of 0.14242986292946308\n",
      "The 41025 th iteration gives loss of 0.14242853332806277\n",
      "The 41026 th iteration gives loss of 0.1424272037567821\n",
      "The 41027 th iteration gives loss of 0.14242587421562075\n",
      "The 41028 th iteration gives loss of 0.14242454470456842\n",
      "The 41029 th iteration gives loss of 0.14242321522362736\n",
      "The 41030 th iteration gives loss of 0.1424218857727932\n",
      "The 41031 th iteration gives loss of 0.1424205563520608\n",
      "The 41032 th iteration gives loss of 0.1424192269614349\n",
      "The 41033 th iteration gives loss of 0.14241789760089735\n",
      "The 41034 th iteration gives loss of 0.14241656827045826\n",
      "The 41035 th iteration gives loss of 0.14241523897011285\n",
      "The 41036 th iteration gives loss of 0.1424139096998501\n",
      "The 41037 th iteration gives loss of 0.14241258045967767\n",
      "The 41038 th iteration gives loss of 0.14241125124958262\n",
      "The 41039 th iteration gives loss of 0.14240992206956563\n",
      "The 41040 th iteration gives loss of 0.14240859291962712\n",
      "The 41041 th iteration gives loss of 0.14240726379976548\n",
      "The 41042 th iteration gives loss of 0.14240593470996815\n",
      "The 41043 th iteration gives loss of 0.142404605650233\n",
      "The 41044 th iteration gives loss of 0.1424032766205658\n",
      "The 41045 th iteration gives loss of 0.14240194762095634\n",
      "The 41046 th iteration gives loss of 0.14240061865140588\n",
      "The 41047 th iteration gives loss of 0.14239928971190902\n",
      "The 41048 th iteration gives loss of 0.142397960802466\n",
      "The 41049 th iteration gives loss of 0.14239663192306679\n",
      "The 41050 th iteration gives loss of 0.14239530307371387\n",
      "The 41051 th iteration gives loss of 0.1423939742544047\n",
      "The 41052 th iteration gives loss of 0.14239264546512703\n",
      "The 41053 th iteration gives loss of 0.14239131670588565\n",
      "The 41054 th iteration gives loss of 0.14238998797668168\n",
      "The 41055 th iteration gives loss of 0.142388659277508\n",
      "The 41056 th iteration gives loss of 0.14238733060835715\n",
      "The 41057 th iteration gives loss of 0.14238600196922696\n",
      "The 41058 th iteration gives loss of 0.14238467336012176\n",
      "The 41059 th iteration gives loss of 0.14238334478102924\n",
      "The 41060 th iteration gives loss of 0.14238201623194852\n",
      "The 41061 th iteration gives loss of 0.1423806877128784\n",
      "The 41062 th iteration gives loss of 0.14237935922381975\n",
      "The 41063 th iteration gives loss of 0.14237803076475877\n",
      "The 41064 th iteration gives loss of 0.14237670233570407\n",
      "The 41065 th iteration gives loss of 0.14237537393664346\n",
      "The 41066 th iteration gives loss of 0.14237404556758446\n",
      "The 41067 th iteration gives loss of 0.1423727172285075\n",
      "The 41068 th iteration gives loss of 0.1423713889194329\n",
      "The 41069 th iteration gives loss of 0.1423700606403337\n",
      "The 41070 th iteration gives loss of 0.14236873239121792\n",
      "The 41071 th iteration gives loss of 0.14236740417208552\n",
      "The 41072 th iteration gives loss of 0.14236607598292725\n",
      "The 41073 th iteration gives loss of 0.14236474782373848\n",
      "The 41074 th iteration gives loss of 0.14236341969452382\n",
      "The 41075 th iteration gives loss of 0.14236209159527724\n",
      "The 41076 th iteration gives loss of 0.14236076352599372\n",
      "The 41077 th iteration gives loss of 0.14235943548666619\n",
      "The 41078 th iteration gives loss of 0.14235810747730482\n",
      "The 41079 th iteration gives loss of 0.14235677949789555\n",
      "The 41080 th iteration gives loss of 0.14235545154844376\n",
      "The 41081 th iteration gives loss of 0.14235412362893318\n",
      "The 41082 th iteration gives loss of 0.14235279573936938\n",
      "The 41083 th iteration gives loss of 0.14235146787974948\n",
      "The 41084 th iteration gives loss of 0.1423501400500649\n",
      "The 41085 th iteration gives loss of 0.14234881225031878\n",
      "The 41086 th iteration gives loss of 0.14234748448050616\n",
      "The 41087 th iteration gives loss of 0.14234615674062578\n",
      "The 41088 th iteration gives loss of 0.1423448290306699\n",
      "The 41089 th iteration gives loss of 0.14234350135063875\n",
      "The 41090 th iteration gives loss of 0.1423421737005253\n",
      "The 41091 th iteration gives loss of 0.14234084608033265\n",
      "The 41092 th iteration gives loss of 0.1423395184900523\n",
      "The 41093 th iteration gives loss of 0.14233819092968564\n",
      "The 41094 th iteration gives loss of 0.14233686339923265\n",
      "The 41095 th iteration gives loss of 0.14233553589867726\n",
      "The 41096 th iteration gives loss of 0.14233420842803024\n",
      "The 41097 th iteration gives loss of 0.14233288098728206\n",
      "The 41098 th iteration gives loss of 0.14233155357642302\n",
      "The 41099 th iteration gives loss of 0.1423302261954664\n",
      "The 41100 th iteration gives loss of 0.1423288988443925\n",
      "The 41101 th iteration gives loss of 0.1423275715232079\n",
      "The 41102 th iteration gives loss of 0.14232624423190549\n",
      "The 41103 th iteration gives loss of 0.14232491697048358\n",
      "The 41104 th iteration gives loss of 0.14232358973894726\n",
      "The 41105 th iteration gives loss of 0.14232226253728092\n",
      "The 41106 th iteration gives loss of 0.1423209353654801\n",
      "The 41107 th iteration gives loss of 0.14231960822355477\n",
      "The 41108 th iteration gives loss of 0.1423182811114922\n",
      "The 41109 th iteration gives loss of 0.1423169540292901\n",
      "The 41110 th iteration gives loss of 0.14231562697695432\n",
      "The 41111 th iteration gives loss of 0.14231429995446773\n",
      "The 41112 th iteration gives loss of 0.14231297296184028\n",
      "The 41113 th iteration gives loss of 0.1423116459990581\n",
      "The 41114 th iteration gives loss of 0.14231031906612657\n",
      "The 41115 th iteration gives loss of 0.14230899216303936\n",
      "The 41116 th iteration gives loss of 0.14230766528979089\n",
      "The 41117 th iteration gives loss of 0.142306338446386\n",
      "The 41118 th iteration gives loss of 0.14230501163280465\n",
      "The 41119 th iteration gives loss of 0.1423036848490612\n",
      "The 41120 th iteration gives loss of 0.14230235809514585\n",
      "The 41121 th iteration gives loss of 0.14230103137105635\n",
      "The 41122 th iteration gives loss of 0.14229970467678565\n",
      "The 41123 th iteration gives loss of 0.1422983780123347\n",
      "The 41124 th iteration gives loss of 0.14229705137770707\n",
      "The 41125 th iteration gives loss of 0.1422957247728852\n",
      "The 41126 th iteration gives loss of 0.14229439819788225\n",
      "The 41127 th iteration gives loss of 0.1422930716526789\n",
      "The 41128 th iteration gives loss of 0.14229174513728207\n",
      "The 41129 th iteration gives loss of 0.14229041865168526\n",
      "The 41130 th iteration gives loss of 0.14228909219588962\n",
      "The 41131 th iteration gives loss of 0.14228776576988475\n",
      "The 41132 th iteration gives loss of 0.14228643937366964\n",
      "The 41133 th iteration gives loss of 0.14228511300725333\n",
      "The 41134 th iteration gives loss of 0.14228378667061678\n",
      "The 41135 th iteration gives loss of 0.14228246036376077\n",
      "The 41136 th iteration gives loss of 0.14228113408668908\n",
      "The 41137 th iteration gives loss of 0.14227980783939004\n",
      "The 41138 th iteration gives loss of 0.142278481621858\n",
      "The 41139 th iteration gives loss of 0.14227715543410246\n",
      "The 41140 th iteration gives loss of 0.14227582927611646\n",
      "The 41141 th iteration gives loss of 0.14227450314789913\n",
      "The 41142 th iteration gives loss of 0.1422731770494405\n",
      "The 41143 th iteration gives loss of 0.14227185098073106\n",
      "The 41144 th iteration gives loss of 0.14227052494179004\n",
      "The 41145 th iteration gives loss of 0.14226919893259116\n",
      "The 41146 th iteration gives loss of 0.14226787295314391\n",
      "The 41147 th iteration gives loss of 0.14226654700344762\n",
      "The 41148 th iteration gives loss of 0.14226522108348855\n",
      "The 41149 th iteration gives loss of 0.14226389519326696\n",
      "The 41150 th iteration gives loss of 0.1422625693327887\n",
      "The 41151 th iteration gives loss of 0.14226124350204206\n",
      "The 41152 th iteration gives loss of 0.14225991770102725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 41153 th iteration gives loss of 0.14225859192973142\n",
      "The 41154 th iteration gives loss of 0.14225726618816692\n",
      "The 41155 th iteration gives loss of 0.14225594047633122\n",
      "The 41156 th iteration gives loss of 0.14225461479420248\n",
      "The 41157 th iteration gives loss of 0.14225328914179913\n",
      "The 41158 th iteration gives loss of 0.1422519635190966\n",
      "The 41159 th iteration gives loss of 0.14225063792611384\n",
      "The 41160 th iteration gives loss of 0.14224931236283297\n",
      "The 41161 th iteration gives loss of 0.14224798682925446\n",
      "The 41162 th iteration gives loss of 0.1422466613253804\n",
      "The 41163 th iteration gives loss of 0.14224533585119628\n",
      "The 41164 th iteration gives loss of 0.14224401040670873\n",
      "The 41165 th iteration gives loss of 0.14224268499191178\n",
      "The 41166 th iteration gives loss of 0.14224135960680773\n",
      "The 41167 th iteration gives loss of 0.1422400342513935\n",
      "The 41168 th iteration gives loss of 0.1422387089256481\n",
      "The 41169 th iteration gives loss of 0.1422373836295901\n",
      "The 41170 th iteration gives loss of 0.1422360583632054\n",
      "The 41171 th iteration gives loss of 0.14223473312648977\n",
      "The 41172 th iteration gives loss of 0.1422334079194494\n",
      "The 41173 th iteration gives loss of 0.14223208274206942\n",
      "The 41174 th iteration gives loss of 0.1422307575943585\n",
      "The 41175 th iteration gives loss of 0.14222943247630826\n",
      "The 41176 th iteration gives loss of 0.14222810738791716\n",
      "The 41177 th iteration gives loss of 0.14222678232917654\n",
      "The 41178 th iteration gives loss of 0.1422254573000922\n",
      "The 41179 th iteration gives loss of 0.14222413230064834\n",
      "The 41180 th iteration gives loss of 0.14222280733086115\n",
      "The 41181 th iteration gives loss of 0.14222148239070734\n",
      "The 41182 th iteration gives loss of 0.1422201574801935\n",
      "The 41183 th iteration gives loss of 0.1422188325993153\n",
      "The 41184 th iteration gives loss of 0.14221750774807415\n",
      "The 41185 th iteration gives loss of 0.14221618292645913\n",
      "The 41186 th iteration gives loss of 0.14221485813447143\n",
      "The 41187 th iteration gives loss of 0.14221353337211323\n",
      "The 41188 th iteration gives loss of 0.14221220863937803\n",
      "The 41189 th iteration gives loss of 0.1422108839362584\n",
      "The 41190 th iteration gives loss of 0.14220955926274775\n",
      "The 41191 th iteration gives loss of 0.14220823461884688\n",
      "The 41192 th iteration gives loss of 0.14220691000456773\n",
      "The 41193 th iteration gives loss of 0.1422055854198884\n",
      "The 41194 th iteration gives loss of 0.14220426086481272\n",
      "The 41195 th iteration gives loss of 0.14220293633933467\n",
      "The 41196 th iteration gives loss of 0.1422016118434572\n",
      "The 41197 th iteration gives loss of 0.14220028737717336\n",
      "The 41198 th iteration gives loss of 0.1421989629404791\n",
      "The 41199 th iteration gives loss of 0.14219763853337167\n",
      "The 41200 th iteration gives loss of 0.14219631415584355\n",
      "The 41201 th iteration gives loss of 0.14219498980790196\n",
      "The 41202 th iteration gives loss of 0.14219366548953938\n",
      "The 41203 th iteration gives loss of 0.14219234120076651\n",
      "The 41204 th iteration gives loss of 0.14219101694155126\n",
      "The 41205 th iteration gives loss of 0.14218969271190257\n",
      "The 41206 th iteration gives loss of 0.14218836851182806\n",
      "The 41207 th iteration gives loss of 0.14218704434131124\n",
      "The 41208 th iteration gives loss of 0.14218572020035736\n",
      "The 41209 th iteration gives loss of 0.1421843960889668\n",
      "The 41210 th iteration gives loss of 0.1421830720071247\n",
      "The 41211 th iteration gives loss of 0.14218174795483696\n",
      "The 41212 th iteration gives loss of 0.14218042393209063\n",
      "The 41213 th iteration gives loss of 0.14217909993890152\n",
      "The 41214 th iteration gives loss of 0.1421777759752506\n",
      "The 41215 th iteration gives loss of 0.1421764520411326\n",
      "The 41216 th iteration gives loss of 0.1421751281365583\n",
      "The 41217 th iteration gives loss of 0.14217380426151147\n",
      "The 41218 th iteration gives loss of 0.14217248041599653\n",
      "The 41219 th iteration gives loss of 0.14217115660000892\n",
      "The 41220 th iteration gives loss of 0.14216983281354956\n",
      "The 41221 th iteration gives loss of 0.14216850905660905\n",
      "The 41222 th iteration gives loss of 0.14216718532918748\n",
      "The 41223 th iteration gives loss of 0.1421658616312819\n",
      "The 41224 th iteration gives loss of 0.14216453796288442\n",
      "The 41225 th iteration gives loss of 0.14216321432399817\n",
      "The 41226 th iteration gives loss of 0.14216189071461927\n",
      "The 41227 th iteration gives loss of 0.14216056713474315\n",
      "The 41228 th iteration gives loss of 0.1421592435843692\n",
      "The 41229 th iteration gives loss of 0.14215792006348268\n",
      "The 41230 th iteration gives loss of 0.14215659657209764\n",
      "The 41231 th iteration gives loss of 0.14215527311020748\n",
      "The 41232 th iteration gives loss of 0.14215394967780123\n",
      "The 41233 th iteration gives loss of 0.14215262627488445\n",
      "The 41234 th iteration gives loss of 0.14215130290145084\n",
      "The 41235 th iteration gives loss of 0.14214997955749165\n",
      "The 41236 th iteration gives loss of 0.1421486562430003\n",
      "The 41237 th iteration gives loss of 0.14214733295799278\n",
      "The 41238 th iteration gives loss of 0.14214600970245014\n",
      "The 41239 th iteration gives loss of 0.1421446864763838\n",
      "The 41240 th iteration gives loss of 0.14214336327976962\n",
      "The 41241 th iteration gives loss of 0.14214204011261927\n",
      "The 41242 th iteration gives loss of 0.1421407169749306\n",
      "The 41243 th iteration gives loss of 0.14213939386669408\n",
      "The 41244 th iteration gives loss of 0.1421380707879104\n",
      "The 41245 th iteration gives loss of 0.14213674773858115\n",
      "The 41246 th iteration gives loss of 0.14213542471869184\n",
      "The 41247 th iteration gives loss of 0.14213410172825308\n",
      "The 41248 th iteration gives loss of 0.14213277876724995\n",
      "The 41249 th iteration gives loss of 0.14213145583568715\n",
      "The 41250 th iteration gives loss of 0.14213013293354412\n",
      "The 41251 th iteration gives loss of 0.14212881006084335\n",
      "The 41252 th iteration gives loss of 0.14212748721756185\n",
      "The 41253 th iteration gives loss of 0.14212616440371395\n",
      "The 41254 th iteration gives loss of 0.14212484161928468\n",
      "The 41255 th iteration gives loss of 0.1421235188642738\n",
      "The 41256 th iteration gives loss of 0.1421221961386779\n",
      "The 41257 th iteration gives loss of 0.1421208734424974\n",
      "The 41258 th iteration gives loss of 0.14211955077572871\n",
      "The 41259 th iteration gives loss of 0.1421182281383651\n",
      "The 41260 th iteration gives loss of 0.14211690553040027\n",
      "The 41261 th iteration gives loss of 0.14211558295185103\n",
      "The 41262 th iteration gives loss of 0.14211426040268402\n",
      "The 41263 th iteration gives loss of 0.14211293788291807\n",
      "The 41264 th iteration gives loss of 0.1421116153925472\n",
      "The 41265 th iteration gives loss of 0.14211029293156452\n",
      "The 41266 th iteration gives loss of 0.1421089704999665\n",
      "The 41267 th iteration gives loss of 0.1421076480977513\n",
      "The 41268 th iteration gives loss of 0.1421063257249132\n",
      "The 41269 th iteration gives loss of 0.142105003381448\n",
      "The 41270 th iteration gives loss of 0.14210368106736362\n",
      "The 41271 th iteration gives loss of 0.14210235878265276\n",
      "The 41272 th iteration gives loss of 0.14210103652730052\n",
      "The 41273 th iteration gives loss of 0.14209971430132276\n",
      "The 41274 th iteration gives loss of 0.1420983921046985\n",
      "The 41275 th iteration gives loss of 0.14209706993744184\n",
      "The 41276 th iteration gives loss of 0.14209574779953485\n",
      "The 41277 th iteration gives loss of 0.14209442569098288\n",
      "The 41278 th iteration gives loss of 0.14209310361177727\n",
      "The 41279 th iteration gives loss of 0.14209178156192293\n",
      "The 41280 th iteration gives loss of 0.14209045954141333\n",
      "The 41281 th iteration gives loss of 0.14208913755024327\n",
      "The 41282 th iteration gives loss of 0.14208781558841524\n",
      "The 41283 th iteration gives loss of 0.14208649365591922\n",
      "The 41284 th iteration gives loss of 0.14208517175275248\n",
      "The 41285 th iteration gives loss of 0.1420838498789186\n",
      "The 41286 th iteration gives loss of 0.14208252803441138\n",
      "The 41287 th iteration gives loss of 0.14208120621922504\n",
      "The 41288 th iteration gives loss of 0.14207988443336397\n",
      "The 41289 th iteration gives loss of 0.1420785626768123\n",
      "The 41290 th iteration gives loss of 0.1420772409495797\n",
      "The 41291 th iteration gives loss of 0.14207591925165391\n",
      "The 41292 th iteration gives loss of 0.14207459758303817\n",
      "The 41293 th iteration gives loss of 0.14207327594372562\n",
      "The 41294 th iteration gives loss of 0.142071954333716\n",
      "The 41295 th iteration gives loss of 0.14207063275300427\n",
      "The 41296 th iteration gives loss of 0.1420693112015946\n",
      "The 41297 th iteration gives loss of 0.14206798967947354\n",
      "The 41298 th iteration gives loss of 0.14206666818664732\n",
      "The 41299 th iteration gives loss of 0.14206534672310686\n",
      "The 41300 th iteration gives loss of 0.1420640252888518\n",
      "The 41301 th iteration gives loss of 0.14206270388387607\n",
      "The 41302 th iteration gives loss of 0.14206138250817604\n",
      "The 41303 th iteration gives loss of 0.14206006116175385\n",
      "The 41304 th iteration gives loss of 0.14205873984460443\n",
      "The 41305 th iteration gives loss of 0.14205741855672135\n",
      "The 41306 th iteration gives loss of 0.14205609729810825\n",
      "The 41307 th iteration gives loss of 0.14205477606876132\n",
      "The 41308 th iteration gives loss of 0.14205345486867207\n",
      "The 41309 th iteration gives loss of 0.14205213369783817\n",
      "The 41310 th iteration gives loss of 0.14205081255625657\n",
      "The 41311 th iteration gives loss of 0.14204949144392806\n",
      "The 41312 th iteration gives loss of 0.142048170360855\n",
      "The 41313 th iteration gives loss of 0.14204684930701728\n",
      "The 41314 th iteration gives loss of 0.14204552828243444\n",
      "The 41315 th iteration gives loss of 0.14204420728708114\n",
      "The 41316 th iteration gives loss of 0.1420428863209677\n",
      "The 41317 th iteration gives loss of 0.14204156538409024\n",
      "The 41318 th iteration gives loss of 0.14204024447643848\n",
      "The 41319 th iteration gives loss of 0.14203892359801745\n",
      "The 41320 th iteration gives loss of 0.14203760274881813\n",
      "The 41321 th iteration gives loss of 0.14203628192884846\n",
      "The 41322 th iteration gives loss of 0.14203496113809277\n",
      "The 41323 th iteration gives loss of 0.14203364037655225\n",
      "The 41324 th iteration gives loss of 0.14203231964422566\n",
      "The 41325 th iteration gives loss of 0.14203099894110455\n",
      "The 41326 th iteration gives loss of 0.14202967826719787\n",
      "The 41327 th iteration gives loss of 0.14202835762249144\n",
      "The 41328 th iteration gives loss of 0.14202703700699001\n",
      "The 41329 th iteration gives loss of 0.14202571642068024\n",
      "The 41330 th iteration gives loss of 0.14202439586356064\n",
      "The 41331 th iteration gives loss of 0.14202307533564373\n",
      "The 41332 th iteration gives loss of 0.14202175483691654\n",
      "The 41333 th iteration gives loss of 0.14202043436737105\n",
      "The 41334 th iteration gives loss of 0.14201911392700914\n",
      "The 41335 th iteration gives loss of 0.14201779351582883\n",
      "The 41336 th iteration gives loss of 0.14201647313382473\n",
      "The 41337 th iteration gives loss of 0.14201515278099708\n",
      "The 41338 th iteration gives loss of 0.14201383245733873\n",
      "The 41339 th iteration gives loss of 0.14201251216284733\n",
      "The 41340 th iteration gives loss of 0.1420111918975262\n",
      "The 41341 th iteration gives loss of 0.14200987166136408\n",
      "The 41342 th iteration gives loss of 0.14200855145436636\n",
      "The 41343 th iteration gives loss of 0.14200723127651427\n",
      "The 41344 th iteration gives loss of 0.14200591112782324\n",
      "The 41345 th iteration gives loss of 0.1420045910082875\n",
      "The 41346 th iteration gives loss of 0.14200327091789525\n",
      "The 41347 th iteration gives loss of 0.142001950856642\n",
      "The 41348 th iteration gives loss of 0.14200063082453898\n",
      "The 41349 th iteration gives loss of 0.1419993108215701\n",
      "The 41350 th iteration gives loss of 0.14199799084774123\n",
      "The 41351 th iteration gives loss of 0.14199667090304532\n",
      "The 41352 th iteration gives loss of 0.14199535098746685\n",
      "The 41353 th iteration gives loss of 0.14199403110102243\n",
      "The 41354 th iteration gives loss of 0.14199271124370527\n",
      "The 41355 th iteration gives loss of 0.141991391415505\n",
      "The 41356 th iteration gives loss of 0.14199007161643118\n",
      "The 41357 th iteration gives loss of 0.14198875184646598\n",
      "The 41358 th iteration gives loss of 0.14198743210561\n",
      "The 41359 th iteration gives loss of 0.14198611239386963\n",
      "The 41360 th iteration gives loss of 0.1419847927112326\n",
      "The 41361 th iteration gives loss of 0.14198347305770467\n",
      "The 41362 th iteration gives loss of 0.14198215343327225\n",
      "The 41363 th iteration gives loss of 0.14198083383793414\n",
      "The 41364 th iteration gives loss of 0.1419795142716979\n",
      "The 41365 th iteration gives loss of 0.14197819473454526\n",
      "The 41366 th iteration gives loss of 0.1419768752264823\n",
      "The 41367 th iteration gives loss of 0.1419755557475133\n",
      "The 41368 th iteration gives loss of 0.141974236297613\n",
      "The 41369 th iteration gives loss of 0.14197291687680588\n",
      "The 41370 th iteration gives loss of 0.1419715974850634\n",
      "The 41371 th iteration gives loss of 0.14197027812241192\n",
      "The 41372 th iteration gives loss of 0.14196895878881452\n",
      "The 41373 th iteration gives loss of 0.1419676394842919\n",
      "The 41374 th iteration gives loss of 0.14196632020883657\n",
      "The 41375 th iteration gives loss of 0.14196500096244014\n",
      "The 41376 th iteration gives loss of 0.1419636817451043\n",
      "The 41377 th iteration gives loss of 0.14196236255681635\n",
      "The 41378 th iteration gives loss of 0.14196104339759116\n",
      "The 41379 th iteration gives loss of 0.14195972426741546\n",
      "The 41380 th iteration gives loss of 0.14195840516628622\n",
      "The 41381 th iteration gives loss of 0.1419570860941961\n",
      "The 41382 th iteration gives loss of 0.1419557670511606\n",
      "The 41383 th iteration gives loss of 0.14195444803715462\n",
      "The 41384 th iteration gives loss of 0.14195312905218743\n",
      "The 41385 th iteration gives loss of 0.14195181009624622\n",
      "The 41386 th iteration gives loss of 0.14195049116934197\n",
      "The 41387 th iteration gives loss of 0.14194917227145856\n",
      "The 41388 th iteration gives loss of 0.14194785340259697\n",
      "The 41389 th iteration gives loss of 0.14194653456276518\n",
      "The 41390 th iteration gives loss of 0.14194521575194483\n",
      "The 41391 th iteration gives loss of 0.141943896970141\n",
      "The 41392 th iteration gives loss of 0.14194257821735162\n",
      "The 41393 th iteration gives loss of 0.14194125949356667\n",
      "The 41394 th iteration gives loss of 0.1419399407987946\n",
      "The 41395 th iteration gives loss of 0.14193862213302044\n",
      "The 41396 th iteration gives loss of 0.1419373034962508\n",
      "The 41397 th iteration gives loss of 0.14193598488847933\n",
      "The 41398 th iteration gives loss of 0.14193466630970017\n",
      "The 41399 th iteration gives loss of 0.14193334775990826\n",
      "The 41400 th iteration gives loss of 0.14193202923910936\n",
      "The 41401 th iteration gives loss of 0.14193071074728605\n",
      "The 41402 th iteration gives loss of 0.14192939228446141\n",
      "The 41403 th iteration gives loss of 0.1419280738506108\n",
      "The 41404 th iteration gives loss of 0.14192675544573138\n",
      "The 41405 th iteration gives loss of 0.14192543706982966\n",
      "The 41406 th iteration gives loss of 0.14192411872290325\n",
      "The 41407 th iteration gives loss of 0.14192280040494293\n",
      "The 41408 th iteration gives loss of 0.14192148211593805\n",
      "The 41409 th iteration gives loss of 0.14192016385590067\n",
      "The 41410 th iteration gives loss of 0.14191884562482526\n",
      "The 41411 th iteration gives loss of 0.14191752742271027\n",
      "The 41412 th iteration gives loss of 0.14191620924954257\n",
      "The 41413 th iteration gives loss of 0.141914891105329\n",
      "The 41414 th iteration gives loss of 0.1419135729900617\n",
      "The 41415 th iteration gives loss of 0.14191225490373896\n",
      "The 41416 th iteration gives loss of 0.14191093684635328\n",
      "The 41417 th iteration gives loss of 0.14190961881791614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 41418 th iteration gives loss of 0.1419083008184058\n",
      "The 41419 th iteration gives loss of 0.14190698284783357\n",
      "The 41420 th iteration gives loss of 0.14190566490619644\n",
      "The 41421 th iteration gives loss of 0.1419043469934823\n",
      "The 41422 th iteration gives loss of 0.14190302910968347\n",
      "The 41423 th iteration gives loss of 0.1419017112548019\n",
      "The 41424 th iteration gives loss of 0.14190039342885288\n",
      "The 41425 th iteration gives loss of 0.1418990756318149\n",
      "The 41426 th iteration gives loss of 0.14189775786368342\n",
      "The 41427 th iteration gives loss of 0.14189644012446753\n",
      "The 41428 th iteration gives loss of 0.1418951224141631\n",
      "The 41429 th iteration gives loss of 0.141893804732756\n",
      "The 41430 th iteration gives loss of 0.14189248708024726\n",
      "The 41431 th iteration gives loss of 0.14189116945664046\n",
      "The 41432 th iteration gives loss of 0.14188985186192873\n",
      "The 41433 th iteration gives loss of 0.14188853429611445\n",
      "The 41434 th iteration gives loss of 0.1418872167591833\n",
      "The 41435 th iteration gives loss of 0.14188589925113237\n",
      "The 41436 th iteration gives loss of 0.1418845817719724\n",
      "The 41437 th iteration gives loss of 0.14188326432169127\n",
      "The 41438 th iteration gives loss of 0.14188194690028658\n",
      "The 41439 th iteration gives loss of 0.1418806295077527\n",
      "The 41440 th iteration gives loss of 0.1418793121440966\n",
      "The 41441 th iteration gives loss of 0.14187799480931185\n",
      "The 41442 th iteration gives loss of 0.14187667750339128\n",
      "The 41443 th iteration gives loss of 0.1418753602263285\n",
      "The 41444 th iteration gives loss of 0.14187404297812817\n",
      "The 41445 th iteration gives loss of 0.14187272575878104\n",
      "The 41446 th iteration gives loss of 0.14187140856828798\n",
      "The 41447 th iteration gives loss of 0.14187009140665693\n",
      "The 41448 th iteration gives loss of 0.14186877427385752\n",
      "The 41449 th iteration gives loss of 0.14186745716991175\n",
      "The 41450 th iteration gives loss of 0.1418661400948156\n",
      "The 41451 th iteration gives loss of 0.141864823048557\n",
      "The 41452 th iteration gives loss of 0.14186350603112236\n",
      "The 41453 th iteration gives loss of 0.1418621890425306\n",
      "The 41454 th iteration gives loss of 0.14186087208277395\n",
      "The 41455 th iteration gives loss of 0.14185955515183918\n",
      "The 41456 th iteration gives loss of 0.1418582382497317\n",
      "The 41457 th iteration gives loss of 0.14185692137644834\n",
      "The 41458 th iteration gives loss of 0.14185560453198262\n",
      "The 41459 th iteration gives loss of 0.14185428771632708\n",
      "The 41460 th iteration gives loss of 0.1418529709294823\n",
      "The 41461 th iteration gives loss of 0.1418516541714528\n",
      "The 41462 th iteration gives loss of 0.1418503374422422\n",
      "The 41463 th iteration gives loss of 0.1418490207418246\n",
      "The 41464 th iteration gives loss of 0.14184770407021108\n",
      "The 41465 th iteration gives loss of 0.14184638742739877\n",
      "The 41466 th iteration gives loss of 0.1418450708133803\n",
      "The 41467 th iteration gives loss of 0.14184375422815235\n",
      "The 41468 th iteration gives loss of 0.14184243767171623\n",
      "The 41469 th iteration gives loss of 0.14184112114406838\n",
      "The 41470 th iteration gives loss of 0.14183980464520424\n",
      "The 41471 th iteration gives loss of 0.14183848817511957\n",
      "The 41472 th iteration gives loss of 0.1418371717338172\n",
      "The 41473 th iteration gives loss of 0.14183585532129037\n",
      "The 41474 th iteration gives loss of 0.14183453893753237\n",
      "The 41475 th iteration gives loss of 0.1418332225825454\n",
      "The 41476 th iteration gives loss of 0.14183190625632908\n",
      "The 41477 th iteration gives loss of 0.14183058995887504\n",
      "The 41478 th iteration gives loss of 0.1418292736901764\n",
      "The 41479 th iteration gives loss of 0.14182795745024374\n",
      "The 41480 th iteration gives loss of 0.14182664123906258\n",
      "The 41481 th iteration gives loss of 0.14182532505663806\n",
      "The 41482 th iteration gives loss of 0.14182400890295613\n",
      "The 41483 th iteration gives loss of 0.14182269277802498\n",
      "The 41484 th iteration gives loss of 0.14182137668183747\n",
      "The 41485 th iteration gives loss of 0.14182006061439154\n",
      "The 41486 th iteration gives loss of 0.1418187445756824\n",
      "The 41487 th iteration gives loss of 0.14181742856570387\n",
      "The 41488 th iteration gives loss of 0.14181611258446838\n",
      "The 41489 th iteration gives loss of 0.14181479663195407\n",
      "The 41490 th iteration gives loss of 0.1418134807081716\n",
      "The 41491 th iteration gives loss of 0.14181216481310738\n",
      "The 41492 th iteration gives loss of 0.14181084894676282\n",
      "The 41493 th iteration gives loss of 0.14180953310913808\n",
      "The 41494 th iteration gives loss of 0.14180821730023618\n",
      "The 41495 th iteration gives loss of 0.14180690152003506\n",
      "The 41496 th iteration gives loss of 0.14180558576854596\n",
      "The 41497 th iteration gives loss of 0.14180427004576238\n",
      "The 41498 th iteration gives loss of 0.14180295435168097\n",
      "The 41499 th iteration gives loss of 0.14180163868629805\n",
      "The 41500 th iteration gives loss of 0.14180032304962925\n",
      "The 41501 th iteration gives loss of 0.14179900744164023\n",
      "The 41502 th iteration gives loss of 0.1417976918623442\n",
      "The 41503 th iteration gives loss of 0.14179637631173297\n",
      "The 41504 th iteration gives loss of 0.14179506078981696\n",
      "The 41505 th iteration gives loss of 0.14179374529657854\n",
      "The 41506 th iteration gives loss of 0.14179242983202234\n",
      "The 41507 th iteration gives loss of 0.14179111439613834\n",
      "The 41508 th iteration gives loss of 0.14178979898893784\n",
      "The 41509 th iteration gives loss of 0.14178848361040536\n",
      "The 41510 th iteration gives loss of 0.1417871682605528\n",
      "The 41511 th iteration gives loss of 0.1417858529393462\n",
      "The 41512 th iteration gives loss of 0.14178453764680357\n",
      "The 41513 th iteration gives loss of 0.14178322238292823\n",
      "The 41514 th iteration gives loss of 0.14178190714771097\n",
      "The 41515 th iteration gives loss of 0.14178059194114337\n",
      "The 41516 th iteration gives loss of 0.14177927676322857\n",
      "The 41517 th iteration gives loss of 0.1417779616139613\n",
      "The 41518 th iteration gives loss of 0.14177664649334282\n",
      "The 41519 th iteration gives loss of 0.14177533140136883\n",
      "The 41520 th iteration gives loss of 0.14177401633803027\n",
      "The 41521 th iteration gives loss of 0.14177270130332925\n",
      "The 41522 th iteration gives loss of 0.1417713862972627\n",
      "The 41523 th iteration gives loss of 0.141770071319829\n",
      "The 41524 th iteration gives loss of 0.1417687563710216\n",
      "The 41525 th iteration gives loss of 0.14176744145084452\n",
      "The 41526 th iteration gives loss of 0.14176612655928467\n",
      "The 41527 th iteration gives loss of 0.1417648116963381\n",
      "The 41528 th iteration gives loss of 0.14176349686202028\n",
      "The 41529 th iteration gives loss of 0.14176218205631916\n",
      "The 41530 th iteration gives loss of 0.14176086727922038\n",
      "The 41531 th iteration gives loss of 0.14175955253072936\n",
      "The 41532 th iteration gives loss of 0.14175823781084385\n",
      "The 41533 th iteration gives loss of 0.1417569231195702\n",
      "The 41534 th iteration gives loss of 0.1417556084568821\n",
      "The 41535 th iteration gives loss of 0.14175429382280028\n",
      "The 41536 th iteration gives loss of 0.14175297921730481\n",
      "The 41537 th iteration gives loss of 0.14175166464039904\n",
      "The 41538 th iteration gives loss of 0.1417503500920957\n",
      "The 41539 th iteration gives loss of 0.14174903557237187\n",
      "The 41540 th iteration gives loss of 0.14174772108122305\n",
      "The 41541 th iteration gives loss of 0.14174640661866042\n",
      "The 41542 th iteration gives loss of 0.1417450921846715\n",
      "The 41543 th iteration gives loss of 0.141743777779262\n",
      "The 41544 th iteration gives loss of 0.14174246340241628\n",
      "The 41545 th iteration gives loss of 0.14174114905413857\n",
      "The 41546 th iteration gives loss of 0.1417398347344251\n",
      "The 41547 th iteration gives loss of 0.14173852044327634\n",
      "The 41548 th iteration gives loss of 0.1417372061806886\n",
      "The 41549 th iteration gives loss of 0.14173589194665082\n",
      "The 41550 th iteration gives loss of 0.14173457774117595\n",
      "The 41551 th iteration gives loss of 0.1417332635642516\n",
      "The 41552 th iteration gives loss of 0.14173194941586656\n",
      "The 41553 th iteration gives loss of 0.14173063529603436\n",
      "The 41554 th iteration gives loss of 0.14172932120474407\n",
      "The 41555 th iteration gives loss of 0.1417280071419894\n",
      "The 41556 th iteration gives loss of 0.14172669310777034\n",
      "The 41557 th iteration gives loss of 0.1417253791020891\n",
      "The 41558 th iteration gives loss of 0.14172406512494076\n",
      "The 41559 th iteration gives loss of 0.14172275117631053\n",
      "The 41560 th iteration gives loss of 0.14172143725620728\n",
      "The 41561 th iteration gives loss of 0.14172012336463513\n",
      "The 41562 th iteration gives loss of 0.14171880950157761\n",
      "The 41563 th iteration gives loss of 0.1417174956670348\n",
      "The 41564 th iteration gives loss of 0.14171618186100615\n",
      "The 41565 th iteration gives loss of 0.14171486808348785\n",
      "The 41566 th iteration gives loss of 0.14171355433448085\n",
      "The 41567 th iteration gives loss of 0.14171224061397697\n",
      "The 41568 th iteration gives loss of 0.14171092692197776\n",
      "The 41569 th iteration gives loss of 0.14170961325847303\n",
      "The 41570 th iteration gives loss of 0.1417082996234677\n",
      "The 41571 th iteration gives loss of 0.1417069860169549\n",
      "The 41572 th iteration gives loss of 0.14170567243893095\n",
      "The 41573 th iteration gives loss of 0.14170435888939562\n",
      "The 41574 th iteration gives loss of 0.14170304536834888\n",
      "The 41575 th iteration gives loss of 0.14170173187578222\n",
      "The 41576 th iteration gives loss of 0.1417004184116972\n",
      "The 41577 th iteration gives loss of 0.14169910497608454\n",
      "The 41578 th iteration gives loss of 0.14169779156894663\n",
      "The 41579 th iteration gives loss of 0.1416964781902766\n",
      "The 41580 th iteration gives loss of 0.14169516484008418\n",
      "The 41581 th iteration gives loss of 0.1416938515183531\n",
      "The 41582 th iteration gives loss of 0.14169253822507955\n",
      "The 41583 th iteration gives loss of 0.14169122496026718\n",
      "The 41584 th iteration gives loss of 0.1416899117239121\n",
      "The 41585 th iteration gives loss of 0.1416885985160094\n",
      "The 41586 th iteration gives loss of 0.14168728533656036\n",
      "The 41587 th iteration gives loss of 0.14168597218555146\n",
      "The 41588 th iteration gives loss of 0.14168465906299943\n",
      "The 41589 th iteration gives loss of 0.14168334596888585\n",
      "The 41590 th iteration gives loss of 0.1416820329032091\n",
      "The 41591 th iteration gives loss of 0.14168071986596747\n",
      "The 41592 th iteration gives loss of 0.14167940685716698\n",
      "The 41593 th iteration gives loss of 0.14167809387679456\n",
      "The 41594 th iteration gives loss of 0.141676780924844\n",
      "The 41595 th iteration gives loss of 0.14167546800132744\n",
      "The 41596 th iteration gives loss of 0.1416741551062259\n",
      "The 41597 th iteration gives loss of 0.14167284223954282\n",
      "The 41598 th iteration gives loss of 0.1416715294012753\n",
      "The 41599 th iteration gives loss of 0.141670216591427\n",
      "The 41600 th iteration gives loss of 0.14166890380999425\n",
      "The 41601 th iteration gives loss of 0.14166759105696136\n",
      "The 41602 th iteration gives loss of 0.1416662783323355\n",
      "The 41603 th iteration gives loss of 0.14166496563611589\n",
      "The 41604 th iteration gives loss of 0.14166365296829306\n",
      "The 41605 th iteration gives loss of 0.14166234032886582\n",
      "The 41606 th iteration gives loss of 0.14166102771783864\n",
      "The 41607 th iteration gives loss of 0.141659715135198\n",
      "The 41608 th iteration gives loss of 0.14165840258093942\n",
      "The 41609 th iteration gives loss of 0.1416570900550787\n",
      "The 41610 th iteration gives loss of 0.14165577755759434\n",
      "The 41611 th iteration gives loss of 0.14165446508849058\n",
      "The 41612 th iteration gives loss of 0.14165315264775816\n",
      "The 41613 th iteration gives loss of 0.14165184023540497\n",
      "The 41614 th iteration gives loss of 0.1416505278514188\n",
      "The 41615 th iteration gives loss of 0.14164921549581339\n",
      "The 41616 th iteration gives loss of 0.14164790316856865\n",
      "The 41617 th iteration gives loss of 0.14164659086967554\n",
      "The 41618 th iteration gives loss of 0.14164527859915624\n",
      "The 41619 th iteration gives loss of 0.14164396635698406\n",
      "The 41620 th iteration gives loss of 0.14164265414317323\n",
      "The 41621 th iteration gives loss of 0.14164134195770706\n",
      "The 41622 th iteration gives loss of 0.1416400298005918\n",
      "The 41623 th iteration gives loss of 0.1416387176718214\n",
      "The 41624 th iteration gives loss of 0.14163740557139706\n",
      "The 41625 th iteration gives loss of 0.1416360934993146\n",
      "The 41626 th iteration gives loss of 0.1416347814555664\n",
      "The 41627 th iteration gives loss of 0.14163346944015984\n",
      "The 41628 th iteration gives loss of 0.14163215745307203\n",
      "The 41629 th iteration gives loss of 0.1416308454943139\n",
      "The 41630 th iteration gives loss of 0.1416295335638869\n",
      "The 41631 th iteration gives loss of 0.14162822166178807\n",
      "The 41632 th iteration gives loss of 0.1416269097879986\n",
      "The 41633 th iteration gives loss of 0.1416255979425323\n",
      "The 41634 th iteration gives loss of 0.14162428612537953\n",
      "The 41635 th iteration gives loss of 0.14162297433653284\n",
      "The 41636 th iteration gives loss of 0.14162166257600067\n",
      "The 41637 th iteration gives loss of 0.14162035084378083\n",
      "The 41638 th iteration gives loss of 0.1416190391398547\n",
      "The 41639 th iteration gives loss of 0.14161772746423737\n",
      "The 41640 th iteration gives loss of 0.1416164158169073\n",
      "The 41641 th iteration gives loss of 0.14161510419787773\n",
      "The 41642 th iteration gives loss of 0.14161379260714382\n",
      "The 41643 th iteration gives loss of 0.14161248104469903\n",
      "The 41644 th iteration gives loss of 0.14161116951053723\n",
      "The 41645 th iteration gives loss of 0.14160985800466075\n",
      "The 41646 th iteration gives loss of 0.14160854652706112\n",
      "The 41647 th iteration gives loss of 0.14160723507773834\n",
      "The 41648 th iteration gives loss of 0.1416059236566985\n",
      "The 41649 th iteration gives loss of 0.14160461226392432\n",
      "The 41650 th iteration gives loss of 0.1416033008994252\n",
      "The 41651 th iteration gives loss of 0.14160198956318693\n",
      "The 41652 th iteration gives loss of 0.14160067825521425\n",
      "The 41653 th iteration gives loss of 0.141599366975504\n",
      "The 41654 th iteration gives loss of 0.1415980557240555\n",
      "The 41655 th iteration gives loss of 0.14159674450086343\n",
      "The 41656 th iteration gives loss of 0.14159543330590835\n",
      "The 41657 th iteration gives loss of 0.14159412213921407\n",
      "The 41658 th iteration gives loss of 0.1415928110007678\n",
      "The 41659 th iteration gives loss of 0.141591499890562\n",
      "The 41660 th iteration gives loss of 0.14159018880859817\n",
      "The 41661 th iteration gives loss of 0.14158887775486848\n",
      "The 41662 th iteration gives loss of 0.14158756672938458\n",
      "The 41663 th iteration gives loss of 0.14158625573212402\n",
      "The 41664 th iteration gives loss of 0.14158494476310193\n",
      "The 41665 th iteration gives loss of 0.14158363382230255\n",
      "The 41666 th iteration gives loss of 0.1415823229097286\n",
      "The 41667 th iteration gives loss of 0.14158101202537116\n",
      "The 41668 th iteration gives loss of 0.14157970116924187\n",
      "The 41669 th iteration gives loss of 0.1415783903413214\n",
      "The 41670 th iteration gives loss of 0.14157707954161633\n",
      "The 41671 th iteration gives loss of 0.14157576877011607\n",
      "The 41672 th iteration gives loss of 0.1415744580268315\n",
      "The 41673 th iteration gives loss of 0.14157314731174678\n",
      "The 41674 th iteration gives loss of 0.14157183662486578\n",
      "The 41675 th iteration gives loss of 0.14157052596618833\n",
      "The 41676 th iteration gives loss of 0.14156921533570324\n",
      "The 41677 th iteration gives loss of 0.14156790473341122\n",
      "The 41678 th iteration gives loss of 0.14156659415931114\n",
      "The 41679 th iteration gives loss of 0.14156528361339485\n",
      "The 41680 th iteration gives loss of 0.14156397309566537\n",
      "The 41681 th iteration gives loss of 0.14156266260612146\n",
      "The 41682 th iteration gives loss of 0.14156135214475327\n",
      "The 41683 th iteration gives loss of 0.14156004171156653\n",
      "The 41684 th iteration gives loss of 0.14155873130654845\n",
      "The 41685 th iteration gives loss of 0.1415574209297014\n",
      "The 41686 th iteration gives loss of 0.1415561105810232\n",
      "The 41687 th iteration gives loss of 0.14155480026051812\n",
      "The 41688 th iteration gives loss of 0.14155348996816838\n",
      "The 41689 th iteration gives loss of 0.14155217970397527\n",
      "The 41690 th iteration gives loss of 0.14155086946794534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 41691 th iteration gives loss of 0.14154955926006754\n",
      "The 41692 th iteration gives loss of 0.1415482490803382\n",
      "The 41693 th iteration gives loss of 0.1415469389287665\n",
      "The 41694 th iteration gives loss of 0.14154562880533197\n",
      "The 41695 th iteration gives loss of 0.14154431871004025\n",
      "The 41696 th iteration gives loss of 0.14154300864288696\n",
      "The 41697 th iteration gives loss of 0.14154169860387236\n",
      "The 41698 th iteration gives loss of 0.14154038859299575\n",
      "The 41699 th iteration gives loss of 0.1415390786102578\n",
      "The 41700 th iteration gives loss of 0.14153776865564838\n",
      "The 41701 th iteration gives loss of 0.14153645872915532\n",
      "The 41702 th iteration gives loss of 0.1415351488307829\n",
      "The 41703 th iteration gives loss of 0.14153383896053606\n",
      "The 41704 th iteration gives loss of 0.1415325291184018\n",
      "The 41705 th iteration gives loss of 0.14153121930439858\n",
      "The 41706 th iteration gives loss of 0.14152990951849037\n",
      "The 41707 th iteration gives loss of 0.14152859976070323\n",
      "The 41708 th iteration gives loss of 0.14152729003101855\n",
      "The 41709 th iteration gives loss of 0.1415259803294372\n",
      "The 41710 th iteration gives loss of 0.14152467065595542\n",
      "The 41711 th iteration gives loss of 0.14152336101057097\n",
      "The 41712 th iteration gives loss of 0.1415220513932902\n",
      "The 41713 th iteration gives loss of 0.1415207418040982\n",
      "The 41714 th iteration gives loss of 0.1415194322429897\n",
      "The 41715 th iteration gives loss of 0.14151812270998074\n",
      "The 41716 th iteration gives loss of 0.1415168132050445\n",
      "The 41717 th iteration gives loss of 0.14151550372819166\n",
      "The 41718 th iteration gives loss of 0.14151419427942177\n",
      "The 41719 th iteration gives loss of 0.14151288485872676\n",
      "The 41720 th iteration gives loss of 0.14151157546610263\n",
      "The 41721 th iteration gives loss of 0.1415102661015449\n",
      "The 41722 th iteration gives loss of 0.1415089567650636\n",
      "The 41723 th iteration gives loss of 0.1415076474566456\n",
      "The 41724 th iteration gives loss of 0.1415063381762915\n",
      "The 41725 th iteration gives loss of 0.141505028923986\n",
      "The 41726 th iteration gives loss of 0.14150371969974515\n",
      "The 41727 th iteration gives loss of 0.14150241050355616\n",
      "The 41728 th iteration gives loss of 0.14150110133541455\n",
      "The 41729 th iteration gives loss of 0.14149979219532596\n",
      "The 41730 th iteration gives loss of 0.14149848308328605\n",
      "The 41731 th iteration gives loss of 0.14149717399927852\n",
      "The 41732 th iteration gives loss of 0.1414958649433144\n",
      "The 41733 th iteration gives loss of 0.14149455591538793\n",
      "The 41734 th iteration gives loss of 0.14149324691550474\n",
      "The 41735 th iteration gives loss of 0.14149193794364834\n",
      "The 41736 th iteration gives loss of 0.14149062899980877\n",
      "The 41737 th iteration gives loss of 0.14148932008400456\n",
      "The 41738 th iteration gives loss of 0.14148801119622242\n",
      "The 41739 th iteration gives loss of 0.14148670233645896\n",
      "The 41740 th iteration gives loss of 0.14148539350471212\n",
      "The 41741 th iteration gives loss of 0.1414840847009843\n",
      "The 41742 th iteration gives loss of 0.14148277592526867\n",
      "The 41743 th iteration gives loss of 0.1414814671775575\n",
      "The 41744 th iteration gives loss of 0.1414801584578515\n",
      "The 41745 th iteration gives loss of 0.141478849766154\n",
      "The 41746 th iteration gives loss of 0.1414775411024549\n",
      "The 41747 th iteration gives loss of 0.14147623246675306\n",
      "The 41748 th iteration gives loss of 0.1414749238590501\n",
      "The 41749 th iteration gives loss of 0.14147361527933877\n",
      "The 41750 th iteration gives loss of 0.14147230672761246\n",
      "The 41751 th iteration gives loss of 0.14147099820387973\n",
      "The 41752 th iteration gives loss of 0.14146968970812243\n",
      "The 41753 th iteration gives loss of 0.14146838124035602\n",
      "The 41754 th iteration gives loss of 0.14146707280056608\n",
      "The 41755 th iteration gives loss of 0.1414657643887373\n",
      "The 41756 th iteration gives loss of 0.14146445600490348\n",
      "The 41757 th iteration gives loss of 0.14146314764903173\n",
      "The 41758 th iteration gives loss of 0.14146183932112505\n",
      "The 41759 th iteration gives loss of 0.14146053102118225\n",
      "The 41760 th iteration gives loss of 0.14145922274920106\n",
      "The 41761 th iteration gives loss of 0.14145791450517975\n",
      "The 41762 th iteration gives loss of 0.14145660628911752\n",
      "The 41763 th iteration gives loss of 0.1414552981010073\n",
      "The 41764 th iteration gives loss of 0.14145398994085076\n",
      "The 41765 th iteration gives loss of 0.14145268180863851\n",
      "The 41766 th iteration gives loss of 0.14145137370437308\n",
      "The 41767 th iteration gives loss of 0.14145006562805296\n",
      "The 41768 th iteration gives loss of 0.14144875757967462\n",
      "The 41769 th iteration gives loss of 0.1414474495592161\n",
      "The 41770 th iteration gives loss of 0.14144614156670765\n",
      "The 41771 th iteration gives loss of 0.14144483360212226\n",
      "The 41772 th iteration gives loss of 0.1414435256654768\n",
      "The 41773 th iteration gives loss of 0.14144221775675023\n",
      "The 41774 th iteration gives loss of 0.1414409098759493\n",
      "The 41775 th iteration gives loss of 0.141439602023071\n",
      "The 41776 th iteration gives loss of 0.14143829419810727\n",
      "The 41777 th iteration gives loss of 0.1414369864010601\n",
      "The 41778 th iteration gives loss of 0.14143567863192294\n",
      "The 41779 th iteration gives loss of 0.14143437089069016\n",
      "The 41780 th iteration gives loss of 0.1414330631773737\n",
      "The 41781 th iteration gives loss of 0.14143175549195477\n",
      "The 41782 th iteration gives loss of 0.14143044783443454\n",
      "The 41783 th iteration gives loss of 0.141429140204818\n",
      "The 41784 th iteration gives loss of 0.1414278326031003\n",
      "The 41785 th iteration gives loss of 0.14142652502926395\n",
      "The 41786 th iteration gives loss of 0.14142521748332784\n",
      "The 41787 th iteration gives loss of 0.14142390996527665\n",
      "The 41788 th iteration gives loss of 0.14142260247511132\n",
      "The 41789 th iteration gives loss of 0.14142129501282225\n",
      "The 41790 th iteration gives loss of 0.14141998757841504\n",
      "The 41791 th iteration gives loss of 0.14141868017188886\n",
      "The 41792 th iteration gives loss of 0.14141737279322913\n",
      "The 41793 th iteration gives loss of 0.1414160654424378\n",
      "The 41794 th iteration gives loss of 0.14141475811952198\n",
      "The 41795 th iteration gives loss of 0.1414134508244711\n",
      "The 41796 th iteration gives loss of 0.1414121435572817\n",
      "The 41797 th iteration gives loss of 0.14141083631794307\n",
      "The 41798 th iteration gives loss of 0.14140952910647359\n",
      "The 41799 th iteration gives loss of 0.1414082219228546\n",
      "The 41800 th iteration gives loss of 0.14140691476708425\n",
      "The 41801 th iteration gives loss of 0.14140560763916493\n",
      "The 41802 th iteration gives loss of 0.1414043005390936\n",
      "The 41803 th iteration gives loss of 0.1414029934668576\n",
      "The 41804 th iteration gives loss of 0.14140168642246792\n",
      "The 41805 th iteration gives loss of 0.14140037940592184\n",
      "The 41806 th iteration gives loss of 0.1413990724171961\n",
      "The 41807 th iteration gives loss of 0.14139776545632257\n",
      "The 41808 th iteration gives loss of 0.1413964585232613\n",
      "The 41809 th iteration gives loss of 0.14139515161803648\n",
      "The 41810 th iteration gives loss of 0.14139384474062927\n",
      "The 41811 th iteration gives loss of 0.14139253789104983\n",
      "The 41812 th iteration gives loss of 0.14139123106928397\n",
      "The 41813 th iteration gives loss of 0.14138992427533673\n",
      "The 41814 th iteration gives loss of 0.14138861750919807\n",
      "The 41815 th iteration gives loss of 0.14138731077087427\n",
      "The 41816 th iteration gives loss of 0.14138600406035276\n",
      "The 41817 th iteration gives loss of 0.14138469737763595\n",
      "The 41818 th iteration gives loss of 0.14138339072272477\n",
      "The 41819 th iteration gives loss of 0.14138208409561392\n",
      "The 41820 th iteration gives loss of 0.14138077749629557\n",
      "The 41821 th iteration gives loss of 0.14137947092477204\n",
      "The 41822 th iteration gives loss of 0.14137816438104317\n",
      "The 41823 th iteration gives loss of 0.1413768578650998\n",
      "The 41824 th iteration gives loss of 0.14137555137694116\n",
      "The 41825 th iteration gives loss of 0.1413742449165625\n",
      "The 41826 th iteration gives loss of 0.14137293848396726\n",
      "The 41827 th iteration gives loss of 0.14137163207914794\n",
      "The 41828 th iteration gives loss of 0.14137032570210184\n",
      "The 41829 th iteration gives loss of 0.14136901935283622\n",
      "The 41830 th iteration gives loss of 0.141367713031329\n",
      "The 41831 th iteration gives loss of 0.1413664067375921\n",
      "The 41832 th iteration gives loss of 0.1413651004716196\n",
      "The 41833 th iteration gives loss of 0.14136379423341208\n",
      "The 41834 th iteration gives loss of 0.14136248802296078\n",
      "The 41835 th iteration gives loss of 0.14136118184025917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 41836 th iteration gives loss of 0.14135987568531286\n",
      "The 41837 th iteration gives loss of 0.14135856955811466\n",
      "The 41838 th iteration gives loss of 0.14135726345867078\n",
      "The 41839 th iteration gives loss of 0.14135595738696669\n",
      "The 41840 th iteration gives loss of 0.14135465134299702\n",
      "The 41841 th iteration gives loss of 0.14135334532677404\n",
      "The 41842 th iteration gives loss of 0.14135203933828297\n",
      "The 41843 th iteration gives loss of 0.14135073337753257\n",
      "The 41844 th iteration gives loss of 0.14134942744450715\n",
      "The 41845 th iteration gives loss of 0.14134812153921275\n",
      "The 41846 th iteration gives loss of 0.14134681566163945\n",
      "The 41847 th iteration gives loss of 0.14134550981179084\n",
      "The 41848 th iteration gives loss of 0.14134420398966716\n",
      "The 41849 th iteration gives loss of 0.1413428981952462\n",
      "The 41850 th iteration gives loss of 0.14134159242855915\n",
      "The 41851 th iteration gives loss of 0.1413402866895738\n",
      "The 41852 th iteration gives loss of 0.14133898097829403\n",
      "The 41853 th iteration gives loss of 0.1413376752947219\n",
      "The 41854 th iteration gives loss of 0.141336369638864\n",
      "The 41855 th iteration gives loss of 0.14133506401069384\n",
      "The 41856 th iteration gives loss of 0.14133375841022297\n",
      "The 41857 th iteration gives loss of 0.14133245283745752\n",
      "The 41858 th iteration gives loss of 0.14133114729237659\n",
      "The 41859 th iteration gives loss of 0.14132984177498703\n",
      "The 41860 th iteration gives loss of 0.14132853628527767\n",
      "The 41861 th iteration gives loss of 0.14132723082326065\n",
      "The 41862 th iteration gives loss of 0.14132592538892322\n",
      "The 41863 th iteration gives loss of 0.14132461998226448\n",
      "The 41864 th iteration gives loss of 0.14132331460328593\n",
      "The 41865 th iteration gives loss of 0.1413220092519759\n",
      "The 41866 th iteration gives loss of 0.14132070392833537\n",
      "The 41867 th iteration gives loss of 0.14131939863236484\n",
      "The 41868 th iteration gives loss of 0.1413180933640609\n",
      "The 41869 th iteration gives loss of 0.14131678812341458\n",
      "The 41870 th iteration gives loss of 0.1413154829104276\n",
      "The 41871 th iteration gives loss of 0.14131417772511207\n",
      "The 41872 th iteration gives loss of 0.14131287256743363\n",
      "The 41873 th iteration gives loss of 0.14131156743741652\n",
      "The 41874 th iteration gives loss of 0.14131026233504185\n",
      "The 41875 th iteration gives loss of 0.1413089572603204\n",
      "The 41876 th iteration gives loss of 0.1413076522132359\n",
      "The 41877 th iteration gives loss of 0.14130634719379956\n",
      "The 41878 th iteration gives loss of 0.14130504220199144\n",
      "The 41879 th iteration gives loss of 0.14130373723782547\n",
      "The 41880 th iteration gives loss of 0.14130243230129425\n",
      "The 41881 th iteration gives loss of 0.14130112739238385\n",
      "The 41882 th iteration gives loss of 0.14129982251110546\n",
      "The 41883 th iteration gives loss of 0.14129851765744914\n",
      "The 41884 th iteration gives loss of 0.1412972128314192\n",
      "The 41885 th iteration gives loss of 0.14129590803300696\n",
      "The 41886 th iteration gives loss of 0.141294603262212\n",
      "The 41887 th iteration gives loss of 0.1412932985190308\n",
      "The 41888 th iteration gives loss of 0.1412919938034571\n",
      "The 41889 th iteration gives loss of 0.1412906891154892\n",
      "The 41890 th iteration gives loss of 0.14128938445513653\n",
      "The 41891 th iteration gives loss of 0.14128807982237665\n",
      "The 41892 th iteration gives loss of 0.14128677521721505\n",
      "The 41893 th iteration gives loss of 0.14128547063966207\n",
      "The 41894 th iteration gives loss of 0.14128416608970124\n",
      "The 41895 th iteration gives loss of 0.14128286156733263\n",
      "The 41896 th iteration gives loss of 0.1412815570725493\n",
      "The 41897 th iteration gives loss of 0.1412802526053493\n",
      "The 41898 th iteration gives loss of 0.14127894816573444\n",
      "The 41899 th iteration gives loss of 0.1412776437537067\n",
      "The 41900 th iteration gives loss of 0.14127633936924638\n",
      "The 41901 th iteration gives loss of 0.14127503501237496\n",
      "The 41902 th iteration gives loss of 0.14127373068306381\n",
      "The 41903 th iteration gives loss of 0.1412724263813361\n",
      "The 41904 th iteration gives loss of 0.14127112210716403\n",
      "The 41905 th iteration gives loss of 0.14126981786056747\n",
      "The 41906 th iteration gives loss of 0.1412685136415208\n",
      "The 41907 th iteration gives loss of 0.14126720945004262\n",
      "The 41908 th iteration gives loss of 0.14126590528611616\n",
      "The 41909 th iteration gives loss of 0.14126460114974296\n",
      "The 41910 th iteration gives loss of 0.14126329704092863\n",
      "The 41911 th iteration gives loss of 0.14126199295965428\n",
      "The 41912 th iteration gives loss of 0.14126068890592838\n",
      "The 41913 th iteration gives loss of 0.14125938487974363\n",
      "The 41914 th iteration gives loss of 0.1412580808810995\n",
      "The 41915 th iteration gives loss of 0.14125677690999341\n",
      "The 41916 th iteration gives loss of 0.1412554729664256\n",
      "The 41917 th iteration gives loss of 0.14125416905038563\n",
      "The 41918 th iteration gives loss of 0.1412528651618823\n",
      "The 41919 th iteration gives loss of 0.14125156130090238\n",
      "The 41920 th iteration gives loss of 0.14125025746745157\n",
      "The 41921 th iteration gives loss of 0.14124895366151874\n",
      "The 41922 th iteration gives loss of 0.1412476498830978\n",
      "The 41923 th iteration gives loss of 0.14124634613219256\n",
      "The 41924 th iteration gives loss of 0.14124504240880967\n",
      "The 41925 th iteration gives loss of 0.14124373871293097\n",
      "The 41926 th iteration gives loss of 0.14124243504455847\n",
      "The 41927 th iteration gives loss of 0.14124113140370148\n",
      "The 41928 th iteration gives loss of 0.1412398277903398\n",
      "The 41929 th iteration gives loss of 0.1412385242044778\n",
      "The 41930 th iteration gives loss of 0.14123722064611322\n",
      "The 41931 th iteration gives loss of 0.14123591711524014\n",
      "The 41932 th iteration gives loss of 0.1412346136118669\n",
      "The 41933 th iteration gives loss of 0.14123331013598317\n",
      "The 41934 th iteration gives loss of 0.14123200668757976\n",
      "The 41935 th iteration gives loss of 0.14123070326666132\n",
      "The 41936 th iteration gives loss of 0.14122939987322697\n",
      "The 41937 th iteration gives loss of 0.14122809650726414\n",
      "The 41938 th iteration gives loss of 0.141226793168779\n",
      "The 41939 th iteration gives loss of 0.1412254898577763\n",
      "The 41940 th iteration gives loss of 0.14122418657423694\n",
      "The 41941 th iteration gives loss of 0.14122288331815736\n",
      "The 41942 th iteration gives loss of 0.1412215800895493\n",
      "The 41943 th iteration gives loss of 0.14122027688840616\n",
      "The 41944 th iteration gives loss of 0.1412189737147216\n",
      "The 41945 th iteration gives loss of 0.1412176705684891\n",
      "The 41946 th iteration gives loss of 0.14121636744971783\n",
      "The 41947 th iteration gives loss of 0.14121506435839126\n",
      "The 41948 th iteration gives loss of 0.14121376129452443\n",
      "The 41949 th iteration gives loss of 0.1412124582580934\n",
      "The 41950 th iteration gives loss of 0.14121115524910816\n",
      "The 41951 th iteration gives loss of 0.14120985226755958\n",
      "The 41952 th iteration gives loss of 0.14120854931345578\n",
      "The 41953 th iteration gives loss of 0.14120724638677756\n",
      "The 41954 th iteration gives loss of 0.14120594348753907\n",
      "The 41955 th iteration gives loss of 0.14120464061572693\n",
      "The 41956 th iteration gives loss of 0.1412033377713496\n",
      "The 41957 th iteration gives loss of 0.14120203495439268\n",
      "The 41958 th iteration gives loss of 0.14120073216484852\n",
      "The 41959 th iteration gives loss of 0.14119942940273242\n",
      "The 41960 th iteration gives loss of 0.14119812666803558\n",
      "The 41961 th iteration gives loss of 0.1411968239607513\n",
      "The 41962 th iteration gives loss of 0.14119552128087318\n",
      "The 41963 th iteration gives loss of 0.14119421862840675\n",
      "The 41964 th iteration gives loss of 0.14119291600334175\n",
      "The 41965 th iteration gives loss of 0.14119161340568476\n",
      "The 41966 th iteration gives loss of 0.14119031083542835\n",
      "The 41967 th iteration gives loss of 0.14118900829256678\n",
      "The 41968 th iteration gives loss of 0.14118770577710232\n",
      "The 41969 th iteration gives loss of 0.14118640328902474\n",
      "The 41970 th iteration gives loss of 0.14118510082834076\n",
      "The 41971 th iteration gives loss of 0.14118379839505088\n",
      "The 41972 th iteration gives loss of 0.14118249598913876\n",
      "The 41973 th iteration gives loss of 0.1411811936106053\n",
      "The 41974 th iteration gives loss of 0.1411798912594542\n",
      "The 41975 th iteration gives loss of 0.14117858893567592\n",
      "The 41976 th iteration gives loss of 0.14117728663927398\n",
      "The 41977 th iteration gives loss of 0.14117598437024328\n",
      "The 41978 th iteration gives loss of 0.14117468212858042\n",
      "The 41979 th iteration gives loss of 0.14117337991428225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 41980 th iteration gives loss of 0.14117207772735307\n",
      "The 41981 th iteration gives loss of 0.14117077556777205\n",
      "The 41982 th iteration gives loss of 0.1411694734355579\n",
      "The 41983 th iteration gives loss of 0.1411681713306903\n",
      "The 41984 th iteration gives loss of 0.14116686925318736\n",
      "The 41985 th iteration gives loss of 0.14116556720302215\n",
      "The 41986 th iteration gives loss of 0.14116426518021014\n",
      "The 41987 th iteration gives loss of 0.14116296318473526\n",
      "The 41988 th iteration gives loss of 0.14116166121660695\n",
      "The 41989 th iteration gives loss of 0.14116035927582496\n",
      "The 41990 th iteration gives loss of 0.14115905736237339\n",
      "The 41991 th iteration gives loss of 0.14115775547624843\n",
      "The 41992 th iteration gives loss of 0.14115645361745957\n",
      "The 41993 th iteration gives loss of 0.1411551517859981\n",
      "The 41994 th iteration gives loss of 0.14115384998186745\n",
      "The 41995 th iteration gives loss of 0.1411525482050452\n",
      "The 41996 th iteration gives loss of 0.14115124645555568\n",
      "The 41997 th iteration gives loss of 0.1411499447333825\n",
      "The 41998 th iteration gives loss of 0.14114864303851934\n",
      "The 41999 th iteration gives loss of 0.1411473413709666\n",
      "The 42000 th iteration gives loss of 0.14114603973073134\n",
      "The 42001 th iteration gives loss of 0.1411447381178006\n",
      "The 42002 th iteration gives loss of 0.1411434365321734\n",
      "The 42003 th iteration gives loss of 0.14114213497384134\n",
      "The 42004 th iteration gives loss of 0.14114083344281472\n",
      "The 42005 th iteration gives loss of 0.14113953193908402\n",
      "The 42006 th iteration gives loss of 0.141138230462651\n",
      "The 42007 th iteration gives loss of 0.14113692901350075\n",
      "The 42008 th iteration gives loss of 0.14113562759163875\n",
      "The 42009 th iteration gives loss of 0.1411343261970693\n",
      "The 42010 th iteration gives loss of 0.1411330248297727\n",
      "The 42011 th iteration gives loss of 0.1411317234897578\n",
      "The 42012 th iteration gives loss of 0.14113042217702254\n",
      "The 42013 th iteration gives loss of 0.14112912089156585\n",
      "The 42014 th iteration gives loss of 0.1411278196333728\n",
      "The 42015 th iteration gives loss of 0.14112651840246107\n",
      "The 42016 th iteration gives loss of 0.14112521719880575\n",
      "The 42017 th iteration gives loss of 0.14112391602241975\n",
      "The 42018 th iteration gives loss of 0.14112261487329314\n",
      "The 42019 th iteration gives loss of 0.1411213137514255\n",
      "The 42020 th iteration gives loss of 0.14112001265681842\n",
      "The 42021 th iteration gives loss of 0.1411187115894532\n",
      "The 42022 th iteration gives loss of 0.14111741054934612\n",
      "The 42023 th iteration gives loss of 0.14111610953649636\n",
      "The 42024 th iteration gives loss of 0.1411148085508793\n",
      "The 42025 th iteration gives loss of 0.14111350759251565\n",
      "The 42026 th iteration gives loss of 0.1411122066613868\n",
      "The 42027 th iteration gives loss of 0.14111090575749427\n",
      "The 42028 th iteration gives loss of 0.14110960488083205\n",
      "The 42029 th iteration gives loss of 0.14110830403140667\n",
      "The 42030 th iteration gives loss of 0.14110700320921218\n",
      "The 42031 th iteration gives loss of 0.14110570241424228\n",
      "The 42032 th iteration gives loss of 0.14110440164650423\n",
      "The 42033 th iteration gives loss of 0.14110310090597708\n",
      "The 42034 th iteration gives loss of 0.14110180019266988\n",
      "The 42035 th iteration gives loss of 0.14110049950658504\n",
      "The 42036 th iteration gives loss of 0.14109919884771435\n",
      "The 42037 th iteration gives loss of 0.14109789821604973\n",
      "The 42038 th iteration gives loss of 0.14109659761159266\n",
      "The 42039 th iteration gives loss of 0.14109529703434304\n",
      "The 42040 th iteration gives loss of 0.1410939964842998\n",
      "The 42041 th iteration gives loss of 0.14109269596145715\n",
      "The 42042 th iteration gives loss of 0.14109139546580687\n",
      "The 42043 th iteration gives loss of 0.14109009499735228\n",
      "The 42044 th iteration gives loss of 0.14108879455609122\n",
      "The 42045 th iteration gives loss of 0.14108749414202085\n",
      "The 42046 th iteration gives loss of 0.14108619375514006\n",
      "The 42047 th iteration gives loss of 0.14108489339543825\n",
      "The 42048 th iteration gives loss of 0.1410835930629258\n",
      "The 42049 th iteration gives loss of 0.14108229275759054\n",
      "The 42050 th iteration gives loss of 0.1410809924794246\n",
      "The 42051 th iteration gives loss of 0.1410796922284417\n",
      "The 42052 th iteration gives loss of 0.1410783920046283\n",
      "The 42053 th iteration gives loss of 0.1410770918079792\n",
      "The 42054 th iteration gives loss of 0.1410757916384974\n",
      "The 42055 th iteration gives loss of 0.14107449149617676\n",
      "The 42056 th iteration gives loss of 0.1410731913810175\n",
      "The 42057 th iteration gives loss of 0.1410718912930203\n",
      "The 42058 th iteration gives loss of 0.14107059123217408\n",
      "The 42059 th iteration gives loss of 0.14106929119848904\n",
      "The 42060 th iteration gives loss of 0.14106799119194594\n",
      "The 42061 th iteration gives loss of 0.14106669121254978\n",
      "The 42062 th iteration gives loss of 0.14106539126030135\n",
      "The 42063 th iteration gives loss of 0.14106409133519093\n",
      "The 42064 th iteration gives loss of 0.1410627914372274\n",
      "The 42065 th iteration gives loss of 0.14106149156640044\n",
      "The 42066 th iteration gives loss of 0.14106019172270684\n",
      "The 42067 th iteration gives loss of 0.1410588919061385\n",
      "The 42068 th iteration gives loss of 0.141057592116701\n",
      "The 42069 th iteration gives loss of 0.14105629235439565\n",
      "The 42070 th iteration gives loss of 0.1410549926192096\n",
      "The 42071 th iteration gives loss of 0.14105369291114678\n",
      "The 42072 th iteration gives loss of 0.1410523932302\n",
      "The 42073 th iteration gives loss of 0.14105109357636386\n",
      "The 42074 th iteration gives loss of 0.1410497939496495\n",
      "The 42075 th iteration gives loss of 0.14104849435003844\n",
      "The 42076 th iteration gives loss of 0.14104719477754596\n",
      "The 42077 th iteration gives loss of 0.1410458952321571\n",
      "The 42078 th iteration gives loss of 0.14104459571386413\n",
      "The 42079 th iteration gives loss of 0.1410432962226775\n",
      "The 42080 th iteration gives loss of 0.14104199675858692\n",
      "The 42081 th iteration gives loss of 0.14104069732158636\n",
      "The 42082 th iteration gives loss of 0.1410393979116754\n",
      "The 42083 th iteration gives loss of 0.14103809852885946\n",
      "The 42084 th iteration gives loss of 0.14103679917313086\n",
      "The 42085 th iteration gives loss of 0.14103549984448088\n",
      "The 42086 th iteration gives loss of 0.14103420054291663\n",
      "The 42087 th iteration gives loss of 0.1410329012684362\n",
      "The 42088 th iteration gives loss of 0.14103160202103385\n",
      "The 42089 th iteration gives loss of 0.14103030280069226\n",
      "The 42090 th iteration gives loss of 0.14102900360742926\n",
      "The 42091 th iteration gives loss of 0.14102770444123386\n",
      "The 42092 th iteration gives loss of 0.1410264053021014\n",
      "The 42093 th iteration gives loss of 0.14102510619003694\n",
      "The 42094 th iteration gives loss of 0.1410238071050305\n",
      "The 42095 th iteration gives loss of 0.1410225080470757\n",
      "The 42096 th iteration gives loss of 0.14102120901618487\n",
      "The 42097 th iteration gives loss of 0.14101991001234773\n",
      "The 42098 th iteration gives loss of 0.14101861103555607\n",
      "The 42099 th iteration gives loss of 0.14101731208580878\n",
      "The 42100 th iteration gives loss of 0.1410160131631115\n",
      "The 42101 th iteration gives loss of 0.1410147142674505\n",
      "The 42102 th iteration gives loss of 0.1410134153988383\n",
      "The 42103 th iteration gives loss of 0.14101211655726423\n",
      "The 42104 th iteration gives loss of 0.1410108177427129\n",
      "The 42105 th iteration gives loss of 0.14100951895520017\n",
      "The 42106 th iteration gives loss of 0.14100822019471174\n",
      "The 42107 th iteration gives loss of 0.1410069214612492\n",
      "The 42108 th iteration gives loss of 0.14100562275482498\n",
      "The 42109 th iteration gives loss of 0.1410043240754089\n",
      "The 42110 th iteration gives loss of 0.1410030254230054\n",
      "The 42111 th iteration gives loss of 0.14100172679762663\n",
      "The 42112 th iteration gives loss of 0.141000428199266\n",
      "The 42113 th iteration gives loss of 0.14099912962791183\n",
      "The 42114 th iteration gives loss of 0.1409978310835638\n",
      "The 42115 th iteration gives loss of 0.14099653256621997\n",
      "The 42116 th iteration gives loss of 0.14099523407587597\n",
      "The 42117 th iteration gives loss of 0.1409939356125401\n",
      "The 42118 th iteration gives loss of 0.14099263717620164\n",
      "The 42119 th iteration gives loss of 0.1409913387668528\n",
      "The 42120 th iteration gives loss of 0.1409900403845007\n",
      "The 42121 th iteration gives loss of 0.14098874202913964\n",
      "The 42122 th iteration gives loss of 0.14098744370076294\n",
      "The 42123 th iteration gives loss of 0.14098614539937093\n",
      "The 42124 th iteration gives loss of 0.1409848471249573\n",
      "The 42125 th iteration gives loss of 0.14098354887752756\n",
      "The 42126 th iteration gives loss of 0.1409822506570707\n",
      "The 42127 th iteration gives loss of 0.14098095246358308\n",
      "The 42128 th iteration gives loss of 0.1409796542970768\n",
      "The 42129 th iteration gives loss of 0.14097835615753507\n",
      "The 42130 th iteration gives loss of 0.14097705804495828\n",
      "The 42131 th iteration gives loss of 0.1409757599593475\n",
      "The 42132 th iteration gives loss of 0.1409744619007011\n",
      "The 42133 th iteration gives loss of 0.14097316386900968\n",
      "The 42134 th iteration gives loss of 0.14097186586427124\n",
      "The 42135 th iteration gives loss of 0.140970567886487\n",
      "The 42136 th iteration gives loss of 0.14096926993564896\n",
      "The 42137 th iteration gives loss of 0.14096797201177036\n",
      "The 42138 th iteration gives loss of 0.14096667411483058\n",
      "The 42139 th iteration gives loss of 0.14096537624484073\n",
      "The 42140 th iteration gives loss of 0.14096407840177755\n",
      "The 42141 th iteration gives loss of 0.1409627805856613\n",
      "The 42142 th iteration gives loss of 0.1409614827964784\n",
      "The 42143 th iteration gives loss of 0.14096018503422048\n",
      "The 42144 th iteration gives loss of 0.14095888729889852\n",
      "The 42145 th iteration gives loss of 0.14095758959050603\n",
      "The 42146 th iteration gives loss of 0.14095629190902792\n",
      "The 42147 th iteration gives loss of 0.14095499425448405\n",
      "The 42148 th iteration gives loss of 0.14095369662685384\n",
      "The 42149 th iteration gives loss of 0.1409523990261442\n",
      "The 42150 th iteration gives loss of 0.14095110145234277\n",
      "The 42151 th iteration gives loss of 0.14094980390545164\n",
      "The 42152 th iteration gives loss of 0.140948506385471\n",
      "The 42153 th iteration gives loss of 0.14094720889239862\n",
      "The 42154 th iteration gives loss of 0.14094591142623528\n",
      "The 42155 th iteration gives loss of 0.14094461398697014\n",
      "The 42156 th iteration gives loss of 0.1409433165745998\n",
      "The 42157 th iteration gives loss of 0.14094201918912214\n",
      "The 42158 th iteration gives loss of 0.14094072183053513\n",
      "The 42159 th iteration gives loss of 0.14093942449885272\n",
      "The 42160 th iteration gives loss of 0.14093812719404755\n",
      "The 42161 th iteration gives loss of 0.1409368299161292\n",
      "The 42162 th iteration gives loss of 0.1409355326650945\n",
      "The 42163 th iteration gives loss of 0.14093423544094\n",
      "The 42164 th iteration gives loss of 0.14093293824367076\n",
      "The 42165 th iteration gives loss of 0.14093164107326475\n",
      "The 42166 th iteration gives loss of 0.1409303439297366\n",
      "The 42167 th iteration gives loss of 0.1409290468130765\n",
      "The 42168 th iteration gives loss of 0.14092774972328087\n",
      "The 42169 th iteration gives loss of 0.14092645266035322\n",
      "The 42170 th iteration gives loss of 0.14092515562429048\n",
      "The 42171 th iteration gives loss of 0.14092385861507942\n",
      "The 42172 th iteration gives loss of 0.14092256163272587\n",
      "The 42173 th iteration gives loss of 0.14092126467723345\n",
      "The 42174 th iteration gives loss of 0.1409199677485881\n",
      "The 42175 th iteration gives loss of 0.14091867084680124\n",
      "The 42176 th iteration gives loss of 0.14091737397185525\n",
      "The 42177 th iteration gives loss of 0.1409160771237408\n",
      "The 42178 th iteration gives loss of 0.14091478030248272\n",
      "The 42179 th iteration gives loss of 0.14091348350805977\n",
      "The 42180 th iteration gives loss of 0.14091218674046901\n",
      "The 42181 th iteration gives loss of 0.140910889999717\n",
      "The 42182 th iteration gives loss of 0.140909593285792\n",
      "The 42183 th iteration gives loss of 0.14090829659868906\n",
      "The 42184 th iteration gives loss of 0.14090699993842493\n",
      "The 42185 th iteration gives loss of 0.1409057033049715\n",
      "The 42186 th iteration gives loss of 0.14090440669834647\n",
      "The 42187 th iteration gives loss of 0.1409031101185394\n",
      "The 42188 th iteration gives loss of 0.14090181356554513\n",
      "The 42189 th iteration gives loss of 0.14090051703935733\n",
      "The 42190 th iteration gives loss of 0.14089922053999435\n",
      "The 42191 th iteration gives loss of 0.14089792406743448\n",
      "The 42192 th iteration gives loss of 0.14089662762167232\n",
      "The 42193 th iteration gives loss of 0.14089533120272346\n",
      "The 42194 th iteration gives loss of 0.1408940348105593\n",
      "The 42195 th iteration gives loss of 0.14089273844520037\n",
      "The 42196 th iteration gives loss of 0.14089144210663226\n",
      "The 42197 th iteration gives loss of 0.14089014579486736\n",
      "The 42198 th iteration gives loss of 0.1408888495098829\n",
      "The 42199 th iteration gives loss of 0.1408875532516918\n",
      "The 42200 th iteration gives loss of 0.14088625702027283\n",
      "The 42201 th iteration gives loss of 0.14088496081564242\n",
      "The 42202 th iteration gives loss of 0.14088366463778956\n",
      "The 42203 th iteration gives loss of 0.14088236848671815\n",
      "The 42204 th iteration gives loss of 0.14088107236241887\n",
      "The 42205 th iteration gives loss of 0.1408797762648922\n",
      "The 42206 th iteration gives loss of 0.14087848019413501\n",
      "The 42207 th iteration gives loss of 0.14087718415013414\n",
      "The 42208 th iteration gives loss of 0.1408758881329017\n",
      "The 42209 th iteration gives loss of 0.14087459214243628\n",
      "The 42210 th iteration gives loss of 0.14087329617872502\n",
      "The 42211 th iteration gives loss of 0.14087200024177143\n",
      "The 42212 th iteration gives loss of 0.14087070433157067\n",
      "The 42213 th iteration gives loss of 0.14086940844811557\n",
      "The 42214 th iteration gives loss of 0.1408681125914157\n",
      "The 42215 th iteration gives loss of 0.14086681676146098\n",
      "The 42216 th iteration gives loss of 0.14086552095824695\n",
      "The 42217 th iteration gives loss of 0.14086422518176653\n",
      "The 42218 th iteration gives loss of 0.1408629294320328\n",
      "The 42219 th iteration gives loss of 0.1408616337090357\n",
      "The 42220 th iteration gives loss of 0.14086033801276765\n",
      "The 42221 th iteration gives loss of 0.14085904234323024\n",
      "The 42222 th iteration gives loss of 0.14085774670041912\n",
      "The 42223 th iteration gives loss of 0.14085645108432518\n",
      "The 42224 th iteration gives loss of 0.14085515549495947\n",
      "The 42225 th iteration gives loss of 0.14085385993232638\n",
      "The 42226 th iteration gives loss of 0.14085256439639796\n",
      "The 42227 th iteration gives loss of 0.1408512688871817\n",
      "The 42228 th iteration gives loss of 0.14084997340468172\n",
      "The 42229 th iteration gives loss of 0.14084867794889672\n",
      "The 42230 th iteration gives loss of 0.1408473825198108\n",
      "The 42231 th iteration gives loss of 0.14084608711743687\n",
      "The 42232 th iteration gives loss of 0.14084479174175504\n",
      "The 42233 th iteration gives loss of 0.14084349639278093\n",
      "The 42234 th iteration gives loss of 0.14084220107049822\n",
      "The 42235 th iteration gives loss of 0.1408409057749125\n",
      "The 42236 th iteration gives loss of 0.14083961050601465\n",
      "The 42237 th iteration gives loss of 0.14083831526380797\n",
      "The 42238 th iteration gives loss of 0.14083702004828438\n",
      "The 42239 th iteration gives loss of 0.1408357248594501\n",
      "The 42240 th iteration gives loss of 0.14083442969729526\n",
      "The 42241 th iteration gives loss of 0.1408331345618233\n",
      "The 42242 th iteration gives loss of 0.14083183945302033\n",
      "The 42243 th iteration gives loss of 0.14083054437089687\n",
      "The 42244 th iteration gives loss of 0.14082924931543525\n",
      "The 42245 th iteration gives loss of 0.14082795428664807\n",
      "The 42246 th iteration gives loss of 0.14082665928452798\n",
      "The 42247 th iteration gives loss of 0.14082536430907222\n",
      "The 42248 th iteration gives loss of 0.14082406936027395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 42249 th iteration gives loss of 0.14082277443813257\n",
      "The 42250 th iteration gives loss of 0.14082147954264665\n",
      "The 42251 th iteration gives loss of 0.14082018467381271\n",
      "The 42252 th iteration gives loss of 0.14081888983163227\n",
      "The 42253 th iteration gives loss of 0.14081759501610563\n",
      "The 42254 th iteration gives loss of 0.14081630022722147\n",
      "The 42255 th iteration gives loss of 0.14081500546497705\n",
      "The 42256 th iteration gives loss of 0.14081371072936957\n",
      "The 42257 th iteration gives loss of 0.14081241602039649\n",
      "The 42258 th iteration gives loss of 0.14081112133806845\n",
      "The 42259 th iteration gives loss of 0.14080982668236958\n",
      "The 42260 th iteration gives loss of 0.14080853205330313\n",
      "The 42261 th iteration gives loss of 0.14080723745086135\n",
      "The 42262 th iteration gives loss of 0.14080594287503884\n",
      "The 42263 th iteration gives loss of 0.14080464832584985\n",
      "The 42264 th iteration gives loss of 0.14080335380327497\n",
      "The 42265 th iteration gives loss of 0.14080205930731227\n",
      "The 42266 th iteration gives loss of 0.14080076483797213\n",
      "The 42267 th iteration gives loss of 0.1407994703952418\n",
      "The 42268 th iteration gives loss of 0.14079817597911892\n",
      "The 42269 th iteration gives loss of 0.14079688158960776\n",
      "The 42270 th iteration gives loss of 0.14079558722669697\n",
      "The 42271 th iteration gives loss of 0.14079429289038733\n",
      "The 42272 th iteration gives loss of 0.14079299858067515\n",
      "The 42273 th iteration gives loss of 0.14079170429755905\n",
      "The 42274 th iteration gives loss of 0.14079041004104564\n",
      "The 42275 th iteration gives loss of 0.14078911581111658\n",
      "The 42276 th iteration gives loss of 0.14078782160778233\n",
      "The 42277 th iteration gives loss of 0.14078652743102715\n",
      "The 42278 th iteration gives loss of 0.1407852332808588\n",
      "The 42279 th iteration gives loss of 0.14078393915727208\n",
      "The 42280 th iteration gives loss of 0.1407826450602672\n",
      "The 42281 th iteration gives loss of 0.14078135098984082\n",
      "The 42282 th iteration gives loss of 0.14078005694598264\n",
      "The 42283 th iteration gives loss of 0.14077876292869176\n",
      "The 42284 th iteration gives loss of 0.14077746893797824\n",
      "The 42285 th iteration gives loss of 0.14077617497382497\n",
      "The 42286 th iteration gives loss of 0.1407748810362395\n",
      "The 42287 th iteration gives loss of 0.14077358712520532\n",
      "The 42288 th iteration gives loss of 0.14077229324073923\n",
      "The 42289 th iteration gives loss of 0.14077099938283452\n",
      "The 42290 th iteration gives loss of 0.14076970555146967\n",
      "The 42291 th iteration gives loss of 0.14076841174666066\n",
      "The 42292 th iteration gives loss of 0.14076711796839955\n",
      "The 42293 th iteration gives loss of 0.14076582421668463\n",
      "The 42294 th iteration gives loss of 0.14076453049151857\n",
      "The 42295 th iteration gives loss of 0.14076323679288297\n",
      "The 42296 th iteration gives loss of 0.14076194312079146\n",
      "The 42297 th iteration gives loss of 0.14076064947523229\n",
      "The 42298 th iteration gives loss of 0.1407593558562124\n",
      "The 42299 th iteration gives loss of 0.14075806226371668\n",
      "The 42300 th iteration gives loss of 0.14075676869774925\n",
      "The 42301 th iteration gives loss of 0.14075547515831277\n",
      "The 42302 th iteration gives loss of 0.14075418164539488\n",
      "The 42303 th iteration gives loss of 0.14075288815900105\n",
      "The 42304 th iteration gives loss of 0.14075159469911583\n",
      "The 42305 th iteration gives loss of 0.14075030126575427\n",
      "The 42306 th iteration gives loss of 0.14074900785890043\n",
      "The 42307 th iteration gives loss of 0.14074771447855663\n",
      "The 42308 th iteration gives loss of 0.14074642112472263\n",
      "The 42309 th iteration gives loss of 0.14074512779739334\n",
      "The 42310 th iteration gives loss of 0.14074383449657218\n",
      "The 42311 th iteration gives loss of 0.14074254122223986\n",
      "The 42312 th iteration gives loss of 0.14074124797441265\n",
      "The 42313 th iteration gives loss of 0.1407399547530772\n",
      "The 42314 th iteration gives loss of 0.1407386615582407\n",
      "The 42315 th iteration gives loss of 0.14073736838988452\n",
      "The 42316 th iteration gives loss of 0.1407360752480209\n",
      "The 42317 th iteration gives loss of 0.14073478213263974\n",
      "The 42318 th iteration gives loss of 0.1407334890437384\n",
      "The 42319 th iteration gives loss of 0.14073219598132405\n",
      "The 42320 th iteration gives loss of 0.1407309029453868\n",
      "The 42321 th iteration gives loss of 0.14072960993591713\n",
      "The 42322 th iteration gives loss of 0.14072831695291818\n",
      "The 42323 th iteration gives loss of 0.14072702399639514\n",
      "The 42324 th iteration gives loss of 0.14072573106633954\n",
      "The 42325 th iteration gives loss of 0.1407244381627421\n",
      "The 42326 th iteration gives loss of 0.14072314528561142\n",
      "The 42327 th iteration gives loss of 0.14072185243493213\n",
      "The 42328 th iteration gives loss of 0.14072055961071903\n",
      "The 42329 th iteration gives loss of 0.14071926681295996\n",
      "The 42330 th iteration gives loss of 0.14071797404164899\n",
      "The 42331 th iteration gives loss of 0.1407166812967843\n",
      "The 42332 th iteration gives loss of 0.14071538857837662\n",
      "The 42333 th iteration gives loss of 0.1407140958864028\n",
      "The 42334 th iteration gives loss of 0.1407128032208755\n",
      "The 42335 th iteration gives loss of 0.14071151058178571\n",
      "The 42336 th iteration gives loss of 0.14071021796913086\n",
      "The 42337 th iteration gives loss of 0.14070892538291638\n",
      "The 42338 th iteration gives loss of 0.14070763282312726\n",
      "The 42339 th iteration gives loss of 0.14070634028976836\n",
      "The 42340 th iteration gives loss of 0.14070504778284276\n",
      "The 42341 th iteration gives loss of 0.1407037553023339\n",
      "The 42342 th iteration gives loss of 0.14070246284825086\n",
      "The 42343 th iteration gives loss of 0.14070117042058433\n",
      "The 42344 th iteration gives loss of 0.14069987801933068\n",
      "The 42345 th iteration gives loss of 0.14069858564448592\n",
      "The 42346 th iteration gives loss of 0.1406972932960687\n",
      "The 42347 th iteration gives loss of 0.14069600097404833\n",
      "The 42348 th iteration gives loss of 0.14069470867844153\n",
      "The 42349 th iteration gives loss of 0.14069341640923047\n",
      "The 42350 th iteration gives loss of 0.1406921241664295\n",
      "The 42351 th iteration gives loss of 0.14069083195002843\n",
      "The 42352 th iteration gives loss of 0.14068953976001244\n",
      "The 42353 th iteration gives loss of 0.1406882475963913\n",
      "The 42354 th iteration gives loss of 0.14068695545916393\n",
      "The 42355 th iteration gives loss of 0.14068566334832489\n",
      "The 42356 th iteration gives loss of 0.14068437126387756\n",
      "The 42357 th iteration gives loss of 0.1406830792058113\n",
      "The 42358 th iteration gives loss of 0.140681787174123\n",
      "The 42359 th iteration gives loss of 0.14068049516881556\n",
      "The 42360 th iteration gives loss of 0.14067920318988417\n",
      "The 42361 th iteration gives loss of 0.14067791123732268\n",
      "The 42362 th iteration gives loss of 0.14067661931114145\n",
      "The 42363 th iteration gives loss of 0.14067532741132055\n",
      "The 42364 th iteration gives loss of 0.14067403553786628\n",
      "The 42365 th iteration gives loss of 0.1406727436907664\n",
      "The 42366 th iteration gives loss of 0.14067145187004332\n",
      "The 42367 th iteration gives loss of 0.14067016007567112\n",
      "The 42368 th iteration gives loss of 0.14066886830765302\n",
      "The 42369 th iteration gives loss of 0.14066757656599752\n",
      "The 42370 th iteration gives loss of 0.1406662848506885\n",
      "The 42371 th iteration gives loss of 0.14066499316172548\n",
      "The 42372 th iteration gives loss of 0.140663701499107\n",
      "The 42373 th iteration gives loss of 0.14066240986283846\n",
      "The 42374 th iteration gives loss of 0.1406611182529025\n",
      "The 42375 th iteration gives loss of 0.1406598266693095\n",
      "The 42376 th iteration gives loss of 0.14065853511205456\n",
      "The 42377 th iteration gives loss of 0.14065724358111853\n",
      "The 42378 th iteration gives loss of 0.14065595207652515\n",
      "The 42379 th iteration gives loss of 0.14065466059826026\n",
      "The 42380 th iteration gives loss of 0.14065336914631904\n",
      "The 42381 th iteration gives loss of 0.14065207772069888\n",
      "The 42382 th iteration gives loss of 0.1406507863214015\n",
      "The 42383 th iteration gives loss of 0.14064949494842846\n",
      "The 42384 th iteration gives loss of 0.14064820360176353\n",
      "The 42385 th iteration gives loss of 0.14064691228141188\n",
      "The 42386 th iteration gives loss of 0.14064562098737657\n",
      "The 42387 th iteration gives loss of 0.14064432971964771\n",
      "The 42388 th iteration gives loss of 0.14064303847821769\n",
      "The 42389 th iteration gives loss of 0.1406417472630987\n",
      "The 42390 th iteration gives loss of 0.14064045607426798\n",
      "The 42391 th iteration gives loss of 0.1406391649117511\n",
      "The 42392 th iteration gives loss of 0.14063787377551265\n",
      "The 42393 th iteration gives loss of 0.14063658266558343\n",
      "The 42394 th iteration gives loss of 0.14063529158193994\n",
      "The 42395 th iteration gives loss of 0.14063400052458563\n",
      "The 42396 th iteration gives loss of 0.1406327094935137\n",
      "The 42397 th iteration gives loss of 0.14063141848872407\n",
      "The 42398 th iteration gives loss of 0.14063012751021706\n",
      "The 42399 th iteration gives loss of 0.14062883655798875\n",
      "The 42400 th iteration gives loss of 0.1406275456320306\n",
      "The 42401 th iteration gives loss of 0.1406262547323506\n",
      "The 42402 th iteration gives loss of 0.14062496385894133\n",
      "The 42403 th iteration gives loss of 0.14062367301179357\n",
      "The 42404 th iteration gives loss of 0.14062238219092302\n",
      "The 42405 th iteration gives loss of 0.14062109139631265\n",
      "The 42406 th iteration gives loss of 0.14061980062795862\n",
      "The 42407 th iteration gives loss of 0.14061850988586305\n",
      "The 42408 th iteration gives loss of 0.1406172191700218\n",
      "The 42409 th iteration gives loss of 0.14061592848043228\n",
      "The 42410 th iteration gives loss of 0.1406146378170965\n",
      "The 42411 th iteration gives loss of 0.14061334718001095\n",
      "The 42412 th iteration gives loss of 0.14061205656917264\n",
      "The 42413 th iteration gives loss of 0.14061076598456473\n",
      "The 42414 th iteration gives loss of 0.14060947542619975\n",
      "The 42415 th iteration gives loss of 0.1406081848940829\n",
      "The 42416 th iteration gives loss of 0.1406068943882014\n",
      "The 42417 th iteration gives loss of 0.14060560390854637\n",
      "The 42418 th iteration gives loss of 0.14060431345512114\n",
      "The 42419 th iteration gives loss of 0.1406030230279338\n",
      "The 42420 th iteration gives loss of 0.14060173262696227\n",
      "The 42421 th iteration gives loss of 0.14060044225222\n",
      "The 42422 th iteration gives loss of 0.1405991519036932\n",
      "The 42423 th iteration gives loss of 0.14059786158138785\n",
      "The 42424 th iteration gives loss of 0.14059657128529654\n",
      "The 42425 th iteration gives loss of 0.14059528101541877\n",
      "The 42426 th iteration gives loss of 0.1405939907717512\n",
      "The 42427 th iteration gives loss of 0.14059270055429277\n",
      "The 42428 th iteration gives loss of 0.14059141036303394\n",
      "The 42429 th iteration gives loss of 0.14059012019798076\n",
      "The 42430 th iteration gives loss of 0.14058883005913328\n",
      "The 42431 th iteration gives loss of 0.14058753994647782\n",
      "The 42432 th iteration gives loss of 0.14058624986002335\n",
      "The 42433 th iteration gives loss of 0.1405849597997588\n",
      "The 42434 th iteration gives loss of 0.14058366976568412\n",
      "The 42435 th iteration gives loss of 0.14058237975780363\n",
      "The 42436 th iteration gives loss of 0.14058108977610223\n",
      "The 42437 th iteration gives loss of 0.14057979982059268\n",
      "The 42438 th iteration gives loss of 0.14057850989125417\n",
      "The 42439 th iteration gives loss of 0.1405772199880958\n",
      "The 42440 th iteration gives loss of 0.14057593011111713\n",
      "The 42441 th iteration gives loss of 0.14057464026030392\n",
      "The 42442 th iteration gives loss of 0.14057335043567268\n",
      "The 42443 th iteration gives loss of 0.14057206063720534\n",
      "The 42444 th iteration gives loss of 0.1405707708648975\n",
      "The 42445 th iteration gives loss of 0.1405694811187601\n",
      "The 42446 th iteration gives loss of 0.14056819139877613\n",
      "The 42447 th iteration gives loss of 0.14056690170495975\n",
      "The 42448 th iteration gives loss of 0.1405656120372969\n",
      "The 42449 th iteration gives loss of 0.14056432239578487\n",
      "The 42450 th iteration gives loss of 0.14056303278041743\n",
      "The 42451 th iteration gives loss of 0.14056174319120363\n",
      "The 42452 th iteration gives loss of 0.14056045362814168\n",
      "The 42453 th iteration gives loss of 0.14055916409122332\n",
      "The 42454 th iteration gives loss of 0.14055787458043192\n",
      "The 42455 th iteration gives loss of 0.14055658509579017\n",
      "The 42456 th iteration gives loss of 0.14055529563728045\n",
      "The 42457 th iteration gives loss of 0.14055400620490652\n",
      "The 42458 th iteration gives loss of 0.14055271679866652\n",
      "The 42459 th iteration gives loss of 0.14055142741855106\n",
      "The 42460 th iteration gives loss of 0.1405501380645601\n",
      "The 42461 th iteration gives loss of 0.14054884873670104\n",
      "The 42462 th iteration gives loss of 0.1405475594349566\n",
      "The 42463 th iteration gives loss of 0.14054627015932486\n",
      "The 42464 th iteration gives loss of 0.14054498090981662\n",
      "The 42465 th iteration gives loss of 0.14054369168642447\n",
      "The 42466 th iteration gives loss of 0.1405424024891467\n",
      "The 42467 th iteration gives loss of 0.14054111331796276\n",
      "The 42468 th iteration gives loss of 0.14053982417289226\n",
      "The 42469 th iteration gives loss of 0.1405385350539291\n",
      "The 42470 th iteration gives loss of 0.14053724596106315\n",
      "The 42471 th iteration gives loss of 0.14053595689429665\n",
      "The 42472 th iteration gives loss of 0.14053466785362742\n",
      "The 42473 th iteration gives loss of 0.14053337883905132\n",
      "The 42474 th iteration gives loss of 0.1405320898505614\n",
      "The 42475 th iteration gives loss of 0.14053080088817047\n",
      "The 42476 th iteration gives loss of 0.14052951195186258\n",
      "The 42477 th iteration gives loss of 0.14052822304163864\n",
      "The 42478 th iteration gives loss of 0.1405269341574865\n",
      "The 42479 th iteration gives loss of 0.1405256452994211\n",
      "The 42480 th iteration gives loss of 0.14052435646743228\n",
      "The 42481 th iteration gives loss of 0.14052306766151898\n",
      "The 42482 th iteration gives loss of 0.14052177888167075\n",
      "The 42483 th iteration gives loss of 0.14052049012790127\n",
      "The 42484 th iteration gives loss of 0.14051920140019716\n",
      "The 42485 th iteration gives loss of 0.14051791269854785\n",
      "The 42486 th iteration gives loss of 0.14051662402296805\n",
      "The 42487 th iteration gives loss of 0.1405153353734476\n",
      "The 42488 th iteration gives loss of 0.14051404674998716\n",
      "The 42489 th iteration gives loss of 0.1405127581525709\n",
      "The 42490 th iteration gives loss of 0.1405114695812134\n",
      "The 42491 th iteration gives loss of 0.14051018103590113\n",
      "The 42492 th iteration gives loss of 0.14050889251664805\n",
      "The 42493 th iteration gives loss of 0.14050760402342652\n",
      "The 42494 th iteration gives loss of 0.14050631555624943\n",
      "The 42495 th iteration gives loss of 0.14050502711510987\n",
      "The 42496 th iteration gives loss of 0.14050373870001298\n",
      "The 42497 th iteration gives loss of 0.14050245031094444\n",
      "The 42498 th iteration gives loss of 0.1405011619479156\n",
      "The 42499 th iteration gives loss of 0.1404998736109057\n",
      "The 42500 th iteration gives loss of 0.14049858529993434\n",
      "The 42501 th iteration gives loss of 0.14049729701498373\n",
      "The 42502 th iteration gives loss of 0.1404960087560455\n",
      "The 42503 th iteration gives loss of 0.14049472052313716\n",
      "The 42504 th iteration gives loss of 0.1404934323162451\n",
      "The 42505 th iteration gives loss of 0.14049214413537045\n",
      "The 42506 th iteration gives loss of 0.1404908559804993\n",
      "The 42507 th iteration gives loss of 0.14048956785164476\n",
      "The 42508 th iteration gives loss of 0.14048827974879663\n",
      "The 42509 th iteration gives loss of 0.14048699167195153\n",
      "The 42510 th iteration gives loss of 0.14048570362110682\n",
      "The 42511 th iteration gives loss of 0.1404844155962725\n",
      "The 42512 th iteration gives loss of 0.14048312759742906\n",
      "The 42513 th iteration gives loss of 0.1404818396245826\n",
      "The 42514 th iteration gives loss of 0.14048055167772464\n",
      "The 42515 th iteration gives loss of 0.14047926375686118\n",
      "The 42516 th iteration gives loss of 0.1404779758619818\n",
      "The 42517 th iteration gives loss of 0.14047668799309018\n",
      "The 42518 th iteration gives loss of 0.14047540015018178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 42519 th iteration gives loss of 0.1404741123332498\n",
      "The 42520 th iteration gives loss of 0.14047282454229768\n",
      "The 42521 th iteration gives loss of 0.14047153677732566\n",
      "The 42522 th iteration gives loss of 0.14047024903832042\n",
      "The 42523 th iteration gives loss of 0.14046896132529285\n",
      "The 42524 th iteration gives loss of 0.1404676736382285\n",
      "The 42525 th iteration gives loss of 0.14046638597713076\n",
      "The 42526 th iteration gives loss of 0.140465098342001\n",
      "The 42527 th iteration gives loss of 0.14046381073282216\n",
      "The 42528 th iteration gives loss of 0.14046252314960145\n",
      "The 42529 th iteration gives loss of 0.1404612355923423\n",
      "The 42530 th iteration gives loss of 0.1404599480610415\n",
      "The 42531 th iteration gives loss of 0.14045866055567838\n",
      "The 42532 th iteration gives loss of 0.1404573730762808\n",
      "The 42533 th iteration gives loss of 0.14045608562281614\n",
      "The 42534 th iteration gives loss of 0.14045479819529524\n",
      "The 42535 th iteration gives loss of 0.14045351079372467\n",
      "The 42536 th iteration gives loss of 0.14045222341808206\n",
      "The 42537 th iteration gives loss of 0.14045093606838746\n",
      "The 42538 th iteration gives loss of 0.1404496487446149\n",
      "The 42539 th iteration gives loss of 0.14044836144678016\n",
      "The 42540 th iteration gives loss of 0.14044707417487684\n",
      "The 42541 th iteration gives loss of 0.14044578692889254\n",
      "The 42542 th iteration gives loss of 0.14044449970883827\n",
      "The 42543 th iteration gives loss of 0.14044321251470313\n",
      "The 42544 th iteration gives loss of 0.14044192534648453\n",
      "The 42545 th iteration gives loss of 0.14044063820419\n",
      "The 42546 th iteration gives loss of 0.14043935108780148\n",
      "The 42547 th iteration gives loss of 0.1404380639973212\n",
      "The 42548 th iteration gives loss of 0.14043677693276574\n",
      "The 42549 th iteration gives loss of 0.14043548989410726\n",
      "The 42550 th iteration gives loss of 0.14043420288135655\n",
      "The 42551 th iteration gives loss of 0.14043291589450219\n",
      "The 42552 th iteration gives loss of 0.1404316289335461\n",
      "The 42553 th iteration gives loss of 0.14043034199849397\n",
      "The 42554 th iteration gives loss of 0.14042905508933046\n",
      "The 42555 th iteration gives loss of 0.14042776820607078\n",
      "The 42556 th iteration gives loss of 0.14042648134869065\n",
      "The 42557 th iteration gives loss of 0.14042519451720123\n",
      "The 42558 th iteration gives loss of 0.14042390771159322\n",
      "The 42559 th iteration gives loss of 0.1404226209318755\n",
      "The 42560 th iteration gives loss of 0.1404213341780335\n",
      "The 42561 th iteration gives loss of 0.14042004745006353\n",
      "The 42562 th iteration gives loss of 0.14041876074797455\n",
      "The 42563 th iteration gives loss of 0.14041747407176083\n",
      "The 42564 th iteration gives loss of 0.1404161874214071\n",
      "The 42565 th iteration gives loss of 0.1404149007969307\n",
      "The 42566 th iteration gives loss of 0.14041361419831525\n",
      "The 42567 th iteration gives loss of 0.1404123276255593\n",
      "The 42568 th iteration gives loss of 0.140411041078669\n",
      "The 42569 th iteration gives loss of 0.1404097545576353\n",
      "The 42570 th iteration gives loss of 0.140408468062463\n",
      "The 42571 th iteration gives loss of 0.14040718159313634\n",
      "The 42572 th iteration gives loss of 0.14040589514966487\n",
      "The 42573 th iteration gives loss of 0.1404046087320428\n",
      "The 42574 th iteration gives loss of 0.14040332234026356\n",
      "The 42575 th iteration gives loss of 0.14040203597432765\n",
      "The 42576 th iteration gives loss of 0.14040074963422647\n",
      "The 42577 th iteration gives loss of 0.1403994633199742\n",
      "The 42578 th iteration gives loss of 0.1403981770315514\n",
      "The 42579 th iteration gives loss of 0.1403968907689653\n",
      "The 42580 th iteration gives loss of 0.14039560453221278\n",
      "The 42581 th iteration gives loss of 0.14039431832128582\n",
      "The 42582 th iteration gives loss of 0.1403930321361859\n",
      "The 42583 th iteration gives loss of 0.14039174597691864\n",
      "The 42584 th iteration gives loss of 0.14039045984345694\n",
      "The 42585 th iteration gives loss of 0.14038917373582016\n",
      "The 42586 th iteration gives loss of 0.14038788765400026\n",
      "The 42587 th iteration gives loss of 0.1403866015979927\n",
      "The 42588 th iteration gives loss of 0.14038531556780273\n",
      "The 42589 th iteration gives loss of 0.14038402956341325\n",
      "The 42590 th iteration gives loss of 0.14038274358484115\n",
      "The 42591 th iteration gives loss of 0.14038145763207283\n",
      "The 42592 th iteration gives loss of 0.14038017170510134\n",
      "The 42593 th iteration gives loss of 0.14037888580393332\n",
      "The 42594 th iteration gives loss of 0.14037759992855486\n",
      "The 42595 th iteration gives loss of 0.14037631407898205\n",
      "The 42596 th iteration gives loss of 0.14037502825519954\n",
      "The 42597 th iteration gives loss of 0.14037374245720166\n",
      "The 42598 th iteration gives loss of 0.14037245668499562\n",
      "The 42599 th iteration gives loss of 0.1403711709385638\n",
      "The 42600 th iteration gives loss of 0.14036988521792576\n",
      "The 42601 th iteration gives loss of 0.14036859952306355\n",
      "The 42602 th iteration gives loss of 0.14036731385398313\n",
      "The 42603 th iteration gives loss of 0.14036602821067712\n",
      "The 42604 th iteration gives loss of 0.14036474259314016\n",
      "The 42605 th iteration gives loss of 0.14036345700137973\n",
      "The 42606 th iteration gives loss of 0.14036217143538784\n",
      "The 42607 th iteration gives loss of 0.14036088589515985\n",
      "The 42608 th iteration gives loss of 0.1403596003806901\n",
      "The 42609 th iteration gives loss of 0.1403583148919894\n",
      "The 42610 th iteration gives loss of 0.14035702942904352\n",
      "The 42611 th iteration gives loss of 0.14035574399185152\n",
      "The 42612 th iteration gives loss of 0.1403544585804094\n",
      "The 42613 th iteration gives loss of 0.1403531731947293\n",
      "The 42614 th iteration gives loss of 0.14035188783478672\n",
      "The 42615 th iteration gives loss of 0.1403506025005943\n",
      "The 42616 th iteration gives loss of 0.1403493171921452\n",
      "The 42617 th iteration gives loss of 0.14034803190943704\n",
      "The 42618 th iteration gives loss of 0.1403467466524723\n",
      "The 42619 th iteration gives loss of 0.1403454614212485\n",
      "The 42620 th iteration gives loss of 0.1403441762157444\n",
      "The 42621 th iteration gives loss of 0.14034289103597977\n",
      "The 42622 th iteration gives loss of 0.14034160588195135\n",
      "The 42623 th iteration gives loss of 0.14034032075363942\n",
      "The 42624 th iteration gives loss of 0.14033903565105577\n",
      "The 42625 th iteration gives loss of 0.14033775057419373\n",
      "The 42626 th iteration gives loss of 0.1403364655230584\n",
      "The 42627 th iteration gives loss of 0.14033518049763055\n",
      "The 42628 th iteration gives loss of 0.14033389549792313\n",
      "The 42629 th iteration gives loss of 0.14033261052393234\n",
      "The 42630 th iteration gives loss of 0.14033132557563593\n",
      "The 42631 th iteration gives loss of 0.1403300406530552\n",
      "The 42632 th iteration gives loss of 0.14032875575617937\n",
      "The 42633 th iteration gives loss of 0.14032747088501407\n",
      "The 42634 th iteration gives loss of 0.1403261860395419\n",
      "The 42635 th iteration gives loss of 0.1403249012197676\n",
      "The 42636 th iteration gives loss of 0.14032361642569702\n",
      "The 42637 th iteration gives loss of 0.1403223316573101\n",
      "The 42638 th iteration gives loss of 0.14032104691462058\n",
      "The 42639 th iteration gives loss of 0.14031976219761924\n",
      "The 42640 th iteration gives loss of 0.14031847750630388\n",
      "The 42641 th iteration gives loss of 0.1403171928406731\n",
      "The 42642 th iteration gives loss of 0.14031590820072004\n",
      "The 42643 th iteration gives loss of 0.1403146235864427\n",
      "The 42644 th iteration gives loss of 0.14031333899784212\n",
      "The 42645 th iteration gives loss of 0.14031205443491418\n",
      "The 42646 th iteration gives loss of 0.14031076989766003\n",
      "The 42647 th iteration gives loss of 0.14030948538608004\n",
      "The 42648 th iteration gives loss of 0.14030820090015925\n",
      "The 42649 th iteration gives loss of 0.14030691643990784\n",
      "The 42650 th iteration gives loss of 0.14030563200531532\n",
      "The 42651 th iteration gives loss of 0.14030434759637997\n",
      "The 42652 th iteration gives loss of 0.1403030632131101\n",
      "The 42653 th iteration gives loss of 0.14030177885548903\n",
      "The 42654 th iteration gives loss of 0.14030049452351884\n",
      "The 42655 th iteration gives loss of 0.14029921021719807\n",
      "The 42656 th iteration gives loss of 0.14029792593652965\n",
      "The 42657 th iteration gives loss of 0.1402966416815043\n",
      "The 42658 th iteration gives loss of 0.1402953574521185\n",
      "The 42659 th iteration gives loss of 0.14029407324837984\n",
      "The 42660 th iteration gives loss of 0.1402927890702757\n",
      "The 42661 th iteration gives loss of 0.14029150491780343\n",
      "The 42662 th iteration gives loss of 0.1402902207909689\n",
      "The 42663 th iteration gives loss of 0.14028893668976358\n",
      "The 42664 th iteration gives loss of 0.14028765261418735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 42665 th iteration gives loss of 0.14028636856423368\n",
      "The 42666 th iteration gives loss of 0.14028508453990246\n",
      "The 42667 th iteration gives loss of 0.1402838005411925\n",
      "The 42668 th iteration gives loss of 0.14028251656810656\n",
      "The 42669 th iteration gives loss of 0.1402812326206285\n",
      "The 42670 th iteration gives loss of 0.14027994869878596\n",
      "The 42671 th iteration gives loss of 0.14027866480253476\n",
      "The 42672 th iteration gives loss of 0.14027738093189504\n",
      "The 42673 th iteration gives loss of 0.140276097086862\n",
      "The 42674 th iteration gives loss of 0.14027481326743407\n",
      "The 42675 th iteration gives loss of 0.14027352947361077\n",
      "The 42676 th iteration gives loss of 0.1402722457053818\n",
      "The 42677 th iteration gives loss of 0.1402709619627535\n",
      "The 42678 th iteration gives loss of 0.14026967824571798\n",
      "The 42679 th iteration gives loss of 0.14026839455427934\n",
      "The 42680 th iteration gives loss of 0.1402671108884322\n",
      "The 42681 th iteration gives loss of 0.1402658272481648\n",
      "The 42682 th iteration gives loss of 0.14026454363348773\n",
      "The 42683 th iteration gives loss of 0.14026326004438094\n",
      "The 42684 th iteration gives loss of 0.14026197648086633\n",
      "The 42685 th iteration gives loss of 0.1402606929429287\n",
      "The 42686 th iteration gives loss of 0.14025940943056547\n",
      "The 42687 th iteration gives loss of 0.1402581259437748\n",
      "The 42688 th iteration gives loss of 0.14025684248255707\n",
      "The 42689 th iteration gives loss of 0.14025555904690096\n",
      "The 42690 th iteration gives loss of 0.14025427563681425\n",
      "The 42691 th iteration gives loss of 0.14025299225229867\n",
      "The 42692 th iteration gives loss of 0.1402517088933427\n",
      "The 42693 th iteration gives loss of 0.1402504255599316\n",
      "The 42694 th iteration gives loss of 0.14024914225208546\n",
      "The 42695 th iteration gives loss of 0.14024785896979777\n",
      "The 42696 th iteration gives loss of 0.14024657571305704\n",
      "The 42697 th iteration gives loss of 0.14024529248186782\n",
      "The 42698 th iteration gives loss of 0.14024400927622044\n",
      "The 42699 th iteration gives loss of 0.14024272609612276\n",
      "The 42700 th iteration gives loss of 0.14024144294155874\n",
      "The 42701 th iteration gives loss of 0.14024015981254373\n",
      "The 42702 th iteration gives loss of 0.14023887670906646\n",
      "The 42703 th iteration gives loss of 0.14023759363112412\n",
      "The 42704 th iteration gives loss of 0.14023631057871364\n",
      "The 42705 th iteration gives loss of 0.1402350275518238\n",
      "The 42706 th iteration gives loss of 0.14023374455047394\n",
      "The 42707 th iteration gives loss of 0.14023246157464453\n",
      "The 42708 th iteration gives loss of 0.14023117862434042\n",
      "The 42709 th iteration gives loss of 0.14022989569954877\n",
      "The 42710 th iteration gives loss of 0.14022861280027882\n",
      "The 42711 th iteration gives loss of 0.14022732992652048\n",
      "The 42712 th iteration gives loss of 0.14022604707828498\n",
      "The 42713 th iteration gives loss of 0.14022476425555808\n",
      "The 42714 th iteration gives loss of 0.1402234814583373\n",
      "The 42715 th iteration gives loss of 0.14022219868662583\n",
      "The 42716 th iteration gives loss of 0.14022091594041727\n",
      "The 42717 th iteration gives loss of 0.14021963321970668\n",
      "The 42718 th iteration gives loss of 0.1402183505244974\n",
      "The 42719 th iteration gives loss of 0.14021706785478402\n",
      "The 42720 th iteration gives loss of 0.14021578521056793\n",
      "The 42721 th iteration gives loss of 0.14021450259184434\n",
      "The 42722 th iteration gives loss of 0.1402132199986098\n",
      "The 42723 th iteration gives loss of 0.1402119374308571\n",
      "The 42724 th iteration gives loss of 0.1402106548885952\n",
      "The 42725 th iteration gives loss of 0.1402093723718116\n",
      "The 42726 th iteration gives loss of 0.140208089880513\n",
      "The 42727 th iteration gives loss of 0.1402068074146859\n",
      "The 42728 th iteration gives loss of 0.1402055249743367\n",
      "The 42729 th iteration gives loss of 0.14020424255946015\n",
      "The 42730 th iteration gives loss of 0.14020296017005812\n",
      "The 42731 th iteration gives loss of 0.14020167780612333\n",
      "The 42732 th iteration gives loss of 0.14020039546765997\n",
      "The 42733 th iteration gives loss of 0.14019911315464634\n",
      "The 42734 th iteration gives loss of 0.1401978308671003\n",
      "The 42735 th iteration gives loss of 0.14019654860501146\n",
      "The 42736 th iteration gives loss of 0.140195266368381\n",
      "The 42737 th iteration gives loss of 0.14019398415720422\n",
      "The 42738 th iteration gives loss of 0.14019270197147665\n",
      "The 42739 th iteration gives loss of 0.14019141981120312\n",
      "The 42740 th iteration gives loss of 0.14019013767637165\n",
      "The 42741 th iteration gives loss of 0.14018885556698377\n",
      "The 42742 th iteration gives loss of 0.1401875734830391\n",
      "The 42743 th iteration gives loss of 0.1401862914245371\n",
      "The 42744 th iteration gives loss of 0.14018500939147444\n",
      "The 42745 th iteration gives loss of 0.1401837273838389\n",
      "The 42746 th iteration gives loss of 0.14018244540164101\n",
      "The 42747 th iteration gives loss of 0.14018116344487527\n",
      "The 42748 th iteration gives loss of 0.1401798815135383\n",
      "The 42749 th iteration gives loss of 0.1401785996076223\n",
      "The 42750 th iteration gives loss of 0.14017731772713\n",
      "The 42751 th iteration gives loss of 0.140176035872058\n",
      "The 42752 th iteration gives loss of 0.14017475404240903\n",
      "The 42753 th iteration gives loss of 0.14017347223817167\n",
      "The 42754 th iteration gives loss of 0.14017219045934948\n",
      "The 42755 th iteration gives loss of 0.14017090870592874\n",
      "The 42756 th iteration gives loss of 0.14016962697792745\n",
      "The 42757 th iteration gives loss of 0.14016834527532962\n",
      "The 42758 th iteration gives loss of 0.14016706359813733\n",
      "The 42759 th iteration gives loss of 0.14016578194634816\n",
      "The 42760 th iteration gives loss of 0.14016450031996755\n",
      "The 42761 th iteration gives loss of 0.14016321871896736\n",
      "The 42762 th iteration gives loss of 0.14016193714336542\n",
      "The 42763 th iteration gives loss of 0.14016065559316107\n",
      "The 42764 th iteration gives loss of 0.14015937406834617\n",
      "The 42765 th iteration gives loss of 0.1401580925689152\n",
      "The 42766 th iteration gives loss of 0.14015681109487443\n",
      "The 42767 th iteration gives loss of 0.14015552964620986\n",
      "The 42768 th iteration gives loss of 0.1401542482229324\n",
      "The 42769 th iteration gives loss of 0.14015296682503003\n",
      "The 42770 th iteration gives loss of 0.14015168545250672\n",
      "The 42771 th iteration gives loss of 0.1401504041053523\n",
      "The 42772 th iteration gives loss of 0.1401491227835723\n",
      "The 42773 th iteration gives loss of 0.14014784148716103\n",
      "The 42774 th iteration gives loss of 0.14014656021611585\n",
      "The 42775 th iteration gives loss of 0.1401452789704372\n",
      "The 42776 th iteration gives loss of 0.1401439977501138\n",
      "The 42777 th iteration gives loss of 0.1401427165551531\n",
      "The 42778 th iteration gives loss of 0.1401414353855485\n",
      "The 42779 th iteration gives loss of 0.14014015424129833\n",
      "The 42780 th iteration gives loss of 0.14013887312240228\n",
      "The 42781 th iteration gives loss of 0.14013759202884601\n",
      "The 42782 th iteration gives loss of 0.14013631096065074\n",
      "The 42783 th iteration gives loss of 0.14013502991780008\n",
      "The 42784 th iteration gives loss of 0.14013374890028835\n",
      "The 42785 th iteration gives loss of 0.14013246790811992\n",
      "The 42786 th iteration gives loss of 0.14013118694128326\n",
      "The 42787 th iteration gives loss of 0.14012990599978978\n",
      "The 42788 th iteration gives loss of 0.1401286250836261\n",
      "The 42789 th iteration gives loss of 0.1401273441927888\n",
      "The 42790 th iteration gives loss of 0.14012606332728605\n",
      "The 42791 th iteration gives loss of 0.14012478248710517\n",
      "The 42792 th iteration gives loss of 0.14012350167225196\n",
      "The 42793 th iteration gives loss of 0.14012222088271414\n",
      "The 42794 th iteration gives loss of 0.1401209401185067\n",
      "The 42795 th iteration gives loss of 0.14011965937961082\n",
      "The 42796 th iteration gives loss of 0.1401183786660311\n",
      "The 42797 th iteration gives loss of 0.14011709797775476\n",
      "The 42798 th iteration gives loss of 0.14011581731479703\n",
      "The 42799 th iteration gives loss of 0.1401145366771476\n",
      "The 42800 th iteration gives loss of 0.1401132560647957\n",
      "The 42801 th iteration gives loss of 0.14011197547775925\n",
      "The 42802 th iteration gives loss of 0.14011069491601774\n",
      "The 42803 th iteration gives loss of 0.14010941437957197\n",
      "The 42804 th iteration gives loss of 0.1401081338684166\n",
      "The 42805 th iteration gives loss of 0.14010685338255907\n",
      "The 42806 th iteration gives loss of 0.14010557292200107\n",
      "The 42807 th iteration gives loss of 0.14010429248672265\n",
      "The 42808 th iteration gives loss of 0.14010301207673287\n",
      "The 42809 th iteration gives loss of 0.14010173169202797\n",
      "The 42810 th iteration gives loss of 0.14010045133260582\n",
      "The 42811 th iteration gives loss of 0.14009917099845362\n",
      "The 42812 th iteration gives loss of 0.14009789068958795\n",
      "The 42813 th iteration gives loss of 0.14009661040599888\n",
      "The 42814 th iteration gives loss of 0.14009533014767545\n",
      "The 42815 th iteration gives loss of 0.14009404991462102\n",
      "The 42816 th iteration gives loss of 0.14009276970684031\n",
      "The 42817 th iteration gives loss of 0.1400914895243244\n",
      "The 42818 th iteration gives loss of 0.14009020936706545\n",
      "The 42819 th iteration gives loss of 0.1400889292350683\n",
      "The 42820 th iteration gives loss of 0.14008764912833355\n",
      "The 42821 th iteration gives loss of 0.1400863690468536\n",
      "The 42822 th iteration gives loss of 0.14008508899062633\n",
      "The 42823 th iteration gives loss of 0.14008380895965503\n",
      "The 42824 th iteration gives loss of 0.14008252895393053\n",
      "The 42825 th iteration gives loss of 0.1400812489734513\n",
      "The 42826 th iteration gives loss of 0.1400799690182146\n",
      "The 42827 th iteration gives loss of 0.14007868908822432\n",
      "The 42828 th iteration gives loss of 0.1400774091834681\n",
      "The 42829 th iteration gives loss of 0.14007612930395186\n",
      "The 42830 th iteration gives loss of 0.14007484944966855\n",
      "The 42831 th iteration gives loss of 0.1400735696206131\n",
      "The 42832 th iteration gives loss of 0.14007228981679629\n",
      "The 42833 th iteration gives loss of 0.1400710100382034\n",
      "The 42834 th iteration gives loss of 0.140069730284839\n",
      "The 42835 th iteration gives loss of 0.14006845055669137\n",
      "The 42836 th iteration gives loss of 0.14006717085377196\n",
      "The 42837 th iteration gives loss of 0.14006589117607116\n",
      "The 42838 th iteration gives loss of 0.14006461152358643\n",
      "The 42839 th iteration gives loss of 0.14006333189630474\n",
      "The 42840 th iteration gives loss of 0.14006205229423957\n",
      "The 42841 th iteration gives loss of 0.140060772717393\n",
      "The 42842 th iteration gives loss of 0.14005949316574998\n",
      "The 42843 th iteration gives loss of 0.1400582136393037\n",
      "The 42844 th iteration gives loss of 0.1400569341380576\n",
      "The 42845 th iteration gives loss of 0.14005565466201864\n",
      "The 42846 th iteration gives loss of 0.14005437521117697\n",
      "The 42847 th iteration gives loss of 0.1400530957855302\n",
      "The 42848 th iteration gives loss of 0.14005181638507713\n",
      "The 42849 th iteration gives loss of 0.1400505370098106\n",
      "The 42850 th iteration gives loss of 0.1400492576597429\n",
      "The 42851 th iteration gives loss of 0.1400479783348463\n",
      "The 42852 th iteration gives loss of 0.14004669903514114\n",
      "The 42853 th iteration gives loss of 0.14004541976061607\n",
      "The 42854 th iteration gives loss of 0.14004414051126804\n",
      "The 42855 th iteration gives loss of 0.14004286128709678\n",
      "The 42856 th iteration gives loss of 0.14004158208810516\n",
      "The 42857 th iteration gives loss of 0.14004030291428246\n",
      "The 42858 th iteration gives loss of 0.14003902376562502\n",
      "The 42859 th iteration gives loss of 0.14003774464213817\n",
      "The 42860 th iteration gives loss of 0.14003646554381116\n",
      "The 42861 th iteration gives loss of 0.14003518647065336\n",
      "The 42862 th iteration gives loss of 0.14003390742265795\n",
      "The 42863 th iteration gives loss of 0.1400326283998148\n",
      "The 42864 th iteration gives loss of 0.1400313494021265\n",
      "The 42865 th iteration gives loss of 0.14003007042959145\n",
      "The 42866 th iteration gives loss of 0.1400287914822104\n",
      "The 42867 th iteration gives loss of 0.14002751255996845\n",
      "The 42868 th iteration gives loss of 0.14002623366289044\n",
      "The 42869 th iteration gives loss of 0.14002495479094276\n",
      "The 42870 th iteration gives loss of 0.1400236759441421\n",
      "The 42871 th iteration gives loss of 0.1400223971224772\n",
      "The 42872 th iteration gives loss of 0.14002111832595387\n",
      "The 42873 th iteration gives loss of 0.14001983955456038\n",
      "The 42874 th iteration gives loss of 0.14001856080830252\n",
      "The 42875 th iteration gives loss of 0.14001728208717523\n",
      "The 42876 th iteration gives loss of 0.14001600339117196\n",
      "The 42877 th iteration gives loss of 0.14001472472029475\n",
      "The 42878 th iteration gives loss of 0.1400134460745451\n",
      "The 42879 th iteration gives loss of 0.14001216745391082\n",
      "The 42880 th iteration gives loss of 0.14001088885839727\n",
      "The 42881 th iteration gives loss of 0.14000961028800213\n",
      "The 42882 th iteration gives loss of 0.14000833174271904\n",
      "The 42883 th iteration gives loss of 0.14000705322254575\n",
      "The 42884 th iteration gives loss of 0.14000577472747353\n",
      "The 42885 th iteration gives loss of 0.14000449625752392\n",
      "The 42886 th iteration gives loss of 0.14000321781266334\n",
      "The 42887 th iteration gives loss of 0.14000193939291877\n",
      "The 42888 th iteration gives loss of 0.1400006609982677\n",
      "The 42889 th iteration gives loss of 0.13999938262871608\n",
      "The 42890 th iteration gives loss of 0.1399981042842539\n",
      "The 42891 th iteration gives loss of 0.1399968259648918\n",
      "The 42892 th iteration gives loss of 0.1399955476706216\n",
      "The 42893 th iteration gives loss of 0.13999426940143134\n",
      "The 42894 th iteration gives loss of 0.13999299115733427\n",
      "The 42895 th iteration gives loss of 0.1399917129383147\n",
      "The 42896 th iteration gives loss of 0.13999043474438808\n",
      "The 42897 th iteration gives loss of 0.1399891565755223\n",
      "The 42898 th iteration gives loss of 0.13998787843174967\n",
      "The 42899 th iteration gives loss of 0.1399866003130415\n",
      "The 42900 th iteration gives loss of 0.13998532221940604\n",
      "The 42901 th iteration gives loss of 0.13998404415084964\n",
      "The 42902 th iteration gives loss of 0.13998276610735122\n",
      "The 42903 th iteration gives loss of 0.13998148808892005\n",
      "The 42904 th iteration gives loss of 0.1399802100955492\n",
      "The 42905 th iteration gives loss of 0.1399789321272457\n",
      "The 42906 th iteration gives loss of 0.13997765418399352\n",
      "The 42907 th iteration gives loss of 0.13997637626580564\n",
      "The 42908 th iteration gives loss of 0.13997509837266084\n",
      "The 42909 th iteration gives loss of 0.13997382050456653\n",
      "The 42910 th iteration gives loss of 0.13997254266153195\n",
      "The 42911 th iteration gives loss of 0.1399712648435394\n",
      "The 42912 th iteration gives loss of 0.1399699870505906\n",
      "The 42913 th iteration gives loss of 0.13996870928268662\n",
      "The 42914 th iteration gives loss of 0.13996743153982164\n",
      "The 42915 th iteration gives loss of 0.13996615382199298\n",
      "The 42916 th iteration gives loss of 0.1399648761291953\n",
      "The 42917 th iteration gives loss of 0.1399635984614307\n",
      "The 42918 th iteration gives loss of 0.1399623208187001\n",
      "The 42919 th iteration gives loss of 0.13996104320099792\n",
      "The 42920 th iteration gives loss of 0.1399597656083207\n",
      "The 42921 th iteration gives loss of 0.13995848804066685\n",
      "The 42922 th iteration gives loss of 0.1399572104980339\n",
      "The 42923 th iteration gives loss of 0.1399559329804218\n",
      "The 42924 th iteration gives loss of 0.13995465548782313\n",
      "The 42925 th iteration gives loss of 0.1399533780202389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 42926 th iteration gives loss of 0.1399521005776711\n",
      "The 42927 th iteration gives loss of 0.139950823160112\n",
      "The 42928 th iteration gives loss of 0.13994954576755692\n",
      "The 42929 th iteration gives loss of 0.13994826840000738\n",
      "The 42930 th iteration gives loss of 0.13994699105746441\n",
      "The 42931 th iteration gives loss of 0.13994571373991527\n",
      "The 42932 th iteration gives loss of 0.13994443644736654\n",
      "The 42933 th iteration gives loss of 0.13994315917982028\n",
      "The 42934 th iteration gives loss of 0.13994188193725998\n",
      "The 42935 th iteration gives loss of 0.13994060471969652\n",
      "The 42936 th iteration gives loss of 0.1399393275271195\n",
      "The 42937 th iteration gives loss of 0.1399380503595317\n",
      "The 42938 th iteration gives loss of 0.13993677321692377\n",
      "The 42939 th iteration gives loss of 0.13993549609930048\n",
      "The 42940 th iteration gives loss of 0.13993421900665332\n",
      "The 42941 th iteration gives loss of 0.13993294193898942\n",
      "The 42942 th iteration gives loss of 0.13993166489629777\n",
      "The 42943 th iteration gives loss of 0.13993038787858292\n",
      "The 42944 th iteration gives loss of 0.13992911088583218\n",
      "The 42945 th iteration gives loss of 0.13992783391805583\n",
      "The 42946 th iteration gives loss of 0.1399265569752374\n",
      "The 42947 th iteration gives loss of 0.1399252800573901\n",
      "The 42948 th iteration gives loss of 0.1399240031645025\n",
      "The 42949 th iteration gives loss of 0.13992272629657143\n",
      "The 42950 th iteration gives loss of 0.13992144945360388\n",
      "The 42951 th iteration gives loss of 0.13992017263557738\n",
      "The 42952 th iteration gives loss of 0.13991889584251807\n",
      "The 42953 th iteration gives loss of 0.13991761907439643\n",
      "The 42954 th iteration gives loss of 0.1399163423312331\n",
      "The 42955 th iteration gives loss of 0.13991506561301817\n",
      "The 42956 th iteration gives loss of 0.13991378891973977\n",
      "The 42957 th iteration gives loss of 0.13991251225140017\n",
      "The 42958 th iteration gives loss of 0.13991123560799873\n",
      "The 42959 th iteration gives loss of 0.13990995898953074\n",
      "The 42960 th iteration gives loss of 0.13990868239600188\n",
      "The 42961 th iteration gives loss of 0.139907405827409\n",
      "The 42962 th iteration gives loss of 0.13990612928373733\n",
      "The 42963 th iteration gives loss of 0.13990485276499398\n",
      "The 42964 th iteration gives loss of 0.13990357627117875\n",
      "The 42965 th iteration gives loss of 0.13990229980228494\n",
      "The 42966 th iteration gives loss of 0.1399010233583141\n",
      "The 42967 th iteration gives loss of 0.13989974693925233\n",
      "The 42968 th iteration gives loss of 0.13989847054511\n",
      "The 42969 th iteration gives loss of 0.13989719417587967\n",
      "The 42970 th iteration gives loss of 0.13989591783156838\n",
      "The 42971 th iteration gives loss of 0.13989464151216302\n",
      "The 42972 th iteration gives loss of 0.13989336521765744\n",
      "The 42973 th iteration gives loss of 0.13989208894806185\n",
      "The 42974 th iteration gives loss of 0.1398908127033639\n",
      "The 42975 th iteration gives loss of 0.1398895364835696\n",
      "The 42976 th iteration gives loss of 0.13988826028866796\n",
      "The 42977 th iteration gives loss of 0.13988698411867095\n",
      "The 42978 th iteration gives loss of 0.1398857079735541\n",
      "The 42979 th iteration gives loss of 0.1398844318533347\n",
      "The 42980 th iteration gives loss of 0.13988315575800406\n",
      "The 42981 th iteration gives loss of 0.13988187968755714\n",
      "The 42982 th iteration gives loss of 0.1398806036419933\n",
      "The 42983 th iteration gives loss of 0.13987932762131458\n",
      "The 42984 th iteration gives loss of 0.13987805162551228\n",
      "The 42985 th iteration gives loss of 0.13987677565458603\n",
      "The 42986 th iteration gives loss of 0.1398754997085381\n",
      "The 42987 th iteration gives loss of 0.13987422378736356\n",
      "The 42988 th iteration gives loss of 0.13987294789104807\n",
      "The 42989 th iteration gives loss of 0.13987167201961395\n",
      "The 42990 th iteration gives loss of 0.13987039617303212\n",
      "The 42991 th iteration gives loss of 0.13986912035132387\n",
      "The 42992 th iteration gives loss of 0.13986784455446963\n",
      "The 42993 th iteration gives loss of 0.1398665687824814\n",
      "The 42994 th iteration gives loss of 0.139865293035342\n",
      "The 42995 th iteration gives loss of 0.13986401731305564\n",
      "The 42996 th iteration gives loss of 0.13986274161563042\n",
      "The 42997 th iteration gives loss of 0.13986146594305018\n",
      "The 42998 th iteration gives loss of 0.13986019029531113\n",
      "The 42999 th iteration gives loss of 0.13985891467242298\n",
      "The 43000 th iteration gives loss of 0.13985763907437304\n",
      "The 43001 th iteration gives loss of 0.13985636350116934\n",
      "The 43002 th iteration gives loss of 0.1398550879528005\n",
      "The 43003 th iteration gives loss of 0.1398538124292642\n",
      "The 43004 th iteration gives loss of 0.13985253693057187\n",
      "The 43005 th iteration gives loss of 0.13985126145670226\n",
      "The 43006 th iteration gives loss of 0.13984998600765466\n",
      "The 43007 th iteration gives loss of 0.13984871058344817\n",
      "The 43008 th iteration gives loss of 0.13984743518406242\n",
      "The 43009 th iteration gives loss of 0.13984615980948864\n",
      "The 43010 th iteration gives loss of 0.13984488445975224\n",
      "The 43011 th iteration gives loss of 0.13984360913481533\n",
      "The 43012 th iteration gives loss of 0.13984233383470054\n",
      "The 43013 th iteration gives loss of 0.13984105855940007\n",
      "The 43014 th iteration gives loss of 0.1398397833089085\n",
      "The 43015 th iteration gives loss of 0.1398385080832289\n",
      "The 43016 th iteration gives loss of 0.13983723288234962\n",
      "The 43017 th iteration gives loss of 0.13983595770627852\n",
      "The 43018 th iteration gives loss of 0.1398346825550096\n",
      "The 43019 th iteration gives loss of 0.13983340742854483\n",
      "The 43020 th iteration gives loss of 0.1398321323268705\n",
      "The 43021 th iteration gives loss of 0.1398308572499917\n",
      "The 43022 th iteration gives loss of 0.13982958219790417\n",
      "The 43023 th iteration gives loss of 0.1398283071706061\n",
      "The 43024 th iteration gives loss of 0.13982703216810016\n",
      "The 43025 th iteration gives loss of 0.13982575719038592\n",
      "The 43026 th iteration gives loss of 0.13982448223744343\n",
      "The 43027 th iteration gives loss of 0.13982320730928643\n",
      "The 43028 th iteration gives loss of 0.13982193240591012\n",
      "The 43029 th iteration gives loss of 0.13982065752730585\n",
      "The 43030 th iteration gives loss of 0.1398193826734746\n",
      "The 43031 th iteration gives loss of 0.1398181078444227\n",
      "The 43032 th iteration gives loss of 0.13981683304013887\n",
      "The 43033 th iteration gives loss of 0.13981555826061792\n",
      "The 43034 th iteration gives loss of 0.1398142835058633\n",
      "The 43035 th iteration gives loss of 0.13981300877587768\n",
      "The 43036 th iteration gives loss of 0.13981173407065117\n",
      "The 43037 th iteration gives loss of 0.13981045939017941\n",
      "The 43038 th iteration gives loss of 0.13980918473446605\n",
      "The 43039 th iteration gives loss of 0.1398079101035098\n",
      "The 43040 th iteration gives loss of 0.13980663549730704\n",
      "The 43041 th iteration gives loss of 0.1398053609158393\n",
      "The 43042 th iteration gives loss of 0.1398040863591299\n",
      "The 43043 th iteration gives loss of 0.13980281182716203\n",
      "The 43044 th iteration gives loss of 0.13980153731994407\n",
      "The 43045 th iteration gives loss of 0.13980026283745892\n",
      "The 43046 th iteration gives loss of 0.13979898837971733\n",
      "The 43047 th iteration gives loss of 0.1397977139467088\n",
      "The 43048 th iteration gives loss of 0.13979643953843882\n",
      "The 43049 th iteration gives loss of 0.1397951651548942\n",
      "The 43050 th iteration gives loss of 0.13979389079608107\n",
      "The 43051 th iteration gives loss of 0.13979261646199403\n",
      "The 43052 th iteration gives loss of 0.13979134215263134\n",
      "The 43053 th iteration gives loss of 0.13979006786799278\n",
      "The 43054 th iteration gives loss of 0.1397887936080744\n",
      "The 43055 th iteration gives loss of 0.1397875193728734\n",
      "The 43056 th iteration gives loss of 0.1397862451623897\n",
      "The 43057 th iteration gives loss of 0.13978497097661324\n",
      "The 43058 th iteration gives loss of 0.13978369681555083\n",
      "The 43059 th iteration gives loss of 0.13978242267919988\n",
      "The 43060 th iteration gives loss of 0.13978114856755358\n",
      "The 43061 th iteration gives loss of 0.13977987448061246\n",
      "The 43062 th iteration gives loss of 0.1397786004183746\n",
      "The 43063 th iteration gives loss of 0.1397773263808351\n",
      "The 43064 th iteration gives loss of 0.13977605236799448\n",
      "The 43065 th iteration gives loss of 0.13977477837984492\n",
      "The 43066 th iteration gives loss of 0.1397735044163921\n",
      "The 43067 th iteration gives loss of 0.1397722304776259\n",
      "The 43068 th iteration gives loss of 0.13977095656355223\n",
      "The 43069 th iteration gives loss of 0.13976968267416987\n",
      "The 43070 th iteration gives loss of 0.1397684088094637\n",
      "The 43071 th iteration gives loss of 0.13976713496943605\n",
      "The 43072 th iteration gives loss of 0.13976586115409717\n",
      "The 43073 th iteration gives loss of 0.13976458736342934\n",
      "The 43074 th iteration gives loss of 0.1397633135974422\n",
      "The 43075 th iteration gives loss of 0.13976203985612415\n",
      "The 43076 th iteration gives loss of 0.1397607661394768\n",
      "The 43077 th iteration gives loss of 0.13975949244749875\n",
      "The 43078 th iteration gives loss of 0.13975821878019168\n",
      "The 43079 th iteration gives loss of 0.13975694513754916\n",
      "The 43080 th iteration gives loss of 0.13975567151956572\n",
      "The 43081 th iteration gives loss of 0.13975439792623204\n",
      "The 43082 th iteration gives loss of 0.13975312435755832\n",
      "The 43083 th iteration gives loss of 0.1397518508135401\n",
      "The 43084 th iteration gives loss of 0.1397505772941816\n",
      "The 43085 th iteration gives loss of 0.13974930379946957\n",
      "The 43086 th iteration gives loss of 0.13974803032940852\n",
      "The 43087 th iteration gives loss of 0.13974675688399\n",
      "The 43088 th iteration gives loss of 0.13974548346321264\n",
      "The 43089 th iteration gives loss of 0.13974421006708085\n",
      "The 43090 th iteration gives loss of 0.13974293669558738\n",
      "The 43091 th iteration gives loss of 0.13974166334873056\n",
      "The 43092 th iteration gives loss of 0.1397403900265077\n",
      "The 43093 th iteration gives loss of 0.13973911672892297\n",
      "The 43094 th iteration gives loss of 0.13973784345595774\n",
      "The 43095 th iteration gives loss of 0.1397365702076283\n",
      "The 43096 th iteration gives loss of 0.1397352969839208\n",
      "The 43097 th iteration gives loss of 0.13973402378483898\n",
      "The 43098 th iteration gives loss of 0.13973275061037907\n",
      "The 43099 th iteration gives loss of 0.13973147746052866\n",
      "The 43100 th iteration gives loss of 0.13973020433530728\n",
      "The 43101 th iteration gives loss of 0.1397289312346943\n",
      "The 43102 th iteration gives loss of 0.13972765815869426\n",
      "The 43103 th iteration gives loss of 0.13972638510730662\n",
      "The 43104 th iteration gives loss of 0.13972511208051802\n",
      "The 43105 th iteration gives loss of 0.13972383907834612\n",
      "The 43106 th iteration gives loss of 0.1397225661007665\n",
      "The 43107 th iteration gives loss of 0.13972129314780146\n",
      "The 43108 th iteration gives loss of 0.1397200202194269\n",
      "The 43109 th iteration gives loss of 0.13971874731565206\n",
      "The 43110 th iteration gives loss of 0.13971747443647128\n",
      "The 43111 th iteration gives loss of 0.13971620158187756\n",
      "The 43112 th iteration gives loss of 0.13971492875187647\n",
      "The 43113 th iteration gives loss of 0.13971365594645688\n",
      "The 43114 th iteration gives loss of 0.13971238316563872\n",
      "The 43115 th iteration gives loss of 0.13971111040939588\n",
      "The 43116 th iteration gives loss of 0.13970983767772965\n",
      "The 43117 th iteration gives loss of 0.13970856497064754\n",
      "The 43118 th iteration gives loss of 0.1397072922881426\n",
      "The 43119 th iteration gives loss of 0.13970601963020648\n",
      "The 43120 th iteration gives loss of 0.13970474699684726\n",
      "The 43121 th iteration gives loss of 0.13970347438804556\n",
      "The 43122 th iteration gives loss of 0.13970220180382856\n",
      "The 43123 th iteration gives loss of 0.1397009292441733\n",
      "The 43124 th iteration gives loss of 0.13969965670906934\n",
      "The 43125 th iteration gives loss of 0.13969838419853325\n",
      "The 43126 th iteration gives loss of 0.1396971117125564\n",
      "The 43127 th iteration gives loss of 0.13969583925113613\n",
      "The 43128 th iteration gives loss of 0.1396945668142717\n",
      "The 43129 th iteration gives loss of 0.13969329440195866\n",
      "The 43130 th iteration gives loss of 0.13969202201419342\n",
      "The 43131 th iteration gives loss of 0.13969074965097047\n",
      "The 43132 th iteration gives loss of 0.13968947731229853\n",
      "The 43133 th iteration gives loss of 0.13968820499816673\n",
      "The 43134 th iteration gives loss of 0.13968693270857557\n",
      "The 43135 th iteration gives loss of 0.13968566044353328\n",
      "The 43136 th iteration gives loss of 0.13968438820301196\n",
      "The 43137 th iteration gives loss of 0.139683115987031\n",
      "The 43138 th iteration gives loss of 0.13968184379558757\n",
      "The 43139 th iteration gives loss of 0.13968057162866526\n",
      "The 43140 th iteration gives loss of 0.1396792994862776\n",
      "The 43141 th iteration gives loss of 0.13967802736840962\n",
      "The 43142 th iteration gives loss of 0.13967675527506693\n",
      "The 43143 th iteration gives loss of 0.1396754832062441\n",
      "The 43144 th iteration gives loss of 0.13967421116193462\n",
      "The 43145 th iteration gives loss of 0.13967293914214962\n",
      "The 43146 th iteration gives loss of 0.1396716671468789\n",
      "The 43147 th iteration gives loss of 0.13967039517611377\n",
      "The 43148 th iteration gives loss of 0.1396691232298604\n",
      "The 43149 th iteration gives loss of 0.13966785130811132\n",
      "The 43150 th iteration gives loss of 0.13966657941087493\n",
      "The 43151 th iteration gives loss of 0.13966530753813838\n",
      "The 43152 th iteration gives loss of 0.13966403568990446\n",
      "The 43153 th iteration gives loss of 0.139662763866161\n",
      "The 43154 th iteration gives loss of 0.13966149206691555\n",
      "The 43155 th iteration gives loss of 0.13966022029216454\n",
      "The 43156 th iteration gives loss of 0.13965894854191513\n",
      "The 43157 th iteration gives loss of 0.139657676816152\n",
      "The 43158 th iteration gives loss of 0.13965640511486818\n",
      "The 43159 th iteration gives loss of 0.1396551334380723\n",
      "The 43160 th iteration gives loss of 0.1396538617857653\n",
      "The 43161 th iteration gives loss of 0.13965259015793163\n",
      "The 43162 th iteration gives loss of 0.1396513185545808\n",
      "The 43163 th iteration gives loss of 0.13965004697570854\n",
      "The 43164 th iteration gives loss of 0.13964877542129855\n",
      "The 43165 th iteration gives loss of 0.1396475038913734\n",
      "The 43166 th iteration gives loss of 0.13964623238591312\n",
      "The 43167 th iteration gives loss of 0.13964496090492043\n",
      "The 43168 th iteration gives loss of 0.13964368944838831\n",
      "The 43169 th iteration gives loss of 0.13964241801633026\n",
      "The 43170 th iteration gives loss of 0.13964114660871674\n",
      "The 43171 th iteration gives loss of 0.13963987522557803\n",
      "The 43172 th iteration gives loss of 0.1396386038668823\n",
      "The 43173 th iteration gives loss of 0.1396373325326512\n",
      "The 43174 th iteration gives loss of 0.13963606122286243\n",
      "The 43175 th iteration gives loss of 0.13963478993752812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 43176 th iteration gives loss of 0.13963351867664126\n",
      "The 43177 th iteration gives loss of 0.13963224744019923\n",
      "The 43178 th iteration gives loss of 0.13963097622820017\n",
      "The 43179 th iteration gives loss of 0.13962970504064434\n",
      "The 43180 th iteration gives loss of 0.13962843387752397\n",
      "The 43181 th iteration gives loss of 0.13962716273884138\n",
      "The 43182 th iteration gives loss of 0.13962589162459435\n",
      "The 43183 th iteration gives loss of 0.1396246205347802\n",
      "The 43184 th iteration gives loss of 0.1396233494693911\n",
      "The 43185 th iteration gives loss of 0.13962207842842755\n",
      "The 43186 th iteration gives loss of 0.1396208074118877\n",
      "The 43187 th iteration gives loss of 0.13961953641978472\n",
      "The 43188 th iteration gives loss of 0.1396182654520907\n",
      "The 43189 th iteration gives loss of 0.13961699450882115\n",
      "The 43190 th iteration gives loss of 0.1396157235899643\n",
      "The 43191 th iteration gives loss of 0.13961445269552122\n",
      "The 43192 th iteration gives loss of 0.13961318182549665\n",
      "The 43193 th iteration gives loss of 0.13961191097987372\n",
      "The 43194 th iteration gives loss of 0.13961064015866176\n",
      "The 43195 th iteration gives loss of 0.13960936936185964\n",
      "The 43196 th iteration gives loss of 0.13960809858946074\n",
      "The 43197 th iteration gives loss of 0.13960682784145506\n",
      "The 43198 th iteration gives loss of 0.13960555711785172\n",
      "The 43199 th iteration gives loss of 0.1396042864186484\n",
      "The 43200 th iteration gives loss of 0.13960301574383796\n",
      "The 43201 th iteration gives loss of 0.13960174509341697\n",
      "The 43202 th iteration gives loss of 0.1396004744673847\n",
      "The 43203 th iteration gives loss of 0.1395992038657404\n",
      "The 43204 th iteration gives loss of 0.13959793328848313\n",
      "The 43205 th iteration gives loss of 0.13959666273561072\n",
      "The 43206 th iteration gives loss of 0.139595392207124\n",
      "The 43207 th iteration gives loss of 0.13959412170300722\n",
      "The 43208 th iteration gives loss of 0.13959285122326823\n",
      "The 43209 th iteration gives loss of 0.13959158076790784\n",
      "The 43210 th iteration gives loss of 0.13959031033691852\n",
      "The 43211 th iteration gives loss of 0.13958903993030056\n",
      "The 43212 th iteration gives loss of 0.13958776954804958\n",
      "The 43213 th iteration gives loss of 0.13958649919016164\n",
      "The 43214 th iteration gives loss of 0.1395852288566373\n",
      "The 43215 th iteration gives loss of 0.13958395854748193\n",
      "The 43216 th iteration gives loss of 0.13958268826268644\n",
      "The 43217 th iteration gives loss of 0.13958141800223756\n",
      "The 43218 th iteration gives loss of 0.1395801477661461\n",
      "The 43219 th iteration gives loss of 0.13957887755440362\n",
      "The 43220 th iteration gives loss of 0.13957760736702154\n",
      "The 43221 th iteration gives loss of 0.1395763372039782\n",
      "The 43222 th iteration gives loss of 0.1395750670652869\n",
      "The 43223 th iteration gives loss of 0.13957379695094033\n",
      "The 43224 th iteration gives loss of 0.13957252686094057\n",
      "The 43225 th iteration gives loss of 0.13957125679526586\n",
      "The 43226 th iteration gives loss of 0.13956998675393073\n",
      "The 43227 th iteration gives loss of 0.13956871673693516\n",
      "The 43228 th iteration gives loss of 0.1395674467442733\n",
      "The 43229 th iteration gives loss of 0.1395661767759407\n",
      "The 43230 th iteration gives loss of 0.13956490683193304\n",
      "The 43231 th iteration gives loss of 0.1395636369122539\n",
      "The 43232 th iteration gives loss of 0.13956236701689959\n",
      "The 43233 th iteration gives loss of 0.13956109714586742\n",
      "The 43234 th iteration gives loss of 0.13955982729915756\n",
      "The 43235 th iteration gives loss of 0.1395585574767606\n",
      "The 43236 th iteration gives loss of 0.13955728767868056\n",
      "The 43237 th iteration gives loss of 0.13955601790491434\n",
      "The 43238 th iteration gives loss of 0.13955474815546068\n",
      "The 43239 th iteration gives loss of 0.13955347843030508\n",
      "The 43240 th iteration gives loss of 0.13955220872946458\n",
      "The 43241 th iteration gives loss of 0.139550939052931\n",
      "The 43242 th iteration gives loss of 0.13954966940069358\n",
      "The 43243 th iteration gives loss of 0.13954839977276193\n",
      "The 43244 th iteration gives loss of 0.1395471301691188\n",
      "The 43245 th iteration gives loss of 0.13954586058977805\n",
      "The 43246 th iteration gives loss of 0.13954459103473268\n",
      "The 43247 th iteration gives loss of 0.1395433215039698\n",
      "The 43248 th iteration gives loss of 0.13954205199750208\n",
      "The 43249 th iteration gives loss of 0.1395407825153274\n",
      "The 43250 th iteration gives loss of 0.1395395130574296\n",
      "The 43251 th iteration gives loss of 0.13953824362380632\n",
      "The 43252 th iteration gives loss of 0.13953697421448155\n",
      "The 43253 th iteration gives loss of 0.13953570482942118\n",
      "The 43254 th iteration gives loss of 0.13953443546863487\n",
      "The 43255 th iteration gives loss of 0.13953316613213082\n",
      "The 43256 th iteration gives loss of 0.13953189681989456\n",
      "The 43257 th iteration gives loss of 0.13953062753192436\n",
      "The 43258 th iteration gives loss of 0.13952935826822735\n",
      "The 43259 th iteration gives loss of 0.1395280890287917\n",
      "The 43260 th iteration gives loss of 0.13952681981361562\n",
      "The 43261 th iteration gives loss of 0.13952555062270278\n",
      "The 43262 th iteration gives loss of 0.13952428145604848\n",
      "The 43263 th iteration gives loss of 0.13952301231365655\n",
      "The 43264 th iteration gives loss of 0.13952174319551422\n",
      "The 43265 th iteration gives loss of 0.13952047410162485\n",
      "The 43266 th iteration gives loss of 0.1395192050319837\n",
      "The 43267 th iteration gives loss of 0.13951793598658557\n",
      "The 43268 th iteration gives loss of 0.1395166669654354\n",
      "The 43269 th iteration gives loss of 0.1395153979685348\n",
      "The 43270 th iteration gives loss of 0.13951412899585888\n",
      "The 43271 th iteration gives loss of 0.13951286004742897\n",
      "The 43272 th iteration gives loss of 0.13951159112324332\n",
      "The 43273 th iteration gives loss of 0.13951032222329038\n",
      "The 43274 th iteration gives loss of 0.13950905334756308\n",
      "The 43275 th iteration gives loss of 0.13950778449606596\n",
      "The 43276 th iteration gives loss of 0.13950651566880215\n",
      "The 43277 th iteration gives loss of 0.139505246865762\n",
      "The 43278 th iteration gives loss of 0.13950397808694723\n",
      "The 43279 th iteration gives loss of 0.13950270933234876\n",
      "The 43280 th iteration gives loss of 0.1395014406019728\n",
      "The 43281 th iteration gives loss of 0.13950017189580755\n",
      "The 43282 th iteration gives loss of 0.1394989032138666\n",
      "The 43283 th iteration gives loss of 0.13949763455613065\n",
      "The 43284 th iteration gives loss of 0.1394963659226071\n",
      "The 43285 th iteration gives loss of 0.13949509731329865\n",
      "The 43286 th iteration gives loss of 0.1394938287281854\n",
      "The 43287 th iteration gives loss of 0.13949256016728068\n",
      "The 43288 th iteration gives loss of 0.13949129163057705\n",
      "The 43289 th iteration gives loss of 0.13949002311807437\n",
      "The 43290 th iteration gives loss of 0.13948875462977064\n",
      "The 43291 th iteration gives loss of 0.1394874861656577\n",
      "The 43292 th iteration gives loss of 0.1394862177257378\n",
      "The 43293 th iteration gives loss of 0.13948494931000852\n",
      "The 43294 th iteration gives loss of 0.13948368091847207\n",
      "The 43295 th iteration gives loss of 0.13948241255112315\n",
      "The 43296 th iteration gives loss of 0.13948114420795305\n",
      "The 43297 th iteration gives loss of 0.13947987588896377\n",
      "The 43298 th iteration gives loss of 0.13947860759415748\n",
      "The 43299 th iteration gives loss of 0.13947733932352677\n",
      "The 43300 th iteration gives loss of 0.1394760710770726\n",
      "The 43301 th iteration gives loss of 0.13947480285479877\n",
      "The 43302 th iteration gives loss of 0.13947353465669346\n",
      "The 43303 th iteration gives loss of 0.1394722664827463\n",
      "The 43304 th iteration gives loss of 0.13947099833297497\n",
      "The 43305 th iteration gives loss of 0.13946973020737324\n",
      "The 43306 th iteration gives loss of 0.13946846210592276\n",
      "The 43307 th iteration gives loss of 0.13946719402864083\n",
      "The 43308 th iteration gives loss of 0.13946592597550847\n",
      "The 43309 th iteration gives loss of 0.13946465794653906\n",
      "The 43310 th iteration gives loss of 0.1394633899417207\n",
      "The 43311 th iteration gives loss of 0.13946212196105573\n",
      "The 43312 th iteration gives loss of 0.13946085400454364\n",
      "The 43313 th iteration gives loss of 0.13945958607217374\n",
      "The 43314 th iteration gives loss of 0.13945831816394905\n",
      "The 43315 th iteration gives loss of 0.13945705027986655\n",
      "The 43316 th iteration gives loss of 0.13945578241992884\n",
      "The 43317 th iteration gives loss of 0.13945451458412564\n",
      "The 43318 th iteration gives loss of 0.1394532467724623\n",
      "The 43319 th iteration gives loss of 0.13945197898493802\n",
      "The 43320 th iteration gives loss of 0.13945071122154\n",
      "The 43321 th iteration gives loss of 0.13944944348227362\n",
      "The 43322 th iteration gives loss of 0.1394481757671258\n",
      "The 43323 th iteration gives loss of 0.13944690807611515\n",
      "The 43324 th iteration gives loss of 0.13944564040922822\n",
      "The 43325 th iteration gives loss of 0.1394443727664612\n",
      "The 43326 th iteration gives loss of 0.1394431051478129\n",
      "The 43327 th iteration gives loss of 0.13944183755326942\n",
      "The 43328 th iteration gives loss of 0.1394405699828576\n",
      "The 43329 th iteration gives loss of 0.1394393024365512\n",
      "The 43330 th iteration gives loss of 0.13943803491435933\n",
      "The 43331 th iteration gives loss of 0.13943676741627165\n",
      "The 43332 th iteration gives loss of 0.13943549994229018\n",
      "The 43333 th iteration gives loss of 0.13943423249241152\n",
      "The 43334 th iteration gives loss of 0.13943296506663697\n",
      "The 43335 th iteration gives loss of 0.1394316976649608\n",
      "The 43336 th iteration gives loss of 0.13943043028739016\n",
      "The 43337 th iteration gives loss of 0.13942916293390922\n",
      "The 43338 th iteration gives loss of 0.1394278956045201\n",
      "The 43339 th iteration gives loss of 0.13942662829922908\n",
      "The 43340 th iteration gives loss of 0.13942536101802047\n",
      "The 43341 th iteration gives loss of 0.13942409376089387\n",
      "The 43342 th iteration gives loss of 0.13942282652785906\n",
      "The 43343 th iteration gives loss of 0.13942155931890887\n",
      "The 43344 th iteration gives loss of 0.13942029213403\n",
      "The 43345 th iteration gives loss of 0.1394190249732401\n",
      "The 43346 th iteration gives loss of 0.13941775783651722\n",
      "The 43347 th iteration gives loss of 0.1394164907238676\n",
      "The 43348 th iteration gives loss of 0.13941522363530404\n",
      "The 43349 th iteration gives loss of 0.1394139565708037\n",
      "The 43350 th iteration gives loss of 0.13941268953035887\n",
      "The 43351 th iteration gives loss of 0.1394114225139887\n",
      "The 43352 th iteration gives loss of 0.1394101555216817\n",
      "The 43353 th iteration gives loss of 0.13940888855343464\n",
      "The 43354 th iteration gives loss of 0.13940762160924405\n",
      "The 43355 th iteration gives loss of 0.13940635468911602\n",
      "The 43356 th iteration gives loss of 0.13940508779304156\n",
      "The 43357 th iteration gives loss of 0.13940382092101358\n",
      "The 43358 th iteration gives loss of 0.13940255407304097\n",
      "The 43359 th iteration gives loss of 0.13940128724911544\n",
      "The 43360 th iteration gives loss of 0.13940002044923497\n",
      "The 43361 th iteration gives loss of 0.13939875367339682\n",
      "The 43362 th iteration gives loss of 0.13939748692160056\n",
      "The 43363 th iteration gives loss of 0.13939622019384926\n",
      "The 43364 th iteration gives loss of 0.1393949534901311\n",
      "The 43365 th iteration gives loss of 0.13939368681044384\n",
      "The 43366 th iteration gives loss of 0.1393924201548023\n",
      "The 43367 th iteration gives loss of 0.1393911535231787\n",
      "The 43368 th iteration gives loss of 0.13938988691558266\n",
      "The 43369 th iteration gives loss of 0.13938862033202298\n",
      "The 43370 th iteration gives loss of 0.13938735377248723\n",
      "The 43371 th iteration gives loss of 0.13938608723695953\n",
      "The 43372 th iteration gives loss of 0.13938482072546657\n",
      "The 43373 th iteration gives loss of 0.13938355423798965\n",
      "The 43374 th iteration gives loss of 0.13938228777453007\n",
      "The 43375 th iteration gives loss of 0.13938102133507388\n",
      "The 43376 th iteration gives loss of 0.1393797549196397\n",
      "The 43377 th iteration gives loss of 0.1393784885282086\n",
      "The 43378 th iteration gives loss of 0.13937722216079382\n",
      "The 43379 th iteration gives loss of 0.13937595581737353\n",
      "The 43380 th iteration gives loss of 0.13937468949796303\n",
      "The 43381 th iteration gives loss of 0.13937342320254625\n",
      "The 43382 th iteration gives loss of 0.13937215693113528\n",
      "The 43383 th iteration gives loss of 0.13937089068372055\n",
      "The 43384 th iteration gives loss of 0.13936962446029616\n",
      "The 43385 th iteration gives loss of 0.1393683582608651\n",
      "The 43386 th iteration gives loss of 0.13936709208542833\n",
      "The 43387 th iteration gives loss of 0.13936582593397268\n",
      "The 43388 th iteration gives loss of 0.13936455980651072\n",
      "The 43389 th iteration gives loss of 0.13936329370302092\n",
      "The 43390 th iteration gives loss of 0.13936202762352343\n",
      "The 43391 th iteration gives loss of 0.13936076156799762\n",
      "The 43392 th iteration gives loss of 0.13935949553645688\n",
      "The 43393 th iteration gives loss of 0.13935822952888335\n",
      "The 43394 th iteration gives loss of 0.13935696354529215\n",
      "The 43395 th iteration gives loss of 0.13935569758566477\n",
      "The 43396 th iteration gives loss of 0.13935443165000025\n",
      "The 43397 th iteration gives loss of 0.139353165738308\n",
      "The 43398 th iteration gives loss of 0.13935189985059032\n",
      "The 43399 th iteration gives loss of 0.13935063398682834\n",
      "The 43400 th iteration gives loss of 0.13934936814702295\n",
      "The 43401 th iteration gives loss of 0.13934810233117492\n",
      "The 43402 th iteration gives loss of 0.13934683653928873\n",
      "The 43403 th iteration gives loss of 0.1393455707713494\n",
      "The 43404 th iteration gives loss of 0.1393443050273636\n",
      "The 43405 th iteration gives loss of 0.13934303930733244\n",
      "The 43406 th iteration gives loss of 0.13934177361124284\n",
      "The 43407 th iteration gives loss of 0.13934050793909808\n",
      "The 43408 th iteration gives loss of 0.1393392422909044\n",
      "The 43409 th iteration gives loss of 0.1393379766666432\n",
      "The 43410 th iteration gives loss of 0.1393367110663259\n",
      "The 43411 th iteration gives loss of 0.13933544548994614\n",
      "The 43412 th iteration gives loss of 0.13933417993749897\n",
      "The 43413 th iteration gives loss of 0.1393329144089777\n",
      "The 43414 th iteration gives loss of 0.139331648904392\n",
      "The 43415 th iteration gives loss of 0.13933038342373755\n",
      "The 43416 th iteration gives loss of 0.13932911796701467\n",
      "The 43417 th iteration gives loss of 0.13932785253420016\n",
      "The 43418 th iteration gives loss of 0.1393265871253065\n",
      "The 43419 th iteration gives loss of 0.13932532174034803\n",
      "The 43420 th iteration gives loss of 0.13932405637929848\n",
      "The 43421 th iteration gives loss of 0.1393227910421665\n",
      "The 43422 th iteration gives loss of 0.13932152572895143\n",
      "The 43423 th iteration gives loss of 0.13932026043963508\n",
      "The 43424 th iteration gives loss of 0.1393189951742406\n",
      "The 43425 th iteration gives loss of 0.13931772993274555\n",
      "The 43426 th iteration gives loss of 0.1393164647151588\n",
      "The 43427 th iteration gives loss of 0.13931519952146648\n",
      "The 43428 th iteration gives loss of 0.13931393435168576\n",
      "The 43429 th iteration gives loss of 0.13931266920579374\n",
      "The 43430 th iteration gives loss of 0.13931140408381054\n",
      "The 43431 th iteration gives loss of 0.13931013898570896\n",
      "The 43432 th iteration gives loss of 0.1393088739115013\n",
      "The 43433 th iteration gives loss of 0.13930760886118967\n",
      "The 43434 th iteration gives loss of 0.13930634383476384\n",
      "The 43435 th iteration gives loss of 0.13930507883221846\n",
      "The 43436 th iteration gives loss of 0.1393038138535637\n",
      "The 43437 th iteration gives loss of 0.13930254889878638\n",
      "The 43438 th iteration gives loss of 0.13930128396788805\n",
      "The 43439 th iteration gives loss of 0.13930001906086412\n",
      "The 43440 th iteration gives loss of 0.13929875417772417\n",
      "The 43441 th iteration gives loss of 0.13929748931844782\n",
      "The 43442 th iteration gives loss of 0.13929622448304654\n",
      "The 43443 th iteration gives loss of 0.13929495967151428\n",
      "The 43444 th iteration gives loss of 0.1392936948838417\n",
      "The 43445 th iteration gives loss of 0.13929243012004514\n",
      "The 43446 th iteration gives loss of 0.13929116538010194\n",
      "The 43447 th iteration gives loss of 0.1392899006640203\n",
      "The 43448 th iteration gives loss of 0.13928863597179922\n",
      "The 43449 th iteration gives loss of 0.13928737130343305\n",
      "The 43450 th iteration gives loss of 0.13928610665892313\n",
      "The 43451 th iteration gives loss of 0.13928484203825908\n",
      "The 43452 th iteration gives loss of 0.13928357744144504\n",
      "The 43453 th iteration gives loss of 0.13928231286848544\n",
      "The 43454 th iteration gives loss of 0.1392810483193663\n",
      "The 43455 th iteration gives loss of 0.13927978379409542\n",
      "The 43456 th iteration gives loss of 0.13927851929265858\n",
      "The 43457 th iteration gives loss of 0.13927725481505893\n",
      "The 43458 th iteration gives loss of 0.13927599036130547\n",
      "The 43459 th iteration gives loss of 0.13927472593137113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 43460 th iteration gives loss of 0.1392734615252893\n",
      "The 43461 th iteration gives loss of 0.13927219714302017\n",
      "The 43462 th iteration gives loss of 0.13927093278458968\n",
      "The 43463 th iteration gives loss of 0.1392696684499828\n",
      "The 43464 th iteration gives loss of 0.13926840413919234\n",
      "The 43465 th iteration gives loss of 0.13926713985223205\n",
      "The 43466 th iteration gives loss of 0.13926587558909223\n",
      "The 43467 th iteration gives loss of 0.13926461134977364\n",
      "The 43468 th iteration gives loss of 0.13926334713425473\n",
      "The 43469 th iteration gives loss of 0.13926208294257197\n",
      "The 43470 th iteration gives loss of 0.1392608187746855\n",
      "The 43471 th iteration gives loss of 0.1392595546306132\n",
      "The 43472 th iteration gives loss of 0.139258290510348\n",
      "The 43473 th iteration gives loss of 0.13925702641388416\n",
      "The 43474 th iteration gives loss of 0.13925576234122836\n",
      "The 43475 th iteration gives loss of 0.13925449829236652\n",
      "The 43476 th iteration gives loss of 0.13925323426730726\n",
      "The 43477 th iteration gives loss of 0.13925197026604286\n",
      "The 43478 th iteration gives loss of 0.13925070628857636\n",
      "The 43479 th iteration gives loss of 0.13924944233489797\n",
      "The 43480 th iteration gives loss of 0.13924817840501538\n",
      "The 43481 th iteration gives loss of 0.13924691449891988\n",
      "The 43482 th iteration gives loss of 0.13924565061660318\n",
      "The 43483 th iteration gives loss of 0.13924438675807552\n",
      "The 43484 th iteration gives loss of 0.1392431229233367\n",
      "The 43485 th iteration gives loss of 0.13924185911236966\n",
      "The 43486 th iteration gives loss of 0.13924059532518626\n",
      "The 43487 th iteration gives loss of 0.1392393315617752\n",
      "The 43488 th iteration gives loss of 0.13923806782213818\n",
      "The 43489 th iteration gives loss of 0.13923680410626757\n",
      "The 43490 th iteration gives loss of 0.13923554041416558\n",
      "The 43491 th iteration gives loss of 0.13923427674583252\n",
      "The 43492 th iteration gives loss of 0.1392330131012647\n",
      "The 43493 th iteration gives loss of 0.1392317494804682\n",
      "The 43494 th iteration gives loss of 0.13923048588342357\n",
      "The 43495 th iteration gives loss of 0.13922922231014123\n",
      "The 43496 th iteration gives loss of 0.13922795876061134\n",
      "The 43497 th iteration gives loss of 0.13922669523484008\n",
      "The 43498 th iteration gives loss of 0.1392254317328172\n",
      "The 43499 th iteration gives loss of 0.13922416825454634\n",
      "The 43500 th iteration gives loss of 0.13922290480002317\n",
      "The 43501 th iteration gives loss of 0.1392216413692531\n",
      "The 43502 th iteration gives loss of 0.13922037796221426\n",
      "The 43503 th iteration gives loss of 0.13921911457892264\n",
      "The 43504 th iteration gives loss of 0.13921785121937877\n",
      "The 43505 th iteration gives loss of 0.13921658788356597\n",
      "The 43506 th iteration gives loss of 0.13921532457148159\n",
      "The 43507 th iteration gives loss of 0.13921406128314787\n",
      "The 43508 th iteration gives loss of 0.1392127980185335\n",
      "The 43509 th iteration gives loss of 0.13921153477764997\n",
      "The 43510 th iteration gives loss of 0.13921027156049454\n",
      "The 43511 th iteration gives loss of 0.13920900836706251\n",
      "The 43512 th iteration gives loss of 0.13920774519734827\n",
      "The 43513 th iteration gives loss of 0.13920648205136094\n",
      "The 43514 th iteration gives loss of 0.13920521892909266\n",
      "The 43515 th iteration gives loss of 0.1392039558305436\n",
      "The 43516 th iteration gives loss of 0.1392026927557078\n",
      "The 43517 th iteration gives loss of 0.1392014297045846\n",
      "The 43518 th iteration gives loss of 0.1392001666771661\n",
      "The 43519 th iteration gives loss of 0.13919890367345542\n",
      "The 43520 th iteration gives loss of 0.1391976406934551\n",
      "The 43521 th iteration gives loss of 0.1391963777371585\n",
      "The 43522 th iteration gives loss of 0.13919511480456428\n",
      "The 43523 th iteration gives loss of 0.13919385189567107\n",
      "The 43524 th iteration gives loss of 0.13919258901047657\n",
      "The 43525 th iteration gives loss of 0.1391913261489733\n",
      "The 43526 th iteration gives loss of 0.13919006331116593\n",
      "The 43527 th iteration gives loss of 0.13918880049704843\n",
      "The 43528 th iteration gives loss of 0.139187537706626\n",
      "The 43529 th iteration gives loss of 0.13918627493987928\n",
      "The 43530 th iteration gives loss of 0.13918501219682536\n",
      "The 43531 th iteration gives loss of 0.1391837494774582\n",
      "The 43532 th iteration gives loss of 0.1391824867817597\n",
      "The 43533 th iteration gives loss of 0.13918122410975645\n",
      "The 43534 th iteration gives loss of 0.13917996146142042\n",
      "The 43535 th iteration gives loss of 0.13917869883676406\n",
      "The 43536 th iteration gives loss of 0.13917743623577244\n",
      "The 43537 th iteration gives loss of 0.1391761736584554\n",
      "The 43538 th iteration gives loss of 0.13917491110480745\n",
      "The 43539 th iteration gives loss of 0.13917364857482664\n",
      "The 43540 th iteration gives loss of 0.1391723860685079\n",
      "The 43541 th iteration gives loss of 0.13917112358585293\n",
      "The 43542 th iteration gives loss of 0.1391698611268581\n",
      "The 43543 th iteration gives loss of 0.1391685986915227\n",
      "The 43544 th iteration gives loss of 0.13916733627983513\n",
      "The 43545 th iteration gives loss of 0.1391660738918076\n",
      "The 43546 th iteration gives loss of 0.13916481152743976\n",
      "The 43547 th iteration gives loss of 0.13916354918671178\n",
      "The 43548 th iteration gives loss of 0.13916228686963522\n",
      "The 43549 th iteration gives loss of 0.1391610245761975\n",
      "The 43550 th iteration gives loss of 0.13915976230640667\n",
      "The 43551 th iteration gives loss of 0.13915850006026523\n",
      "The 43552 th iteration gives loss of 0.1391572378377525\n",
      "The 43553 th iteration gives loss of 0.13915597563888152\n",
      "The 43554 th iteration gives loss of 0.13915471346364136\n",
      "The 43555 th iteration gives loss of 0.13915345131203968\n",
      "The 43556 th iteration gives loss of 0.1391521891840699\n",
      "The 43557 th iteration gives loss of 0.1391509270797273\n",
      "The 43558 th iteration gives loss of 0.13914966499900078\n",
      "The 43559 th iteration gives loss of 0.13914840294191333\n",
      "The 43560 th iteration gives loss of 0.13914714090844066\n",
      "The 43561 th iteration gives loss of 0.13914587889859806\n",
      "The 43562 th iteration gives loss of 0.13914461691236116\n",
      "The 43563 th iteration gives loss of 0.13914335494974225\n",
      "The 43564 th iteration gives loss of 0.13914209301074243\n",
      "The 43565 th iteration gives loss of 0.13914083109536027\n",
      "The 43566 th iteration gives loss of 0.1391395692035797\n",
      "The 43567 th iteration gives loss of 0.13913830733540522\n",
      "The 43568 th iteration gives loss of 0.13913704549083195\n",
      "The 43569 th iteration gives loss of 0.13913578366988116\n",
      "The 43570 th iteration gives loss of 0.13913452187251568\n",
      "The 43571 th iteration gives loss of 0.13913326009875873\n",
      "The 43572 th iteration gives loss of 0.13913199834859574\n",
      "The 43573 th iteration gives loss of 0.13913073662202088\n",
      "The 43574 th iteration gives loss of 0.13912947491905575\n",
      "The 43575 th iteration gives loss of 0.13912821323967114\n",
      "The 43576 th iteration gives loss of 0.13912695158388028\n",
      "The 43577 th iteration gives loss of 0.13912568995167943\n",
      "The 43578 th iteration gives loss of 0.13912442834305247\n",
      "The 43579 th iteration gives loss of 0.13912316675801895\n",
      "The 43580 th iteration gives loss of 0.13912190519655973\n",
      "The 43581 th iteration gives loss of 0.1391206436586848\n",
      "The 43582 th iteration gives loss of 0.13911938214438252\n",
      "The 43583 th iteration gives loss of 0.13911812065365572\n",
      "The 43584 th iteration gives loss of 0.1391168591865019\n",
      "The 43585 th iteration gives loss of 0.13911559774291654\n",
      "The 43586 th iteration gives loss of 0.13911433632290632\n",
      "The 43587 th iteration gives loss of 0.13911307492645564\n",
      "The 43588 th iteration gives loss of 0.139111813553573\n",
      "The 43589 th iteration gives loss of 0.13911055220424318\n",
      "The 43590 th iteration gives loss of 0.139109290878484\n",
      "The 43591 th iteration gives loss of 0.13910802957628066\n",
      "The 43592 th iteration gives loss of 0.1391067682976355\n",
      "The 43593 th iteration gives loss of 0.1391055070425473\n",
      "The 43594 th iteration gives loss of 0.13910424581100195\n",
      "The 43595 th iteration gives loss of 0.13910298460300946\n",
      "The 43596 th iteration gives loss of 0.139101723418562\n",
      "The 43597 th iteration gives loss of 0.13910046225766\n",
      "The 43598 th iteration gives loss of 0.13909920112031413\n",
      "The 43599 th iteration gives loss of 0.13909794000649853\n",
      "The 43600 th iteration gives loss of 0.13909667891622182\n",
      "The 43601 th iteration gives loss of 0.13909541784948956\n",
      "The 43602 th iteration gives loss of 0.1390941568062833\n",
      "The 43603 th iteration gives loss of 0.1390928957866192\n",
      "The 43604 th iteration gives loss of 0.13909163479048148\n",
      "The 43605 th iteration gives loss of 0.1390903738178726\n",
      "The 43606 th iteration gives loss of 0.13908911286879647\n",
      "The 43607 th iteration gives loss of 0.13908785194324147\n",
      "The 43608 th iteration gives loss of 0.1390865910412038\n",
      "The 43609 th iteration gives loss of 0.13908533016268906\n",
      "The 43610 th iteration gives loss of 0.13908406930769784\n",
      "The 43611 th iteration gives loss of 0.13908280847622537\n",
      "The 43612 th iteration gives loss of 0.1390815476682596\n",
      "The 43613 th iteration gives loss of 0.13908028688381185\n",
      "The 43614 th iteration gives loss of 0.1390790261228726\n",
      "The 43615 th iteration gives loss of 0.1390777653854375\n",
      "The 43616 th iteration gives loss of 0.13907650467151686\n",
      "The 43617 th iteration gives loss of 0.13907524398109763\n",
      "The 43618 th iteration gives loss of 0.13907398331417886\n",
      "The 43619 th iteration gives loss of 0.13907272267076556\n",
      "The 43620 th iteration gives loss of 0.13907146205084045\n",
      "The 43621 th iteration gives loss of 0.13907020145441476\n",
      "The 43622 th iteration gives loss of 0.1390689408814927\n",
      "The 43623 th iteration gives loss of 0.13906768033205183\n",
      "The 43624 th iteration gives loss of 0.13906641980610815\n",
      "The 43625 th iteration gives loss of 0.13906515930364738\n",
      "The 43626 th iteration gives loss of 0.1390638988246748\n",
      "The 43627 th iteration gives loss of 0.13906263836918362\n",
      "The 43628 th iteration gives loss of 0.13906137793717144\n",
      "The 43629 th iteration gives loss of 0.1390601175286454\n",
      "The 43630 th iteration gives loss of 0.13905885714359542\n",
      "The 43631 th iteration gives loss of 0.13905759678202226\n",
      "The 43632 th iteration gives loss of 0.13905633644391913\n",
      "The 43633 th iteration gives loss of 0.13905507612927795\n",
      "The 43634 th iteration gives loss of 0.13905381583811718\n",
      "The 43635 th iteration gives loss of 0.13905255557042803\n",
      "The 43636 th iteration gives loss of 0.13905129532619762\n",
      "The 43637 th iteration gives loss of 0.1390500351054357\n",
      "The 43638 th iteration gives loss of 0.13904877490812972\n",
      "The 43639 th iteration gives loss of 0.13904751473428364\n",
      "The 43640 th iteration gives loss of 0.1390462545838954\n",
      "The 43641 th iteration gives loss of 0.1390449944569592\n",
      "The 43642 th iteration gives loss of 0.13904373435347187\n",
      "The 43643 th iteration gives loss of 0.13904247427343958\n",
      "The 43644 th iteration gives loss of 0.13904121421685847\n",
      "The 43645 th iteration gives loss of 0.13903995418371912\n",
      "The 43646 th iteration gives loss of 0.13903869417402526\n",
      "The 43647 th iteration gives loss of 0.13903743418777958\n",
      "The 43648 th iteration gives loss of 0.13903617422496722\n",
      "The 43649 th iteration gives loss of 0.13903491428559914\n",
      "The 43650 th iteration gives loss of 0.13903365436966778\n",
      "The 43651 th iteration gives loss of 0.13903239447717636\n",
      "The 43652 th iteration gives loss of 0.13903113460810382\n",
      "The 43653 th iteration gives loss of 0.13902987476246126\n",
      "The 43654 th iteration gives loss of 0.13902861494025268\n",
      "The 43655 th iteration gives loss of 0.13902735514146947\n",
      "The 43656 th iteration gives loss of 0.13902609536611837\n",
      "The 43657 th iteration gives loss of 0.13902483561417778\n",
      "The 43658 th iteration gives loss of 0.13902357588566153\n",
      "The 43659 th iteration gives loss of 0.1390223161805624\n",
      "The 43660 th iteration gives loss of 0.13902105649887894\n",
      "The 43661 th iteration gives loss of 0.1390197968406122\n",
      "The 43662 th iteration gives loss of 0.13901853720575127\n",
      "The 43663 th iteration gives loss of 0.1390172775943032\n",
      "The 43664 th iteration gives loss of 0.13901601800625762\n",
      "The 43665 th iteration gives loss of 0.13901475844163075\n",
      "The 43666 th iteration gives loss of 0.139013498900392\n",
      "The 43667 th iteration gives loss of 0.13901223938256047\n",
      "The 43668 th iteration gives loss of 0.1390109798881367\n",
      "The 43669 th iteration gives loss of 0.1390097204171033\n",
      "The 43670 th iteration gives loss of 0.1390084609694608\n",
      "The 43671 th iteration gives loss of 0.139007201545214\n",
      "The 43672 th iteration gives loss of 0.1390059421443604\n",
      "The 43673 th iteration gives loss of 0.13900468276689942\n",
      "The 43674 th iteration gives loss of 0.1390034234128235\n",
      "The 43675 th iteration gives loss of 0.13900216408213223\n",
      "The 43676 th iteration gives loss of 0.1390009047748235\n",
      "The 43677 th iteration gives loss of 0.13899964549089752\n",
      "The 43678 th iteration gives loss of 0.13899838623034824\n",
      "The 43679 th iteration gives loss of 0.13899712699316907\n",
      "The 43680 th iteration gives loss of 0.13899586777937928\n",
      "The 43681 th iteration gives loss of 0.13899460858895002\n",
      "The 43682 th iteration gives loss of 0.138993349421903\n",
      "The 43683 th iteration gives loss of 0.13899209027821607\n",
      "The 43684 th iteration gives loss of 0.13899083115789548\n",
      "The 43685 th iteration gives loss of 0.1389895720609368\n",
      "The 43686 th iteration gives loss of 0.13898831298734787\n",
      "The 43687 th iteration gives loss of 0.13898705393711605\n",
      "The 43688 th iteration gives loss of 0.1389857949102452\n",
      "The 43689 th iteration gives loss of 0.13898453590673024\n",
      "The 43690 th iteration gives loss of 0.13898327692657247\n",
      "The 43691 th iteration gives loss of 0.13898201796976342\n",
      "The 43692 th iteration gives loss of 0.13898075903630308\n",
      "The 43693 th iteration gives loss of 0.13897950012619364\n",
      "The 43694 th iteration gives loss of 0.13897824123943248\n",
      "The 43695 th iteration gives loss of 0.13897698237601336\n",
      "The 43696 th iteration gives loss of 0.13897572353593504\n",
      "The 43697 th iteration gives loss of 0.1389744647191964\n",
      "The 43698 th iteration gives loss of 0.1389732059257989\n",
      "The 43699 th iteration gives loss of 0.13897194715573452\n",
      "The 43700 th iteration gives loss of 0.138970688409004\n",
      "The 43701 th iteration gives loss of 0.13896942968561132\n",
      "The 43702 th iteration gives loss of 0.13896817098553918\n",
      "The 43703 th iteration gives loss of 0.1389669123088068\n",
      "The 43704 th iteration gives loss of 0.13896565365538613\n",
      "The 43705 th iteration gives loss of 0.13896439502530464\n",
      "The 43706 th iteration gives loss of 0.13896313641853525\n",
      "The 43707 th iteration gives loss of 0.13896187783508446\n",
      "The 43708 th iteration gives loss of 0.13896061927495487\n",
      "The 43709 th iteration gives loss of 0.13895936073814238\n",
      "The 43710 th iteration gives loss of 0.13895810222463947\n",
      "The 43711 th iteration gives loss of 0.13895684373445372\n",
      "The 43712 th iteration gives loss of 0.1389555852675732\n",
      "The 43713 th iteration gives loss of 0.13895432682400227\n",
      "The 43714 th iteration gives loss of 0.1389530684037406\n",
      "The 43715 th iteration gives loss of 0.13895181000677606\n",
      "The 43716 th iteration gives loss of 0.13895055163311648\n",
      "The 43717 th iteration gives loss of 0.13894929328274977\n",
      "The 43718 th iteration gives loss of 0.13894803495568886\n",
      "The 43719 th iteration gives loss of 0.1389467766519214\n",
      "The 43720 th iteration gives loss of 0.1389455183714453\n",
      "The 43721 th iteration gives loss of 0.13894426011426037\n",
      "The 43722 th iteration gives loss of 0.13894300188036515\n",
      "The 43723 th iteration gives loss of 0.1389417436697581\n",
      "The 43724 th iteration gives loss of 0.13894048548243318\n",
      "The 43725 th iteration gives loss of 0.138939227318395\n",
      "The 43726 th iteration gives loss of 0.1389379691776385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 43727 th iteration gives loss of 0.13893671106015779\n",
      "The 43728 th iteration gives loss of 0.1389354529659585\n",
      "The 43729 th iteration gives loss of 0.13893419489502734\n",
      "The 43730 th iteration gives loss of 0.13893293684737001\n",
      "The 43731 th iteration gives loss of 0.1389316788229903\n",
      "The 43732 th iteration gives loss of 0.13893042082187812\n",
      "The 43733 th iteration gives loss of 0.13892916284403237\n",
      "The 43734 th iteration gives loss of 0.1389279048894502\n",
      "The 43735 th iteration gives loss of 0.13892664695813003\n",
      "The 43736 th iteration gives loss of 0.1389253890500722\n",
      "The 43737 th iteration gives loss of 0.1389241311652742\n",
      "The 43738 th iteration gives loss of 0.13892287330372874\n",
      "The 43739 th iteration gives loss of 0.13892161546543794\n",
      "The 43740 th iteration gives loss of 0.13892035765041094\n",
      "The 43741 th iteration gives loss of 0.1389190998586268\n",
      "The 43742 th iteration gives loss of 0.13891784209009048\n",
      "The 43743 th iteration gives loss of 0.13891658434480134\n",
      "The 43744 th iteration gives loss of 0.1389153266227593\n",
      "The 43745 th iteration gives loss of 0.1389140689239553\n",
      "The 43746 th iteration gives loss of 0.13891281124839477\n",
      "The 43747 th iteration gives loss of 0.1389115535960745\n",
      "The 43748 th iteration gives loss of 0.1389102959669861\n",
      "The 43749 th iteration gives loss of 0.13890903836113136\n",
      "The 43750 th iteration gives loss of 0.1389077807785178\n",
      "The 43751 th iteration gives loss of 0.13890652321913124\n",
      "The 43752 th iteration gives loss of 0.13890526568296493\n",
      "The 43753 th iteration gives loss of 0.13890400817003606\n",
      "The 43754 th iteration gives loss of 0.13890275068032573\n",
      "The 43755 th iteration gives loss of 0.13890149321384496\n",
      "The 43756 th iteration gives loss of 0.1389002357705764\n",
      "The 43757 th iteration gives loss of 0.13889897835053164\n",
      "The 43758 th iteration gives loss of 0.13889772095370168\n",
      "The 43759 th iteration gives loss of 0.13889646358008112\n",
      "The 43760 th iteration gives loss of 0.13889520622968013\n",
      "The 43761 th iteration gives loss of 0.13889394890248485\n",
      "The 43762 th iteration gives loss of 0.1388926915985007\n",
      "The 43763 th iteration gives loss of 0.13889143431772188\n",
      "The 43764 th iteration gives loss of 0.13889017706014667\n",
      "The 43765 th iteration gives loss of 0.1388889198257683\n",
      "The 43766 th iteration gives loss of 0.1388876626146059\n",
      "The 43767 th iteration gives loss of 0.13888640542663147\n",
      "The 43768 th iteration gives loss of 0.13888514826185136\n",
      "The 43769 th iteration gives loss of 0.13888389112026597\n",
      "The 43770 th iteration gives loss of 0.1388826340018678\n",
      "The 43771 th iteration gives loss of 0.13888137690667335\n",
      "The 43772 th iteration gives loss of 0.1388801198346658\n",
      "The 43773 th iteration gives loss of 0.13887886278583314\n",
      "The 43774 th iteration gives loss of 0.1388776057601914\n",
      "The 43775 th iteration gives loss of 0.13887634875772856\n",
      "The 43776 th iteration gives loss of 0.13887509177844645\n",
      "The 43777 th iteration gives loss of 0.13887383482234375\n",
      "The 43778 th iteration gives loss of 0.13887257788941726\n",
      "The 43779 th iteration gives loss of 0.1388713209796709\n",
      "The 43780 th iteration gives loss of 0.13887006409308314\n",
      "The 43781 th iteration gives loss of 0.1388688072296791\n",
      "The 43782 th iteration gives loss of 0.13886755038943918\n",
      "The 43783 th iteration gives loss of 0.13886629357235847\n",
      "The 43784 th iteration gives loss of 0.1388650367784466\n",
      "The 43785 th iteration gives loss of 0.13886378000770025\n",
      "The 43786 th iteration gives loss of 0.1388625232601029\n",
      "The 43787 th iteration gives loss of 0.13886126653567057\n",
      "The 43788 th iteration gives loss of 0.13886000983438912\n",
      "The 43789 th iteration gives loss of 0.13885875315626897\n",
      "The 43790 th iteration gives loss of 0.13885749650130316\n",
      "The 43791 th iteration gives loss of 0.1388562398694816\n",
      "The 43792 th iteration gives loss of 0.1388549832608061\n",
      "The 43793 th iteration gives loss of 0.13885372667527351\n",
      "The 43794 th iteration gives loss of 0.1388524701128883\n",
      "The 43795 th iteration gives loss of 0.13885121357364907\n",
      "The 43796 th iteration gives loss of 0.13884995705754766\n",
      "The 43797 th iteration gives loss of 0.13884870056458434\n",
      "The 43798 th iteration gives loss of 0.13884744409476021\n",
      "The 43799 th iteration gives loss of 0.1388461876480611\n",
      "The 43800 th iteration gives loss of 0.1388449312244991\n",
      "The 43801 th iteration gives loss of 0.13884367482406615\n",
      "The 43802 th iteration gives loss of 0.13884241844676593\n",
      "The 43803 th iteration gives loss of 0.13884116209258535\n",
      "The 43804 th iteration gives loss of 0.13883990576152813\n",
      "The 43805 th iteration gives loss of 0.1388386494535948\n",
      "The 43806 th iteration gives loss of 0.1388373931687844\n",
      "The 43807 th iteration gives loss of 0.13883613690709154\n",
      "The 43808 th iteration gives loss of 0.13883488066851132\n",
      "The 43809 th iteration gives loss of 0.13883362445304548\n",
      "The 43810 th iteration gives loss of 0.13883236826068826\n",
      "The 43811 th iteration gives loss of 0.138831112091441\n",
      "The 43812 th iteration gives loss of 0.13882985594530992\n",
      "The 43813 th iteration gives loss of 0.13882859982227355\n",
      "The 43814 th iteration gives loss of 0.13882734372234906\n",
      "The 43815 th iteration gives loss of 0.1388260876455285\n",
      "The 43816 th iteration gives loss of 0.138824831591803\n",
      "The 43817 th iteration gives loss of 0.1388235755611706\n",
      "The 43818 th iteration gives loss of 0.13882231955364047\n",
      "The 43819 th iteration gives loss of 0.1388210635692022\n",
      "The 43820 th iteration gives loss of 0.13881980760785514\n",
      "The 43821 th iteration gives loss of 0.13881855166959095\n",
      "The 43822 th iteration gives loss of 0.13881729575443094\n",
      "The 43823 th iteration gives loss of 0.13881603986234403\n",
      "The 43824 th iteration gives loss of 0.13881478399334637\n",
      "The 43825 th iteration gives loss of 0.13881352814743034\n",
      "The 43826 th iteration gives loss of 0.13881227232459703\n",
      "The 43827 th iteration gives loss of 0.13881101652483308\n",
      "The 43828 th iteration gives loss of 0.13880976074815055\n",
      "The 43829 th iteration gives loss of 0.13880850499453956\n",
      "The 43830 th iteration gives loss of 0.1388072492640045\n",
      "The 43831 th iteration gives loss of 0.13880599355653295\n",
      "The 43832 th iteration gives loss of 0.13880473787213263\n",
      "The 43833 th iteration gives loss of 0.13880348221079383\n",
      "The 43834 th iteration gives loss of 0.13880222657252325\n",
      "The 43835 th iteration gives loss of 0.13880097095731478\n",
      "The 43836 th iteration gives loss of 0.13879971536516456\n",
      "The 43837 th iteration gives loss of 0.13879845979607602\n",
      "The 43838 th iteration gives loss of 0.13879720425003955\n",
      "The 43839 th iteration gives loss of 0.13879594872706355\n",
      "The 43840 th iteration gives loss of 0.13879469322713353\n",
      "The 43841 th iteration gives loss of 0.13879343775025085\n",
      "The 43842 th iteration gives loss of 0.13879218229641554\n",
      "The 43843 th iteration gives loss of 0.138790926865631\n",
      "The 43844 th iteration gives loss of 0.13878967145788662\n",
      "The 43845 th iteration gives loss of 0.13878841607318787\n",
      "The 43846 th iteration gives loss of 0.1387871607115249\n",
      "The 43847 th iteration gives loss of 0.13878590537290106\n",
      "The 43848 th iteration gives loss of 0.13878465005732002\n",
      "The 43849 th iteration gives loss of 0.13878339476476184\n",
      "The 43850 th iteration gives loss of 0.1387821394952425\n",
      "The 43851 th iteration gives loss of 0.13878088424874893\n",
      "The 43852 th iteration gives loss of 0.13877962902529214\n",
      "The 43853 th iteration gives loss of 0.13877837382484748\n",
      "The 43854 th iteration gives loss of 0.1387771186474384\n",
      "The 43855 th iteration gives loss of 0.13877586349304694\n",
      "The 43856 th iteration gives loss of 0.13877460836167135\n",
      "The 43857 th iteration gives loss of 0.1387733532533274\n",
      "The 43858 th iteration gives loss of 0.13877209816798833\n",
      "The 43859 th iteration gives loss of 0.13877084310566268\n",
      "The 43860 th iteration gives loss of 0.13876958806635942\n",
      "The 43861 th iteration gives loss of 0.1387683330500553\n",
      "The 43862 th iteration gives loss of 0.1387670780567574\n",
      "The 43863 th iteration gives loss of 0.13876582308647104\n",
      "The 43864 th iteration gives loss of 0.1387645681391903\n",
      "The 43865 th iteration gives loss of 0.13876331321491103\n",
      "The 43866 th iteration gives loss of 0.1387620583136254\n",
      "The 43867 th iteration gives loss of 0.13876080343535335\n",
      "The 43868 th iteration gives loss of 0.1387595485800633\n",
      "The 43869 th iteration gives loss of 0.1387582937477788\n",
      "The 43870 th iteration gives loss of 0.13875703893847088\n",
      "The 43871 th iteration gives loss of 0.1387557841521601\n",
      "The 43872 th iteration gives loss of 0.13875452938883923\n",
      "The 43873 th iteration gives loss of 0.13875327464850556\n",
      "The 43874 th iteration gives loss of 0.138752019931159\n",
      "The 43875 th iteration gives loss of 0.138750765236785\n",
      "The 43876 th iteration gives loss of 0.13874951056539758\n",
      "The 43877 th iteration gives loss of 0.13874825591699277\n",
      "The 43878 th iteration gives loss of 0.13874700129155443\n",
      "The 43879 th iteration gives loss of 0.13874574668910333\n",
      "The 43880 th iteration gives loss of 0.13874449210961287\n",
      "The 43881 th iteration gives loss of 0.13874323755310078\n",
      "The 43882 th iteration gives loss of 0.13874198301954965\n",
      "The 43883 th iteration gives loss of 0.13874072850896885\n",
      "The 43884 th iteration gives loss of 0.13873947402134684\n",
      "The 43885 th iteration gives loss of 0.13873821955669247\n",
      "The 43886 th iteration gives loss of 0.13873696511500094\n",
      "The 43887 th iteration gives loss of 0.1387357106962668\n",
      "The 43888 th iteration gives loss of 0.13873445630048317\n",
      "The 43889 th iteration gives loss of 0.13873320192766167\n",
      "The 43890 th iteration gives loss of 0.13873194757778598\n",
      "The 43891 th iteration gives loss of 0.13873069325086626\n",
      "The 43892 th iteration gives loss of 0.1387294389468905\n",
      "The 43893 th iteration gives loss of 0.13872818466586853\n",
      "The 43894 th iteration gives loss of 0.13872693040778655\n",
      "The 43895 th iteration gives loss of 0.13872567617264728\n",
      "The 43896 th iteration gives loss of 0.1387244219604523\n",
      "The 43897 th iteration gives loss of 0.13872316777118815\n",
      "The 43898 th iteration gives loss of 0.13872191360486966\n",
      "The 43899 th iteration gives loss of 0.13872065946148032\n",
      "The 43900 th iteration gives loss of 0.13871940534102498\n",
      "The 43901 th iteration gives loss of 0.13871815124350637\n",
      "The 43902 th iteration gives loss of 0.13871689716890942\n",
      "The 43903 th iteration gives loss of 0.13871564311724383\n",
      "The 43904 th iteration gives loss of 0.13871438908850042\n",
      "The 43905 th iteration gives loss of 0.13871313508267966\n",
      "The 43906 th iteration gives loss of 0.13871188109978272\n",
      "The 43907 th iteration gives loss of 0.13871062713979818\n",
      "The 43908 th iteration gives loss of 0.13870937320272936\n",
      "The 43909 th iteration gives loss of 0.1387081192885941\n",
      "The 43910 th iteration gives loss of 0.1387068653973508\n",
      "The 43911 th iteration gives loss of 0.13870561152903269\n",
      "The 43912 th iteration gives loss of 0.13870435768361197\n",
      "The 43913 th iteration gives loss of 0.13870310386110318\n",
      "The 43914 th iteration gives loss of 0.13870185006149666\n",
      "The 43915 th iteration gives loss of 0.1387005962847998\n",
      "The 43916 th iteration gives loss of 0.13869934253100763\n",
      "The 43917 th iteration gives loss of 0.13869808880010412\n",
      "The 43918 th iteration gives loss of 0.1386968350921058\n",
      "The 43919 th iteration gives loss of 0.13869558140699753\n",
      "The 43920 th iteration gives loss of 0.13869432774478385\n",
      "The 43921 th iteration gives loss of 0.1386930741054647\n",
      "The 43922 th iteration gives loss of 0.13869182048903272\n",
      "The 43923 th iteration gives loss of 0.13869056689548623\n",
      "The 43924 th iteration gives loss of 0.13868931332482984\n",
      "The 43925 th iteration gives loss of 0.13868805977705687\n",
      "The 43926 th iteration gives loss of 0.13868680625216714\n",
      "The 43927 th iteration gives loss of 0.13868555275014588\n",
      "The 43928 th iteration gives loss of 0.13868429927101364\n",
      "The 43929 th iteration gives loss of 0.13868304581475444\n",
      "The 43930 th iteration gives loss of 0.13868179238136746\n",
      "The 43931 th iteration gives loss of 0.13868053897085258\n",
      "The 43932 th iteration gives loss of 0.13867928558320294\n",
      "The 43933 th iteration gives loss of 0.13867803221842967\n",
      "The 43934 th iteration gives loss of 0.1386767788765169\n",
      "The 43935 th iteration gives loss of 0.1386755255574707\n",
      "The 43936 th iteration gives loss of 0.13867427226128196\n",
      "The 43937 th iteration gives loss of 0.13867301898796222\n",
      "The 43938 th iteration gives loss of 0.13867176573748444\n",
      "The 43939 th iteration gives loss of 0.13867051250988002\n",
      "The 43940 th iteration gives loss of 0.13866925930511756\n",
      "The 43941 th iteration gives loss of 0.13866800612321306\n",
      "The 43942 th iteration gives loss of 0.13866675296415712\n",
      "The 43943 th iteration gives loss of 0.13866549982794996\n",
      "The 43944 th iteration gives loss of 0.1386642467145968\n",
      "The 43945 th iteration gives loss of 0.1386629936240696\n",
      "The 43946 th iteration gives loss of 0.13866174055639927\n",
      "The 43947 th iteration gives loss of 0.13866048751156262\n",
      "The 43948 th iteration gives loss of 0.13865923448957307\n",
      "The 43949 th iteration gives loss of 0.13865798149040903\n",
      "The 43950 th iteration gives loss of 0.13865672851408223\n",
      "The 43951 th iteration gives loss of 0.13865547556059926\n",
      "The 43952 th iteration gives loss of 0.13865422262993138\n",
      "The 43953 th iteration gives loss of 0.13865296972209937\n",
      "The 43954 th iteration gives loss of 0.13865171683709163\n",
      "The 43955 th iteration gives loss of 0.13865046397491174\n",
      "The 43956 th iteration gives loss of 0.13864921113555595\n",
      "The 43957 th iteration gives loss of 0.13864795831901028\n",
      "The 43958 th iteration gives loss of 0.13864670552529373\n",
      "The 43959 th iteration gives loss of 0.13864545275438933\n",
      "The 43960 th iteration gives loss of 0.13864420000630426\n",
      "The 43961 th iteration gives loss of 0.1386429472810311\n",
      "The 43962 th iteration gives loss of 0.1386416945785651\n",
      "The 43963 th iteration gives loss of 0.13864044189891397\n",
      "The 43964 th iteration gives loss of 0.13863918924206858\n",
      "The 43965 th iteration gives loss of 0.1386379366080164\n",
      "The 43966 th iteration gives loss of 0.13863668399677928\n",
      "The 43967 th iteration gives loss of 0.1386354314083498\n",
      "The 43968 th iteration gives loss of 0.13863417884270865\n",
      "The 43969 th iteration gives loss of 0.13863292629986357\n",
      "The 43970 th iteration gives loss of 0.13863167377981545\n",
      "The 43971 th iteration gives loss of 0.13863042128256356\n",
      "The 43972 th iteration gives loss of 0.13862916880810236\n",
      "The 43973 th iteration gives loss of 0.13862791635642982\n",
      "The 43974 th iteration gives loss of 0.13862666392754627\n",
      "The 43975 th iteration gives loss of 0.13862541152144878\n",
      "The 43976 th iteration gives loss of 0.13862415913813775\n",
      "The 43977 th iteration gives loss of 0.1386229067776\n",
      "The 43978 th iteration gives loss of 0.13862165443985078\n",
      "The 43979 th iteration gives loss of 0.1386204021248751\n",
      "The 43980 th iteration gives loss of 0.13861914983267634\n",
      "The 43981 th iteration gives loss of 0.13861789756325038\n",
      "The 43982 th iteration gives loss of 0.1386166453165958\n",
      "The 43983 th iteration gives loss of 0.138615393092708\n",
      "The 43984 th iteration gives loss of 0.13861414089159083\n",
      "The 43985 th iteration gives loss of 0.1386128887132419\n",
      "The 43986 th iteration gives loss of 0.13861163655765768\n",
      "The 43987 th iteration gives loss of 0.13861038442483156\n",
      "The 43988 th iteration gives loss of 0.13860913231477748\n",
      "The 43989 th iteration gives loss of 0.13860788022747064\n",
      "The 43990 th iteration gives loss of 0.13860662816291724\n",
      "The 43991 th iteration gives loss of 0.13860537612112103\n",
      "The 43992 th iteration gives loss of 0.1386041241020836\n",
      "The 43993 th iteration gives loss of 0.13860287210579433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 43994 th iteration gives loss of 0.13860162013225272\n",
      "The 43995 th iteration gives loss of 0.13860036818145788\n",
      "The 43996 th iteration gives loss of 0.1385991162534002\n",
      "The 43997 th iteration gives loss of 0.13859786434809834\n",
      "The 43998 th iteration gives loss of 0.13859661246552518\n",
      "The 43999 th iteration gives loss of 0.13859536060570027\n",
      "The 44000 th iteration gives loss of 0.1385941087686122\n",
      "The 44001 th iteration gives loss of 0.13859285695425422\n",
      "The 44002 th iteration gives loss of 0.13859160516262997\n",
      "The 44003 th iteration gives loss of 0.13859035339372988\n",
      "The 44004 th iteration gives loss of 0.13858910164757443\n",
      "The 44005 th iteration gives loss of 0.1385878499241352\n",
      "The 44006 th iteration gives loss of 0.1385865982234199\n",
      "The 44007 th iteration gives loss of 0.13858534654543125\n",
      "The 44008 th iteration gives loss of 0.13858409489017162\n",
      "The 44009 th iteration gives loss of 0.1385828432576241\n",
      "The 44010 th iteration gives loss of 0.13858159164779357\n",
      "The 44011 th iteration gives loss of 0.1385803400606817\n",
      "The 44012 th iteration gives loss of 0.13857908849627362\n",
      "The 44013 th iteration gives loss of 0.1385778369545878\n",
      "The 44014 th iteration gives loss of 0.13857658543560733\n",
      "The 44015 th iteration gives loss of 0.1385753339393468\n",
      "The 44016 th iteration gives loss of 0.13857408246577574\n",
      "The 44017 th iteration gives loss of 0.1385728310149145\n",
      "The 44018 th iteration gives loss of 0.13857157958674887\n",
      "The 44019 th iteration gives loss of 0.1385703281812888\n",
      "The 44020 th iteration gives loss of 0.13856907679853284\n",
      "The 44021 th iteration gives loss of 0.138567825438462\n",
      "The 44022 th iteration gives loss of 0.13856657410108653\n",
      "The 44023 th iteration gives loss of 0.13856532278641287\n",
      "The 44024 th iteration gives loss of 0.13856407149443156\n",
      "The 44025 th iteration gives loss of 0.1385628202251348\n",
      "The 44026 th iteration gives loss of 0.13856156897851699\n",
      "The 44027 th iteration gives loss of 0.13856031775459188\n",
      "The 44028 th iteration gives loss of 0.13855906655333838\n",
      "The 44029 th iteration gives loss of 0.13855781537476528\n",
      "The 44030 th iteration gives loss of 0.1385565642188841\n",
      "The 44031 th iteration gives loss of 0.13855531308567962\n",
      "The 44032 th iteration gives loss of 0.1385540619751445\n",
      "The 44033 th iteration gives loss of 0.13855281088727836\n",
      "The 44034 th iteration gives loss of 0.1385515598220933\n",
      "The 44035 th iteration gives loss of 0.13855030877956534\n",
      "The 44036 th iteration gives loss of 0.13854905775971096\n",
      "The 44037 th iteration gives loss of 0.13854780676251782\n",
      "The 44038 th iteration gives loss of 0.13854655578798847\n",
      "The 44039 th iteration gives loss of 0.13854530483612643\n",
      "The 44040 th iteration gives loss of 0.1385440539069223\n",
      "The 44041 th iteration gives loss of 0.13854280300037858\n",
      "The 44042 th iteration gives loss of 0.13854155211648078\n",
      "The 44043 th iteration gives loss of 0.13854030125524408\n",
      "The 44044 th iteration gives loss of 0.13853905041666315\n",
      "The 44045 th iteration gives loss of 0.13853779960071774\n",
      "The 44046 th iteration gives loss of 0.13853654880743144\n",
      "The 44047 th iteration gives loss of 0.13853529803678383\n",
      "The 44048 th iteration gives loss of 0.13853404728878577\n",
      "The 44049 th iteration gives loss of 0.138532796563426\n",
      "The 44050 th iteration gives loss of 0.13853154586070734\n",
      "The 44051 th iteration gives loss of 0.13853029518062898\n",
      "The 44052 th iteration gives loss of 0.13852904452318726\n",
      "The 44053 th iteration gives loss of 0.1385277938883778\n",
      "The 44054 th iteration gives loss of 0.13852654327620284\n",
      "The 44055 th iteration gives loss of 0.13852529268665878\n",
      "The 44056 th iteration gives loss of 0.13852404211973746\n",
      "The 44057 th iteration gives loss of 0.1385227915754467\n",
      "The 44058 th iteration gives loss of 0.13852154105378345\n",
      "The 44059 th iteration gives loss of 0.13852029055474185\n",
      "The 44060 th iteration gives loss of 0.1385190400783151\n",
      "The 44061 th iteration gives loss of 0.13851778962451317\n",
      "The 44062 th iteration gives loss of 0.13851653919332013\n",
      "The 44063 th iteration gives loss of 0.13851528878475178\n",
      "The 44064 th iteration gives loss of 0.1385140383987967\n",
      "The 44065 th iteration gives loss of 0.13851278803544606\n",
      "The 44066 th iteration gives loss of 0.1385115376947114\n",
      "The 44067 th iteration gives loss of 0.13851028737657503\n",
      "The 44068 th iteration gives loss of 0.13850903708105083\n",
      "The 44069 th iteration gives loss of 0.13850778680812809\n",
      "The 44070 th iteration gives loss of 0.13850653655781303\n",
      "The 44071 th iteration gives loss of 0.1385052863300879\n",
      "The 44072 th iteration gives loss of 0.13850403612496914\n",
      "The 44073 th iteration gives loss of 0.13850278594244025\n",
      "The 44074 th iteration gives loss of 0.13850153578250096\n",
      "The 44075 th iteration gives loss of 0.1385002856451665\n",
      "The 44076 th iteration gives loss of 0.13849903553041712\n",
      "The 44077 th iteration gives loss of 0.1384977854382546\n",
      "The 44078 th iteration gives loss of 0.13849653536868536\n",
      "The 44079 th iteration gives loss of 0.138495285321692\n",
      "The 44080 th iteration gives loss of 0.13849403529728158\n",
      "The 44081 th iteration gives loss of 0.1384927852954503\n",
      "The 44082 th iteration gives loss of 0.13849153531619893\n",
      "The 44083 th iteration gives loss of 0.13849028535952132\n",
      "The 44084 th iteration gives loss of 0.13848903542542776\n",
      "The 44085 th iteration gives loss of 0.13848778551390103\n",
      "The 44086 th iteration gives loss of 0.13848653562494118\n",
      "The 44087 th iteration gives loss of 0.13848528575856006\n",
      "The 44088 th iteration gives loss of 0.138484035914739\n",
      "The 44089 th iteration gives loss of 0.13848278609348422\n",
      "The 44090 th iteration gives loss of 0.13848153629479326\n",
      "The 44091 th iteration gives loss of 0.13848028651866193\n",
      "The 44092 th iteration gives loss of 0.13847903676509105\n",
      "The 44093 th iteration gives loss of 0.13847778703408073\n",
      "The 44094 th iteration gives loss of 0.13847653732562082\n",
      "The 44095 th iteration gives loss of 0.13847528763972006\n",
      "The 44096 th iteration gives loss of 0.13847403797636548\n",
      "The 44097 th iteration gives loss of 0.13847278833556556\n",
      "The 44098 th iteration gives loss of 0.13847153871730974\n",
      "The 44099 th iteration gives loss of 0.13847028912160655\n",
      "The 44100 th iteration gives loss of 0.13846903954843798\n",
      "The 44101 th iteration gives loss of 0.13846778999781575\n",
      "The 44102 th iteration gives loss of 0.1384665404697333\n",
      "The 44103 th iteration gives loss of 0.13846529096418864\n",
      "The 44104 th iteration gives loss of 0.13846404148119099\n",
      "The 44105 th iteration gives loss of 0.1384627920207194\n",
      "The 44106 th iteration gives loss of 0.1384615425827771\n",
      "The 44107 th iteration gives loss of 0.13846029316736752\n",
      "The 44108 th iteration gives loss of 0.13845904377449095\n",
      "The 44109 th iteration gives loss of 0.13845779440413045\n",
      "The 44110 th iteration gives loss of 0.13845654505630253\n",
      "The 44111 th iteration gives loss of 0.13845529573099585\n",
      "The 44112 th iteration gives loss of 0.13845404642820622\n",
      "The 44113 th iteration gives loss of 0.13845279714794442\n",
      "The 44114 th iteration gives loss of 0.13845154789020123\n",
      "The 44115 th iteration gives loss of 0.13845029865496836\n",
      "The 44116 th iteration gives loss of 0.13844904944224293\n",
      "The 44117 th iteration gives loss of 0.1384478002520348\n",
      "The 44118 th iteration gives loss of 0.1384465510843421\n",
      "The 44119 th iteration gives loss of 0.1384453019391498\n",
      "The 44120 th iteration gives loss of 0.1384440528164674\n",
      "The 44121 th iteration gives loss of 0.1384428037162878\n",
      "The 44122 th iteration gives loss of 0.1384415546386124\n",
      "The 44123 th iteration gives loss of 0.13844030558343293\n",
      "The 44124 th iteration gives loss of 0.13843905655074862\n",
      "The 44125 th iteration gives loss of 0.13843780754056895\n",
      "The 44126 th iteration gives loss of 0.13843655855287676\n",
      "The 44127 th iteration gives loss of 0.13843530958767863\n",
      "The 44128 th iteration gives loss of 0.13843406064497243\n",
      "The 44129 th iteration gives loss of 0.1384328117247623\n",
      "The 44130 th iteration gives loss of 0.13843156282703756\n",
      "The 44131 th iteration gives loss of 0.138430313951791\n",
      "The 44132 th iteration gives loss of 0.1384290650990288\n",
      "The 44133 th iteration gives loss of 0.13842781626874706\n",
      "The 44134 th iteration gives loss of 0.13842656746094867\n",
      "The 44135 th iteration gives loss of 0.13842531867561514\n",
      "The 44136 th iteration gives loss of 0.13842406991276796\n",
      "The 44137 th iteration gives loss of 0.13842282117238658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 44138 th iteration gives loss of 0.1384215724544839\n",
      "The 44139 th iteration gives loss of 0.13842032375904534\n",
      "The 44140 th iteration gives loss of 0.13841907508608345\n",
      "The 44141 th iteration gives loss of 0.13841782643559072\n",
      "The 44142 th iteration gives loss of 0.1384165778075472\n",
      "The 44143 th iteration gives loss of 0.13841532920197136\n",
      "The 44144 th iteration gives loss of 0.13841408061885926\n",
      "The 44145 th iteration gives loss of 0.13841283205820293\n",
      "The 44146 th iteration gives loss of 0.1384115835199996\n",
      "The 44147 th iteration gives loss of 0.13841033500426042\n",
      "The 44148 th iteration gives loss of 0.13840908651096692\n",
      "The 44149 th iteration gives loss of 0.13840783804011964\n",
      "The 44150 th iteration gives loss of 0.1384065895917265\n",
      "The 44151 th iteration gives loss of 0.13840534116577988\n",
      "The 44152 th iteration gives loss of 0.13840409276228058\n",
      "The 44153 th iteration gives loss of 0.13840284438122272\n",
      "The 44154 th iteration gives loss of 0.13840159602260774\n",
      "The 44155 th iteration gives loss of 0.13840034768642887\n",
      "The 44156 th iteration gives loss of 0.13839909937269307\n",
      "The 44157 th iteration gives loss of 0.13839785108138386\n",
      "The 44158 th iteration gives loss of 0.13839660281251637\n",
      "The 44159 th iteration gives loss of 0.13839535456607552\n",
      "The 44160 th iteration gives loss of 0.13839410634206809\n",
      "The 44161 th iteration gives loss of 0.13839285814048263\n",
      "The 44162 th iteration gives loss of 0.13839160996132197\n",
      "The 44163 th iteration gives loss of 0.13839036180459396\n",
      "The 44164 th iteration gives loss of 0.1383891136702869\n",
      "The 44165 th iteration gives loss of 0.1383878655583993\n",
      "The 44166 th iteration gives loss of 0.1383866174689309\n",
      "The 44167 th iteration gives loss of 0.13838536940187365\n",
      "The 44168 th iteration gives loss of 0.13838412135723338\n",
      "The 44169 th iteration gives loss of 0.1383828733350036\n",
      "The 44170 th iteration gives loss of 0.13838162533519166\n",
      "The 44171 th iteration gives loss of 0.1383803773577815\n",
      "The 44172 th iteration gives loss of 0.1383791294027861\n",
      "The 44173 th iteration gives loss of 0.138377881470188\n",
      "The 44174 th iteration gives loss of 0.1383766335599992\n",
      "The 44175 th iteration gives loss of 0.1383753856722118\n",
      "The 44176 th iteration gives loss of 0.13837413780682406\n",
      "The 44177 th iteration gives loss of 0.13837288996382635\n",
      "The 44178 th iteration gives loss of 0.13837164214323058\n",
      "The 44179 th iteration gives loss of 0.1383703943450306\n",
      "The 44180 th iteration gives loss of 0.13836914656921748\n",
      "The 44181 th iteration gives loss of 0.1383678988157986\n",
      "The 44182 th iteration gives loss of 0.1383666510847651\n",
      "The 44183 th iteration gives loss of 0.13836540337612885\n",
      "The 44184 th iteration gives loss of 0.13836415568985944\n",
      "The 44185 th iteration gives loss of 0.13836290802597728\n",
      "The 44186 th iteration gives loss of 0.13836166038447997\n",
      "The 44187 th iteration gives loss of 0.13836041276536\n",
      "The 44188 th iteration gives loss of 0.1383591651686117\n",
      "The 44189 th iteration gives loss of 0.13835791759424598\n",
      "The 44190 th iteration gives loss of 0.13835667004225885\n",
      "The 44191 th iteration gives loss of 0.13835542251263333\n",
      "The 44192 th iteration gives loss of 0.13835417500538347\n",
      "The 44193 th iteration gives loss of 0.13835292752049377\n",
      "The 44194 th iteration gives loss of 0.13835168005796888\n",
      "The 44195 th iteration gives loss of 0.13835043261781135\n",
      "The 44196 th iteration gives loss of 0.13834918520002032\n",
      "The 44197 th iteration gives loss of 0.13834793780457938\n",
      "The 44198 th iteration gives loss of 0.1383466904315028\n",
      "The 44199 th iteration gives loss of 0.13834544308078087\n",
      "The 44200 th iteration gives loss of 0.1383441957524156\n",
      "The 44201 th iteration gives loss of 0.13834294844639902\n",
      "The 44202 th iteration gives loss of 0.1383417011627362\n",
      "The 44203 th iteration gives loss of 0.13834045390142188\n",
      "The 44204 th iteration gives loss of 0.13833920666245164\n",
      "The 44205 th iteration gives loss of 0.13833795944583127\n",
      "The 44206 th iteration gives loss of 0.13833671225154764\n",
      "The 44207 th iteration gives loss of 0.13833546507960895\n",
      "The 44208 th iteration gives loss of 0.13833421793000408\n",
      "The 44209 th iteration gives loss of 0.13833297080274445\n",
      "The 44210 th iteration gives loss of 0.1383317236978114\n",
      "The 44211 th iteration gives loss of 0.13833047661521441\n",
      "The 44212 th iteration gives loss of 0.1383292295549585\n",
      "The 44213 th iteration gives loss of 0.1383279825170172\n",
      "The 44214 th iteration gives loss of 0.13832673550141902\n",
      "The 44215 th iteration gives loss of 0.13832548850813797\n",
      "The 44216 th iteration gives loss of 0.13832424153718398\n",
      "The 44217 th iteration gives loss of 0.1383229945885566\n",
      "The 44218 th iteration gives loss of 0.1383217476622377\n",
      "The 44219 th iteration gives loss of 0.13832050075824603\n",
      "The 44220 th iteration gives loss of 0.13831925387656696\n",
      "The 44221 th iteration gives loss of 0.13831800701720473\n",
      "The 44222 th iteration gives loss of 0.13831676018015104\n",
      "The 44223 th iteration gives loss of 0.1383155133654176\n",
      "The 44224 th iteration gives loss of 0.13831426657298132\n",
      "The 44225 th iteration gives loss of 0.13831301980286875\n",
      "The 44226 th iteration gives loss of 0.13831177305504938\n",
      "The 44227 th iteration gives loss of 0.13831052632952953\n",
      "The 44228 th iteration gives loss of 0.13830927962632117\n",
      "The 44229 th iteration gives loss of 0.13830803294540853\n",
      "The 44230 th iteration gives loss of 0.13830678628678914\n",
      "The 44231 th iteration gives loss of 0.13830553965047873\n",
      "The 44232 th iteration gives loss of 0.13830429303644998\n",
      "The 44233 th iteration gives loss of 0.138303046444722\n",
      "The 44234 th iteration gives loss of 0.13830179987528188\n",
      "The 44235 th iteration gives loss of 0.1383005533281307\n",
      "The 44236 th iteration gives loss of 0.138299306803261\n",
      "The 44237 th iteration gives loss of 0.13829806030067904\n",
      "The 44238 th iteration gives loss of 0.13829681382037756\n",
      "The 44239 th iteration gives loss of 0.13829556736236714\n",
      "The 44240 th iteration gives loss of 0.13829432092663205\n",
      "The 44241 th iteration gives loss of 0.13829307451316722\n",
      "The 44242 th iteration gives loss of 0.13829182812197616\n",
      "The 44243 th iteration gives loss of 0.13829058175306427\n",
      "The 44244 th iteration gives loss of 0.13828933540643173\n",
      "The 44245 th iteration gives loss of 0.13828808908206108\n",
      "The 44246 th iteration gives loss of 0.1382868427799605\n",
      "The 44247 th iteration gives loss of 0.13828559650011799\n",
      "The 44248 th iteration gives loss of 0.13828435024254668\n",
      "The 44249 th iteration gives loss of 0.13828310400723565\n",
      "The 44250 th iteration gives loss of 0.138281857794187\n",
      "The 44251 th iteration gives loss of 0.13828061160339689\n",
      "The 44252 th iteration gives loss of 0.1382793654348705\n",
      "The 44253 th iteration gives loss of 0.13827811928858547\n",
      "The 44254 th iteration gives loss of 0.13827687316456286\n",
      "The 44255 th iteration gives loss of 0.13827562706278218\n",
      "The 44256 th iteration gives loss of 0.13827438098325842\n",
      "The 44257 th iteration gives loss of 0.1382731349259825\n",
      "The 44258 th iteration gives loss of 0.1382718888909541\n",
      "The 44259 th iteration gives loss of 0.13827064287816448\n",
      "The 44260 th iteration gives loss of 0.1382693968876204\n",
      "The 44261 th iteration gives loss of 0.1382681509193032\n",
      "The 44262 th iteration gives loss of 0.13826690497324162\n",
      "The 44263 th iteration gives loss of 0.13826565904940477\n",
      "The 44264 th iteration gives loss of 0.13826441314780855\n",
      "The 44265 th iteration gives loss of 0.13826316726844337\n",
      "The 44266 th iteration gives loss of 0.13826192141130622\n",
      "The 44267 th iteration gives loss of 0.13826067557640037\n",
      "The 44268 th iteration gives loss of 0.13825942976371935\n",
      "The 44269 th iteration gives loss of 0.13825818397326176\n",
      "The 44270 th iteration gives loss of 0.13825693820503038\n",
      "The 44271 th iteration gives loss of 0.1382556924590193\n",
      "The 44272 th iteration gives loss of 0.13825444673522871\n",
      "The 44273 th iteration gives loss of 0.13825320103365513\n",
      "The 44274 th iteration gives loss of 0.13825195535430115\n",
      "The 44275 th iteration gives loss of 0.13825070969715259\n",
      "The 44276 th iteration gives loss of 0.1382494640622259\n",
      "The 44277 th iteration gives loss of 0.13824821844949492\n",
      "The 44278 th iteration gives loss of 0.13824697285898713\n",
      "The 44279 th iteration gives loss of 0.13824572729067863\n",
      "The 44280 th iteration gives loss of 0.1382444817445756\n",
      "The 44281 th iteration gives loss of 0.1382432362206804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 44282 th iteration gives loss of 0.13824199071898086\n",
      "The 44283 th iteration gives loss of 0.13824074523948232\n",
      "The 44284 th iteration gives loss of 0.13823949978217764\n",
      "The 44285 th iteration gives loss of 0.13823825434707138\n",
      "The 44286 th iteration gives loss of 0.13823700893415594\n",
      "The 44287 th iteration gives loss of 0.1382357635434335\n",
      "The 44288 th iteration gives loss of 0.13823451817489849\n",
      "The 44289 th iteration gives loss of 0.1382332728285536\n",
      "The 44290 th iteration gives loss of 0.13823202750439761\n",
      "The 44291 th iteration gives loss of 0.13823078220241902\n",
      "The 44292 th iteration gives loss of 0.1382295369226295\n",
      "The 44293 th iteration gives loss of 0.13822829166501296\n",
      "The 44294 th iteration gives loss of 0.13822704642958347\n",
      "The 44295 th iteration gives loss of 0.1382258012163275\n",
      "The 44296 th iteration gives loss of 0.1382245560252437\n",
      "The 44297 th iteration gives loss of 0.13822331085633002\n",
      "The 44298 th iteration gives loss of 0.13822206570959725\n",
      "The 44299 th iteration gives loss of 0.13822082058502172\n",
      "The 44300 th iteration gives loss of 0.13821957548262578\n",
      "The 44301 th iteration gives loss of 0.13821833040238385\n",
      "The 44302 th iteration gives loss of 0.13821708534431498\n",
      "The 44303 th iteration gives loss of 0.1382158403084042\n",
      "The 44304 th iteration gives loss of 0.1382145952946529\n",
      "The 44305 th iteration gives loss of 0.13821335030306195\n",
      "The 44306 th iteration gives loss of 0.13821210533362777\n",
      "The 44307 th iteration gives loss of 0.13821086038634203\n",
      "The 44308 th iteration gives loss of 0.13820961546121385\n",
      "The 44309 th iteration gives loss of 0.13820837055823051\n",
      "The 44310 th iteration gives loss of 0.13820712567740248\n",
      "The 44311 th iteration gives loss of 0.1382058808187264\n",
      "The 44312 th iteration gives loss of 0.13820463598218605\n",
      "The 44313 th iteration gives loss of 0.13820339116779026\n",
      "The 44314 th iteration gives loss of 0.13820214637554398\n",
      "The 44315 th iteration gives loss of 0.1382009016054265\n",
      "The 44316 th iteration gives loss of 0.13819965685744873\n",
      "The 44317 th iteration gives loss of 0.13819841213161388\n",
      "The 44318 th iteration gives loss of 0.13819716742790383\n",
      "The 44319 th iteration gives loss of 0.13819592274633827\n",
      "The 44320 th iteration gives loss of 0.13819467808689348\n",
      "The 44321 th iteration gives loss of 0.13819343344958546\n",
      "The 44322 th iteration gives loss of 0.13819218883439094\n",
      "The 44323 th iteration gives loss of 0.13819094424133305\n",
      "The 44324 th iteration gives loss of 0.13818969967039763\n",
      "The 44325 th iteration gives loss of 0.1381884551215777\n",
      "The 44326 th iteration gives loss of 0.13818721059487696\n",
      "The 44327 th iteration gives loss of 0.13818596609029693\n",
      "The 44328 th iteration gives loss of 0.1381847216078321\n",
      "The 44329 th iteration gives loss of 0.1381834771474826\n",
      "The 44330 th iteration gives loss of 0.1381822327092441\n",
      "The 44331 th iteration gives loss of 0.1381809882931136\n",
      "The 44332 th iteration gives loss of 0.13817974389908125\n",
      "The 44333 th iteration gives loss of 0.1381784995271737\n",
      "The 44334 th iteration gives loss of 0.1381772551773597\n",
      "The 44335 th iteration gives loss of 0.1381760108496486\n",
      "The 44336 th iteration gives loss of 0.13817476654404454\n",
      "The 44337 th iteration gives loss of 0.138173522260536\n",
      "The 44338 th iteration gives loss of 0.13817227799912377\n",
      "The 44339 th iteration gives loss of 0.13817103375981107\n",
      "The 44340 th iteration gives loss of 0.13816978954259002\n",
      "The 44341 th iteration gives loss of 0.1381685453474601\n",
      "The 44342 th iteration gives loss of 0.13816730117441803\n",
      "The 44343 th iteration gives loss of 0.1381660570234565\n",
      "The 44344 th iteration gives loss of 0.13816481289459215\n",
      "The 44345 th iteration gives loss of 0.13816356878781613\n",
      "The 44346 th iteration gives loss of 0.13816232470312415\n",
      "The 44347 th iteration gives loss of 0.13816108064050006\n",
      "The 44348 th iteration gives loss of 0.1381598365999642\n",
      "The 44349 th iteration gives loss of 0.13815859258149754\n",
      "The 44350 th iteration gives loss of 0.13815734858511428\n",
      "The 44351 th iteration gives loss of 0.1381561046107948\n",
      "The 44352 th iteration gives loss of 0.1381548606585457\n",
      "The 44353 th iteration gives loss of 0.13815361672837512\n",
      "The 44354 th iteration gives loss of 0.13815237282026802\n",
      "The 44355 th iteration gives loss of 0.1381511289342252\n",
      "The 44356 th iteration gives loss of 0.13814988507025103\n",
      "The 44357 th iteration gives loss of 0.1381486412283374\n",
      "The 44358 th iteration gives loss of 0.13814739740848403\n",
      "The 44359 th iteration gives loss of 0.13814615361068883\n",
      "The 44360 th iteration gives loss of 0.1381449098349513\n",
      "The 44361 th iteration gives loss of 0.13814366608126008\n",
      "The 44362 th iteration gives loss of 0.13814242234963273\n",
      "The 44363 th iteration gives loss of 0.13814117864005593\n",
      "The 44364 th iteration gives loss of 0.1381399349525232\n",
      "The 44365 th iteration gives loss of 0.1381386912870397\n",
      "The 44366 th iteration gives loss of 0.13813744764360672\n",
      "The 44367 th iteration gives loss of 0.13813620402221605\n",
      "The 44368 th iteration gives loss of 0.13813496042286813\n",
      "The 44369 th iteration gives loss of 0.1381337168455582\n",
      "The 44370 th iteration gives loss of 0.13813247329028192\n",
      "The 44371 th iteration gives loss of 0.13813122975703976\n",
      "The 44372 th iteration gives loss of 0.13812998624584932\n",
      "The 44373 th iteration gives loss of 0.13812874275667889\n",
      "The 44374 th iteration gives loss of 0.13812749928954093\n",
      "The 44375 th iteration gives loss of 0.13812625584443552\n",
      "The 44376 th iteration gives loss of 0.13812501242135236\n",
      "The 44377 th iteration gives loss of 0.13812376902029805\n",
      "The 44378 th iteration gives loss of 0.13812252564126684\n",
      "The 44379 th iteration gives loss of 0.1381212822842569\n",
      "The 44380 th iteration gives loss of 0.1381200389492678\n",
      "The 44381 th iteration gives loss of 0.13811879563629445\n",
      "The 44382 th iteration gives loss of 0.1381175523453357\n",
      "The 44383 th iteration gives loss of 0.13811630907639272\n",
      "The 44384 th iteration gives loss of 0.13811506582947383\n",
      "The 44385 th iteration gives loss of 0.13811382260455704\n",
      "The 44386 th iteration gives loss of 0.13811257940164126\n",
      "The 44387 th iteration gives loss of 0.13811133622074787\n",
      "The 44388 th iteration gives loss of 0.1381100930618497\n",
      "The 44389 th iteration gives loss of 0.13810884992496086\n",
      "The 44390 th iteration gives loss of 0.13810760681006962\n",
      "The 44391 th iteration gives loss of 0.13810636371717977\n",
      "The 44392 th iteration gives loss of 0.1381051206462845\n",
      "The 44393 th iteration gives loss of 0.13810387759739717\n",
      "The 44394 th iteration gives loss of 0.13810263457049637\n",
      "The 44395 th iteration gives loss of 0.1381013915655826\n",
      "The 44396 th iteration gives loss of 0.13810014858266675\n",
      "The 44397 th iteration gives loss of 0.13809890562174235\n",
      "The 44398 th iteration gives loss of 0.13809766268280058\n",
      "The 44399 th iteration gives loss of 0.13809641976584516\n",
      "The 44400 th iteration gives loss of 0.13809517687086675\n",
      "The 44401 th iteration gives loss of 0.13809393399787864\n",
      "The 44402 th iteration gives loss of 0.13809269114686437\n",
      "The 44403 th iteration gives loss of 0.1380914483178341\n",
      "The 44404 th iteration gives loss of 0.13809020551078005\n",
      "The 44405 th iteration gives loss of 0.1380889627256978\n",
      "The 44406 th iteration gives loss of 0.13808771996259012\n",
      "The 44407 th iteration gives loss of 0.1380864772214478\n",
      "The 44408 th iteration gives loss of 0.13808523450227725\n",
      "The 44409 th iteration gives loss of 0.1380839918050707\n",
      "The 44410 th iteration gives loss of 0.1380827491298357\n",
      "The 44411 th iteration gives loss of 0.1380815064765637\n",
      "The 44412 th iteration gives loss of 0.13808026384524702\n",
      "The 44413 th iteration gives loss of 0.138079021235894\n",
      "The 44414 th iteration gives loss of 0.13807777864850357\n",
      "The 44415 th iteration gives loss of 0.13807653608306594\n",
      "The 44416 th iteration gives loss of 0.1380752935395764\n",
      "The 44417 th iteration gives loss of 0.13807405101804465\n",
      "The 44418 th iteration gives loss of 0.13807280851846362\n",
      "The 44419 th iteration gives loss of 0.13807156604082796\n",
      "The 44420 th iteration gives loss of 0.1380703235851475\n",
      "The 44421 th iteration gives loss of 0.13806908115140648\n",
      "The 44422 th iteration gives loss of 0.1380678387396164\n",
      "The 44423 th iteration gives loss of 0.13806659634975468\n",
      "The 44424 th iteration gives loss of 0.13806535398183759\n",
      "The 44425 th iteration gives loss of 0.13806411163586468\n",
      "The 44426 th iteration gives loss of 0.13806286931181613\n",
      "The 44427 th iteration gives loss of 0.13806162700971877\n",
      "The 44428 th iteration gives loss of 0.13806038472954757\n",
      "The 44429 th iteration gives loss of 0.13805914247129805\n",
      "The 44430 th iteration gives loss of 0.1380579002349804\n",
      "The 44431 th iteration gives loss of 0.1380566580205904\n",
      "The 44432 th iteration gives loss of 0.13805541582813668\n",
      "The 44433 th iteration gives loss of 0.13805417365759884\n",
      "The 44434 th iteration gives loss of 0.1380529315089814\n",
      "The 44435 th iteration gives loss of 0.13805168938228507\n",
      "The 44436 th iteration gives loss of 0.1380504472775055\n",
      "The 44437 th iteration gives loss of 0.138049205194637\n",
      "The 44438 th iteration gives loss of 0.138047963133688\n",
      "The 44439 th iteration gives loss of 0.13804672109465427\n",
      "The 44440 th iteration gives loss of 0.13804547907752537\n",
      "The 44441 th iteration gives loss of 0.13804423708231078\n",
      "The 44442 th iteration gives loss of 0.13804299510899784\n",
      "The 44443 th iteration gives loss of 0.13804175315759415\n",
      "The 44444 th iteration gives loss of 0.13804051122809521\n",
      "The 44445 th iteration gives loss of 0.13803926932049734\n",
      "The 44446 th iteration gives loss of 0.1380380274347948\n",
      "The 44447 th iteration gives loss of 0.13803678557099042\n",
      "The 44448 th iteration gives loss of 0.13803554372909055\n",
      "The 44449 th iteration gives loss of 0.13803430190908317\n",
      "The 44450 th iteration gives loss of 0.13803306011096025\n",
      "The 44451 th iteration gives loss of 0.13803181833473485\n",
      "The 44452 th iteration gives loss of 0.13803057658039664\n",
      "The 44453 th iteration gives loss of 0.13802933484794236\n",
      "The 44454 th iteration gives loss of 0.1380280931373724\n",
      "The 44455 th iteration gives loss of 0.1380268514486899\n",
      "The 44456 th iteration gives loss of 0.13802560978189268\n",
      "The 44457 th iteration gives loss of 0.13802436813697255\n",
      "The 44458 th iteration gives loss of 0.13802312651392815\n",
      "The 44459 th iteration gives loss of 0.1380218849127581\n",
      "The 44460 th iteration gives loss of 0.13802064333347225\n",
      "The 44461 th iteration gives loss of 0.13801940177605826\n",
      "The 44462 th iteration gives loss of 0.1380181602404977\n",
      "The 44463 th iteration gives loss of 0.1380169187268244\n",
      "The 44464 th iteration gives loss of 0.13801567723500946\n",
      "The 44465 th iteration gives loss of 0.13801443576507222\n",
      "The 44466 th iteration gives loss of 0.13801319431698617\n",
      "The 44467 th iteration gives loss of 0.13801195289076879\n",
      "The 44468 th iteration gives loss of 0.13801071148640584\n",
      "The 44469 th iteration gives loss of 0.13800947010390743\n",
      "The 44470 th iteration gives loss of 0.1380082287432605\n",
      "The 44471 th iteration gives loss of 0.13800698740446893\n",
      "The 44472 th iteration gives loss of 0.13800574608752866\n",
      "The 44473 th iteration gives loss of 0.13800450479244455\n",
      "The 44474 th iteration gives loss of 0.13800326351919917\n",
      "The 44475 th iteration gives loss of 0.1380020222678099\n",
      "The 44476 th iteration gives loss of 0.13800078103826227\n",
      "The 44477 th iteration gives loss of 0.13799953983057137\n",
      "The 44478 th iteration gives loss of 0.13799829864470642\n",
      "The 44479 th iteration gives loss of 0.13799705748069147\n",
      "The 44480 th iteration gives loss of 0.13799581633850946\n",
      "The 44481 th iteration gives loss of 0.13799457521816882\n",
      "The 44482 th iteration gives loss of 0.13799333411966197\n",
      "The 44483 th iteration gives loss of 0.13799209304298826\n",
      "The 44484 th iteration gives loss of 0.1379908519881376\n",
      "The 44485 th iteration gives loss of 0.13798961095512566\n",
      "The 44486 th iteration gives loss of 0.13798836994393657\n",
      "The 44487 th iteration gives loss of 0.13798712895457732\n",
      "The 44488 th iteration gives loss of 0.13798588798703665\n",
      "The 44489 th iteration gives loss of 0.13798464704131716\n",
      "The 44490 th iteration gives loss of 0.13798340611742196\n",
      "The 44491 th iteration gives loss of 0.13798216521534915\n",
      "The 44492 th iteration gives loss of 0.13798092433509454\n",
      "The 44493 th iteration gives loss of 0.1379796834766372\n",
      "The 44494 th iteration gives loss of 0.13797844264001669\n",
      "The 44495 th iteration gives loss of 0.1379772018251968\n",
      "The 44496 th iteration gives loss of 0.13797596103219278\n",
      "The 44497 th iteration gives loss of 0.13797472026098323\n",
      "The 44498 th iteration gives loss of 0.13797347951158428\n",
      "The 44499 th iteration gives loss of 0.13797223878399945\n",
      "The 44500 th iteration gives loss of 0.13797099807820964\n",
      "The 44501 th iteration gives loss of 0.13796975739421619\n",
      "The 44502 th iteration gives loss of 0.13796851673202398\n",
      "The 44503 th iteration gives loss of 0.13796727609163484\n",
      "The 44504 th iteration gives loss of 0.1379660354730339\n",
      "The 44505 th iteration gives loss of 0.13796479487623606\n",
      "The 44506 th iteration gives loss of 0.13796355430121968\n",
      "The 44507 th iteration gives loss of 0.13796231374799192\n",
      "The 44508 th iteration gives loss of 0.1379610732165542\n",
      "The 44509 th iteration gives loss of 0.1379598327069159\n",
      "The 44510 th iteration gives loss of 0.13795859221904602\n",
      "The 44511 th iteration gives loss of 0.13795735175296253\n",
      "The 44512 th iteration gives loss of 0.1379561113086626\n",
      "The 44513 th iteration gives loss of 0.1379548708861465\n",
      "The 44514 th iteration gives loss of 0.1379536304854039\n",
      "The 44515 th iteration gives loss of 0.1379523901064408\n",
      "The 44516 th iteration gives loss of 0.13795114974924186\n",
      "The 44517 th iteration gives loss of 0.1379499094138197\n",
      "The 44518 th iteration gives loss of 0.13794866910016643\n",
      "The 44519 th iteration gives loss of 0.13794742880828234\n",
      "The 44520 th iteration gives loss of 0.13794618853816387\n",
      "The 44521 th iteration gives loss of 0.13794494828980952\n",
      "The 44522 th iteration gives loss of 0.13794370806321715\n",
      "The 44523 th iteration gives loss of 0.1379424678583996\n",
      "The 44524 th iteration gives loss of 0.1379412276753303\n",
      "The 44525 th iteration gives loss of 0.1379399875140157\n",
      "The 44526 th iteration gives loss of 0.1379387473744573\n",
      "The 44527 th iteration gives loss of 0.13793750725665954\n",
      "The 44528 th iteration gives loss of 0.13793626716060525\n",
      "The 44529 th iteration gives loss of 0.13793502708630742\n",
      "The 44530 th iteration gives loss of 0.1379337870337566\n",
      "The 44531 th iteration gives loss of 0.13793254700296143\n",
      "The 44532 th iteration gives loss of 0.1379313069938982\n",
      "The 44533 th iteration gives loss of 0.13793006700658106\n",
      "The 44534 th iteration gives loss of 0.1379288270410079\n",
      "The 44535 th iteration gives loss of 0.1379275870971783\n",
      "The 44536 th iteration gives loss of 0.13792634717508528\n",
      "The 44537 th iteration gives loss of 0.1379251072747216\n",
      "The 44538 th iteration gives loss of 0.13792386739610027\n",
      "The 44539 th iteration gives loss of 0.13792262753920623\n",
      "The 44540 th iteration gives loss of 0.13792138770405152\n",
      "The 44541 th iteration gives loss of 0.13792014789061885\n",
      "The 44542 th iteration gives loss of 0.13791890809891796\n",
      "The 44543 th iteration gives loss of 0.1379176683289325\n",
      "The 44544 th iteration gives loss of 0.13791642858067796\n",
      "The 44545 th iteration gives loss of 0.1379151888541465\n",
      "The 44546 th iteration gives loss of 0.13791394914933472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 44547 th iteration gives loss of 0.13791270946623754\n",
      "The 44548 th iteration gives loss of 0.1379114698048525\n",
      "The 44549 th iteration gives loss of 0.1379102301651896\n",
      "The 44550 th iteration gives loss of 0.13790899054723962\n",
      "The 44551 th iteration gives loss of 0.13790775095099778\n",
      "The 44552 th iteration gives loss of 0.13790651137646703\n",
      "The 44553 th iteration gives loss of 0.13790527182364157\n",
      "The 44554 th iteration gives loss of 0.13790403229252998\n",
      "The 44555 th iteration gives loss of 0.1379027927831179\n",
      "The 44556 th iteration gives loss of 0.13790155329540976\n",
      "The 44557 th iteration gives loss of 0.1379003138294016\n",
      "The 44558 th iteration gives loss of 0.137899074385084\n",
      "The 44559 th iteration gives loss of 0.1378978349624719\n",
      "The 44560 th iteration gives loss of 0.1378965955615521\n",
      "The 44561 th iteration gives loss of 0.13789535618232932\n",
      "The 44562 th iteration gives loss of 0.13789411682479089\n",
      "The 44563 th iteration gives loss of 0.13789287748894746\n",
      "The 44564 th iteration gives loss of 0.13789163817479158\n",
      "The 44565 th iteration gives loss of 0.13789039888231638\n",
      "The 44566 th iteration gives loss of 0.13788915961154044\n",
      "The 44567 th iteration gives loss of 0.1378879203624326\n",
      "The 44568 th iteration gives loss of 0.13788668113501457\n",
      "The 44569 th iteration gives loss of 0.13788544192926827\n",
      "The 44570 th iteration gives loss of 0.13788420274520335\n",
      "The 44571 th iteration gives loss of 0.13788296358281912\n",
      "The 44572 th iteration gives loss of 0.13788172444210106\n",
      "The 44573 th iteration gives loss of 0.137880485323052\n",
      "The 44574 th iteration gives loss of 0.13787924622568223\n",
      "The 44575 th iteration gives loss of 0.1378780071499789\n",
      "The 44576 th iteration gives loss of 0.13787676809594354\n",
      "The 44577 th iteration gives loss of 0.1378755290635726\n",
      "The 44578 th iteration gives loss of 0.13787429005285715\n",
      "The 44579 th iteration gives loss of 0.13787305106381223\n",
      "The 44580 th iteration gives loss of 0.13787181209642743\n",
      "The 44581 th iteration gives loss of 0.1378705731506948\n",
      "The 44582 th iteration gives loss of 0.13786933422661934\n",
      "The 44583 th iteration gives loss of 0.13786809532419889\n",
      "The 44584 th iteration gives loss of 0.13786685644343435\n",
      "The 44585 th iteration gives loss of 0.13786561758431715\n",
      "The 44586 th iteration gives loss of 0.13786437874684648\n",
      "The 44587 th iteration gives loss of 0.1378631399310217\n",
      "The 44588 th iteration gives loss of 0.13786190113685054\n",
      "The 44589 th iteration gives loss of 0.1378606623643257\n",
      "The 44590 th iteration gives loss of 0.1378594236134264\n",
      "The 44591 th iteration gives loss of 0.13785818488418025\n",
      "The 44592 th iteration gives loss of 0.13785694617657027\n",
      "The 44593 th iteration gives loss of 0.13785570749059597\n",
      "The 44594 th iteration gives loss of 0.137854468826256\n",
      "The 44595 th iteration gives loss of 0.13785323018355353\n",
      "The 44596 th iteration gives loss of 0.13785199156247815\n",
      "The 44597 th iteration gives loss of 0.13785075296303803\n",
      "The 44598 th iteration gives loss of 0.13784951438521187\n",
      "The 44599 th iteration gives loss of 0.13784827582902065\n",
      "The 44600 th iteration gives loss of 0.13784703729445194\n",
      "The 44601 th iteration gives loss of 0.13784579878150435\n",
      "The 44602 th iteration gives loss of 0.13784456029018977\n",
      "The 44603 th iteration gives loss of 0.13784332182048348\n",
      "The 44604 th iteration gives loss of 0.13784208337239057\n",
      "The 44605 th iteration gives loss of 0.13784084494591736\n",
      "The 44606 th iteration gives loss of 0.13783960654105806\n",
      "The 44607 th iteration gives loss of 0.13783836815781075\n",
      "The 44608 th iteration gives loss of 0.13783712979617074\n",
      "The 44609 th iteration gives loss of 0.1378358914561445\n",
      "The 44610 th iteration gives loss of 0.1378346531377286\n",
      "The 44611 th iteration gives loss of 0.13783341484090275\n",
      "The 44612 th iteration gives loss of 0.13783217656569197\n",
      "The 44613 th iteration gives loss of 0.1378309383120774\n",
      "The 44614 th iteration gives loss of 0.13782970008006645\n",
      "The 44615 th iteration gives loss of 0.13782846186965375\n",
      "The 44616 th iteration gives loss of 0.13782722368082861\n",
      "The 44617 th iteration gives loss of 0.13782598551360248\n",
      "The 44618 th iteration gives loss of 0.13782474736797173\n",
      "The 44619 th iteration gives loss of 0.13782350924392658\n",
      "The 44620 th iteration gives loss of 0.137822271141478\n",
      "The 44621 th iteration gives loss of 0.13782103306060023\n",
      "The 44622 th iteration gives loss of 0.1378197950013278\n",
      "The 44623 th iteration gives loss of 0.13781855696362702\n",
      "The 44624 th iteration gives loss of 0.13781731894751276\n",
      "The 44625 th iteration gives loss of 0.13781608095297815\n",
      "The 44626 th iteration gives loss of 0.13781484298002075\n",
      "The 44627 th iteration gives loss of 0.13781360502864318\n",
      "The 44628 th iteration gives loss of 0.13781236709883574\n",
      "The 44629 th iteration gives loss of 0.1378111291906092\n",
      "The 44630 th iteration gives loss of 0.13780989130394167\n",
      "The 44631 th iteration gives loss of 0.13780865343885357\n",
      "The 44632 th iteration gives loss of 0.1378074155953339\n",
      "The 44633 th iteration gives loss of 0.1378061777733762\n",
      "The 44634 th iteration gives loss of 0.13780493997298215\n",
      "The 44635 th iteration gives loss of 0.1378037021941476\n",
      "The 44636 th iteration gives loss of 0.1378024644368766\n",
      "The 44637 th iteration gives loss of 0.13780122670116995\n",
      "The 44638 th iteration gives loss of 0.13779998898701096\n",
      "The 44639 th iteration gives loss of 0.13779875129441846\n",
      "The 44640 th iteration gives loss of 0.13779751362336717\n",
      "The 44641 th iteration gives loss of 0.1377962759738736\n",
      "The 44642 th iteration gives loss of 0.1377950383459313\n",
      "The 44643 th iteration gives loss of 0.13779380073954042\n",
      "The 44644 th iteration gives loss of 0.1377925631546978\n",
      "The 44645 th iteration gives loss of 0.13779132559139298\n",
      "The 44646 th iteration gives loss of 0.13779008804963278\n",
      "The 44647 th iteration gives loss of 0.13778885052941547\n",
      "The 44648 th iteration gives loss of 0.13778761303073303\n",
      "The 44649 th iteration gives loss of 0.1377863755535929\n",
      "The 44650 th iteration gives loss of 0.13778513809799933\n",
      "The 44651 th iteration gives loss of 0.13778390066392338\n",
      "The 44652 th iteration gives loss of 0.13778266325139418\n",
      "The 44653 th iteration gives loss of 0.13778142586037712\n",
      "The 44654 th iteration gives loss of 0.1377801884909085\n",
      "The 44655 th iteration gives loss of 0.1377789511429575\n",
      "The 44656 th iteration gives loss of 0.13777771381653325\n",
      "The 44657 th iteration gives loss of 0.137776476511639\n",
      "The 44658 th iteration gives loss of 0.13777523922825546\n",
      "The 44659 th iteration gives loss of 0.13777400196640524\n",
      "The 44660 th iteration gives loss of 0.13777276472606487\n",
      "The 44661 th iteration gives loss of 0.13777152750724617\n",
      "The 44662 th iteration gives loss of 0.13777029030993868\n",
      "The 44663 th iteration gives loss of 0.13776905313414525\n",
      "The 44664 th iteration gives loss of 0.13776781597985974\n",
      "The 44665 th iteration gives loss of 0.1377665788470944\n",
      "The 44666 th iteration gives loss of 0.1377653417358315\n",
      "The 44667 th iteration gives loss of 0.13776410464606967\n",
      "The 44668 th iteration gives loss of 0.13776286757781925\n",
      "The 44669 th iteration gives loss of 0.13776163053106694\n",
      "The 44670 th iteration gives loss of 0.13776039350582545\n",
      "The 44671 th iteration gives loss of 0.13775915650207562\n",
      "The 44672 th iteration gives loss of 0.1377579195198245\n",
      "The 44673 th iteration gives loss of 0.13775668255906906\n",
      "The 44674 th iteration gives loss of 0.1377554456198099\n",
      "The 44675 th iteration gives loss of 0.13775420870204066\n",
      "The 44676 th iteration gives loss of 0.13775297180576448\n",
      "The 44677 th iteration gives loss of 0.1377517349309737\n",
      "The 44678 th iteration gives loss of 0.13775049807767004\n",
      "The 44679 th iteration gives loss of 0.13774926124585074\n",
      "The 44680 th iteration gives loss of 0.13774802443552275\n",
      "The 44681 th iteration gives loss of 0.1377467876466729\n",
      "The 44682 th iteration gives loss of 0.13774555087929818\n",
      "The 44683 th iteration gives loss of 0.13774431413340954\n",
      "The 44684 th iteration gives loss of 0.13774307740899577\n",
      "The 44685 th iteration gives loss of 0.13774184070605\n",
      "The 44686 th iteration gives loss of 0.13774060402458696\n",
      "The 44687 th iteration gives loss of 0.13773936736458997\n",
      "The 44688 th iteration gives loss of 0.13773813072606522\n",
      "The 44689 th iteration gives loss of 0.13773689410900705\n",
      "The 44690 th iteration gives loss of 0.13773565751341638\n",
      "The 44691 th iteration gives loss of 0.13773442093928986\n",
      "The 44692 th iteration gives loss of 0.13773318438662271\n",
      "The 44693 th iteration gives loss of 0.1377319478554212\n",
      "The 44694 th iteration gives loss of 0.13773071134567658\n",
      "The 44695 th iteration gives loss of 0.1377294748573896\n",
      "The 44696 th iteration gives loss of 0.1377282383905574\n",
      "The 44697 th iteration gives loss of 0.13772700194518106\n",
      "The 44698 th iteration gives loss of 0.13772576552124993\n",
      "The 44699 th iteration gives loss of 0.13772452911878055\n",
      "The 44700 th iteration gives loss of 0.13772329273775683\n",
      "The 44701 th iteration gives loss of 0.13772205637817972\n",
      "The 44702 th iteration gives loss of 0.13772082004003866\n",
      "The 44703 th iteration gives loss of 0.13771958372335058\n",
      "The 44704 th iteration gives loss of 0.13771834742810463\n",
      "The 44705 th iteration gives loss of 0.13771711115430302\n",
      "The 44706 th iteration gives loss of 0.13771587490192852\n",
      "The 44707 th iteration gives loss of 0.13771463867100042\n",
      "The 44708 th iteration gives loss of 0.13771340246150218\n",
      "The 44709 th iteration gives loss of 0.13771216627344435\n",
      "The 44710 th iteration gives loss of 0.13771093010680668\n",
      "The 44711 th iteration gives loss of 0.1377096939615999\n",
      "The 44712 th iteration gives loss of 0.1377084578378264\n",
      "The 44713 th iteration gives loss of 0.137707221735477\n",
      "The 44714 th iteration gives loss of 0.13770598565455636\n",
      "The 44715 th iteration gives loss of 0.1377047495950551\n",
      "The 44716 th iteration gives loss of 0.13770351355697263\n",
      "The 44717 th iteration gives loss of 0.13770227754030878\n",
      "The 44718 th iteration gives loss of 0.13770104154506832\n",
      "The 44719 th iteration gives loss of 0.13769980557123504\n",
      "The 44720 th iteration gives loss of 0.1376985696188215\n",
      "The 44721 th iteration gives loss of 0.13769733368782344\n",
      "The 44722 th iteration gives loss of 0.13769609777823463\n",
      "The 44723 th iteration gives loss of 0.13769486189004929\n",
      "The 44724 th iteration gives loss of 0.13769362602326865\n",
      "The 44725 th iteration gives loss of 0.13769239017790397\n",
      "The 44726 th iteration gives loss of 0.1376911543539363\n",
      "The 44727 th iteration gives loss of 0.13768991855137172\n",
      "The 44728 th iteration gives loss of 0.13768868277020727\n",
      "The 44729 th iteration gives loss of 0.13768744701045038\n",
      "The 44730 th iteration gives loss of 0.13768621127207478\n",
      "The 44731 th iteration gives loss of 0.13768497555510484\n",
      "The 44732 th iteration gives loss of 0.13768373985951665\n",
      "The 44733 th iteration gives loss of 0.1376825041853344\n",
      "The 44734 th iteration gives loss of 0.13768126853253404\n",
      "The 44735 th iteration gives loss of 0.13768003290112107\n",
      "The 44736 th iteration gives loss of 0.13767879729109664\n",
      "The 44737 th iteration gives loss of 0.137677561702458\n",
      "The 44738 th iteration gives loss of 0.1376763261351983\n",
      "The 44739 th iteration gives loss of 0.13767509058932256\n",
      "The 44740 th iteration gives loss of 0.13767385506482768\n",
      "The 44741 th iteration gives loss of 0.1376726195617112\n",
      "The 44742 th iteration gives loss of 0.13767138407996582\n",
      "The 44743 th iteration gives loss of 0.1376701486195988\n",
      "The 44744 th iteration gives loss of 0.1376689131806023\n",
      "The 44745 th iteration gives loss of 0.1376676777629726\n",
      "The 44746 th iteration gives loss of 0.13766644236671938\n",
      "The 44747 th iteration gives loss of 0.13766520699183396\n",
      "The 44748 th iteration gives loss of 0.13766397163830668\n",
      "The 44749 th iteration gives loss of 0.13766273630614453\n",
      "The 44750 th iteration gives loss of 0.1376615009953507\n",
      "The 44751 th iteration gives loss of 0.13766026570590834\n",
      "The 44752 th iteration gives loss of 0.13765903043783428\n",
      "The 44753 th iteration gives loss of 0.13765779519111207\n",
      "The 44754 th iteration gives loss of 0.13765655996574805\n",
      "The 44755 th iteration gives loss of 0.13765532476173456\n",
      "The 44756 th iteration gives loss of 0.13765408957907393\n",
      "The 44757 th iteration gives loss of 0.13765285441776207\n",
      "The 44758 th iteration gives loss of 0.1376516192778033\n",
      "The 44759 th iteration gives loss of 0.13765038415918773\n",
      "The 44760 th iteration gives loss of 0.1376491490619074\n",
      "The 44761 th iteration gives loss of 0.13764791398598047\n",
      "The 44762 th iteration gives loss of 0.13764667893139362\n",
      "The 44763 th iteration gives loss of 0.1376454438981476\n",
      "The 44764 th iteration gives loss of 0.1376442088862379\n",
      "The 44765 th iteration gives loss of 0.13764297389567795\n",
      "The 44766 th iteration gives loss of 0.13764173892643808\n",
      "The 44767 th iteration gives loss of 0.13764050397853425\n",
      "The 44768 th iteration gives loss of 0.1376392690519578\n",
      "The 44769 th iteration gives loss of 0.13763803414671108\n",
      "The 44770 th iteration gives loss of 0.1376367992627907\n",
      "The 44771 th iteration gives loss of 0.13763556440019578\n",
      "The 44772 th iteration gives loss of 0.13763432955892815\n",
      "The 44773 th iteration gives loss of 0.1376330947389945\n",
      "The 44774 th iteration gives loss of 0.13763185994037017\n",
      "The 44775 th iteration gives loss of 0.13763062516305702\n",
      "The 44776 th iteration gives loss of 0.13762939040707559\n",
      "The 44777 th iteration gives loss of 0.1376281556724024\n",
      "The 44778 th iteration gives loss of 0.13762692095905177\n",
      "The 44779 th iteration gives loss of 0.1376256862669985\n",
      "The 44780 th iteration gives loss of 0.13762445159626943\n",
      "The 44781 th iteration gives loss of 0.13762321694684335\n",
      "The 44782 th iteration gives loss of 0.1376219823187183\n",
      "The 44783 th iteration gives loss of 0.13762074771190233\n",
      "The 44784 th iteration gives loss of 0.137619513126391\n",
      "The 44785 th iteration gives loss of 0.1376182785621848\n",
      "The 44786 th iteration gives loss of 0.13761704401927702\n",
      "The 44787 th iteration gives loss of 0.13761580949767016\n",
      "The 44788 th iteration gives loss of 0.1376145749973508\n",
      "The 44789 th iteration gives loss of 0.13761334051833943\n",
      "The 44790 th iteration gives loss of 0.13761210606061\n",
      "The 44791 th iteration gives loss of 0.13761087162417623\n",
      "The 44792 th iteration gives loss of 0.13760963720903857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 44793 th iteration gives loss of 0.13760840281517842\n",
      "The 44794 th iteration gives loss of 0.1376071684426101\n",
      "The 44795 th iteration gives loss of 0.13760593409132696\n",
      "The 44796 th iteration gives loss of 0.1376046997613286\n",
      "The 44797 th iteration gives loss of 0.13760346545260482\n",
      "The 44798 th iteration gives loss of 0.13760223116516343\n",
      "The 44799 th iteration gives loss of 0.1376009968989968\n",
      "The 44800 th iteration gives loss of 0.13759976265411292\n",
      "The 44801 th iteration gives loss of 0.13759852843050335\n",
      "The 44802 th iteration gives loss of 0.13759729422816624\n",
      "The 44803 th iteration gives loss of 0.13759606004710148\n",
      "The 44804 th iteration gives loss of 0.13759482588729993\n",
      "The 44805 th iteration gives loss of 0.13759359174877023\n",
      "The 44806 th iteration gives loss of 0.13759235763150332\n",
      "The 44807 th iteration gives loss of 0.13759112353549546\n",
      "The 44808 th iteration gives loss of 0.13758988946075884\n",
      "The 44809 th iteration gives loss of 0.13758865540728196\n",
      "The 44810 th iteration gives loss of 0.13758742137505822\n",
      "The 44811 th iteration gives loss of 0.13758618736409395\n",
      "The 44812 th iteration gives loss of 0.13758495337438922\n",
      "The 44813 th iteration gives loss of 0.13758371940593192\n",
      "The 44814 th iteration gives loss of 0.13758248545873697\n",
      "The 44815 th iteration gives loss of 0.1375812515327869\n",
      "The 44816 th iteration gives loss of 0.13758001762808691\n",
      "The 44817 th iteration gives loss of 0.13757878374463686\n",
      "The 44818 th iteration gives loss of 0.1375775498824311\n",
      "The 44819 th iteration gives loss of 0.13757631604145848\n",
      "The 44820 th iteration gives loss of 0.13757508222174158\n",
      "The 44821 th iteration gives loss of 0.13757384842325077\n",
      "The 44822 th iteration gives loss of 0.1375726146460083\n",
      "The 44823 th iteration gives loss of 0.1375713808900037\n",
      "The 44824 th iteration gives loss of 0.13757014715522622\n",
      "The 44825 th iteration gives loss of 0.1375689134416873\n",
      "The 44826 th iteration gives loss of 0.1375676797493835\n",
      "The 44827 th iteration gives loss of 0.1375664460783051\n",
      "The 44828 th iteration gives loss of 0.1375652124284528\n",
      "The 44829 th iteration gives loss of 0.1375639787998373\n",
      "The 44830 th iteration gives loss of 0.13756274519243508\n",
      "The 44831 th iteration gives loss of 0.13756151160626343\n",
      "The 44832 th iteration gives loss of 0.1375602780413064\n",
      "The 44833 th iteration gives loss of 0.13755904449757644\n",
      "The 44834 th iteration gives loss of 0.13755781097505962\n",
      "The 44835 th iteration gives loss of 0.1375565774737587\n",
      "The 44836 th iteration gives loss of 0.13755534399367092\n",
      "The 44837 th iteration gives loss of 0.13755411053480032\n",
      "The 44838 th iteration gives loss of 0.13755287709714012\n",
      "The 44839 th iteration gives loss of 0.13755164368069425\n",
      "The 44840 th iteration gives loss of 0.13755041028545045\n",
      "The 44841 th iteration gives loss of 0.13754917691141602\n",
      "The 44842 th iteration gives loss of 0.13754794355857752\n",
      "The 44843 th iteration gives loss of 0.1375467102269514\n",
      "The 44844 th iteration gives loss of 0.13754547691651434\n",
      "The 44845 th iteration gives loss of 0.13754424362729667\n",
      "The 44846 th iteration gives loss of 0.1375430103592574\n",
      "The 44847 th iteration gives loss of 0.13754177711242482\n",
      "The 44848 th iteration gives loss of 0.1375405438867865\n",
      "The 44849 th iteration gives loss of 0.13753931068233569\n",
      "The 44850 th iteration gives loss of 0.13753807749907937\n",
      "The 44851 th iteration gives loss of 0.1375368443370177\n",
      "The 44852 th iteration gives loss of 0.13753561119612515\n",
      "The 44853 th iteration gives loss of 0.13753437807642932\n",
      "The 44854 th iteration gives loss of 0.137533144977919\n",
      "The 44855 th iteration gives loss of 0.13753191190058753\n",
      "The 44856 th iteration gives loss of 0.13753067884444184\n",
      "The 44857 th iteration gives loss of 0.13752944580946944\n",
      "The 44858 th iteration gives loss of 0.13752821279567898\n",
      "The 44859 th iteration gives loss of 0.1375269798030574\n",
      "The 44860 th iteration gives loss of 0.13752574683161523\n",
      "The 44861 th iteration gives loss of 0.13752451388134768\n",
      "The 44862 th iteration gives loss of 0.13752328095224067\n",
      "The 44863 th iteration gives loss of 0.13752204804430748\n",
      "The 44864 th iteration gives loss of 0.13752081515753928\n",
      "The 44865 th iteration gives loss of 0.13751958229193917\n",
      "The 44866 th iteration gives loss of 0.13751834944750688\n",
      "The 44867 th iteration gives loss of 0.13751711662423177\n",
      "The 44868 th iteration gives loss of 0.13751588382212357\n",
      "The 44869 th iteration gives loss of 0.13751465104116786\n",
      "The 44870 th iteration gives loss of 0.13751341828137303\n",
      "The 44871 th iteration gives loss of 0.13751218554272565\n",
      "The 44872 th iteration gives loss of 0.1375109528252371\n",
      "The 44873 th iteration gives loss of 0.1375097201289011\n",
      "The 44874 th iteration gives loss of 0.13750848745370403\n",
      "The 44875 th iteration gives loss of 0.13750725479966563\n",
      "The 44876 th iteration gives loss of 0.13750602216677282\n",
      "The 44877 th iteration gives loss of 0.13750478955502485\n",
      "The 44878 th iteration gives loss of 0.1375035569644181\n",
      "The 44879 th iteration gives loss of 0.13750232439495913\n",
      "The 44880 th iteration gives loss of 0.13750109184663722\n",
      "The 44881 th iteration gives loss of 0.13749985931944778\n",
      "The 44882 th iteration gives loss of 0.13749862681339692\n",
      "The 44883 th iteration gives loss of 0.1374973943284884\n",
      "The 44884 th iteration gives loss of 0.13749616186470534\n",
      "The 44885 th iteration gives loss of 0.13749492942205913\n",
      "The 44886 th iteration gives loss of 0.13749369700053543\n",
      "The 44887 th iteration gives loss of 0.13749246460014672\n",
      "The 44888 th iteration gives loss of 0.13749123222088094\n",
      "The 44889 th iteration gives loss of 0.13748999986274404\n",
      "The 44890 th iteration gives loss of 0.13748876752572453\n",
      "The 44891 th iteration gives loss of 0.13748753520983187\n",
      "The 44892 th iteration gives loss of 0.13748630291505679\n",
      "The 44893 th iteration gives loss of 0.13748507064140428\n",
      "The 44894 th iteration gives loss of 0.13748383838886466\n",
      "The 44895 th iteration gives loss of 0.1374826061574368\n",
      "The 44896 th iteration gives loss of 0.1374813739471138\n",
      "The 44897 th iteration gives loss of 0.1374801417579137\n",
      "The 44898 th iteration gives loss of 0.1374789095898183\n",
      "The 44899 th iteration gives loss of 0.13747767744283404\n",
      "The 44900 th iteration gives loss of 0.13747644531695397\n",
      "The 44901 th iteration gives loss of 0.13747521321218253\n",
      "The 44902 th iteration gives loss of 0.1374739811285102\n",
      "The 44903 th iteration gives loss of 0.13747274906594037\n",
      "The 44904 th iteration gives loss of 0.1374715170244657\n",
      "The 44905 th iteration gives loss of 0.13747028500409056\n",
      "The 44906 th iteration gives loss of 0.1374690530048157\n",
      "The 44907 th iteration gives loss of 0.1374678210266359\n",
      "The 44908 th iteration gives loss of 0.13746658906954548\n",
      "The 44909 th iteration gives loss of 0.13746535713354424\n",
      "The 44910 th iteration gives loss of 0.13746412521863302\n",
      "The 44911 th iteration gives loss of 0.13746289332480474\n",
      "The 44912 th iteration gives loss of 0.13746166145206726\n",
      "The 44913 th iteration gives loss of 0.1374604296004203\n",
      "The 44914 th iteration gives loss of 0.13745919776985338\n",
      "The 44915 th iteration gives loss of 0.13745796596035895\n",
      "The 44916 th iteration gives loss of 0.13745673417194684\n",
      "The 44917 th iteration gives loss of 0.13745550240461585\n",
      "The 44918 th iteration gives loss of 0.13745427065836033\n",
      "The 44919 th iteration gives loss of 0.13745303893318087\n",
      "The 44920 th iteration gives loss of 0.13745180722907074\n",
      "The 44921 th iteration gives loss of 0.13745057554602783\n",
      "The 44922 th iteration gives loss of 0.13744934388406008\n",
      "The 44923 th iteration gives loss of 0.13744811224315767\n",
      "The 44924 th iteration gives loss of 0.13744688062332155\n",
      "The 44925 th iteration gives loss of 0.13744564902455225\n",
      "The 44926 th iteration gives loss of 0.1374444174468376\n",
      "The 44927 th iteration gives loss of 0.13744318589018747\n",
      "The 44928 th iteration gives loss of 0.13744195435459847\n",
      "The 44929 th iteration gives loss of 0.13744072284005868\n",
      "The 44930 th iteration gives loss of 0.13743949134659053\n",
      "The 44931 th iteration gives loss of 0.1374382598741628\n",
      "The 44932 th iteration gives loss of 0.13743702842279337\n",
      "The 44933 th iteration gives loss of 0.1374357969924745\n",
      "The 44934 th iteration gives loss of 0.13743456558319703\n",
      "The 44935 th iteration gives loss of 0.13743333419497414\n",
      "The 44936 th iteration gives loss of 0.13743210282779109\n",
      "The 44937 th iteration gives loss of 0.13743087148166813\n",
      "The 44938 th iteration gives loss of 0.13742964015657066\n",
      "The 44939 th iteration gives loss of 0.13742840885251761\n",
      "The 44940 th iteration gives loss of 0.1374271775695039\n",
      "The 44941 th iteration gives loss of 0.13742594630752392\n",
      "The 44942 th iteration gives loss of 0.13742471506658918\n",
      "The 44943 th iteration gives loss of 0.13742348384667918\n",
      "The 44944 th iteration gives loss of 0.1374222526478102\n",
      "The 44945 th iteration gives loss of 0.13742102146996366\n",
      "The 44946 th iteration gives loss of 0.1374197903131433\n",
      "The 44947 th iteration gives loss of 0.13741855917736515\n",
      "The 44948 th iteration gives loss of 0.1374173280625961\n",
      "The 44949 th iteration gives loss of 0.13741609696885193\n",
      "The 44950 th iteration gives loss of 0.13741486589614196\n",
      "The 44951 th iteration gives loss of 0.13741363484444546\n",
      "The 44952 th iteration gives loss of 0.13741240381377168\n",
      "The 44953 th iteration gives loss of 0.1374111728041133\n",
      "The 44954 th iteration gives loss of 0.13740994181546834\n",
      "The 44955 th iteration gives loss of 0.137408710847836\n",
      "The 44956 th iteration gives loss of 0.13740747990121552\n",
      "The 44957 th iteration gives loss of 0.1374062489756074\n",
      "The 44958 th iteration gives loss of 0.13740501807100922\n",
      "The 44959 th iteration gives loss of 0.13740378718742186\n",
      "The 44960 th iteration gives loss of 0.13740255632483298\n",
      "The 44961 th iteration gives loss of 0.1374013254832513\n",
      "The 44962 th iteration gives loss of 0.13740009466267278\n",
      "The 44963 th iteration gives loss of 0.13739886386308575\n",
      "The 44964 th iteration gives loss of 0.13739763308450298\n",
      "The 44965 th iteration gives loss of 0.13739640232692113\n",
      "The 44966 th iteration gives loss of 0.13739517159033238\n",
      "The 44967 th iteration gives loss of 0.13739394087474024\n",
      "The 44968 th iteration gives loss of 0.13739271018014082\n",
      "The 44969 th iteration gives loss of 0.137391479506521\n",
      "The 44970 th iteration gives loss of 0.1373902488538973\n",
      "The 44971 th iteration gives loss of 0.13738901822226618\n",
      "The 44972 th iteration gives loss of 0.13738778761161566\n",
      "The 44973 th iteration gives loss of 0.13738655702194552\n",
      "The 44974 th iteration gives loss of 0.13738532645325943\n",
      "The 44975 th iteration gives loss of 0.13738409590554784\n",
      "The 44976 th iteration gives loss of 0.13738286537882996\n",
      "The 44977 th iteration gives loss of 0.13738163487307983\n",
      "The 44978 th iteration gives loss of 0.13738040438830873\n",
      "The 44979 th iteration gives loss of 0.1373791739245014\n",
      "The 44980 th iteration gives loss of 0.13737794348167093\n",
      "The 44981 th iteration gives loss of 0.1373767130598173\n",
      "The 44982 th iteration gives loss of 0.1373754826589252\n",
      "The 44983 th iteration gives loss of 0.137374252279004\n",
      "The 44984 th iteration gives loss of 0.13737302192004777\n",
      "The 44985 th iteration gives loss of 0.13737179158204596\n",
      "The 44986 th iteration gives loss of 0.13737056126502348\n",
      "The 44987 th iteration gives loss of 0.13736933096895326\n",
      "The 44988 th iteration gives loss of 0.1373681006938459\n",
      "The 44989 th iteration gives loss of 0.13736687043968923\n",
      "The 44990 th iteration gives loss of 0.1373656402064915\n",
      "The 44991 th iteration gives loss of 0.13736440999424424\n",
      "The 44992 th iteration gives loss of 0.13736317980295432\n",
      "The 44993 th iteration gives loss of 0.1373619496326077\n",
      "The 44994 th iteration gives loss of 0.13736071948321577\n",
      "The 44995 th iteration gives loss of 0.13735948935477046\n",
      "The 44996 th iteration gives loss of 0.1373582592472704\n",
      "The 44997 th iteration gives loss of 0.13735702916071096\n",
      "The 44998 th iteration gives loss of 0.13735579909509624\n",
      "The 44999 th iteration gives loss of 0.1373545690504225\n",
      "The 45000 th iteration gives loss of 0.1373533390266818\n",
      "The 45001 th iteration gives loss of 0.13735210902388936\n",
      "The 45002 th iteration gives loss of 0.13735087904202525\n",
      "The 45003 th iteration gives loss of 0.13734964908110123\n",
      "The 45004 th iteration gives loss of 0.13734841914109971\n",
      "The 45005 th iteration gives loss of 0.13734718922202765\n",
      "The 45006 th iteration gives loss of 0.1373459593238965\n",
      "The 45007 th iteration gives loss of 0.1373447294466818\n",
      "The 45008 th iteration gives loss of 0.1373434995904016\n",
      "The 45009 th iteration gives loss of 0.1373422697550357\n",
      "The 45010 th iteration gives loss of 0.13734103994060032\n",
      "The 45011 th iteration gives loss of 0.13733981014707444\n",
      "The 45012 th iteration gives loss of 0.1373385803744801\n",
      "The 45013 th iteration gives loss of 0.1373373506227951\n",
      "The 45014 th iteration gives loss of 0.13733612089202785\n",
      "The 45015 th iteration gives loss of 0.13733489118217673\n",
      "The 45016 th iteration gives loss of 0.13733366149323428\n",
      "The 45017 th iteration gives loss of 0.1373324318252103\n",
      "The 45018 th iteration gives loss of 0.1373312021780866\n",
      "The 45019 th iteration gives loss of 0.13732997255187096\n",
      "The 45020 th iteration gives loss of 0.13732874294656555\n",
      "The 45021 th iteration gives loss of 0.13732751336216212\n",
      "The 45022 th iteration gives loss of 0.1373262837986653\n",
      "The 45023 th iteration gives loss of 0.1373250542560694\n",
      "The 45024 th iteration gives loss of 0.1373238247343626\n",
      "The 45025 th iteration gives loss of 0.13732259523356125\n",
      "The 45026 th iteration gives loss of 0.13732136575365056\n",
      "The 45027 th iteration gives loss of 0.13732013629463702\n",
      "The 45028 th iteration gives loss of 0.1373189068565106\n",
      "The 45029 th iteration gives loss of 0.13731767743928666\n",
      "The 45030 th iteration gives loss of 0.1373164480429422\n",
      "The 45031 th iteration gives loss of 0.13731521866748292\n",
      "The 45032 th iteration gives loss of 0.13731398931291594\n",
      "The 45033 th iteration gives loss of 0.13731275997922673\n",
      "The 45034 th iteration gives loss of 0.13731153066642912\n",
      "The 45035 th iteration gives loss of 0.1373103013745105\n",
      "The 45036 th iteration gives loss of 0.13730907210346727\n",
      "The 45037 th iteration gives loss of 0.13730784285330347\n",
      "The 45038 th iteration gives loss of 0.1373066136240159\n",
      "The 45039 th iteration gives loss of 0.13730538441560108\n",
      "The 45040 th iteration gives loss of 0.13730415522806255\n",
      "The 45041 th iteration gives loss of 0.1373029260613938\n",
      "The 45042 th iteration gives loss of 0.13730169691559135\n",
      "The 45043 th iteration gives loss of 0.1373004677906595\n",
      "The 45044 th iteration gives loss of 0.1372992386865897\n",
      "The 45045 th iteration gives loss of 0.13729800960338553\n",
      "The 45046 th iteration gives loss of 0.1372967805410427\n",
      "The 45047 th iteration gives loss of 0.13729555149956288\n",
      "The 45048 th iteration gives loss of 0.13729432247894263\n",
      "The 45049 th iteration gives loss of 0.13729309347918217\n",
      "The 45050 th iteration gives loss of 0.13729186450027944\n",
      "The 45051 th iteration gives loss of 0.13729063554222745\n",
      "The 45052 th iteration gives loss of 0.1372894066050309\n",
      "The 45053 th iteration gives loss of 0.13728817768868867\n",
      "The 45054 th iteration gives loss of 0.1372869487931867\n",
      "The 45055 th iteration gives loss of 0.1372857199185371\n",
      "The 45056 th iteration gives loss of 0.13728449106473303\n",
      "The 45057 th iteration gives loss of 0.1372832622317752\n",
      "The 45058 th iteration gives loss of 0.13728203341965436\n",
      "The 45059 th iteration gives loss of 0.13728080462838352\n",
      "The 45060 th iteration gives loss of 0.13727957585794545\n",
      "The 45061 th iteration gives loss of 0.1372783471083479\n",
      "The 45062 th iteration gives loss of 0.1372771183795876\n",
      "The 45063 th iteration gives loss of 0.13727588967166923\n",
      "The 45064 th iteration gives loss of 0.137274660984577\n",
      "The 45065 th iteration gives loss of 0.1372734323183117\n",
      "The 45066 th iteration gives loss of 0.13727220367287682\n",
      "The 45067 th iteration gives loss of 0.13727097504827954\n",
      "The 45068 th iteration gives loss of 0.13726974644449697\n",
      "The 45069 th iteration gives loss of 0.13726851786154798\n",
      "The 45070 th iteration gives loss of 0.13726728929942325\n",
      "The 45071 th iteration gives loss of 0.13726606075811953\n",
      "The 45072 th iteration gives loss of 0.13726483223762304\n",
      "The 45073 th iteration gives loss of 0.13726360373795735\n",
      "The 45074 th iteration gives loss of 0.1372623752591063\n",
      "The 45075 th iteration gives loss of 0.1372611468010734\n",
      "The 45076 th iteration gives loss of 0.13725991836385157\n",
      "The 45077 th iteration gives loss of 0.13725868994743334\n",
      "The 45078 th iteration gives loss of 0.13725746155183188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 45079 th iteration gives loss of 0.13725623317703822\n",
      "The 45080 th iteration gives loss of 0.13725500482305444\n",
      "The 45081 th iteration gives loss of 0.13725377648987647\n",
      "The 45082 th iteration gives loss of 0.1372525481774963\n",
      "The 45083 th iteration gives loss of 0.13725131988592282\n",
      "The 45084 th iteration gives loss of 0.13725009161514734\n",
      "The 45085 th iteration gives loss of 0.13724886336516973\n",
      "The 45086 th iteration gives loss of 0.1372476351359884\n",
      "The 45087 th iteration gives loss of 0.13724640692760628\n",
      "The 45088 th iteration gives loss of 0.1372451787400187\n",
      "The 45089 th iteration gives loss of 0.13724395057321914\n",
      "The 45090 th iteration gives loss of 0.137242722427216\n",
      "The 45091 th iteration gives loss of 0.13724149430199656\n",
      "The 45092 th iteration gives loss of 0.13724026619756946\n",
      "The 45093 th iteration gives loss of 0.13723903811392188\n",
      "The 45094 th iteration gives loss of 0.13723781005105526\n",
      "The 45095 th iteration gives loss of 0.13723658200897454\n",
      "The 45096 th iteration gives loss of 0.13723535398767717\n",
      "The 45097 th iteration gives loss of 0.1372341259871582\n",
      "The 45098 th iteration gives loss of 0.13723289800741012\n",
      "The 45099 th iteration gives loss of 0.13723167004844428\n",
      "The 45100 th iteration gives loss of 0.13723044211025273\n",
      "The 45101 th iteration gives loss of 0.13722921419283762\n",
      "The 45102 th iteration gives loss of 0.13722798629618518\n",
      "The 45103 th iteration gives loss of 0.13722675842030138\n",
      "The 45104 th iteration gives loss of 0.1372255305651861\n",
      "The 45105 th iteration gives loss of 0.13722430273084263\n",
      "The 45106 th iteration gives loss of 0.1372230749172592\n",
      "The 45107 th iteration gives loss of 0.1372218471244419\n",
      "The 45108 th iteration gives loss of 0.1372206193523807\n",
      "The 45109 th iteration gives loss of 0.13721939160108193\n",
      "The 45110 th iteration gives loss of 0.1372181638705423\n",
      "The 45111 th iteration gives loss of 0.13721693616075648\n",
      "The 45112 th iteration gives loss of 0.13721570847172548\n",
      "The 45113 th iteration gives loss of 0.13721448080345172\n",
      "The 45114 th iteration gives loss of 0.13721325315592164\n",
      "The 45115 th iteration gives loss of 0.13721202552914374\n",
      "The 45116 th iteration gives loss of 0.13721079792311985\n",
      "The 45117 th iteration gives loss of 0.13720957033783673\n",
      "The 45118 th iteration gives loss of 0.137208342773294\n",
      "The 45119 th iteration gives loss of 0.13720711522949786\n",
      "The 45120 th iteration gives loss of 0.13720588770644584\n",
      "The 45121 th iteration gives loss of 0.13720466020413782\n",
      "The 45122 th iteration gives loss of 0.13720343272256005\n",
      "The 45123 th iteration gives loss of 0.1372022052617211\n",
      "The 45124 th iteration gives loss of 0.1372009778216157\n",
      "The 45125 th iteration gives loss of 0.13719975040223845\n",
      "The 45126 th iteration gives loss of 0.137198523003598\n",
      "The 45127 th iteration gives loss of 0.13719729562569033\n",
      "The 45128 th iteration gives loss of 0.13719606826851788\n",
      "The 45129 th iteration gives loss of 0.13719484093206247\n",
      "The 45130 th iteration gives loss of 0.1371936136163364\n",
      "The 45131 th iteration gives loss of 0.13719238632133338\n",
      "The 45132 th iteration gives loss of 0.13719115904704868\n",
      "The 45133 th iteration gives loss of 0.13718993179348582\n",
      "The 45134 th iteration gives loss of 0.13718870456063836\n",
      "The 45135 th iteration gives loss of 0.1371874773485136\n",
      "The 45136 th iteration gives loss of 0.1371862501571034\n",
      "The 45137 th iteration gives loss of 0.13718502298640955\n",
      "The 45138 th iteration gives loss of 0.13718379583641682\n",
      "The 45139 th iteration gives loss of 0.13718256870714698\n",
      "The 45140 th iteration gives loss of 0.1371813415985877\n",
      "The 45141 th iteration gives loss of 0.13718011451072526\n",
      "The 45142 th iteration gives loss of 0.1371788874435713\n",
      "The 45143 th iteration gives loss of 0.13717766039712137\n",
      "The 45144 th iteration gives loss of 0.13717643337137744\n",
      "The 45145 th iteration gives loss of 0.13717520636633232\n",
      "The 45146 th iteration gives loss of 0.13717397938197812\n",
      "The 45147 th iteration gives loss of 0.13717275241834026\n",
      "The 45148 th iteration gives loss of 0.13717152547538447\n",
      "The 45149 th iteration gives loss of 0.1371702985531263\n",
      "The 45150 th iteration gives loss of 0.13716907165156356\n",
      "The 45151 th iteration gives loss of 0.13716784477068647\n",
      "The 45152 th iteration gives loss of 0.13716661791049933\n",
      "The 45153 th iteration gives loss of 0.1371653910710059\n",
      "The 45154 th iteration gives loss of 0.13716416425219252\n",
      "The 45155 th iteration gives loss of 0.13716293745406752\n",
      "The 45156 th iteration gives loss of 0.13716171067662677\n",
      "The 45157 th iteration gives loss of 0.1371604839198644\n",
      "The 45158 th iteration gives loss of 0.1371592571837816\n",
      "The 45159 th iteration gives loss of 0.1371580304683766\n",
      "The 45160 th iteration gives loss of 0.13715680377364992\n",
      "The 45161 th iteration gives loss of 0.1371555770996059\n",
      "The 45162 th iteration gives loss of 0.1371543504462184\n",
      "The 45163 th iteration gives loss of 0.1371531238135143\n",
      "The 45164 th iteration gives loss of 0.13715189720147386\n",
      "The 45165 th iteration gives loss of 0.13715067061010724\n",
      "The 45166 th iteration gives loss of 0.1371494440394026\n",
      "The 45167 th iteration gives loss of 0.13714821748936812\n",
      "The 45168 th iteration gives loss of 0.1371469909599879\n",
      "The 45169 th iteration gives loss of 0.1371457644512752\n",
      "The 45170 th iteration gives loss of 0.13714453796322637\n",
      "The 45171 th iteration gives loss of 0.1371433114958339\n",
      "The 45172 th iteration gives loss of 0.1371420850490963\n",
      "The 45173 th iteration gives loss of 0.13714085862301537\n",
      "The 45174 th iteration gives loss of 0.13713963221759168\n",
      "The 45175 th iteration gives loss of 0.13713840583282264\n",
      "The 45176 th iteration gives loss of 0.13713717946870463\n",
      "The 45177 th iteration gives loss of 0.13713595312522517\n",
      "The 45178 th iteration gives loss of 0.13713472680240288\n",
      "The 45179 th iteration gives loss of 0.13713350050022113\n",
      "The 45180 th iteration gives loss of 0.13713227421868196\n",
      "The 45181 th iteration gives loss of 0.1371310479577895\n",
      "The 45182 th iteration gives loss of 0.1371298217175334\n",
      "The 45183 th iteration gives loss of 0.1371285954979188\n",
      "The 45184 th iteration gives loss of 0.13712736929894162\n",
      "The 45185 th iteration gives loss of 0.1371261431205985\n",
      "The 45186 th iteration gives loss of 0.1371249169628939\n",
      "The 45187 th iteration gives loss of 0.13712369082583142\n",
      "The 45188 th iteration gives loss of 0.13712246470938427\n",
      "The 45189 th iteration gives loss of 0.13712123861357517\n",
      "The 45190 th iteration gives loss of 0.13712001253839742\n",
      "The 45191 th iteration gives loss of 0.13711878648383397\n",
      "The 45192 th iteration gives loss of 0.13711756044990428\n",
      "The 45193 th iteration gives loss of 0.13711633443659746\n",
      "The 45194 th iteration gives loss of 0.13711510844390712\n",
      "The 45195 th iteration gives loss of 0.13711388247184145\n",
      "The 45196 th iteration gives loss of 0.13711265652039312\n",
      "The 45197 th iteration gives loss of 0.1371114305895578\n",
      "The 45198 th iteration gives loss of 0.13711020467934318\n",
      "The 45199 th iteration gives loss of 0.13710897878974188\n",
      "The 45200 th iteration gives loss of 0.1371077529207574\n",
      "The 45201 th iteration gives loss of 0.13710652707236992\n",
      "The 45202 th iteration gives loss of 0.13710530124459794\n",
      "The 45203 th iteration gives loss of 0.13710407543743314\n",
      "The 45204 th iteration gives loss of 0.13710284965087072\n",
      "The 45205 th iteration gives loss of 0.1371016238849189\n",
      "The 45206 th iteration gives loss of 0.13710039813956768\n",
      "The 45207 th iteration gives loss of 0.1370991724148134\n",
      "The 45208 th iteration gives loss of 0.13709794671066003\n",
      "The 45209 th iteration gives loss of 0.1370967210271053\n",
      "The 45210 th iteration gives loss of 0.13709549536414448\n",
      "The 45211 th iteration gives loss of 0.13709426972177624\n",
      "The 45212 th iteration gives loss of 0.1370930440999985\n",
      "The 45213 th iteration gives loss of 0.1370918184988236\n",
      "The 45214 th iteration gives loss of 0.13709059291822948\n",
      "The 45215 th iteration gives loss of 0.1370893673582209\n",
      "The 45216 th iteration gives loss of 0.13708814181880816\n",
      "The 45217 th iteration gives loss of 0.1370869162999793\n",
      "The 45218 th iteration gives loss of 0.13708569080172242\n",
      "The 45219 th iteration gives loss of 0.13708446532405325\n",
      "The 45220 th iteration gives loss of 0.1370832398669666\n",
      "The 45221 th iteration gives loss of 0.13708201443045784\n",
      "The 45222 th iteration gives loss of 0.1370807890145217\n",
      "The 45223 th iteration gives loss of 0.137079563619156\n",
      "The 45224 th iteration gives loss of 0.13707833824437254\n",
      "The 45225 th iteration gives loss of 0.13707711289015764\n",
      "The 45226 th iteration gives loss of 0.13707588755651412\n",
      "The 45227 th iteration gives loss of 0.13707466224344267\n",
      "The 45228 th iteration gives loss of 0.13707343695093485\n",
      "The 45229 th iteration gives loss of 0.13707221167898742\n",
      "The 45230 th iteration gives loss of 0.1370709864276095\n",
      "The 45231 th iteration gives loss of 0.13706976119679662\n",
      "The 45232 th iteration gives loss of 0.13706853598653665\n",
      "The 45233 th iteration gives loss of 0.1370673107968416\n",
      "The 45234 th iteration gives loss of 0.1370660856277019\n",
      "The 45235 th iteration gives loss of 0.13706486047912383\n",
      "The 45236 th iteration gives loss of 0.13706363535108934\n",
      "The 45237 th iteration gives loss of 0.13706241024361387\n",
      "The 45238 th iteration gives loss of 0.13706118515668092\n",
      "The 45239 th iteration gives loss of 0.13705996009031207\n",
      "The 45240 th iteration gives loss of 0.13705873504447647\n",
      "The 45241 th iteration gives loss of 0.13705751001920083\n",
      "The 45242 th iteration gives loss of 0.13705628501445985\n",
      "The 45243 th iteration gives loss of 0.13705506003027018\n",
      "The 45244 th iteration gives loss of 0.1370538350666143\n",
      "The 45245 th iteration gives loss of 0.13705261012349215\n",
      "The 45246 th iteration gives loss of 0.13705138520091856\n",
      "The 45247 th iteration gives loss of 0.13705016029888276\n",
      "The 45248 th iteration gives loss of 0.13704893541737417\n",
      "The 45249 th iteration gives loss of 0.13704771055641163\n",
      "The 45250 th iteration gives loss of 0.1370464857159723\n",
      "The 45251 th iteration gives loss of 0.13704526089605687\n",
      "The 45252 th iteration gives loss of 0.13704403609667884\n",
      "The 45253 th iteration gives loss of 0.13704281131783333\n",
      "The 45254 th iteration gives loss of 0.13704158655950444\n",
      "The 45255 th iteration gives loss of 0.13704036182170107\n",
      "The 45256 th iteration gives loss of 0.1370391371044206\n",
      "The 45257 th iteration gives loss of 0.13703791240765814\n",
      "The 45258 th iteration gives loss of 0.13703668773141964\n",
      "The 45259 th iteration gives loss of 0.13703546307569622\n",
      "The 45260 th iteration gives loss of 0.13703423844049076\n",
      "The 45261 th iteration gives loss of 0.13703301382580044\n",
      "The 45262 th iteration gives loss of 0.13703178923161372\n",
      "The 45263 th iteration gives loss of 0.13703056465794225\n",
      "The 45264 th iteration gives loss of 0.13702934010478224\n",
      "The 45265 th iteration gives loss of 0.13702811557212846\n",
      "The 45266 th iteration gives loss of 0.13702689105998772\n",
      "The 45267 th iteration gives loss of 0.13702566656834625\n",
      "The 45268 th iteration gives loss of 0.13702444209720624\n",
      "The 45269 th iteration gives loss of 0.13702321764657147\n",
      "The 45270 th iteration gives loss of 0.1370219932164355\n",
      "The 45271 th iteration gives loss of 0.13702076880679961\n",
      "The 45272 th iteration gives loss of 0.1370195444176566\n",
      "The 45273 th iteration gives loss of 0.13701832004901543\n",
      "The 45274 th iteration gives loss of 0.13701709570085774\n",
      "The 45275 th iteration gives loss of 0.1370158713732003\n",
      "The 45276 th iteration gives loss of 0.13701464706603295\n",
      "The 45277 th iteration gives loss of 0.13701342277935555\n",
      "The 45278 th iteration gives loss of 0.13701219851315083\n",
      "The 45279 th iteration gives loss of 0.13701097426744477\n",
      "The 45280 th iteration gives loss of 0.13700975004222335\n",
      "The 45281 th iteration gives loss of 0.13700852583748063\n",
      "The 45282 th iteration gives loss of 0.13700730165322103\n",
      "The 45283 th iteration gives loss of 0.13700607748944482\n",
      "The 45284 th iteration gives loss of 0.1370048533461375\n",
      "The 45285 th iteration gives loss of 0.13700362922330622\n",
      "The 45286 th iteration gives loss of 0.13700240512095732\n",
      "The 45287 th iteration gives loss of 0.13700118103907052\n",
      "The 45288 th iteration gives loss of 0.13699995697766568\n",
      "The 45289 th iteration gives loss of 0.13699873293672593\n",
      "The 45290 th iteration gives loss of 0.13699750891625595\n",
      "The 45291 th iteration gives loss of 0.13699628491625465\n",
      "The 45292 th iteration gives loss of 0.1369950609367144\n",
      "The 45293 th iteration gives loss of 0.1369938369776411\n",
      "The 45294 th iteration gives loss of 0.1369926130390322\n",
      "The 45295 th iteration gives loss of 0.13699138912087436\n",
      "The 45296 th iteration gives loss of 0.13699016522318047\n",
      "The 45297 th iteration gives loss of 0.13698894134593603\n",
      "The 45298 th iteration gives loss of 0.13698771748915906\n",
      "The 45299 th iteration gives loss of 0.13698649365283347\n",
      "The 45300 th iteration gives loss of 0.1369852698369641\n",
      "The 45301 th iteration gives loss of 0.1369840460415369\n",
      "The 45302 th iteration gives loss of 0.13698282226656214\n",
      "The 45303 th iteration gives loss of 0.13698159851202757\n",
      "The 45304 th iteration gives loss of 0.136980374777946\n",
      "The 45305 th iteration gives loss of 0.13697915106430286\n",
      "The 45306 th iteration gives loss of 0.13697792737111283\n",
      "The 45307 th iteration gives loss of 0.13697670369835738\n",
      "The 45308 th iteration gives loss of 0.13697548004604765\n",
      "The 45309 th iteration gives loss of 0.1369742564141771\n",
      "The 45310 th iteration gives loss of 0.13697303280272827\n",
      "The 45311 th iteration gives loss of 0.13697180921172675\n",
      "The 45312 th iteration gives loss of 0.1369705856411567\n",
      "The 45313 th iteration gives loss of 0.13696936209101934\n",
      "The 45314 th iteration gives loss of 0.1369681385613142\n",
      "The 45315 th iteration gives loss of 0.13696691505202946\n",
      "The 45316 th iteration gives loss of 0.13696569156317356\n",
      "The 45317 th iteration gives loss of 0.1369644680947489\n",
      "The 45318 th iteration gives loss of 0.136963244646743\n",
      "The 45319 th iteration gives loss of 0.13696202121916212\n",
      "The 45320 th iteration gives loss of 0.13696079781200715\n",
      "The 45321 th iteration gives loss of 0.13695957442526366\n",
      "The 45322 th iteration gives loss of 0.1369583510589444\n",
      "The 45323 th iteration gives loss of 0.13695712771303328\n",
      "The 45324 th iteration gives loss of 0.1369559043875367\n",
      "The 45325 th iteration gives loss of 0.13695468108245637\n",
      "The 45326 th iteration gives loss of 0.13695345779778792\n",
      "The 45327 th iteration gives loss of 0.13695223453352806\n",
      "The 45328 th iteration gives loss of 0.13695101128967296\n",
      "The 45329 th iteration gives loss of 0.13694978806622668\n",
      "The 45330 th iteration gives loss of 0.1369485648631904\n",
      "The 45331 th iteration gives loss of 0.13694734168055772\n",
      "The 45332 th iteration gives loss of 0.13694611851831817\n",
      "The 45333 th iteration gives loss of 0.13694489537648646\n",
      "The 45334 th iteration gives loss of 0.13694367225504922\n",
      "The 45335 th iteration gives loss of 0.13694244915401077\n",
      "The 45336 th iteration gives loss of 0.1369412260733612\n",
      "The 45337 th iteration gives loss of 0.13694000301311232\n",
      "The 45338 th iteration gives loss of 0.13693877997326345\n",
      "The 45339 th iteration gives loss of 0.13693755695379606\n",
      "The 45340 th iteration gives loss of 0.13693633395471244\n",
      "The 45341 th iteration gives loss of 0.13693511097602748\n",
      "The 45342 th iteration gives loss of 0.13693388801772216\n",
      "The 45343 th iteration gives loss of 0.1369326650798027\n",
      "The 45344 th iteration gives loss of 0.13693144216226716\n",
      "The 45345 th iteration gives loss of 0.13693021926511412\n",
      "The 45346 th iteration gives loss of 0.1369289963883368\n",
      "The 45347 th iteration gives loss of 0.1369277735319308\n",
      "The 45348 th iteration gives loss of 0.1369265506959139\n",
      "The 45349 th iteration gives loss of 0.1369253278802675\n",
      "The 45350 th iteration gives loss of 0.13692410508499844\n",
      "The 45351 th iteration gives loss of 0.13692288231010397\n",
      "The 45352 th iteration gives loss of 0.1369216595555739\n",
      "The 45353 th iteration gives loss of 0.13692043682141491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 45354 th iteration gives loss of 0.13691921410761887\n",
      "The 45355 th iteration gives loss of 0.13691799141418917\n",
      "The 45356 th iteration gives loss of 0.13691676874112857\n",
      "The 45357 th iteration gives loss of 0.13691554608842715\n",
      "The 45358 th iteration gives loss of 0.13691432345608828\n",
      "The 45359 th iteration gives loss of 0.13691310084410577\n",
      "The 45360 th iteration gives loss of 0.1369118782524768\n",
      "The 45361 th iteration gives loss of 0.13691065568121288\n",
      "The 45362 th iteration gives loss of 0.1369094331303063\n",
      "The 45363 th iteration gives loss of 0.1369082105997392\n",
      "The 45364 th iteration gives loss of 0.13690698808953536\n",
      "The 45365 th iteration gives loss of 0.13690576559967413\n",
      "The 45366 th iteration gives loss of 0.13690454313016334\n",
      "The 45367 th iteration gives loss of 0.13690332068100305\n",
      "The 45368 th iteration gives loss of 0.13690209825218586\n",
      "The 45369 th iteration gives loss of 0.13690087584371274\n",
      "The 45370 th iteration gives loss of 0.13689965345558247\n",
      "The 45371 th iteration gives loss of 0.13689843108779215\n",
      "The 45372 th iteration gives loss of 0.13689720874033584\n",
      "The 45373 th iteration gives loss of 0.13689598641322084\n",
      "The 45374 th iteration gives loss of 0.13689476410644\n",
      "The 45375 th iteration gives loss of 0.13689354181999913\n",
      "The 45376 th iteration gives loss of 0.1368923195538841\n",
      "The 45377 th iteration gives loss of 0.13689109730810817\n",
      "The 45378 th iteration gives loss of 0.13688987508265646\n",
      "The 45379 th iteration gives loss of 0.13688865287753976\n",
      "The 45380 th iteration gives loss of 0.1368874306927411\n",
      "The 45381 th iteration gives loss of 0.13688620852826724\n",
      "The 45382 th iteration gives loss of 0.13688498638412153\n",
      "The 45383 th iteration gives loss of 0.1368837642602832\n",
      "The 45384 th iteration gives loss of 0.13688254215678894\n",
      "The 45385 th iteration gives loss of 0.1368813200735982\n",
      "The 45386 th iteration gives loss of 0.13688009801073067\n",
      "The 45387 th iteration gives loss of 0.1368788759681756\n",
      "The 45388 th iteration gives loss of 0.1368776539459331\n",
      "The 45389 th iteration gives loss of 0.13687643194399982\n",
      "The 45390 th iteration gives loss of 0.13687520996238273\n",
      "The 45391 th iteration gives loss of 0.13687398800107445\n",
      "The 45392 th iteration gives loss of 0.13687276606007232\n",
      "The 45393 th iteration gives loss of 0.1368715441393812\n",
      "The 45394 th iteration gives loss of 0.13687032223898946\n",
      "The 45395 th iteration gives loss of 0.13686910035890462\n",
      "The 45396 th iteration gives loss of 0.13686787849911505\n",
      "The 45397 th iteration gives loss of 0.1368666566596303\n",
      "The 45398 th iteration gives loss of 0.1368654348404429\n",
      "The 45399 th iteration gives loss of 0.1368642130415526\n",
      "The 45400 th iteration gives loss of 0.13686299126295035\n",
      "The 45401 th iteration gives loss of 0.13686176950464998\n",
      "The 45402 th iteration gives loss of 0.13686054776664205\n",
      "The 45403 th iteration gives loss of 0.13685932604891898\n",
      "The 45404 th iteration gives loss of 0.13685810435149334\n",
      "The 45405 th iteration gives loss of 0.1368568826743556\n",
      "The 45406 th iteration gives loss of 0.13685566101749105\n",
      "The 45407 th iteration gives loss of 0.1368544393809214\n",
      "The 45408 th iteration gives loss of 0.1368532177646365\n",
      "The 45409 th iteration gives loss of 0.13685199616862828\n",
      "The 45410 th iteration gives loss of 0.1368507745928921\n",
      "The 45411 th iteration gives loss of 0.13684955303744734\n",
      "The 45412 th iteration gives loss of 0.13684833150227552\n",
      "The 45413 th iteration gives loss of 0.13684710998737593\n",
      "The 45414 th iteration gives loss of 0.13684588849274854\n",
      "The 45415 th iteration gives loss of 0.1368446670183969\n",
      "The 45416 th iteration gives loss of 0.13684344556431127\n",
      "The 45417 th iteration gives loss of 0.13684222413049313\n",
      "The 45418 th iteration gives loss of 0.1368410027169447\n",
      "The 45419 th iteration gives loss of 0.1368397813236658\n",
      "The 45420 th iteration gives loss of 0.13683855995064703\n",
      "The 45421 th iteration gives loss of 0.13683733859789346\n",
      "The 45422 th iteration gives loss of 0.13683611726539271\n",
      "The 45423 th iteration gives loss of 0.1368348959531677\n",
      "The 45424 th iteration gives loss of 0.13683367466118626\n",
      "The 45425 th iteration gives loss of 0.1368324533894704\n",
      "The 45426 th iteration gives loss of 0.13683123213799978\n",
      "The 45427 th iteration gives loss of 0.1368300109067916\n",
      "The 45428 th iteration gives loss of 0.13682878969582665\n",
      "The 45429 th iteration gives loss of 0.13682756850511202\n",
      "The 45430 th iteration gives loss of 0.13682634733465365\n",
      "The 45431 th iteration gives loss of 0.13682512618443915\n",
      "The 45432 th iteration gives loss of 0.13682390505447042\n",
      "The 45433 th iteration gives loss of 0.1368226839447522\n",
      "The 45434 th iteration gives loss of 0.13682146285526273\n",
      "The 45435 th iteration gives loss of 0.13682024178601676\n",
      "The 45436 th iteration gives loss of 0.1368190207370176\n",
      "The 45437 th iteration gives loss of 0.13681779970825447\n",
      "The 45438 th iteration gives loss of 0.13681657869972585\n",
      "The 45439 th iteration gives loss of 0.13681535771142733\n",
      "The 45440 th iteration gives loss of 0.13681413674336299\n",
      "The 45441 th iteration gives loss of 0.13681291579553514\n",
      "The 45442 th iteration gives loss of 0.13681169486793165\n",
      "The 45443 th iteration gives loss of 0.13681047396056537\n",
      "The 45444 th iteration gives loss of 0.13680925307341957\n",
      "The 45445 th iteration gives loss of 0.13680803220649904\n",
      "The 45446 th iteration gives loss of 0.13680681135980244\n",
      "The 45447 th iteration gives loss of 0.13680559053332966\n",
      "The 45448 th iteration gives loss of 0.13680436972707827\n",
      "The 45449 th iteration gives loss of 0.1368031489410427\n",
      "The 45450 th iteration gives loss of 0.13680192817522513\n",
      "The 45451 th iteration gives loss of 0.1368007074296311\n",
      "The 45452 th iteration gives loss of 0.13679948670424447\n",
      "The 45453 th iteration gives loss of 0.13679826599906902\n",
      "The 45454 th iteration gives loss of 0.13679704531410833\n",
      "The 45455 th iteration gives loss of 0.13679582464935286\n",
      "The 45456 th iteration gives loss of 0.13679460400480978\n",
      "The 45457 th iteration gives loss of 0.13679338338047403\n",
      "The 45458 th iteration gives loss of 0.13679216277634282\n",
      "The 45459 th iteration gives loss of 0.13679094219241328\n",
      "The 45460 th iteration gives loss of 0.13678972162869327\n",
      "The 45461 th iteration gives loss of 0.13678850108516546\n",
      "The 45462 th iteration gives loss of 0.1367872805618377\n",
      "The 45463 th iteration gives loss of 0.13678606005871133\n",
      "The 45464 th iteration gives loss of 0.13678483957577967\n",
      "The 45465 th iteration gives loss of 0.13678361911305006\n",
      "The 45466 th iteration gives loss of 0.13678239867050437\n",
      "The 45467 th iteration gives loss of 0.1367811782481475\n",
      "The 45468 th iteration gives loss of 0.1367799578459801\n",
      "The 45469 th iteration gives loss of 0.1367787374640064\n",
      "The 45470 th iteration gives loss of 0.13677751710221822\n",
      "The 45471 th iteration gives loss of 0.13677629676062078\n",
      "The 45472 th iteration gives loss of 0.1367750764391992\n",
      "The 45473 th iteration gives loss of 0.13677385613795598\n",
      "The 45474 th iteration gives loss of 0.13677263585690153\n",
      "The 45475 th iteration gives loss of 0.13677141559602182\n",
      "The 45476 th iteration gives loss of 0.13677019535532325\n",
      "The 45477 th iteration gives loss of 0.13676897513479389\n",
      "The 45478 th iteration gives loss of 0.13676775493444504\n",
      "The 45479 th iteration gives loss of 0.1367665347542635\n",
      "The 45480 th iteration gives loss of 0.13676531459426167\n",
      "The 45481 th iteration gives loss of 0.13676409445442878\n",
      "The 45482 th iteration gives loss of 0.13676287433475953\n",
      "The 45483 th iteration gives loss of 0.13676165423525177\n",
      "The 45484 th iteration gives loss of 0.13676043415591593\n",
      "The 45485 th iteration gives loss of 0.13675921409674913\n",
      "The 45486 th iteration gives loss of 0.13675799405773126\n",
      "The 45487 th iteration gives loss of 0.1367567740388881\n",
      "The 45488 th iteration gives loss of 0.1367555540401862\n",
      "The 45489 th iteration gives loss of 0.13675433406165932\n",
      "The 45490 th iteration gives loss of 0.1367531141032749\n",
      "The 45491 th iteration gives loss of 0.13675189416505198\n",
      "The 45492 th iteration gives loss of 0.1367506742469796\n",
      "The 45493 th iteration gives loss of 0.13674945434905766\n",
      "The 45494 th iteration gives loss of 0.1367482344712902\n",
      "The 45495 th iteration gives loss of 0.13674701461367356\n",
      "The 45496 th iteration gives loss of 0.1367457947761961\n",
      "The 45497 th iteration gives loss of 0.13674457495886724\n",
      "The 45498 th iteration gives loss of 0.13674335516168448\n",
      "The 45499 th iteration gives loss of 0.13674213538463764\n",
      "The 45500 th iteration gives loss of 0.1367409156277377\n",
      "The 45501 th iteration gives loss of 0.13673969589096488\n",
      "The 45502 th iteration gives loss of 0.13673847617434043\n",
      "The 45503 th iteration gives loss of 0.13673725647784937\n",
      "The 45504 th iteration gives loss of 0.13673603680149138\n",
      "The 45505 th iteration gives loss of 0.13673481714527064\n",
      "The 45506 th iteration gives loss of 0.13673359750917097\n",
      "The 45507 th iteration gives loss of 0.1367323778932151\n",
      "The 45508 th iteration gives loss of 0.13673115829738192\n",
      "The 45509 th iteration gives loss of 0.13672993872167127\n",
      "The 45510 th iteration gives loss of 0.13672871916609128\n",
      "The 45511 th iteration gives loss of 0.13672749963063446\n",
      "The 45512 th iteration gives loss of 0.1367262801152936\n",
      "The 45513 th iteration gives loss of 0.13672506062007952\n",
      "The 45514 th iteration gives loss of 0.1367238411449836\n",
      "The 45515 th iteration gives loss of 0.13672262169000618\n",
      "The 45516 th iteration gives loss of 0.13672140225513998\n",
      "The 45517 th iteration gives loss of 0.13672018284039392\n",
      "The 45518 th iteration gives loss of 0.13671896344575357\n",
      "The 45519 th iteration gives loss of 0.13671774407123047\n",
      "The 45520 th iteration gives loss of 0.13671652471681564\n",
      "The 45521 th iteration gives loss of 0.13671530538251248\n",
      "The 45522 th iteration gives loss of 0.13671408606831423\n",
      "The 45523 th iteration gives loss of 0.13671286677421945\n",
      "The 45524 th iteration gives loss of 0.13671164750022735\n",
      "The 45525 th iteration gives loss of 0.13671042824634586\n",
      "The 45526 th iteration gives loss of 0.1367092090125588\n",
      "The 45527 th iteration gives loss of 0.13670798979886947\n",
      "The 45528 th iteration gives loss of 0.13670677060528022\n",
      "The 45529 th iteration gives loss of 0.13670555143178148\n",
      "The 45530 th iteration gives loss of 0.13670433227838213\n",
      "The 45531 th iteration gives loss of 0.13670311314508263\n",
      "The 45532 th iteration gives loss of 0.13670189403186403\n",
      "The 45533 th iteration gives loss of 0.1367006749387342\n",
      "The 45534 th iteration gives loss of 0.1366994558657034\n",
      "The 45535 th iteration gives loss of 0.13669823681275528\n",
      "The 45536 th iteration gives loss of 0.13669701777989035\n",
      "The 45537 th iteration gives loss of 0.13669579876711024\n",
      "The 45538 th iteration gives loss of 0.13669457977441585\n",
      "The 45539 th iteration gives loss of 0.13669336080179223\n",
      "The 45540 th iteration gives loss of 0.13669214184926648\n",
      "The 45541 th iteration gives loss of 0.13669092291680593\n",
      "The 45542 th iteration gives loss of 0.13668970400442584\n",
      "The 45543 th iteration gives loss of 0.1366884851121087\n",
      "The 45544 th iteration gives loss of 0.1366872662398779\n",
      "The 45545 th iteration gives loss of 0.13668604738771073\n",
      "The 45546 th iteration gives loss of 0.1366848285556205\n",
      "The 45547 th iteration gives loss of 0.1366836097435935\n",
      "The 45548 th iteration gives loss of 0.13668239095163853\n",
      "The 45549 th iteration gives loss of 0.1366811721797488\n",
      "The 45550 th iteration gives loss of 0.13667995342792585\n",
      "The 45551 th iteration gives loss of 0.13667873469615963\n",
      "The 45552 th iteration gives loss of 0.13667751598445568\n",
      "The 45553 th iteration gives loss of 0.1366762972928149\n",
      "The 45554 th iteration gives loss of 0.1366750786212318\n",
      "The 45555 th iteration gives loss of 0.13667385996970297\n",
      "The 45556 th iteration gives loss of 0.1366726413382209\n",
      "The 45557 th iteration gives loss of 0.1366714227268002\n",
      "The 45558 th iteration gives loss of 0.1366702041354298\n",
      "The 45559 th iteration gives loss of 0.13666898556411056\n",
      "The 45560 th iteration gives loss of 0.13666776701284572\n",
      "The 45561 th iteration gives loss of 0.1366665484816208\n",
      "The 45562 th iteration gives loss of 0.1366653299704413\n",
      "The 45563 th iteration gives loss of 0.13666411147931645\n",
      "The 45564 th iteration gives loss of 0.13666289300822493\n",
      "The 45565 th iteration gives loss of 0.13666167455717498\n",
      "The 45566 th iteration gives loss of 0.1366604561261716\n",
      "The 45567 th iteration gives loss of 0.13665923771519775\n",
      "The 45568 th iteration gives loss of 0.13665801932426758\n",
      "The 45569 th iteration gives loss of 0.13665680095337257\n",
      "The 45570 th iteration gives loss of 0.13665558260250246\n",
      "The 45571 th iteration gives loss of 0.1366543642716806\n",
      "The 45572 th iteration gives loss of 0.13665314596088646\n",
      "The 45573 th iteration gives loss of 0.1366519276701047\n",
      "The 45574 th iteration gives loss of 0.13665070939936008\n",
      "The 45575 th iteration gives loss of 0.1366494911486441\n",
      "The 45576 th iteration gives loss of 0.13664827291795542\n",
      "The 45577 th iteration gives loss of 0.13664705470727834\n",
      "The 45578 th iteration gives loss of 0.1366458365166272\n",
      "The 45579 th iteration gives loss of 0.13664461834600072\n",
      "The 45580 th iteration gives loss of 0.13664340019539128\n",
      "The 45581 th iteration gives loss of 0.13664218206479684\n",
      "The 45582 th iteration gives loss of 0.13664096395422037\n",
      "The 45583 th iteration gives loss of 0.13663974586365282\n",
      "The 45584 th iteration gives loss of 0.1366385277931031\n",
      "The 45585 th iteration gives loss of 0.13663730974255706\n",
      "The 45586 th iteration gives loss of 0.13663609171203422\n",
      "The 45587 th iteration gives loss of 0.13663487370150496\n",
      "The 45588 th iteration gives loss of 0.1366336557109961\n",
      "The 45589 th iteration gives loss of 0.1366324377404865\n",
      "The 45590 th iteration gives loss of 0.13663121978997086\n",
      "The 45591 th iteration gives loss of 0.13663000185947002\n",
      "The 45592 th iteration gives loss of 0.13662878394896197\n",
      "The 45593 th iteration gives loss of 0.13662756605845688\n",
      "The 45594 th iteration gives loss of 0.1366263481879402\n",
      "The 45595 th iteration gives loss of 0.13662513033743534\n",
      "The 45596 th iteration gives loss of 0.13662391250691294\n",
      "The 45597 th iteration gives loss of 0.1366226946963882\n",
      "The 45598 th iteration gives loss of 0.1366214769058507\n",
      "The 45599 th iteration gives loss of 0.13662025913530626\n",
      "The 45600 th iteration gives loss of 0.1366190413847532\n",
      "The 45601 th iteration gives loss of 0.13661782365418784\n",
      "The 45602 th iteration gives loss of 0.13661660594359612\n",
      "The 45603 th iteration gives loss of 0.13661538825299507\n",
      "The 45604 th iteration gives loss of 0.13661417058237948\n",
      "The 45605 th iteration gives loss of 0.1366129529317423\n",
      "The 45606 th iteration gives loss of 0.1366117353010794\n",
      "The 45607 th iteration gives loss of 0.13661051769039728\n",
      "The 45608 th iteration gives loss of 0.13660930009969652\n",
      "The 45609 th iteration gives loss of 0.13660808252896847\n",
      "The 45610 th iteration gives loss of 0.13660686497820365\n",
      "The 45611 th iteration gives loss of 0.1366056474474206\n",
      "The 45612 th iteration gives loss of 0.1366044299366033\n",
      "The 45613 th iteration gives loss of 0.13660321244575965\n",
      "The 45614 th iteration gives loss of 0.13660199497487757\n",
      "The 45615 th iteration gives loss of 0.1366007775239684\n",
      "The 45616 th iteration gives loss of 0.13659956009301702\n",
      "The 45617 th iteration gives loss of 0.13659834268202992\n",
      "The 45618 th iteration gives loss of 0.13659712529100637\n",
      "The 45619 th iteration gives loss of 0.13659590791993667\n",
      "The 45620 th iteration gives loss of 0.13659469056883058\n",
      "The 45621 th iteration gives loss of 0.13659347323768156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 45622 th iteration gives loss of 0.13659225592648006\n",
      "The 45623 th iteration gives loss of 0.1365910386352348\n",
      "The 45624 th iteration gives loss of 0.1365898213639491\n",
      "The 45625 th iteration gives loss of 0.1365886041126128\n",
      "The 45626 th iteration gives loss of 0.13658738688122257\n",
      "The 45627 th iteration gives loss of 0.1365861696697821\n",
      "The 45628 th iteration gives loss of 0.1365849524782774\n",
      "The 45629 th iteration gives loss of 0.1365837353067295\n",
      "The 45630 th iteration gives loss of 0.13658251815511516\n",
      "The 45631 th iteration gives loss of 0.13658130102345414\n",
      "The 45632 th iteration gives loss of 0.1365800839117237\n",
      "The 45633 th iteration gives loss of 0.13657886681993647\n",
      "The 45634 th iteration gives loss of 0.136577649748078\n",
      "The 45635 th iteration gives loss of 0.13657643269616432\n",
      "The 45636 th iteration gives loss of 0.13657521566418035\n",
      "The 45637 th iteration gives loss of 0.13657399865213335\n",
      "The 45638 th iteration gives loss of 0.13657278166001544\n",
      "The 45639 th iteration gives loss of 0.13657156468782608\n",
      "The 45640 th iteration gives loss of 0.1365703477355587\n",
      "The 45641 th iteration gives loss of 0.1365691308032261\n",
      "The 45642 th iteration gives loss of 0.13656791389082065\n",
      "The 45643 th iteration gives loss of 0.13656669699833365\n",
      "The 45644 th iteration gives loss of 0.13656548012576394\n",
      "The 45645 th iteration gives loss of 0.13656426327312488\n",
      "The 45646 th iteration gives loss of 0.13656304644039785\n",
      "The 45647 th iteration gives loss of 0.13656182962758875\n",
      "The 45648 th iteration gives loss of 0.13656061283469478\n",
      "The 45649 th iteration gives loss of 0.13655939606172085\n",
      "The 45650 th iteration gives loss of 0.13655817930865896\n",
      "The 45651 th iteration gives loss of 0.1365569625755116\n",
      "The 45652 th iteration gives loss of 0.13655574586226768\n",
      "The 45653 th iteration gives loss of 0.13655452916893346\n",
      "The 45654 th iteration gives loss of 0.13655331249550473\n",
      "The 45655 th iteration gives loss of 0.13655209584198774\n",
      "The 45656 th iteration gives loss of 0.13655087920837414\n",
      "The 45657 th iteration gives loss of 0.13654966259466605\n",
      "The 45658 th iteration gives loss of 0.13654844600084837\n",
      "The 45659 th iteration gives loss of 0.13654722942693517\n",
      "The 45660 th iteration gives loss of 0.1365460128729165\n",
      "The 45661 th iteration gives loss of 0.13654479633880023\n",
      "The 45662 th iteration gives loss of 0.13654357982457674\n",
      "The 45663 th iteration gives loss of 0.13654236333024888\n",
      "The 45664 th iteration gives loss of 0.13654114685581095\n",
      "The 45665 th iteration gives loss of 0.13653993040126206\n",
      "The 45666 th iteration gives loss of 0.1365387139666092\n",
      "The 45667 th iteration gives loss of 0.13653749755183886\n",
      "The 45668 th iteration gives loss of 0.13653628115694916\n",
      "The 45669 th iteration gives loss of 0.13653506478195093\n",
      "The 45670 th iteration gives loss of 0.13653384842683394\n",
      "The 45671 th iteration gives loss of 0.13653263209159897\n",
      "The 45672 th iteration gives loss of 0.13653141577624514\n",
      "The 45673 th iteration gives loss of 0.1365301994807642\n",
      "The 45674 th iteration gives loss of 0.13652898320517132\n",
      "The 45675 th iteration gives loss of 0.13652776694945012\n",
      "The 45676 th iteration gives loss of 0.13652655071359535\n",
      "The 45677 th iteration gives loss of 0.13652533449761783\n",
      "The 45678 th iteration gives loss of 0.13652411830151479\n",
      "The 45679 th iteration gives loss of 0.13652290212528007\n",
      "The 45680 th iteration gives loss of 0.13652168596890818\n",
      "The 45681 th iteration gives loss of 0.13652046983240762\n",
      "The 45682 th iteration gives loss of 0.13651925371577844\n",
      "The 45683 th iteration gives loss of 0.13651803761900397\n",
      "The 45684 th iteration gives loss of 0.13651682154209596\n",
      "The 45685 th iteration gives loss of 0.13651560548504685\n",
      "The 45686 th iteration gives loss of 0.13651438944786323\n",
      "The 45687 th iteration gives loss of 0.1365131734305222\n",
      "The 45688 th iteration gives loss of 0.1365119574330494\n",
      "The 45689 th iteration gives loss of 0.13651074145542663\n",
      "The 45690 th iteration gives loss of 0.13650952549766424\n",
      "The 45691 th iteration gives loss of 0.1365083095597411\n",
      "The 45692 th iteration gives loss of 0.13650709364167177\n",
      "The 45693 th iteration gives loss of 0.1365058777434594\n",
      "The 45694 th iteration gives loss of 0.13650466186509247\n",
      "The 45695 th iteration gives loss of 0.13650344600656772\n",
      "The 45696 th iteration gives loss of 0.1365022301678909\n",
      "The 45697 th iteration gives loss of 0.13650101434905687\n",
      "The 45698 th iteration gives loss of 0.13649979855005542\n",
      "The 45699 th iteration gives loss of 0.13649858277089832\n",
      "The 45700 th iteration gives loss of 0.1364973670115921\n",
      "The 45701 th iteration gives loss of 0.1364961512721119\n",
      "The 45702 th iteration gives loss of 0.1364949355524636\n",
      "The 45703 th iteration gives loss of 0.13649371985264808\n",
      "The 45704 th iteration gives loss of 0.13649250417267214\n",
      "The 45705 th iteration gives loss of 0.13649128851252793\n",
      "The 45706 th iteration gives loss of 0.13649007287220924\n",
      "The 45707 th iteration gives loss of 0.13648885725172077\n",
      "The 45708 th iteration gives loss of 0.13648764165105937\n",
      "The 45709 th iteration gives loss of 0.13648642607022363\n",
      "The 45710 th iteration gives loss of 0.1364852105092106\n",
      "The 45711 th iteration gives loss of 0.13648399496801514\n",
      "The 45712 th iteration gives loss of 0.13648277944664056\n",
      "The 45713 th iteration gives loss of 0.13648156394509028\n",
      "The 45714 th iteration gives loss of 0.1364803484633538\n",
      "The 45715 th iteration gives loss of 0.13647913300143788\n",
      "The 45716 th iteration gives loss of 0.1364779175593328\n",
      "The 45717 th iteration gives loss of 0.13647670213704194\n",
      "The 45718 th iteration gives loss of 0.13647548673456036\n",
      "The 45719 th iteration gives loss of 0.1364742713518993\n",
      "The 45720 th iteration gives loss of 0.13647305598903228\n",
      "The 45721 th iteration gives loss of 0.13647184064598106\n",
      "The 45722 th iteration gives loss of 0.13647062532273002\n",
      "The 45723 th iteration gives loss of 0.13646941001928745\n",
      "The 45724 th iteration gives loss of 0.13646819473565375\n",
      "The 45725 th iteration gives loss of 0.13646697947180852\n",
      "The 45726 th iteration gives loss of 0.1364657642277704\n",
      "The 45727 th iteration gives loss of 0.13646454900353064\n",
      "The 45728 th iteration gives loss of 0.1364633337990908\n",
      "The 45729 th iteration gives loss of 0.13646211861443774\n",
      "The 45730 th iteration gives loss of 0.1364609034495782\n",
      "The 45731 th iteration gives loss of 0.13645968830451902\n",
      "The 45732 th iteration gives loss of 0.13645847317924764\n",
      "The 45733 th iteration gives loss of 0.13645725807376285\n",
      "The 45734 th iteration gives loss of 0.13645604298807334\n",
      "The 45735 th iteration gives loss of 0.13645482792216496\n",
      "The 45736 th iteration gives loss of 0.1364536128760389\n",
      "The 45737 th iteration gives loss of 0.13645239784969923\n",
      "The 45738 th iteration gives loss of 0.13645118284314411\n",
      "The 45739 th iteration gives loss of 0.1364499678563701\n",
      "The 45740 th iteration gives loss of 0.13644875288937555\n",
      "The 45741 th iteration gives loss of 0.13644753794215364\n",
      "The 45742 th iteration gives loss of 0.13644632301471152\n",
      "The 45743 th iteration gives loss of 0.13644510810704005\n",
      "The 45744 th iteration gives loss of 0.1364438932191467\n",
      "The 45745 th iteration gives loss of 0.13644267835102214\n",
      "The 45746 th iteration gives loss of 0.13644146350267103\n",
      "The 45747 th iteration gives loss of 0.1364402486740837\n",
      "The 45748 th iteration gives loss of 0.13643903386526082\n",
      "The 45749 th iteration gives loss of 0.13643781907621158\n",
      "The 45750 th iteration gives loss of 0.13643660430692683\n",
      "The 45751 th iteration gives loss of 0.1364353895574002\n",
      "The 45752 th iteration gives loss of 0.13643417482763623\n",
      "The 45753 th iteration gives loss of 0.13643296011763764\n",
      "The 45754 th iteration gives loss of 0.13643174542739034\n",
      "The 45755 th iteration gives loss of 0.13643053075690056\n",
      "The 45756 th iteration gives loss of 0.1364293161061695\n",
      "The 45757 th iteration gives loss of 0.1364281014751892\n",
      "The 45758 th iteration gives loss of 0.13642688686396026\n",
      "The 45759 th iteration gives loss of 0.13642567227248464\n",
      "The 45760 th iteration gives loss of 0.13642445770076334\n",
      "The 45761 th iteration gives loss of 0.13642324314878856\n",
      "The 45762 th iteration gives loss of 0.13642202861655492\n",
      "The 45763 th iteration gives loss of 0.13642081410407134\n",
      "The 45764 th iteration gives loss of 0.1364195996113299\n",
      "The 45765 th iteration gives loss of 0.136418385138332\n",
      "The 45766 th iteration gives loss of 0.1364171706850701\n",
      "The 45767 th iteration gives loss of 0.13641595625155606\n",
      "The 45768 th iteration gives loss of 0.13641474183777416\n",
      "The 45769 th iteration gives loss of 0.13641352744372565\n",
      "The 45770 th iteration gives loss of 0.1364123130694134\n",
      "The 45771 th iteration gives loss of 0.13641109871483373\n",
      "The 45772 th iteration gives loss of 0.13640988437998733\n",
      "The 45773 th iteration gives loss of 0.13640867006487623\n",
      "The 45774 th iteration gives loss of 0.1364074557694884\n",
      "The 45775 th iteration gives loss of 0.13640624149382957\n",
      "The 45776 th iteration gives loss of 0.13640502723790113\n",
      "The 45777 th iteration gives loss of 0.13640381300169702\n",
      "The 45778 th iteration gives loss of 0.13640259878521133\n",
      "The 45779 th iteration gives loss of 0.13640138458844178\n",
      "The 45780 th iteration gives loss of 0.1364001704114008\n",
      "The 45781 th iteration gives loss of 0.1363989562540827\n",
      "The 45782 th iteration gives loss of 0.13639774211647474\n",
      "The 45783 th iteration gives loss of 0.13639652799858423\n",
      "The 45784 th iteration gives loss of 0.13639531390040602\n",
      "The 45785 th iteration gives loss of 0.13639409982194167\n",
      "The 45786 th iteration gives loss of 0.13639288576319158\n",
      "The 45787 th iteration gives loss of 0.1363916717241469\n",
      "The 45788 th iteration gives loss of 0.1363904577048144\n",
      "The 45789 th iteration gives loss of 0.13638924370518596\n",
      "The 45790 th iteration gives loss of 0.1363880297252674\n",
      "The 45791 th iteration gives loss of 0.13638681576505043\n",
      "The 45792 th iteration gives loss of 0.13638560182453419\n",
      "The 45793 th iteration gives loss of 0.1363843879037212\n",
      "The 45794 th iteration gives loss of 0.13638317400260902\n",
      "The 45795 th iteration gives loss of 0.13638196012118672\n",
      "The 45796 th iteration gives loss of 0.13638074625947022\n",
      "The 45797 th iteration gives loss of 0.13637953241744993\n",
      "The 45798 th iteration gives loss of 0.13637831859511224\n",
      "The 45799 th iteration gives loss of 0.13637710479247625\n",
      "The 45800 th iteration gives loss of 0.13637589100952602\n",
      "The 45801 th iteration gives loss of 0.13637467724626784\n",
      "The 45802 th iteration gives loss of 0.13637346350269763\n",
      "The 45803 th iteration gives loss of 0.13637224977881932\n",
      "The 45804 th iteration gives loss of 0.13637103607462198\n",
      "The 45805 th iteration gives loss of 0.13636982239010753\n",
      "The 45806 th iteration gives loss of 0.13636860872527273\n",
      "The 45807 th iteration gives loss of 0.13636739508011644\n",
      "The 45808 th iteration gives loss of 0.1363661814546473\n",
      "The 45809 th iteration gives loss of 0.13636496784885221\n",
      "The 45810 th iteration gives loss of 0.13636375426272898\n",
      "The 45811 th iteration gives loss of 0.13636254069628262\n",
      "The 45812 th iteration gives loss of 0.13636132714951374\n",
      "The 45813 th iteration gives loss of 0.13636011362241424\n",
      "The 45814 th iteration gives loss of 0.13635890011498514\n",
      "The 45815 th iteration gives loss of 0.13635768662722575\n",
      "The 45816 th iteration gives loss of 0.13635647315913701\n",
      "The 45817 th iteration gives loss of 0.1363552597107095\n",
      "The 45818 th iteration gives loss of 0.13635404628194767\n",
      "The 45819 th iteration gives loss of 0.13635283287284664\n",
      "The 45820 th iteration gives loss of 0.13635161948340832\n",
      "The 45821 th iteration gives loss of 0.13635040611363414\n",
      "The 45822 th iteration gives loss of 0.13634919276351468\n",
      "The 45823 th iteration gives loss of 0.1363479794330569\n",
      "The 45824 th iteration gives loss of 0.13634676612224625\n",
      "The 45825 th iteration gives loss of 0.13634555283109517\n",
      "The 45826 th iteration gives loss of 0.13634433955960132\n",
      "The 45827 th iteration gives loss of 0.13634312630775128\n",
      "The 45828 th iteration gives loss of 0.13634191307555457\n",
      "The 45829 th iteration gives loss of 0.13634069986301012\n",
      "The 45830 th iteration gives loss of 0.1363394866701049\n",
      "The 45831 th iteration gives loss of 0.13633827349685326\n",
      "The 45832 th iteration gives loss of 0.1363370603432404\n",
      "The 45833 th iteration gives loss of 0.13633584720926895\n",
      "The 45834 th iteration gives loss of 0.13633463409494284\n",
      "The 45835 th iteration gives loss of 0.13633342100025336\n",
      "The 45836 th iteration gives loss of 0.13633220792519657\n",
      "The 45837 th iteration gives loss of 0.13633099486978809\n",
      "The 45838 th iteration gives loss of 0.13632978183401223\n",
      "The 45839 th iteration gives loss of 0.13632856881785912\n",
      "The 45840 th iteration gives loss of 0.1363273558213556\n",
      "The 45841 th iteration gives loss of 0.1363261428444735\n",
      "The 45842 th iteration gives loss of 0.13632492988722872\n",
      "The 45843 th iteration gives loss of 0.13632371694960435\n",
      "The 45844 th iteration gives loss of 0.1363225040316092\n",
      "The 45845 th iteration gives loss of 0.1363212911332384\n",
      "The 45846 th iteration gives loss of 0.13632007825449252\n",
      "The 45847 th iteration gives loss of 0.13631886539535906\n",
      "The 45848 th iteration gives loss of 0.13631765255586065\n",
      "The 45849 th iteration gives loss of 0.1363164397359738\n",
      "The 45850 th iteration gives loss of 0.13631522693571024\n",
      "The 45851 th iteration gives loss of 0.1363140141550591\n",
      "The 45852 th iteration gives loss of 0.13631280139403307\n",
      "The 45853 th iteration gives loss of 0.1363115886526042\n",
      "The 45854 th iteration gives loss of 0.13631037593079276\n",
      "The 45855 th iteration gives loss of 0.1363091632285983\n",
      "The 45856 th iteration gives loss of 0.1363079505460101\n",
      "The 45857 th iteration gives loss of 0.13630673788302666\n",
      "The 45858 th iteration gives loss of 0.1363055252396486\n",
      "The 45859 th iteration gives loss of 0.13630431261587983\n",
      "The 45860 th iteration gives loss of 0.13630310001171567\n",
      "The 45861 th iteration gives loss of 0.13630188742713925\n",
      "The 45862 th iteration gives loss of 0.13630067486217906\n",
      "The 45863 th iteration gives loss of 0.13629946231681603\n",
      "The 45864 th iteration gives loss of 0.13629824979104443\n",
      "The 45865 th iteration gives loss of 0.13629703728487153\n",
      "The 45866 th iteration gives loss of 0.13629582479829808\n",
      "The 45867 th iteration gives loss of 0.1362946123313184\n",
      "The 45868 th iteration gives loss of 0.1362933998839202\n",
      "The 45869 th iteration gives loss of 0.13629218745612415\n",
      "The 45870 th iteration gives loss of 0.13629097504791166\n",
      "The 45871 th iteration gives loss of 0.1362897626592821\n",
      "The 45872 th iteration gives loss of 0.13628855029024164\n",
      "The 45873 th iteration gives loss of 0.13628733794078857\n",
      "The 45874 th iteration gives loss of 0.1362861256109197\n",
      "The 45875 th iteration gives loss of 0.13628491330063236\n",
      "The 45876 th iteration gives loss of 0.13628370100991932\n",
      "The 45877 th iteration gives loss of 0.13628248873879006\n",
      "The 45878 th iteration gives loss of 0.13628127648723054\n",
      "The 45879 th iteration gives loss of 0.1362800642552571\n",
      "The 45880 th iteration gives loss of 0.1362788520428595\n",
      "The 45881 th iteration gives loss of 0.13627763985002897\n",
      "The 45882 th iteration gives loss of 0.136276427676765\n",
      "The 45883 th iteration gives loss of 0.13627521552307922\n",
      "The 45884 th iteration gives loss of 0.13627400338895732\n",
      "The 45885 th iteration gives loss of 0.13627279127440067\n",
      "The 45886 th iteration gives loss of 0.13627157917941535\n",
      "The 45887 th iteration gives loss of 0.1362703671040011\n",
      "The 45888 th iteration gives loss of 0.13626915504813653\n",
      "The 45889 th iteration gives loss of 0.13626794301184159\n",
      "The 45890 th iteration gives loss of 0.13626673099510622\n",
      "The 45891 th iteration gives loss of 0.13626551899792086\n",
      "The 45892 th iteration gives loss of 0.13626430702030307\n",
      "The 45893 th iteration gives loss of 0.13626309506223444\n",
      "The 45894 th iteration gives loss of 0.1362618831237193\n",
      "The 45895 th iteration gives loss of 0.1362606712047561\n",
      "The 45896 th iteration gives loss of 0.13625945930534925\n",
      "The 45897 th iteration gives loss of 0.13625824742549358\n",
      "The 45898 th iteration gives loss of 0.136257035565182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 45899 th iteration gives loss of 0.13625582372442246\n",
      "The 45900 th iteration gives loss of 0.1362546119032024\n",
      "The 45901 th iteration gives loss of 0.1362534001015248\n",
      "The 45902 th iteration gives loss of 0.1362521883194015\n",
      "The 45903 th iteration gives loss of 0.13625097655681323\n",
      "The 45904 th iteration gives loss of 0.13624976481375572\n",
      "The 45905 th iteration gives loss of 0.1362485530902498\n",
      "The 45906 th iteration gives loss of 0.1362473413862702\n",
      "The 45907 th iteration gives loss of 0.13624612970183797\n",
      "The 45908 th iteration gives loss of 0.13624491803693142\n",
      "The 45909 th iteration gives loss of 0.13624370639156258\n",
      "The 45910 th iteration gives loss of 0.13624249476571956\n",
      "The 45911 th iteration gives loss of 0.1362412831594083\n",
      "The 45912 th iteration gives loss of 0.13624007157262416\n",
      "The 45913 th iteration gives loss of 0.13623886000537377\n",
      "The 45914 th iteration gives loss of 0.1362376484576432\n",
      "The 45915 th iteration gives loss of 0.1362364369294376\n",
      "The 45916 th iteration gives loss of 0.13623522542074773\n",
      "The 45917 th iteration gives loss of 0.13623401393159082\n",
      "The 45918 th iteration gives loss of 0.1362328024619427\n",
      "The 45919 th iteration gives loss of 0.13623159101181967\n",
      "The 45920 th iteration gives loss of 0.1362303795812116\n",
      "The 45921 th iteration gives loss of 0.13622916817012418\n",
      "The 45922 th iteration gives loss of 0.13622795677854407\n",
      "The 45923 th iteration gives loss of 0.13622674540647647\n",
      "The 45924 th iteration gives loss of 0.13622553405392548\n",
      "The 45925 th iteration gives loss of 0.13622432272087892\n",
      "The 45926 th iteration gives loss of 0.13622311140733656\n",
      "The 45927 th iteration gives loss of 0.13622190011330892\n",
      "The 45928 th iteration gives loss of 0.13622068883878294\n",
      "The 45929 th iteration gives loss of 0.13621947758376612\n",
      "The 45930 th iteration gives loss of 0.13621826634824857\n",
      "The 45931 th iteration gives loss of 0.13621705513222382\n",
      "The 45932 th iteration gives loss of 0.13621584393570532\n",
      "The 45933 th iteration gives loss of 0.13621463275868773\n",
      "The 45934 th iteration gives loss of 0.1362134216011645\n",
      "The 45935 th iteration gives loss of 0.13621221046313525\n",
      "The 45936 th iteration gives loss of 0.1362109993446015\n",
      "The 45937 th iteration gives loss of 0.13620978824555965\n",
      "The 45938 th iteration gives loss of 0.1362085771660074\n",
      "The 45939 th iteration gives loss of 0.1362073661059422\n",
      "The 45940 th iteration gives loss of 0.13620615506537495\n",
      "The 45941 th iteration gives loss of 0.13620494404429048\n",
      "The 45942 th iteration gives loss of 0.13620373304268485\n",
      "The 45943 th iteration gives loss of 0.13620252206057182\n",
      "The 45944 th iteration gives loss of 0.1362013110979304\n",
      "The 45945 th iteration gives loss of 0.13620010015478148\n",
      "The 45946 th iteration gives loss of 0.13619888923110335\n",
      "The 45947 th iteration gives loss of 0.13619767832690458\n",
      "The 45948 th iteration gives loss of 0.13619646744218863\n",
      "The 45949 th iteration gives loss of 0.13619525657694934\n",
      "The 45950 th iteration gives loss of 0.1361940457311765\n",
      "The 45951 th iteration gives loss of 0.13619283490486722\n",
      "The 45952 th iteration gives loss of 0.13619162409804658\n",
      "The 45953 th iteration gives loss of 0.1361904133106924\n",
      "The 45954 th iteration gives loss of 0.13618920254279648\n",
      "The 45955 th iteration gives loss of 0.13618799179437413\n",
      "The 45956 th iteration gives loss of 0.13618678106542134\n",
      "The 45957 th iteration gives loss of 0.13618557035592238\n",
      "The 45958 th iteration gives loss of 0.13618435966589873\n",
      "The 45959 th iteration gives loss of 0.1361831489953265\n",
      "The 45960 th iteration gives loss of 0.13618193834421127\n",
      "The 45961 th iteration gives loss of 0.1361807277125594\n",
      "The 45962 th iteration gives loss of 0.13617951710036094\n",
      "The 45963 th iteration gives loss of 0.13617830650762766\n",
      "The 45964 th iteration gives loss of 0.1361770959343381\n",
      "The 45965 th iteration gives loss of 0.13617588538050537\n",
      "The 45966 th iteration gives loss of 0.13617467484611728\n",
      "The 45967 th iteration gives loss of 0.13617346433118332\n",
      "The 45968 th iteration gives loss of 0.1361722538356971\n",
      "The 45969 th iteration gives loss of 0.13617104335965324\n",
      "The 45970 th iteration gives loss of 0.13616983290305096\n",
      "The 45971 th iteration gives loss of 0.1361686224659035\n",
      "The 45972 th iteration gives loss of 0.1361674120482014\n",
      "The 45973 th iteration gives loss of 0.13616620164992999\n",
      "The 45974 th iteration gives loss of 0.13616499127110446\n",
      "The 45975 th iteration gives loss of 0.1361637809117148\n",
      "The 45976 th iteration gives loss of 0.1361625705717679\n",
      "The 45977 th iteration gives loss of 0.1361613602512475\n",
      "The 45978 th iteration gives loss of 0.13616014995016099\n",
      "The 45979 th iteration gives loss of 0.13615893966850617\n",
      "The 45980 th iteration gives loss of 0.13615772940629023\n",
      "The 45981 th iteration gives loss of 0.1361565191634949\n",
      "The 45982 th iteration gives loss of 0.13615530894013028\n",
      "The 45983 th iteration gives loss of 0.13615409873619444\n",
      "The 45984 th iteration gives loss of 0.1361528885516879\n",
      "The 45985 th iteration gives loss of 0.13615167838660527\n",
      "The 45986 th iteration gives loss of 0.13615046824093974\n",
      "The 45987 th iteration gives loss of 0.1361492581146932\n",
      "The 45988 th iteration gives loss of 0.136148048007869\n",
      "The 45989 th iteration gives loss of 0.13614683792046717\n",
      "The 45990 th iteration gives loss of 0.13614562785247367\n",
      "The 45991 th iteration gives loss of 0.1361444178039025\n",
      "The 45992 th iteration gives loss of 0.1361432077747422\n",
      "The 45993 th iteration gives loss of 0.1361419977649967\n",
      "The 45994 th iteration gives loss of 0.1361407877746634\n",
      "The 45995 th iteration gives loss of 0.13613957780373706\n",
      "The 45996 th iteration gives loss of 0.13613836785221275\n",
      "The 45997 th iteration gives loss of 0.13613715792010622\n",
      "The 45998 th iteration gives loss of 0.13613594800740242\n",
      "The 45999 th iteration gives loss of 0.1361347381140929\n",
      "The 46000 th iteration gives loss of 0.13613352824019717\n",
      "The 46001 th iteration gives loss of 0.1361323183857019\n",
      "The 46002 th iteration gives loss of 0.13613110855060162\n",
      "The 46003 th iteration gives loss of 0.1361298987349033\n",
      "The 46004 th iteration gives loss of 0.13612868893860336\n",
      "The 46005 th iteration gives loss of 0.13612747916168982\n",
      "The 46006 th iteration gives loss of 0.1361262694041845\n",
      "The 46007 th iteration gives loss of 0.13612505966606092\n",
      "The 46008 th iteration gives loss of 0.13612384994732765\n",
      "The 46009 th iteration gives loss of 0.13612264024798565\n",
      "The 46010 th iteration gives loss of 0.13612143056804113\n",
      "The 46011 th iteration gives loss of 0.13612022090747383\n",
      "The 46012 th iteration gives loss of 0.13611901126629108\n",
      "The 46013 th iteration gives loss of 0.13611780164449996\n",
      "The 46014 th iteration gives loss of 0.13611659204208454\n",
      "The 46015 th iteration gives loss of 0.136115382459058\n",
      "The 46016 th iteration gives loss of 0.13611417289541017\n",
      "The 46017 th iteration gives loss of 0.13611296335113726\n",
      "The 46018 th iteration gives loss of 0.1361117538262425\n",
      "The 46019 th iteration gives loss of 0.13611054432072575\n",
      "The 46020 th iteration gives loss of 0.13610933483457663\n",
      "The 46021 th iteration gives loss of 0.13610812536781275\n",
      "The 46022 th iteration gives loss of 0.13610691592041252\n",
      "The 46023 th iteration gives loss of 0.1361057064923824\n",
      "The 46024 th iteration gives loss of 0.13610449708371214\n",
      "The 46025 th iteration gives loss of 0.13610328769441432\n",
      "The 46026 th iteration gives loss of 0.13610207832448937\n",
      "The 46027 th iteration gives loss of 0.13610086897392015\n",
      "The 46028 th iteration gives loss of 0.13609965964272397\n",
      "The 46029 th iteration gives loss of 0.13609845033088\n",
      "The 46030 th iteration gives loss of 0.1360972410384019\n",
      "The 46031 th iteration gives loss of 0.13609603176528326\n",
      "The 46032 th iteration gives loss of 0.13609482251151298\n",
      "The 46033 th iteration gives loss of 0.1360936132771061\n",
      "The 46034 th iteration gives loss of 0.13609240406205572\n",
      "The 46035 th iteration gives loss of 0.13609119486635163\n",
      "The 46036 th iteration gives loss of 0.13608998568999922\n",
      "The 46037 th iteration gives loss of 0.1360887765330017\n",
      "The 46038 th iteration gives loss of 0.13608756739535688\n",
      "The 46039 th iteration gives loss of 0.1360863582770492\n",
      "The 46040 th iteration gives loss of 0.13608514917809997\n",
      "The 46041 th iteration gives loss of 0.13608394009848096\n",
      "The 46042 th iteration gives loss of 0.13608273103820656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 46043 th iteration gives loss of 0.1360815219972828\n",
      "The 46044 th iteration gives loss of 0.13608031297569562\n",
      "The 46045 th iteration gives loss of 0.13607910397345277\n",
      "The 46046 th iteration gives loss of 0.1360778949905402\n",
      "The 46047 th iteration gives loss of 0.13607668602697287\n",
      "The 46048 th iteration gives loss of 0.1360754770827317\n",
      "The 46049 th iteration gives loss of 0.13607426815782045\n",
      "The 46050 th iteration gives loss of 0.13607305925225094\n",
      "The 46051 th iteration gives loss of 0.1360718503660065\n",
      "The 46052 th iteration gives loss of 0.13607064149909087\n",
      "The 46053 th iteration gives loss of 0.13606943265151122\n",
      "The 46054 th iteration gives loss of 0.13606822382325043\n",
      "The 46055 th iteration gives loss of 0.13606701501430893\n",
      "The 46056 th iteration gives loss of 0.13606580622470218\n",
      "The 46057 th iteration gives loss of 0.1360645974544103\n",
      "The 46058 th iteration gives loss of 0.1360633887034428\n",
      "The 46059 th iteration gives loss of 0.13606217997178985\n",
      "The 46060 th iteration gives loss of 0.13606097125945513\n",
      "The 46061 th iteration gives loss of 0.13605976256644403\n",
      "The 46062 th iteration gives loss of 0.13605855389273813\n",
      "The 46063 th iteration gives loss of 0.13605734523834995\n",
      "The 46064 th iteration gives loss of 0.13605613660327734\n",
      "The 46065 th iteration gives loss of 0.13605492798751062\n",
      "The 46066 th iteration gives loss of 0.13605371939105973\n",
      "The 46067 th iteration gives loss of 0.13605251081391037\n",
      "The 46068 th iteration gives loss of 0.13605130225607062\n",
      "The 46069 th iteration gives loss of 0.1360500937175375\n",
      "The 46070 th iteration gives loss of 0.1360488851983081\n",
      "The 46071 th iteration gives loss of 0.13604767669837972\n",
      "The 46072 th iteration gives loss of 0.13604646821775096\n",
      "The 46073 th iteration gives loss of 0.13604525975642587\n",
      "The 46074 th iteration gives loss of 0.13604405131439215\n",
      "The 46075 th iteration gives loss of 0.1360428428916599\n",
      "The 46076 th iteration gives loss of 0.1360416344882277\n",
      "The 46077 th iteration gives loss of 0.13604042610407585\n",
      "The 46078 th iteration gives loss of 0.13603921773923017\n",
      "The 46079 th iteration gives loss of 0.13603800939367358\n",
      "The 46080 th iteration gives loss of 0.13603680106740032\n",
      "The 46081 th iteration gives loss of 0.1360355927604201\n",
      "The 46082 th iteration gives loss of 0.1360343844727219\n",
      "The 46083 th iteration gives loss of 0.1360331762043177\n",
      "The 46084 th iteration gives loss of 0.13603196795518818\n",
      "The 46085 th iteration gives loss of 0.13603075972534662\n",
      "The 46086 th iteration gives loss of 0.13602955151478172\n",
      "The 46087 th iteration gives loss of 0.13602834332350383\n",
      "The 46088 th iteration gives loss of 0.13602713515150247\n",
      "The 46089 th iteration gives loss of 0.13602592699877536\n",
      "The 46090 th iteration gives loss of 0.13602471886532705\n",
      "The 46091 th iteration gives loss of 0.13602351075115168\n",
      "The 46092 th iteration gives loss of 0.1360223026562488\n",
      "The 46093 th iteration gives loss of 0.13602109458061312\n",
      "The 46094 th iteration gives loss of 0.13601988652425331\n",
      "The 46095 th iteration gives loss of 0.13601867848716814\n",
      "The 46096 th iteration gives loss of 0.13601747046934648\n",
      "The 46097 th iteration gives loss of 0.1360162624707866\n",
      "The 46098 th iteration gives loss of 0.13601505449149226\n",
      "The 46099 th iteration gives loss of 0.13601384653146378\n",
      "The 46100 th iteration gives loss of 0.13601263859069598\n",
      "The 46101 th iteration gives loss of 0.1360114306691869\n",
      "The 46102 th iteration gives loss of 0.13601022276694025\n",
      "The 46103 th iteration gives loss of 0.13600901488395603\n",
      "The 46104 th iteration gives loss of 0.13600780702021184\n",
      "The 46105 th iteration gives loss of 0.13600659917573413\n",
      "The 46106 th iteration gives loss of 0.13600539135050513\n",
      "The 46107 th iteration gives loss of 0.13600418354453223\n",
      "The 46108 th iteration gives loss of 0.13600297575780942\n",
      "The 46109 th iteration gives loss of 0.1360017679903345\n",
      "The 46110 th iteration gives loss of 0.1360005602421096\n",
      "The 46111 th iteration gives loss of 0.13599935251313128\n",
      "The 46112 th iteration gives loss of 0.13599814480339203\n",
      "The 46113 th iteration gives loss of 0.13599693711289906\n",
      "The 46114 th iteration gives loss of 0.13599572944165636\n",
      "The 46115 th iteration gives loss of 0.13599452178964874\n",
      "The 46116 th iteration gives loss of 0.13599331415686963\n",
      "The 46117 th iteration gives loss of 0.1359921065433435\n",
      "The 46118 th iteration gives loss of 0.13599089894904978\n",
      "The 46119 th iteration gives loss of 0.13598969137398323\n",
      "The 46120 th iteration gives loss of 0.13598848381815878\n",
      "The 46121 th iteration gives loss of 0.13598727628157092\n",
      "The 46122 th iteration gives loss of 0.1359860687642073\n",
      "The 46123 th iteration gives loss of 0.13598486126607331\n",
      "The 46124 th iteration gives loss of 0.1359836537871751\n",
      "The 46125 th iteration gives loss of 0.13598244632749643\n",
      "The 46126 th iteration gives loss of 0.13598123888704328\n",
      "The 46127 th iteration gives loss of 0.1359800314658196\n",
      "The 46128 th iteration gives loss of 0.13597882406381465\n",
      "The 46129 th iteration gives loss of 0.13597761668103134\n",
      "The 46130 th iteration gives loss of 0.13597640931746968\n",
      "The 46131 th iteration gives loss of 0.1359752019731242\n",
      "The 46132 th iteration gives loss of 0.13597399464800466\n",
      "The 46133 th iteration gives loss of 0.13597278734208804\n",
      "The 46134 th iteration gives loss of 0.13597158005539398\n",
      "The 46135 th iteration gives loss of 0.13597037278791502\n",
      "The 46136 th iteration gives loss of 0.13596916553963811\n",
      "The 46137 th iteration gives loss of 0.13596795831057162\n",
      "The 46138 th iteration gives loss of 0.13596675110072462\n",
      "The 46139 th iteration gives loss of 0.13596554391007964\n",
      "The 46140 th iteration gives loss of 0.13596433673864103\n",
      "The 46141 th iteration gives loss of 0.1359631295864081\n",
      "The 46142 th iteration gives loss of 0.13596192245337488\n",
      "The 46143 th iteration gives loss of 0.13596071533953913\n",
      "The 46144 th iteration gives loss of 0.13595950824491537\n",
      "The 46145 th iteration gives loss of 0.13595830116949253\n",
      "The 46146 th iteration gives loss of 0.13595709411325707\n",
      "The 46147 th iteration gives loss of 0.13595588707622128\n",
      "The 46148 th iteration gives loss of 0.13595468005837916\n",
      "The 46149 th iteration gives loss of 0.13595347305973457\n",
      "The 46150 th iteration gives loss of 0.13595226608028294\n",
      "The 46151 th iteration gives loss of 0.13595105912001773\n",
      "The 46152 th iteration gives loss of 0.13594985217894173\n",
      "The 46153 th iteration gives loss of 0.13594864525705597\n",
      "The 46154 th iteration gives loss of 0.135947438354356\n",
      "The 46155 th iteration gives loss of 0.13594623147084212\n",
      "The 46156 th iteration gives loss of 0.1359450246065198\n",
      "The 46157 th iteration gives loss of 0.13594381776136616\n",
      "The 46158 th iteration gives loss of 0.1359426109354002\n",
      "The 46159 th iteration gives loss of 0.13594140412861686\n",
      "The 46160 th iteration gives loss of 0.1359401973410128\n",
      "The 46161 th iteration gives loss of 0.13593899057257752\n",
      "The 46162 th iteration gives loss of 0.1359377838233207\n",
      "The 46163 th iteration gives loss of 0.1359365770932432\n",
      "The 46164 th iteration gives loss of 0.13593537038233544\n",
      "The 46165 th iteration gives loss of 0.13593416369059844\n",
      "The 46166 th iteration gives loss of 0.13593295701803032\n",
      "The 46167 th iteration gives loss of 0.1359317503646347\n",
      "The 46168 th iteration gives loss of 0.13593054373039662\n",
      "The 46169 th iteration gives loss of 0.135929337115341\n",
      "The 46170 th iteration gives loss of 0.13592813051943503\n",
      "The 46171 th iteration gives loss of 0.13592692394269656\n",
      "The 46172 th iteration gives loss of 0.1359257173851208\n",
      "The 46173 th iteration gives loss of 0.13592451084670695\n",
      "The 46174 th iteration gives loss of 0.13592330432744992\n",
      "The 46175 th iteration gives loss of 0.13592209782736042\n",
      "The 46176 th iteration gives loss of 0.1359208913464137\n",
      "The 46177 th iteration gives loss of 0.1359196848846302\n",
      "The 46178 th iteration gives loss of 0.13591847844199154\n",
      "The 46179 th iteration gives loss of 0.13591727201851292\n",
      "The 46180 th iteration gives loss of 0.1359160656141814\n",
      "The 46181 th iteration gives loss of 0.13591485922899854\n",
      "The 46182 th iteration gives loss of 0.13591365286296703\n",
      "The 46183 th iteration gives loss of 0.1359124465160862\n",
      "The 46184 th iteration gives loss of 0.13591124018834616\n",
      "The 46185 th iteration gives loss of 0.13591003387974354\n",
      "The 46186 th iteration gives loss of 0.13590882759028775\n",
      "The 46187 th iteration gives loss of 0.13590762131998302\n",
      "The 46188 th iteration gives loss of 0.13590641506879922\n",
      "The 46189 th iteration gives loss of 0.1359052088367747\n",
      "The 46190 th iteration gives loss of 0.13590400262387065\n",
      "The 46191 th iteration gives loss of 0.1359027964301066\n",
      "The 46192 th iteration gives loss of 0.13590159025547247\n",
      "The 46193 th iteration gives loss of 0.13590038409998176\n",
      "The 46194 th iteration gives loss of 0.13589917796361703\n",
      "The 46195 th iteration gives loss of 0.13589797184638835\n",
      "The 46196 th iteration gives loss of 0.135896765748281\n",
      "The 46197 th iteration gives loss of 0.13589555966929814\n",
      "The 46198 th iteration gives loss of 0.13589435360945024\n",
      "The 46199 th iteration gives loss of 0.1358931475687141\n",
      "The 46200 th iteration gives loss of 0.13589194154711481\n",
      "The 46201 th iteration gives loss of 0.13589073554463713\n",
      "The 46202 th iteration gives loss of 0.13588952956127878\n",
      "The 46203 th iteration gives loss of 0.1358883235970315\n",
      "The 46204 th iteration gives loss of 0.13588711765190026\n",
      "The 46205 th iteration gives loss of 0.13588591172589332\n",
      "The 46206 th iteration gives loss of 0.13588470581900147\n",
      "The 46207 th iteration gives loss of 0.13588349993122598\n",
      "The 46208 th iteration gives loss of 0.13588229406255337\n",
      "The 46209 th iteration gives loss of 0.13588108821299813\n",
      "The 46210 th iteration gives loss of 0.13587988238254686\n",
      "The 46211 th iteration gives loss of 0.13587867657119937\n",
      "The 46212 th iteration gives loss of 0.13587747077897347\n",
      "The 46213 th iteration gives loss of 0.13587626500584535\n",
      "The 46214 th iteration gives loss of 0.13587505925182036\n",
      "The 46215 th iteration gives loss of 0.13587385351690076\n",
      "The 46216 th iteration gives loss of 0.13587264780108246\n",
      "The 46217 th iteration gives loss of 0.13587144210436\n",
      "The 46218 th iteration gives loss of 0.1358702364267395\n",
      "The 46219 th iteration gives loss of 0.13586903076821463\n",
      "The 46220 th iteration gives loss of 0.1358678251287912\n",
      "The 46221 th iteration gives loss of 0.1358666195084584\n",
      "The 46222 th iteration gives loss of 0.13586541390721552\n",
      "The 46223 th iteration gives loss of 0.1358642083250659\n",
      "The 46224 th iteration gives loss of 0.13586300276200441\n",
      "The 46225 th iteration gives loss of 0.13586179721803512\n",
      "The 46226 th iteration gives loss of 0.13586059169315456\n",
      "The 46227 th iteration gives loss of 0.13585938618735768\n",
      "The 46228 th iteration gives loss of 0.13585818070064834\n",
      "The 46229 th iteration gives loss of 0.13585697523301313\n",
      "The 46230 th iteration gives loss of 0.13585576978447614\n",
      "The 46231 th iteration gives loss of 0.13585456435500187\n",
      "The 46232 th iteration gives loss of 0.13585335894461723\n",
      "The 46233 th iteration gives loss of 0.1358521535533088\n",
      "The 46234 th iteration gives loss of 0.13585094818107601\n",
      "The 46235 th iteration gives loss of 0.1358497428279168\n",
      "The 46236 th iteration gives loss of 0.13584853749383716\n",
      "The 46237 th iteration gives loss of 0.13584733217882775\n",
      "The 46238 th iteration gives loss of 0.13584612688288544\n",
      "The 46239 th iteration gives loss of 0.13584492160601894\n",
      "The 46240 th iteration gives loss of 0.13584371634821674\n",
      "The 46241 th iteration gives loss of 0.1358425111094808\n",
      "The 46242 th iteration gives loss of 0.13584130588981227\n",
      "The 46243 th iteration gives loss of 0.1358401006892134\n",
      "The 46244 th iteration gives loss of 0.1358388955076666\n",
      "The 46245 th iteration gives loss of 0.135837690345182\n",
      "The 46246 th iteration gives loss of 0.13583648520177108\n",
      "The 46247 th iteration gives loss of 0.1358352800774104\n",
      "The 46248 th iteration gives loss of 0.13583407497210695\n",
      "The 46249 th iteration gives loss of 0.1358328698858568\n",
      "The 46250 th iteration gives loss of 0.13583166481866607\n",
      "The 46251 th iteration gives loss of 0.13583045977052052\n",
      "The 46252 th iteration gives loss of 0.13582925474143992\n",
      "The 46253 th iteration gives loss of 0.1358280497314059\n",
      "The 46254 th iteration gives loss of 0.13582684474042192\n",
      "The 46255 th iteration gives loss of 0.13582563976848175\n",
      "The 46256 th iteration gives loss of 0.13582443481558892\n",
      "The 46257 th iteration gives loss of 0.13582322988174392\n",
      "The 46258 th iteration gives loss of 0.13582202496693774\n",
      "The 46259 th iteration gives loss of 0.1358208200711814\n",
      "The 46260 th iteration gives loss of 0.13581961519445937\n",
      "The 46261 th iteration gives loss of 0.13581841033678224\n",
      "The 46262 th iteration gives loss of 0.13581720549813697\n",
      "The 46263 th iteration gives loss of 0.13581600067852906\n",
      "The 46264 th iteration gives loss of 0.13581479587796408\n",
      "The 46265 th iteration gives loss of 0.13581359109642913\n",
      "The 46266 th iteration gives loss of 0.1358123863339266\n",
      "The 46267 th iteration gives loss of 0.1358111815904546\n",
      "The 46268 th iteration gives loss of 0.1358099768660142\n",
      "The 46269 th iteration gives loss of 0.13580877216060977\n",
      "The 46270 th iteration gives loss of 0.13580756747422398\n",
      "The 46271 th iteration gives loss of 0.13580636280686242\n",
      "The 46272 th iteration gives loss of 0.1358051581585261\n",
      "The 46273 th iteration gives loss of 0.1358039535292188\n",
      "The 46274 th iteration gives loss of 0.13580274891892713\n",
      "The 46275 th iteration gives loss of 0.1358015443276611\n",
      "The 46276 th iteration gives loss of 0.13580033975542483\n",
      "The 46277 th iteration gives loss of 0.13579913520219195\n",
      "The 46278 th iteration gives loss of 0.1357979306679812\n",
      "The 46279 th iteration gives loss of 0.1357967261527792\n",
      "The 46280 th iteration gives loss of 0.13579552165659547\n",
      "The 46281 th iteration gives loss of 0.13579431717942114\n",
      "The 46282 th iteration gives loss of 0.13579311272126013\n",
      "The 46283 th iteration gives loss of 0.13579190828211013\n",
      "The 46284 th iteration gives loss of 0.1357907038619706\n",
      "The 46285 th iteration gives loss of 0.13578949946083635\n",
      "The 46286 th iteration gives loss of 0.1357882950787092\n",
      "The 46287 th iteration gives loss of 0.13578709071558528\n",
      "The 46288 th iteration gives loss of 0.13578588637146188\n",
      "The 46289 th iteration gives loss of 0.13578468204634392\n",
      "The 46290 th iteration gives loss of 0.1357834777402199\n",
      "The 46291 th iteration gives loss of 0.13578227345310287\n",
      "The 46292 th iteration gives loss of 0.1357810691849816\n",
      "The 46293 th iteration gives loss of 0.13577986493585303\n",
      "The 46294 th iteration gives loss of 0.1357786607057272\n",
      "The 46295 th iteration gives loss of 0.1357774564945826\n",
      "The 46296 th iteration gives loss of 0.1357762523024386\n",
      "The 46297 th iteration gives loss of 0.13577504812928026\n",
      "The 46298 th iteration gives loss of 0.13577384397511502\n",
      "The 46299 th iteration gives loss of 0.13577263983994567\n",
      "The 46300 th iteration gives loss of 0.1357714357237535\n",
      "The 46301 th iteration gives loss of 0.13577023162654284\n",
      "The 46302 th iteration gives loss of 0.13576902754831618\n",
      "The 46303 th iteration gives loss of 0.13576782348908642\n",
      "The 46304 th iteration gives loss of 0.13576661944882137\n",
      "The 46305 th iteration gives loss of 0.13576541542754392\n",
      "The 46306 th iteration gives loss of 0.13576421142525225\n",
      "The 46307 th iteration gives loss of 0.13576300744192935\n",
      "The 46308 th iteration gives loss of 0.13576180347758804\n",
      "The 46309 th iteration gives loss of 0.1357605995322211\n",
      "The 46310 th iteration gives loss of 0.13575939560581726\n",
      "The 46311 th iteration gives loss of 0.1357581916983955\n",
      "The 46312 th iteration gives loss of 0.13575698780994155\n",
      "The 46313 th iteration gives loss of 0.13575578394045176\n",
      "The 46314 th iteration gives loss of 0.13575458008993416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 46315 th iteration gives loss of 0.1357533762583832\n",
      "The 46316 th iteration gives loss of 0.13575217244579718\n",
      "The 46317 th iteration gives loss of 0.13575096865217354\n",
      "The 46318 th iteration gives loss of 0.13574976487751875\n",
      "The 46319 th iteration gives loss of 0.1357485611218204\n",
      "The 46320 th iteration gives loss of 0.13574735738507873\n",
      "The 46321 th iteration gives loss of 0.13574615366730022\n",
      "The 46322 th iteration gives loss of 0.13574494996847708\n",
      "The 46323 th iteration gives loss of 0.13574374628860403\n",
      "The 46324 th iteration gives loss of 0.13574254262769347\n",
      "The 46325 th iteration gives loss of 0.13574133898573312\n",
      "The 46326 th iteration gives loss of 0.13574013536272536\n",
      "The 46327 th iteration gives loss of 0.13573893175866597\n",
      "The 46328 th iteration gives loss of 0.13573772817355081\n",
      "The 46329 th iteration gives loss of 0.135736524607388\n",
      "The 46330 th iteration gives loss of 0.13573532106017852\n",
      "The 46331 th iteration gives loss of 0.1357341175319098\n",
      "The 46332 th iteration gives loss of 0.135732914022585\n",
      "The 46333 th iteration gives loss of 0.135731710532203\n",
      "The 46334 th iteration gives loss of 0.1357305070607541\n",
      "The 46335 th iteration gives loss of 0.13572930360824825\n",
      "The 46336 th iteration gives loss of 0.13572810017467687\n",
      "The 46337 th iteration gives loss of 0.1357268967600535\n",
      "The 46338 th iteration gives loss of 0.13572569336436266\n",
      "The 46339 th iteration gives loss of 0.13572448998760062\n",
      "The 46340 th iteration gives loss of 0.13572328662977565\n",
      "The 46341 th iteration gives loss of 0.13572208329087762\n",
      "The 46342 th iteration gives loss of 0.13572087997091048\n",
      "The 46343 th iteration gives loss of 0.13571967666986764\n",
      "The 46344 th iteration gives loss of 0.13571847338776155\n",
      "The 46345 th iteration gives loss of 0.13571727012457582\n",
      "The 46346 th iteration gives loss of 0.13571606688032375\n",
      "The 46347 th iteration gives loss of 0.13571486365499147\n",
      "The 46348 th iteration gives loss of 0.1357136604485755\n",
      "The 46349 th iteration gives loss of 0.13571245726107953\n",
      "The 46350 th iteration gives loss of 0.13571125409251228\n",
      "The 46351 th iteration gives loss of 0.1357100509428606\n",
      "The 46352 th iteration gives loss of 0.1357088478121195\n",
      "The 46353 th iteration gives loss of 0.13570764470029384\n",
      "The 46354 th iteration gives loss of 0.1357064416073835\n",
      "The 46355 th iteration gives loss of 0.135705238533389\n",
      "The 46356 th iteration gives loss of 0.1357040354783073\n",
      "The 46357 th iteration gives loss of 0.13570283244212725\n",
      "The 46358 th iteration gives loss of 0.135701629424862\n",
      "The 46359 th iteration gives loss of 0.1357004264265023\n",
      "The 46360 th iteration gives loss of 0.13569922344705013\n",
      "The 46361 th iteration gives loss of 0.13569802048650512\n",
      "The 46362 th iteration gives loss of 0.13569681754485283\n",
      "The 46363 th iteration gives loss of 0.13569561462211133\n",
      "The 46364 th iteration gives loss of 0.135694411718265\n",
      "The 46365 th iteration gives loss of 0.135693208833328\n",
      "The 46366 th iteration gives loss of 0.13569200596728756\n",
      "The 46367 th iteration gives loss of 0.135690803120131\n",
      "The 46368 th iteration gives loss of 0.13568960029188235\n",
      "The 46369 th iteration gives loss of 0.13568839748251604\n",
      "The 46370 th iteration gives loss of 0.1356871946920514\n",
      "The 46371 th iteration gives loss of 0.13568599192047656\n",
      "The 46372 th iteration gives loss of 0.1356847891677913\n",
      "The 46373 th iteration gives loss of 0.1356835864339904\n",
      "The 46374 th iteration gives loss of 0.13568238371907665\n",
      "The 46375 th iteration gives loss of 0.13568118102305504\n",
      "The 46376 th iteration gives loss of 0.13567997834591744\n",
      "The 46377 th iteration gives loss of 0.13567877568765824\n",
      "The 46378 th iteration gives loss of 0.13567757304828193\n",
      "The 46379 th iteration gives loss of 0.13567637042778968\n",
      "The 46380 th iteration gives loss of 0.1356751678261732\n",
      "The 46381 th iteration gives loss of 0.13567396524343292\n",
      "The 46382 th iteration gives loss of 0.13567276267957923\n",
      "The 46383 th iteration gives loss of 0.13567156013458256\n",
      "The 46384 th iteration gives loss of 0.13567035760846943\n",
      "The 46385 th iteration gives loss of 0.13566915510122954\n",
      "The 46386 th iteration gives loss of 0.13566795261285877\n",
      "The 46387 th iteration gives loss of 0.13566675014336477\n",
      "The 46388 th iteration gives loss of 0.13566554769273817\n",
      "The 46389 th iteration gives loss of 0.13566434526097088\n",
      "The 46390 th iteration gives loss of 0.1356631428480725\n",
      "The 46391 th iteration gives loss of 0.13566194045403532\n",
      "The 46392 th iteration gives loss of 0.1356607380788652\n",
      "The 46393 th iteration gives loss of 0.13565953572255815\n",
      "The 46394 th iteration gives loss of 0.13565833338510916\n",
      "The 46395 th iteration gives loss of 0.13565713106652263\n",
      "The 46396 th iteration gives loss of 0.13565592876679536\n",
      "The 46397 th iteration gives loss of 0.13565472648591645\n",
      "The 46398 th iteration gives loss of 0.13565352422390053\n",
      "The 46399 th iteration gives loss of 0.13565232198073096\n",
      "The 46400 th iteration gives loss of 0.1356511197564211\n",
      "The 46401 th iteration gives loss of 0.1356499175509525\n",
      "The 46402 th iteration gives loss of 0.13564871536434472\n",
      "The 46403 th iteration gives loss of 0.13564751319658058\n",
      "The 46404 th iteration gives loss of 0.1356463110476609\n",
      "The 46405 th iteration gives loss of 0.13564510891759107\n",
      "The 46406 th iteration gives loss of 0.13564390680636995\n",
      "The 46407 th iteration gives loss of 0.1356427047139796\n",
      "The 46408 th iteration gives loss of 0.13564150264044406\n",
      "The 46409 th iteration gives loss of 0.13564030058574314\n",
      "The 46410 th iteration gives loss of 0.13563909854988185\n",
      "The 46411 th iteration gives loss of 0.13563789653285624\n",
      "The 46412 th iteration gives loss of 0.13563669453466734\n",
      "The 46413 th iteration gives loss of 0.13563549255531696\n",
      "The 46414 th iteration gives loss of 0.1356342905947946\n",
      "The 46415 th iteration gives loss of 0.13563308865310975\n",
      "The 46416 th iteration gives loss of 0.13563188673025595\n",
      "The 46417 th iteration gives loss of 0.135630684826234\n",
      "The 46418 th iteration gives loss of 0.13562948294103833\n",
      "The 46419 th iteration gives loss of 0.13562828107466995\n",
      "The 46420 th iteration gives loss of 0.13562707922712489\n",
      "The 46421 th iteration gives loss of 0.13562587739840373\n",
      "The 46422 th iteration gives loss of 0.13562467558850988\n",
      "The 46423 th iteration gives loss of 0.13562347379743575\n",
      "The 46424 th iteration gives loss of 0.1356222720251844\n",
      "The 46425 th iteration gives loss of 0.13562107027175088\n",
      "The 46426 th iteration gives loss of 0.1356198685371301\n",
      "The 46427 th iteration gives loss of 0.13561866682133172\n",
      "The 46428 th iteration gives loss of 0.1356174651243433\n",
      "The 46429 th iteration gives loss of 0.13561626344617767\n",
      "The 46430 th iteration gives loss of 0.13561506178682184\n",
      "The 46431 th iteration gives loss of 0.13561386014626764\n",
      "The 46432 th iteration gives loss of 0.13561265852452786\n",
      "The 46433 th iteration gives loss of 0.13561145692160434\n",
      "The 46434 th iteration gives loss of 0.13561025533748564\n",
      "The 46435 th iteration gives loss of 0.13560905377217042\n",
      "The 46436 th iteration gives loss of 0.13560785222566232\n",
      "The 46437 th iteration gives loss of 0.13560665069795447\n",
      "The 46438 th iteration gives loss of 0.13560544918904396\n",
      "The 46439 th iteration gives loss of 0.1356042476989439\n",
      "The 46440 th iteration gives loss of 0.13560304622764194\n",
      "The 46441 th iteration gives loss of 0.1356018447751244\n",
      "The 46442 th iteration gives loss of 0.13560064334142166\n",
      "The 46443 th iteration gives loss of 0.13559944192650134\n",
      "The 46444 th iteration gives loss of 0.13559824053038716\n",
      "The 46445 th iteration gives loss of 0.1355970391530575\n",
      "The 46446 th iteration gives loss of 0.13559583779452172\n",
      "The 46447 th iteration gives loss of 0.1355946364547686\n",
      "The 46448 th iteration gives loss of 0.1355934351338137\n",
      "The 46449 th iteration gives loss of 0.13559223383164018\n",
      "The 46450 th iteration gives loss of 0.13559103254825744\n",
      "The 46451 th iteration gives loss of 0.13558983128365729\n",
      "The 46452 th iteration gives loss of 0.13558863003784902\n",
      "The 46453 th iteration gives loss of 0.13558742881081434\n",
      "The 46454 th iteration gives loss of 0.13558622760256042\n",
      "The 46455 th iteration gives loss of 0.13558502641308562\n",
      "The 46456 th iteration gives loss of 0.13558382524239193\n",
      "The 46457 th iteration gives loss of 0.13558262409046776\n",
      "The 46458 th iteration gives loss of 0.1355814229573144\n",
      "The 46459 th iteration gives loss of 0.13558022184295532\n",
      "The 46460 th iteration gives loss of 0.13557902074735872\n",
      "The 46461 th iteration gives loss of 0.1355778196705311\n",
      "The 46462 th iteration gives loss of 0.13557661861247838\n",
      "The 46463 th iteration gives loss of 0.13557541757319286\n",
      "The 46464 th iteration gives loss of 0.13557421655267263\n",
      "The 46465 th iteration gives loss of 0.13557301555092\n",
      "The 46466 th iteration gives loss of 0.135571814567934\n",
      "The 46467 th iteration gives loss of 0.13557061360370945\n",
      "The 46468 th iteration gives loss of 0.13556941265825076\n",
      "The 46469 th iteration gives loss of 0.13556821173154987\n",
      "The 46470 th iteration gives loss of 0.13556701082360498\n",
      "The 46471 th iteration gives loss of 0.1355658099344227\n",
      "The 46472 th iteration gives loss of 0.13556460906400228\n",
      "The 46473 th iteration gives loss of 0.13556340821233143\n",
      "The 46474 th iteration gives loss of 0.13556220737941688\n",
      "The 46475 th iteration gives loss of 0.13556100656525755\n",
      "The 46476 th iteration gives loss of 0.13555980576984747\n",
      "The 46477 th iteration gives loss of 0.13555860499319192\n",
      "The 46478 th iteration gives loss of 0.13555740423527518\n",
      "The 46479 th iteration gives loss of 0.13555620349611258\n",
      "The 46480 th iteration gives loss of 0.1355550027756983\n",
      "The 46481 th iteration gives loss of 0.13555380207403164\n",
      "The 46482 th iteration gives loss of 0.1355526013910991\n",
      "The 46483 th iteration gives loss of 0.1355514007269132\n",
      "The 46484 th iteration gives loss of 0.13555020008147556\n",
      "The 46485 th iteration gives loss of 0.13554899945477122\n",
      "The 46486 th iteration gives loss of 0.13554779884680523\n",
      "The 46487 th iteration gives loss of 0.13554659825758275\n",
      "The 46488 th iteration gives loss of 0.13554539768708498\n",
      "The 46489 th iteration gives loss of 0.1355441971353238\n",
      "The 46490 th iteration gives loss of 0.1355429966023042\n",
      "The 46491 th iteration gives loss of 0.13554179608801303\n",
      "The 46492 th iteration gives loss of 0.13554059559245815\n",
      "The 46493 th iteration gives loss of 0.13553939511563132\n",
      "The 46494 th iteration gives loss of 0.13553819465752226\n",
      "The 46495 th iteration gives loss of 0.1355369942181471\n",
      "The 46496 th iteration gives loss of 0.1355357937974941\n",
      "The 46497 th iteration gives loss of 0.13553459339557197\n",
      "The 46498 th iteration gives loss of 0.1355333930123635\n",
      "The 46499 th iteration gives loss of 0.1355321926478849\n",
      "The 46500 th iteration gives loss of 0.13553099230211937\n",
      "The 46501 th iteration gives loss of 0.13552979197508078\n",
      "The 46502 th iteration gives loss of 0.13552859166675557\n",
      "The 46503 th iteration gives loss of 0.13552739137714265\n",
      "The 46504 th iteration gives loss of 0.1355261911062538\n",
      "The 46505 th iteration gives loss of 0.13552499085406225\n",
      "The 46506 th iteration gives loss of 0.13552379062060246\n",
      "The 46507 th iteration gives loss of 0.13552259040584255\n",
      "The 46508 th iteration gives loss of 0.13552139020980106\n",
      "The 46509 th iteration gives loss of 0.1355201900324588\n",
      "The 46510 th iteration gives loss of 0.13551898987382904\n",
      "The 46511 th iteration gives loss of 0.13551778973390075\n",
      "The 46512 th iteration gives loss of 0.13551658961268384\n",
      "The 46513 th iteration gives loss of 0.13551538951016745\n",
      "The 46514 th iteration gives loss of 0.1355141894263512\n",
      "The 46515 th iteration gives loss of 0.13551298936123507\n",
      "The 46516 th iteration gives loss of 0.13551178931481717\n",
      "The 46517 th iteration gives loss of 0.13551058928709528\n",
      "The 46518 th iteration gives loss of 0.13550938927807782\n",
      "The 46519 th iteration gives loss of 0.1355081892877507\n",
      "The 46520 th iteration gives loss of 0.13550698931611133\n",
      "The 46521 th iteration gives loss of 0.13550578936317922\n",
      "The 46522 th iteration gives loss of 0.13550458942892862\n",
      "The 46523 th iteration gives loss of 0.13550338951336846\n",
      "The 46524 th iteration gives loss of 0.1355021896165001\n",
      "The 46525 th iteration gives loss of 0.1355009897383117\n",
      "The 46526 th iteration gives loss of 0.13549978987881703\n",
      "The 46527 th iteration gives loss of 0.13549859003801645\n",
      "The 46528 th iteration gives loss of 0.1354973902158832\n",
      "The 46529 th iteration gives loss of 0.13549619041244226\n",
      "The 46530 th iteration gives loss of 0.13549499062767426\n",
      "The 46531 th iteration gives loss of 0.1354937908615902\n",
      "The 46532 th iteration gives loss of 0.13549259111418485\n",
      "The 46533 th iteration gives loss of 0.13549139138545568\n",
      "The 46534 th iteration gives loss of 0.13549019167540036\n",
      "The 46535 th iteration gives loss of 0.13548899198401945\n",
      "The 46536 th iteration gives loss of 0.1354877923113157\n",
      "The 46537 th iteration gives loss of 0.13548659265727966\n",
      "The 46538 th iteration gives loss of 0.13548539302192067\n",
      "The 46539 th iteration gives loss of 0.13548419340522647\n",
      "The 46540 th iteration gives loss of 0.1354829938071946\n",
      "The 46541 th iteration gives loss of 0.13548179422784135\n",
      "The 46542 th iteration gives loss of 0.13548059466714626\n",
      "The 46543 th iteration gives loss of 0.13547939512511092\n",
      "The 46544 th iteration gives loss of 0.13547819560174135\n",
      "The 46545 th iteration gives loss of 0.13547699609703745\n",
      "The 46546 th iteration gives loss of 0.13547579661099352\n",
      "The 46547 th iteration gives loss of 0.13547459714360433\n",
      "The 46548 th iteration gives loss of 0.13547339769487465\n",
      "The 46549 th iteration gives loss of 0.13547219826480175\n",
      "The 46550 th iteration gives loss of 0.13547099885338407\n",
      "The 46551 th iteration gives loss of 0.1354697994606175\n",
      "The 46552 th iteration gives loss of 0.1354686000865081\n",
      "The 46553 th iteration gives loss of 0.13546740073104385\n",
      "The 46554 th iteration gives loss of 0.13546620139423113\n",
      "The 46555 th iteration gives loss of 0.1354650020760731\n",
      "The 46556 th iteration gives loss of 0.1354638027765532\n",
      "The 46557 th iteration gives loss of 0.13546260349568032\n",
      "The 46558 th iteration gives loss of 0.1354614042334538\n",
      "The 46559 th iteration gives loss of 0.1354602049898759\n",
      "The 46560 th iteration gives loss of 0.1354590057649286\n",
      "The 46561 th iteration gives loss of 0.13545780655863116\n",
      "The 46562 th iteration gives loss of 0.13545660737096565\n",
      "The 46563 th iteration gives loss of 0.135455408201948\n",
      "The 46564 th iteration gives loss of 0.13545420905155733\n",
      "The 46565 th iteration gives loss of 0.13545300991979783\n",
      "The 46566 th iteration gives loss of 0.13545181080668142\n",
      "The 46567 th iteration gives loss of 0.13545061171220008\n",
      "The 46568 th iteration gives loss of 0.13544941263635027\n",
      "The 46569 th iteration gives loss of 0.13544821357912282\n",
      "The 46570 th iteration gives loss of 0.13544701454052693\n",
      "The 46571 th iteration gives loss of 0.13544581552056212\n",
      "The 46572 th iteration gives loss of 0.1354446165192229\n",
      "The 46573 th iteration gives loss of 0.13544341753650857\n",
      "The 46574 th iteration gives loss of 0.13544221857240868\n",
      "The 46575 th iteration gives loss of 0.13544101962694732\n",
      "The 46576 th iteration gives loss of 0.13543982070009714\n",
      "The 46577 th iteration gives loss of 0.13543862179187102\n",
      "The 46578 th iteration gives loss of 0.13543742290225544\n",
      "The 46579 th iteration gives loss of 0.13543622403126412\n",
      "The 46580 th iteration gives loss of 0.13543502517889144\n",
      "The 46581 th iteration gives loss of 0.1354338263451265\n",
      "The 46582 th iteration gives loss of 0.13543262752998073\n",
      "The 46583 th iteration gives loss of 0.1354314287334478\n",
      "The 46584 th iteration gives loss of 0.13543022995551895\n",
      "The 46585 th iteration gives loss of 0.13542903119619995\n",
      "The 46586 th iteration gives loss of 0.13542783245549145\n",
      "The 46587 th iteration gives loss of 0.13542663373338754\n",
      "The 46588 th iteration gives loss of 0.1354254350298952\n",
      "The 46589 th iteration gives loss of 0.13542423634500925\n",
      "The 46590 th iteration gives loss of 0.13542303767871275\n",
      "The 46591 th iteration gives loss of 0.13542183903102692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 46592 th iteration gives loss of 0.13542064040193622\n",
      "The 46593 th iteration gives loss of 0.13541944179145277\n",
      "The 46594 th iteration gives loss of 0.13541824319956244\n",
      "The 46595 th iteration gives loss of 0.13541704462626894\n",
      "The 46596 th iteration gives loss of 0.13541584607157145\n",
      "The 46597 th iteration gives loss of 0.1354146475354695\n",
      "The 46598 th iteration gives loss of 0.13541344901795552\n",
      "The 46599 th iteration gives loss of 0.135412250519034\n",
      "The 46600 th iteration gives loss of 0.13541105203870207\n",
      "The 46601 th iteration gives loss of 0.1354098535769655\n",
      "The 46602 th iteration gives loss of 0.1354086551338045\n",
      "The 46603 th iteration gives loss of 0.13540745670924215\n",
      "The 46604 th iteration gives loss of 0.13540625830325453\n",
      "The 46605 th iteration gives loss of 0.13540505991586058\n",
      "The 46606 th iteration gives loss of 0.13540386154703882\n",
      "The 46607 th iteration gives loss of 0.1354026631968012\n",
      "The 46608 th iteration gives loss of 0.13540146486514737\n",
      "The 46609 th iteration gives loss of 0.13540026655206971\n",
      "The 46610 th iteration gives loss of 0.1353990682575727\n",
      "The 46611 th iteration gives loss of 0.1353978699816458\n",
      "The 46612 th iteration gives loss of 0.13539667172429593\n",
      "The 46613 th iteration gives loss of 0.13539547348552114\n",
      "The 46614 th iteration gives loss of 0.1353942752653156\n",
      "The 46615 th iteration gives loss of 0.13539307706367643\n",
      "The 46616 th iteration gives loss of 0.1353918788806158\n",
      "The 46617 th iteration gives loss of 0.13539068071612312\n",
      "The 46618 th iteration gives loss of 0.13538948257018635\n",
      "The 46619 th iteration gives loss of 0.13538828444281947\n",
      "The 46620 th iteration gives loss of 0.1353870863340196\n",
      "The 46621 th iteration gives loss of 0.13538588824378398\n",
      "The 46622 th iteration gives loss of 0.13538469017211133\n",
      "The 46623 th iteration gives loss of 0.13538349211899794\n",
      "The 46624 th iteration gives loss of 0.1353822940844391\n",
      "The 46625 th iteration gives loss of 0.13538109606843896\n",
      "The 46626 th iteration gives loss of 0.1353798980709956\n",
      "The 46627 th iteration gives loss of 0.13537870009211148\n",
      "The 46628 th iteration gives loss of 0.1353775021317718\n",
      "The 46629 th iteration gives loss of 0.13537630418999855\n",
      "The 46630 th iteration gives loss of 0.13537510626677085\n",
      "The 46631 th iteration gives loss of 0.13537390836209415\n",
      "The 46632 th iteration gives loss of 0.13537271047596242\n",
      "The 46633 th iteration gives loss of 0.1353715126083773\n",
      "The 46634 th iteration gives loss of 0.13537031475935127\n",
      "The 46635 th iteration gives loss of 0.13536911692885614\n",
      "The 46636 th iteration gives loss of 0.13536791911690974\n",
      "The 46637 th iteration gives loss of 0.13536672132350208\n",
      "The 46638 th iteration gives loss of 0.13536552354863632\n",
      "The 46639 th iteration gives loss of 0.1353643257923133\n",
      "The 46640 th iteration gives loss of 0.13536312805453588\n",
      "The 46641 th iteration gives loss of 0.13536193033527766\n",
      "The 46642 th iteration gives loss of 0.13536073263457002\n",
      "The 46643 th iteration gives loss of 0.1353595349523947\n",
      "The 46644 th iteration gives loss of 0.1353583372887486\n",
      "The 46645 th iteration gives loss of 0.1353571396436393\n",
      "The 46646 th iteration gives loss of 0.13535594201705467\n",
      "The 46647 th iteration gives loss of 0.13535474440900283\n",
      "The 46648 th iteration gives loss of 0.13535354681947975\n",
      "The 46649 th iteration gives loss of 0.1353523492484813\n",
      "The 46650 th iteration gives loss of 0.13535115169601195\n",
      "The 46651 th iteration gives loss of 0.13534995416207102\n",
      "The 46652 th iteration gives loss of 0.1353487566466407\n",
      "The 46653 th iteration gives loss of 0.13534755914973703\n",
      "The 46654 th iteration gives loss of 0.1353463616713607\n",
      "The 46655 th iteration gives loss of 0.1353451642114978\n",
      "The 46656 th iteration gives loss of 0.1353439667701496\n",
      "The 46657 th iteration gives loss of 0.13534276934732922\n",
      "The 46658 th iteration gives loss of 0.13534157194301552\n",
      "The 46659 th iteration gives loss of 0.13534037455721976\n",
      "The 46660 th iteration gives loss of 0.13533917718993663\n",
      "The 46661 th iteration gives loss of 0.1353379798411661\n",
      "The 46662 th iteration gives loss of 0.13533678251090467\n",
      "The 46663 th iteration gives loss of 0.13533558519915562\n",
      "The 46664 th iteration gives loss of 0.13533438790590874\n",
      "The 46665 th iteration gives loss of 0.13533319063117674\n",
      "The 46666 th iteration gives loss of 0.1353319933749292\n",
      "The 46667 th iteration gives loss of 0.1353307961372027\n",
      "The 46668 th iteration gives loss of 0.13532959891798796\n",
      "The 46669 th iteration gives loss of 0.13532840171725571\n",
      "The 46670 th iteration gives loss of 0.13532720453503555\n",
      "The 46671 th iteration gives loss of 0.13532600737130526\n",
      "The 46672 th iteration gives loss of 0.13532481022607165\n",
      "The 46673 th iteration gives loss of 0.1353236130993448\n",
      "The 46674 th iteration gives loss of 0.13532241599110387\n",
      "The 46675 th iteration gives loss of 0.13532121890136126\n",
      "The 46676 th iteration gives loss of 0.1353200218301069\n",
      "The 46677 th iteration gives loss of 0.1353188247773461\n",
      "The 46678 th iteration gives loss of 0.135317627743079\n",
      "The 46679 th iteration gives loss of 0.13531643072729715\n",
      "The 46680 th iteration gives loss of 0.13531523373000962\n",
      "The 46681 th iteration gives loss of 0.13531403675120127\n",
      "The 46682 th iteration gives loss of 0.13531283979087547\n",
      "The 46683 th iteration gives loss of 0.13531164284903666\n",
      "The 46684 th iteration gives loss of 0.13531044592567956\n",
      "The 46685 th iteration gives loss of 0.13530924902079292\n",
      "The 46686 th iteration gives loss of 0.13530805213440117\n",
      "The 46687 th iteration gives loss of 0.13530685526648506\n",
      "The 46688 th iteration gives loss of 0.1353056584170416\n",
      "The 46689 th iteration gives loss of 0.13530446158607853\n",
      "The 46690 th iteration gives loss of 0.13530326477358687\n",
      "The 46691 th iteration gives loss of 0.13530206797957206\n",
      "The 46692 th iteration gives loss of 0.1353008712040255\n",
      "The 46693 th iteration gives loss of 0.135299674446951\n",
      "The 46694 th iteration gives loss of 0.13529847770835352\n",
      "The 46695 th iteration gives loss of 0.13529728098821128\n",
      "The 46696 th iteration gives loss of 0.13529608428654216\n",
      "The 46697 th iteration gives loss of 0.13529488760333805\n",
      "The 46698 th iteration gives loss of 0.13529369093859805\n",
      "The 46699 th iteration gives loss of 0.1352924942923248\n",
      "The 46700 th iteration gives loss of 0.1352912976645079\n",
      "The 46701 th iteration gives loss of 0.1352901010551523\n",
      "The 46702 th iteration gives loss of 0.13528890446426098\n",
      "The 46703 th iteration gives loss of 0.13528770789182598\n",
      "The 46704 th iteration gives loss of 0.1352865113378369\n",
      "The 46705 th iteration gives loss of 0.13528531480231262\n",
      "The 46706 th iteration gives loss of 0.1352841182852463\n",
      "The 46707 th iteration gives loss of 0.13528292178663381\n",
      "The 46708 th iteration gives loss of 0.13528172530646396\n",
      "The 46709 th iteration gives loss of 0.13528052884474637\n",
      "The 46710 th iteration gives loss of 0.1352793324014855\n",
      "The 46711 th iteration gives loss of 0.1352781359766666\n",
      "The 46712 th iteration gives loss of 0.1352769395702938\n",
      "The 46713 th iteration gives loss of 0.13527574318237207\n",
      "The 46714 th iteration gives loss of 0.13527454681288675\n",
      "The 46715 th iteration gives loss of 0.1352733504618495\n",
      "The 46716 th iteration gives loss of 0.13527215412925445\n",
      "The 46717 th iteration gives loss of 0.13527095781509743\n",
      "The 46718 th iteration gives loss of 0.13526976151937703\n",
      "The 46719 th iteration gives loss of 0.13526856524209957\n",
      "The 46720 th iteration gives loss of 0.13526736898325722\n",
      "The 46721 th iteration gives loss of 0.13526617274283995\n",
      "The 46722 th iteration gives loss of 0.13526497652087166\n",
      "The 46723 th iteration gives loss of 0.13526378031733077\n",
      "The 46724 th iteration gives loss of 0.1352625841322174\n",
      "The 46725 th iteration gives loss of 0.1352613879655333\n",
      "The 46726 th iteration gives loss of 0.13526019181728113\n",
      "The 46727 th iteration gives loss of 0.13525899568746402\n",
      "The 46728 th iteration gives loss of 0.13525779957606326\n",
      "The 46729 th iteration gives loss of 0.1352566034830917\n",
      "The 46730 th iteration gives loss of 0.13525540740854428\n",
      "The 46731 th iteration gives loss of 0.13525421135241886\n",
      "The 46732 th iteration gives loss of 0.13525301531470796\n",
      "The 46733 th iteration gives loss of 0.13525181929542748\n",
      "The 46734 th iteration gives loss of 0.13525062329456394\n",
      "The 46735 th iteration gives loss of 0.1352494273121122\n",
      "The 46736 th iteration gives loss of 0.13524823134808836\n",
      "The 46737 th iteration gives loss of 0.1352470354024559\n",
      "The 46738 th iteration gives loss of 0.1352458394752604\n",
      "The 46739 th iteration gives loss of 0.1352446435664734\n",
      "The 46740 th iteration gives loss of 0.13524344767609298\n",
      "The 46741 th iteration gives loss of 0.13524225180411872\n",
      "The 46742 th iteration gives loss of 0.13524105595055713\n",
      "The 46743 th iteration gives loss of 0.13523986011540393\n",
      "The 46744 th iteration gives loss of 0.13523866429866338\n",
      "The 46745 th iteration gives loss of 0.1352374685003169\n",
      "The 46746 th iteration gives loss of 0.13523627272037808\n",
      "The 46747 th iteration gives loss of 0.13523507695883613\n",
      "The 46748 th iteration gives loss of 0.13523388121570254\n",
      "The 46749 th iteration gives loss of 0.13523268549096826\n",
      "The 46750 th iteration gives loss of 0.1352314897846264\n",
      "The 46751 th iteration gives loss of 0.13523029409668372\n",
      "The 46752 th iteration gives loss of 0.13522909842714276\n",
      "The 46753 th iteration gives loss of 0.13522790277599317\n",
      "The 46754 th iteration gives loss of 0.1352267071432485\n",
      "The 46755 th iteration gives loss of 0.13522551152887807\n",
      "The 46756 th iteration gives loss of 0.13522431593290685\n",
      "The 46757 th iteration gives loss of 0.13522312035531667\n",
      "The 46758 th iteration gives loss of 0.13522192479612669\n",
      "The 46759 th iteration gives loss of 0.1352207292553231\n",
      "The 46760 th iteration gives loss of 0.13521953373289383\n",
      "The 46761 th iteration gives loss of 0.1352183382288624\n",
      "The 46762 th iteration gives loss of 0.13521714274320648\n",
      "The 46763 th iteration gives loss of 0.1352159472759323\n",
      "The 46764 th iteration gives loss of 0.1352147518270442\n",
      "The 46765 th iteration gives loss of 0.13521355639653504\n",
      "The 46766 th iteration gives loss of 0.13521236098439696\n",
      "The 46767 th iteration gives loss of 0.13521116559064614\n",
      "The 46768 th iteration gives loss of 0.13520997021526762\n",
      "The 46769 th iteration gives loss of 0.13520877485826074\n",
      "The 46770 th iteration gives loss of 0.1352075795196301\n",
      "The 46771 th iteration gives loss of 0.13520638419937228\n",
      "The 46772 th iteration gives loss of 0.13520518889748298\n",
      "The 46773 th iteration gives loss of 0.13520399361396357\n",
      "The 46774 th iteration gives loss of 0.13520279834881446\n",
      "The 46775 th iteration gives loss of 0.13520160310202897\n",
      "The 46776 th iteration gives loss of 0.1352004078736132\n",
      "The 46777 th iteration gives loss of 0.13519921266356041\n",
      "The 46778 th iteration gives loss of 0.1351980174718676\n",
      "The 46779 th iteration gives loss of 0.1351968222985432\n",
      "The 46780 th iteration gives loss of 0.13519562714357433\n",
      "The 46781 th iteration gives loss of 0.13519443200696626\n",
      "The 46782 th iteration gives loss of 0.1351932368887216\n",
      "The 46783 th iteration gives loss of 0.13519204178882482\n",
      "The 46784 th iteration gives loss of 0.1351908467072837\n",
      "The 46785 th iteration gives loss of 0.13518965164410715\n",
      "The 46786 th iteration gives loss of 0.13518845659927467\n",
      "The 46787 th iteration gives loss of 0.1351872615727985\n",
      "The 46788 th iteration gives loss of 0.1351860665646664\n",
      "The 46789 th iteration gives loss of 0.13518487157489065\n",
      "The 46790 th iteration gives loss of 0.13518367660346559\n",
      "The 46791 th iteration gives loss of 0.13518248165038188\n",
      "The 46792 th iteration gives loss of 0.1351812867156551\n",
      "The 46793 th iteration gives loss of 0.13518009179926635\n",
      "The 46794 th iteration gives loss of 0.13517889690121346\n",
      "The 46795 th iteration gives loss of 0.13517770202149929\n",
      "The 46796 th iteration gives loss of 0.13517650716014074\n",
      "The 46797 th iteration gives loss of 0.1351753123171147\n",
      "The 46798 th iteration gives loss of 0.13517411749242494\n",
      "The 46799 th iteration gives loss of 0.13517292268607134\n",
      "The 46800 th iteration gives loss of 0.135171727898063\n",
      "The 46801 th iteration gives loss of 0.1351705331283795\n",
      "The 46802 th iteration gives loss of 0.13516933837704023\n",
      "The 46803 th iteration gives loss of 0.1351681436440192\n",
      "The 46804 th iteration gives loss of 0.1351669489293315\n",
      "The 46805 th iteration gives loss of 0.13516575423298183\n",
      "The 46806 th iteration gives loss of 0.1351645595549484\n",
      "The 46807 th iteration gives loss of 0.13516336489525563\n",
      "The 46808 th iteration gives loss of 0.13516217025387678\n",
      "The 46809 th iteration gives loss of 0.13516097563082657\n",
      "The 46810 th iteration gives loss of 0.13515978102609794\n",
      "The 46811 th iteration gives loss of 0.13515858643969733\n",
      "The 46812 th iteration gives loss of 0.13515739187160697\n",
      "The 46813 th iteration gives loss of 0.1351561973218452\n",
      "The 46814 th iteration gives loss of 0.13515500279039946\n",
      "The 46815 th iteration gives loss of 0.13515380827727444\n",
      "The 46816 th iteration gives loss of 0.13515261378245413\n",
      "The 46817 th iteration gives loss of 0.1351514193059591\n",
      "The 46818 th iteration gives loss of 0.1351502248477732\n",
      "The 46819 th iteration gives loss of 0.13514903040789852\n",
      "The 46820 th iteration gives loss of 0.13514783598633598\n",
      "The 46821 th iteration gives loss of 0.13514664158308487\n",
      "The 46822 th iteration gives loss of 0.13514544719813626\n",
      "The 46823 th iteration gives loss of 0.13514425283149817\n",
      "The 46824 th iteration gives loss of 0.13514305848316313\n",
      "The 46825 th iteration gives loss of 0.13514186415313612\n",
      "The 46826 th iteration gives loss of 0.13514066984141446\n",
      "The 46827 th iteration gives loss of 0.1351394755479921\n",
      "The 46828 th iteration gives loss of 0.13513828127286612\n",
      "The 46829 th iteration gives loss of 0.13513708701604382\n",
      "The 46830 th iteration gives loss of 0.1351358927775192\n",
      "The 46831 th iteration gives loss of 0.13513469855728497\n",
      "The 46832 th iteration gives loss of 0.1351335043553551\n",
      "The 46833 th iteration gives loss of 0.13513231017171873\n",
      "The 46834 th iteration gives loss of 0.13513111600637226\n",
      "The 46835 th iteration gives loss of 0.13512992185932582\n",
      "The 46836 th iteration gives loss of 0.13512872773055976\n",
      "The 46837 th iteration gives loss of 0.13512753362008495\n",
      "The 46838 th iteration gives loss of 0.1351263395278977\n",
      "The 46839 th iteration gives loss of 0.13512514545400886\n",
      "The 46840 th iteration gives loss of 0.13512395139839373\n",
      "The 46841 th iteration gives loss of 0.1351227573610675\n",
      "The 46842 th iteration gives loss of 0.1351215633420254\n",
      "The 46843 th iteration gives loss of 0.135120369341263\n",
      "The 46844 th iteration gives loss of 0.13511917535878035\n",
      "The 46845 th iteration gives loss of 0.13511798139457862\n",
      "The 46846 th iteration gives loss of 0.13511678744865813\n",
      "The 46847 th iteration gives loss of 0.13511559352101254\n",
      "The 46848 th iteration gives loss of 0.13511439961164062\n",
      "The 46849 th iteration gives loss of 0.13511320572054214\n",
      "The 46850 th iteration gives loss of 0.1351120118477253\n",
      "The 46851 th iteration gives loss of 0.13511081799317215\n",
      "The 46852 th iteration gives loss of 0.1351096241568946\n",
      "The 46853 th iteration gives loss of 0.13510843033889106\n",
      "The 46854 th iteration gives loss of 0.1351072365391521\n",
      "The 46855 th iteration gives loss of 0.1351060427576762\n",
      "The 46856 th iteration gives loss of 0.1351048489944651\n",
      "The 46857 th iteration gives loss of 0.1351036552495197\n",
      "The 46858 th iteration gives loss of 0.13510246152284247\n",
      "The 46859 th iteration gives loss of 0.13510126781442616\n",
      "The 46860 th iteration gives loss of 0.13510007412426792\n",
      "The 46861 th iteration gives loss of 0.13509888045237642\n",
      "The 46862 th iteration gives loss of 0.13509768679873393\n",
      "The 46863 th iteration gives loss of 0.13509649316335176\n",
      "The 46864 th iteration gives loss of 0.1350952995462238\n",
      "The 46865 th iteration gives loss of 0.13509410594735702\n",
      "The 46866 th iteration gives loss of 0.13509291236673676\n",
      "The 46867 th iteration gives loss of 0.135091718804377\n",
      "The 46868 th iteration gives loss of 0.13509052526025853\n",
      "The 46869 th iteration gives loss of 0.13508933173439497\n",
      "The 46870 th iteration gives loss of 0.13508813822677854\n",
      "The 46871 th iteration gives loss of 0.1350869447374131\n",
      "The 46872 th iteration gives loss of 0.1350857512662904\n",
      "The 46873 th iteration gives loss of 0.13508455781341697\n",
      "The 46874 th iteration gives loss of 0.1350833643787822\n",
      "The 46875 th iteration gives loss of 0.13508217096239145\n",
      "The 46876 th iteration gives loss of 0.1350809775642411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 46877 th iteration gives loss of 0.13507978418432598\n",
      "The 46878 th iteration gives loss of 0.13507859082265838\n",
      "The 46879 th iteration gives loss of 0.13507739747921557\n",
      "The 46880 th iteration gives loss of 0.13507620415401958\n",
      "The 46881 th iteration gives loss of 0.13507501084705492\n",
      "The 46882 th iteration gives loss of 0.1350738175583278\n",
      "The 46883 th iteration gives loss of 0.1350726242878385\n",
      "The 46884 th iteration gives loss of 0.13507143103557062\n",
      "The 46885 th iteration gives loss of 0.13507023780153665\n",
      "The 46886 th iteration gives loss of 0.13506904458572652\n",
      "The 46887 th iteration gives loss of 0.1350678513881443\n",
      "The 46888 th iteration gives loss of 0.13506665820879493\n",
      "The 46889 th iteration gives loss of 0.13506546504766997\n",
      "The 46890 th iteration gives loss of 0.13506427190476197\n",
      "The 46891 th iteration gives loss of 0.1350630787800789\n",
      "The 46892 th iteration gives loss of 0.13506188567361985\n",
      "The 46893 th iteration gives loss of 0.1350606925853795\n",
      "The 46894 th iteration gives loss of 0.1350594995153581\n",
      "The 46895 th iteration gives loss of 0.13505830646355288\n",
      "The 46896 th iteration gives loss of 0.13505711342996146\n",
      "The 46897 th iteration gives loss of 0.13505592041459868\n",
      "The 46898 th iteration gives loss of 0.13505472741743715\n",
      "The 46899 th iteration gives loss of 0.1350535344384936\n",
      "The 46900 th iteration gives loss of 0.13505234147775658\n",
      "The 46901 th iteration gives loss of 0.1350511485352372\n",
      "The 46902 th iteration gives loss of 0.13504995561091987\n",
      "The 46903 th iteration gives loss of 0.13504876270480345\n",
      "The 46904 th iteration gives loss of 0.1350475698169062\n",
      "The 46905 th iteration gives loss of 0.13504637694720842\n",
      "The 46906 th iteration gives loss of 0.1350451840957229\n",
      "The 46907 th iteration gives loss of 0.13504399126243355\n",
      "The 46908 th iteration gives loss of 0.135042798447347\n",
      "The 46909 th iteration gives loss of 0.13504160565046264\n",
      "The 46910 th iteration gives loss of 0.13504041287177257\n",
      "The 46911 th iteration gives loss of 0.1350392201112805\n",
      "The 46912 th iteration gives loss of 0.13503802736899567\n",
      "The 46913 th iteration gives loss of 0.1350368346448956\n",
      "The 46914 th iteration gives loss of 0.1350356419389929\n",
      "The 46915 th iteration gives loss of 0.1350344492512803\n",
      "The 46916 th iteration gives loss of 0.1350332565817608\n",
      "The 46917 th iteration gives loss of 0.13503206393043443\n",
      "The 46918 th iteration gives loss of 0.1350308712972986\n",
      "The 46919 th iteration gives loss of 0.13502967868234744\n",
      "The 46920 th iteration gives loss of 0.13502848608558687\n",
      "The 46921 th iteration gives loss of 0.1350272935070135\n",
      "The 46922 th iteration gives loss of 0.13502610094661344\n",
      "The 46923 th iteration gives loss of 0.13502490840440545\n",
      "The 46924 th iteration gives loss of 0.13502371588037837\n",
      "The 46925 th iteration gives loss of 0.13502252337452644\n",
      "The 46926 th iteration gives loss of 0.13502133088686333\n",
      "The 46927 th iteration gives loss of 0.13502013841737923\n",
      "The 46928 th iteration gives loss of 0.13501894596606215\n",
      "The 46929 th iteration gives loss of 0.13501775353292583\n",
      "The 46930 th iteration gives loss of 0.13501656111795982\n",
      "The 46931 th iteration gives loss of 0.1350153687211723\n",
      "The 46932 th iteration gives loss of 0.13501417634255744\n",
      "The 46933 th iteration gives loss of 0.13501298398211414\n",
      "The 46934 th iteration gives loss of 0.13501179163983162\n",
      "The 46935 th iteration gives loss of 0.13501059931573006\n",
      "The 46936 th iteration gives loss of 0.13500940700978759\n",
      "The 46937 th iteration gives loss of 0.13500821472201205\n",
      "The 46938 th iteration gives loss of 0.13500702245240487\n",
      "The 46939 th iteration gives loss of 0.13500583020095686\n",
      "The 46940 th iteration gives loss of 0.13500463796767223\n",
      "The 46941 th iteration gives loss of 0.13500344575254336\n",
      "The 46942 th iteration gives loss of 0.1350022535555854\n",
      "The 46943 th iteration gives loss of 0.13500106137678494\n",
      "The 46944 th iteration gives loss of 0.13499986921614146\n",
      "The 46945 th iteration gives loss of 0.13499867707364754\n",
      "The 46946 th iteration gives loss of 0.13499748494930922\n",
      "The 46947 th iteration gives loss of 0.13499629284312872\n",
      "The 46948 th iteration gives loss of 0.1349951007550994\n",
      "The 46949 th iteration gives loss of 0.1349939086852206\n",
      "The 46950 th iteration gives loss of 0.13499271663349774\n",
      "The 46951 th iteration gives loss of 0.13499152459991173\n",
      "The 46952 th iteration gives loss of 0.13499033258448478\n",
      "The 46953 th iteration gives loss of 0.13498914058719844\n",
      "The 46954 th iteration gives loss of 0.13498794860805843\n",
      "The 46955 th iteration gives loss of 0.13498675664706786\n",
      "The 46956 th iteration gives loss of 0.13498556470421344\n",
      "The 46957 th iteration gives loss of 0.13498437277950326\n",
      "The 46958 th iteration gives loss of 0.13498318087293426\n",
      "The 46959 th iteration gives loss of 0.13498198898449515\n",
      "The 46960 th iteration gives loss of 0.13498079711420197\n",
      "The 46961 th iteration gives loss of 0.13497960526204944\n",
      "The 46962 th iteration gives loss of 0.13497841342802191\n",
      "The 46963 th iteration gives loss of 0.13497722161213713\n",
      "The 46964 th iteration gives loss of 0.1349760298143761\n",
      "The 46965 th iteration gives loss of 0.13497483803475757\n",
      "The 46966 th iteration gives loss of 0.1349736462732637\n",
      "The 46967 th iteration gives loss of 0.13497245452989978\n",
      "The 46968 th iteration gives loss of 0.13497126280466745\n",
      "The 46969 th iteration gives loss of 0.13497007109755818\n",
      "The 46970 th iteration gives loss of 0.13496887940857377\n",
      "The 46971 th iteration gives loss of 0.1349676877377171\n",
      "The 46972 th iteration gives loss of 0.1349664960849862\n",
      "The 46973 th iteration gives loss of 0.13496530445037405\n",
      "The 46974 th iteration gives loss of 0.13496411283388052\n",
      "The 46975 th iteration gives loss of 0.13496292123550374\n",
      "The 46976 th iteration gives loss of 0.1349617296552528\n",
      "The 46977 th iteration gives loss of 0.1349605380931144\n",
      "The 46978 th iteration gives loss of 0.13495934654909045\n",
      "The 46979 th iteration gives loss of 0.13495815502318517\n",
      "The 46980 th iteration gives loss of 0.13495696351539077\n",
      "The 46981 th iteration gives loss of 0.1349557720257109\n",
      "The 46982 th iteration gives loss of 0.13495458055414006\n",
      "The 46983 th iteration gives loss of 0.13495338910068352\n",
      "The 46984 th iteration gives loss of 0.13495219766532887\n",
      "The 46985 th iteration gives loss of 0.13495100624808887\n",
      "The 46986 th iteration gives loss of 0.1349498148489469\n",
      "The 46987 th iteration gives loss of 0.13494862346791692\n",
      "The 46988 th iteration gives loss of 0.13494743210499338\n",
      "The 46989 th iteration gives loss of 0.13494624076016126\n",
      "The 46990 th iteration gives loss of 0.1349450494334348\n",
      "The 46991 th iteration gives loss of 0.13494385812481552\n",
      "The 46992 th iteration gives loss of 0.13494266683428566\n",
      "The 46993 th iteration gives loss of 0.13494147556186298\n",
      "The 46994 th iteration gives loss of 0.13494028430752797\n",
      "The 46995 th iteration gives loss of 0.13493909307129373\n",
      "The 46996 th iteration gives loss of 0.1349379018531445\n",
      "The 46997 th iteration gives loss of 0.13493671065309729\n",
      "The 46998 th iteration gives loss of 0.13493551947113938\n",
      "The 46999 th iteration gives loss of 0.13493432830727708\n",
      "The 47000 th iteration gives loss of 0.1349331371614952\n",
      "The 47001 th iteration gives loss of 0.13493194603381267\n",
      "The 47002 th iteration gives loss of 0.13493075492420253\n",
      "The 47003 th iteration gives loss of 0.13492956383268676\n",
      "The 47004 th iteration gives loss of 0.13492837275923888\n",
      "The 47005 th iteration gives loss of 0.1349271817038923\n",
      "The 47006 th iteration gives loss of 0.13492599066662633\n",
      "The 47007 th iteration gives loss of 0.13492479964743698\n",
      "The 47008 th iteration gives loss of 0.1349236086463329\n",
      "The 47009 th iteration gives loss of 0.13492241766329793\n",
      "The 47010 th iteration gives loss of 0.13492122669835033\n",
      "The 47011 th iteration gives loss of 0.1349200357514733\n",
      "The 47012 th iteration gives loss of 0.1349188448226638\n",
      "The 47013 th iteration gives loss of 0.1349176539119372\n",
      "The 47014 th iteration gives loss of 0.134916463019279\n",
      "The 47015 th iteration gives loss of 0.1349152721446905\n",
      "The 47016 th iteration gives loss of 0.1349140812881755\n",
      "The 47017 th iteration gives loss of 0.13491289044972832\n",
      "The 47018 th iteration gives loss of 0.13491169962934776\n",
      "The 47019 th iteration gives loss of 0.13491050882703448\n",
      "The 47020 th iteration gives loss of 0.13490931804277767\n",
      "The 47021 th iteration gives loss of 0.13490812727659524\n",
      "The 47022 th iteration gives loss of 0.13490693652847335\n",
      "The 47023 th iteration gives loss of 0.1349057457984165\n",
      "The 47024 th iteration gives loss of 0.13490455508641125\n",
      "The 47025 th iteration gives loss of 0.13490336439246647\n",
      "The 47026 th iteration gives loss of 0.13490217371658164\n",
      "The 47027 th iteration gives loss of 0.1349009830587568\n",
      "The 47028 th iteration gives loss of 0.1348997924189769\n",
      "The 47029 th iteration gives loss of 0.13489860179725796\n",
      "The 47030 th iteration gives loss of 0.13489741119359336\n",
      "The 47031 th iteration gives loss of 0.1348962206079816\n",
      "The 47032 th iteration gives loss of 0.13489503004041395\n",
      "The 47033 th iteration gives loss of 0.1348938394908932\n",
      "The 47034 th iteration gives loss of 0.13489264895943595\n",
      "The 47035 th iteration gives loss of 0.13489145844600828\n",
      "The 47036 th iteration gives loss of 0.1348902679506306\n",
      "The 47037 th iteration gives loss of 0.13488907747330264\n",
      "The 47038 th iteration gives loss of 0.13488788701401866\n",
      "The 47039 th iteration gives loss of 0.13488669657277277\n",
      "The 47040 th iteration gives loss of 0.13488550614956882\n",
      "The 47041 th iteration gives loss of 0.13488431574440546\n",
      "The 47042 th iteration gives loss of 0.13488312535727565\n",
      "The 47043 th iteration gives loss of 0.1348819349881852\n",
      "The 47044 th iteration gives loss of 0.13488074463713196\n",
      "The 47045 th iteration gives loss of 0.13487955430411017\n",
      "The 47046 th iteration gives loss of 0.1348783639891334\n",
      "The 47047 th iteration gives loss of 0.13487717369217042\n",
      "The 47048 th iteration gives loss of 0.13487598341324822\n",
      "The 47049 th iteration gives loss of 0.13487479315235992\n",
      "The 47050 th iteration gives loss of 0.13487360290949973\n",
      "The 47051 th iteration gives loss of 0.1348724126846628\n",
      "The 47052 th iteration gives loss of 0.13487122247785585\n",
      "The 47053 th iteration gives loss of 0.13487003228907363\n",
      "The 47054 th iteration gives loss of 0.1348688421183091\n",
      "The 47055 th iteration gives loss of 0.13486765196557407\n",
      "The 47056 th iteration gives loss of 0.1348664618308563\n",
      "The 47057 th iteration gives loss of 0.13486527171415477\n",
      "The 47058 th iteration gives loss of 0.13486408161548222\n",
      "The 47059 th iteration gives loss of 0.13486289153482076\n",
      "The 47060 th iteration gives loss of 0.13486170147218016\n",
      "The 47061 th iteration gives loss of 0.1348605114275614\n",
      "The 47062 th iteration gives loss of 0.13485932140094845\n",
      "The 47063 th iteration gives loss of 0.1348581313923431\n",
      "The 47064 th iteration gives loss of 0.13485694140175736\n",
      "The 47065 th iteration gives loss of 0.13485575142918263\n",
      "The 47066 th iteration gives loss of 0.13485456147461644\n",
      "The 47067 th iteration gives loss of 0.13485337153806187\n",
      "The 47068 th iteration gives loss of 0.13485218161950607\n",
      "The 47069 th iteration gives loss of 0.13485099171896006\n",
      "The 47070 th iteration gives loss of 0.13484980183641854\n",
      "The 47071 th iteration gives loss of 0.13484861197187495\n",
      "The 47072 th iteration gives loss of 0.13484742212534512\n",
      "The 47073 th iteration gives loss of 0.13484623229680873\n",
      "The 47074 th iteration gives loss of 0.13484504248627366\n",
      "The 47075 th iteration gives loss of 0.13484385269374122\n",
      "The 47076 th iteration gives loss of 0.1348426629192016\n",
      "The 47077 th iteration gives loss of 0.13484147316266049\n",
      "The 47078 th iteration gives loss of 0.13484028342411275\n",
      "The 47079 th iteration gives loss of 0.13483909370356378\n",
      "The 47080 th iteration gives loss of 0.13483790400100387\n",
      "The 47081 th iteration gives loss of 0.13483671431644226\n",
      "The 47082 th iteration gives loss of 0.13483552464985452\n",
      "The 47083 th iteration gives loss of 0.13483433500126268\n",
      "The 47084 th iteration gives loss of 0.13483314537066354\n",
      "The 47085 th iteration gives loss of 0.13483195575805285\n",
      "The 47086 th iteration gives loss of 0.13483076616342285\n",
      "The 47087 th iteration gives loss of 0.13482957658677996\n",
      "The 47088 th iteration gives loss of 0.13482838702812439\n",
      "The 47089 th iteration gives loss of 0.13482719748743202\n",
      "The 47090 th iteration gives loss of 0.13482600796473354\n",
      "The 47091 th iteration gives loss of 0.13482481846001282\n",
      "The 47092 th iteration gives loss of 0.1348236289732757\n",
      "The 47093 th iteration gives loss of 0.1348224395045117\n",
      "The 47094 th iteration gives loss of 0.13482125005372333\n",
      "The 47095 th iteration gives loss of 0.13482006062091229\n",
      "The 47096 th iteration gives loss of 0.13481887120607247\n",
      "The 47097 th iteration gives loss of 0.13481768180920545\n",
      "The 47098 th iteration gives loss of 0.1348164924303017\n",
      "The 47099 th iteration gives loss of 0.13481530306937883\n",
      "The 47100 th iteration gives loss of 0.1348141137264236\n",
      "The 47101 th iteration gives loss of 0.13481292440142773\n",
      "The 47102 th iteration gives loss of 0.13481173509440775\n",
      "The 47103 th iteration gives loss of 0.13481054580535354\n",
      "The 47104 th iteration gives loss of 0.1348093565342579\n",
      "The 47105 th iteration gives loss of 0.13480816728112494\n",
      "The 47106 th iteration gives loss of 0.13480697804595437\n",
      "The 47107 th iteration gives loss of 0.13480578882874564\n",
      "The 47108 th iteration gives loss of 0.13480459962950245\n",
      "The 47109 th iteration gives loss of 0.13480341044819846\n",
      "The 47110 th iteration gives loss of 0.13480222128486566\n",
      "The 47111 th iteration gives loss of 0.13480103213949066\n",
      "The 47112 th iteration gives loss of 0.13479984301206208\n",
      "The 47113 th iteration gives loss of 0.1347986539025859\n",
      "The 47114 th iteration gives loss of 0.13479746481106694\n",
      "The 47115 th iteration gives loss of 0.13479627573750105\n",
      "The 47116 th iteration gives loss of 0.1347950866818797\n",
      "The 47117 th iteration gives loss of 0.1347938976442098\n",
      "The 47118 th iteration gives loss of 0.13479270862448764\n",
      "The 47119 th iteration gives loss of 0.1347915196227065\n",
      "The 47120 th iteration gives loss of 0.13479033063887544\n",
      "The 47121 th iteration gives loss of 0.13478914167298667\n",
      "The 47122 th iteration gives loss of 0.13478795272503888\n",
      "The 47123 th iteration gives loss of 0.13478676379503138\n",
      "The 47124 th iteration gives loss of 0.13478557488296736\n",
      "The 47125 th iteration gives loss of 0.1347843859888414\n",
      "The 47126 th iteration gives loss of 0.13478319711264863\n",
      "The 47127 th iteration gives loss of 0.1347820082543994\n",
      "The 47128 th iteration gives loss of 0.13478081941408315\n",
      "The 47129 th iteration gives loss of 0.1347796305917022\n",
      "The 47130 th iteration gives loss of 0.13477844178725096\n",
      "The 47131 th iteration gives loss of 0.13477725300073606\n",
      "The 47132 th iteration gives loss of 0.13477606423214772\n",
      "The 47133 th iteration gives loss of 0.13477487548149297\n",
      "The 47134 th iteration gives loss of 0.13477368674876355\n",
      "The 47135 th iteration gives loss of 0.13477249803396074\n",
      "The 47136 th iteration gives loss of 0.13477130933708978\n",
      "The 47137 th iteration gives loss of 0.1347701206581313\n",
      "The 47138 th iteration gives loss of 0.13476893199710455\n",
      "The 47139 th iteration gives loss of 0.13476774335399688\n",
      "The 47140 th iteration gives loss of 0.1347665547288137\n",
      "The 47141 th iteration gives loss of 0.1347653661215478\n",
      "The 47142 th iteration gives loss of 0.13476417753220404\n",
      "The 47143 th iteration gives loss of 0.13476298896077252\n",
      "The 47144 th iteration gives loss of 0.13476180040726068\n",
      "The 47145 th iteration gives loss of 0.13476061187166719\n",
      "The 47146 th iteration gives loss of 0.13475942335398194\n",
      "The 47147 th iteration gives loss of 0.1347582348542071\n",
      "The 47148 th iteration gives loss of 0.13475704637234992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 47149 th iteration gives loss of 0.1347558579084043\n",
      "The 47150 th iteration gives loss of 0.1347546694623652\n",
      "The 47151 th iteration gives loss of 0.13475348103423757\n",
      "The 47152 th iteration gives loss of 0.13475229262400676\n",
      "The 47153 th iteration gives loss of 0.13475110423169379\n",
      "The 47154 th iteration gives loss of 0.1347499158572779\n",
      "The 47155 th iteration gives loss of 0.13474872750076786\n",
      "The 47156 th iteration gives loss of 0.1347475391621602\n",
      "The 47157 th iteration gives loss of 0.13474635084145503\n",
      "The 47158 th iteration gives loss of 0.1347451625386477\n",
      "The 47159 th iteration gives loss of 0.1347439742537409\n",
      "The 47160 th iteration gives loss of 0.13474278598673203\n",
      "The 47161 th iteration gives loss of 0.13474159773761826\n",
      "The 47162 th iteration gives loss of 0.1347404095063951\n",
      "The 47163 th iteration gives loss of 0.13473922129306673\n",
      "The 47164 th iteration gives loss of 0.1347380330976344\n",
      "The 47165 th iteration gives loss of 0.13473684492009078\n",
      "The 47166 th iteration gives loss of 0.134735656760441\n",
      "The 47167 th iteration gives loss of 0.1347344686186819\n",
      "The 47168 th iteration gives loss of 0.13473328049479527\n",
      "The 47169 th iteration gives loss of 0.1347320923888125\n",
      "The 47170 th iteration gives loss of 0.13473090430071152\n",
      "The 47171 th iteration gives loss of 0.13472971623049984\n",
      "The 47172 th iteration gives loss of 0.13472852817816136\n",
      "The 47173 th iteration gives loss of 0.1347273401437062\n",
      "The 47174 th iteration gives loss of 0.13472615212713115\n",
      "The 47175 th iteration gives loss of 0.13472496412844057\n",
      "The 47176 th iteration gives loss of 0.1347237761476287\n",
      "The 47177 th iteration gives loss of 0.13472258818469304\n",
      "The 47178 th iteration gives loss of 0.13472140023963808\n",
      "The 47179 th iteration gives loss of 0.13472021231245165\n",
      "The 47180 th iteration gives loss of 0.13471902440313363\n",
      "The 47181 th iteration gives loss of 0.13471783651169236\n",
      "The 47182 th iteration gives loss of 0.13471664863812524\n",
      "The 47183 th iteration gives loss of 0.13471546078243055\n",
      "The 47184 th iteration gives loss of 0.13471427294460822\n",
      "The 47185 th iteration gives loss of 0.13471308512464883\n",
      "The 47186 th iteration gives loss of 0.1347118973225516\n",
      "The 47187 th iteration gives loss of 0.13471070953832343\n",
      "The 47188 th iteration gives loss of 0.134709521771956\n",
      "The 47189 th iteration gives loss of 0.13470833402345597\n",
      "The 47190 th iteration gives loss of 0.13470714629281805\n",
      "The 47191 th iteration gives loss of 0.13470595858004064\n",
      "The 47192 th iteration gives loss of 0.13470477088512134\n",
      "The 47193 th iteration gives loss of 0.1347035832080656\n",
      "The 47194 th iteration gives loss of 0.13470239554886299\n",
      "The 47195 th iteration gives loss of 0.13470120790750972\n",
      "The 47196 th iteration gives loss of 0.13470002028401779\n",
      "The 47197 th iteration gives loss of 0.13469883267838018\n",
      "The 47198 th iteration gives loss of 0.13469764509059598\n",
      "The 47199 th iteration gives loss of 0.1346964575206684\n",
      "The 47200 th iteration gives loss of 0.1346952699685818\n",
      "The 47201 th iteration gives loss of 0.13469408243435185\n",
      "The 47202 th iteration gives loss of 0.13469289491796246\n",
      "The 47203 th iteration gives loss of 0.13469170741942565\n",
      "The 47204 th iteration gives loss of 0.1346905199387237\n",
      "The 47205 th iteration gives loss of 0.13468933247588252\n",
      "The 47206 th iteration gives loss of 0.13468814503087634\n",
      "The 47207 th iteration gives loss of 0.13468695760370514\n",
      "The 47208 th iteration gives loss of 0.13468577019438413\n",
      "The 47209 th iteration gives loss of 0.1346845828029019\n",
      "The 47210 th iteration gives loss of 0.1346833954292547\n",
      "The 47211 th iteration gives loss of 0.13468220807344777\n",
      "The 47212 th iteration gives loss of 0.13468102073547525\n",
      "The 47213 th iteration gives loss of 0.1346798334153374\n",
      "The 47214 th iteration gives loss of 0.13467864611303293\n",
      "The 47215 th iteration gives loss of 0.13467745882855464\n",
      "The 47216 th iteration gives loss of 0.1346762715619194\n",
      "The 47217 th iteration gives loss of 0.13467508431311256\n",
      "The 47218 th iteration gives loss of 0.13467389708212474\n",
      "The 47219 th iteration gives loss of 0.13467270986897875\n",
      "The 47220 th iteration gives loss of 0.13467152267365637\n",
      "The 47221 th iteration gives loss of 0.13467033549615548\n",
      "The 47222 th iteration gives loss of 0.13466914833647775\n",
      "The 47223 th iteration gives loss of 0.13466796119462607\n",
      "The 47224 th iteration gives loss of 0.13466677407059452\n",
      "The 47225 th iteration gives loss of 0.13466558696438702\n",
      "The 47226 th iteration gives loss of 0.13466439987599432\n",
      "The 47227 th iteration gives loss of 0.13466321280541993\n",
      "The 47228 th iteration gives loss of 0.1346620257526623\n",
      "The 47229 th iteration gives loss of 0.13466083871772955\n",
      "The 47230 th iteration gives loss of 0.13465965170060665\n",
      "The 47231 th iteration gives loss of 0.1346584647012957\n",
      "The 47232 th iteration gives loss of 0.13465727771980066\n",
      "The 47233 th iteration gives loss of 0.1346560907561128\n",
      "The 47234 th iteration gives loss of 0.13465490381023407\n",
      "The 47235 th iteration gives loss of 0.13465371688217465\n",
      "The 47236 th iteration gives loss of 0.1346525299719137\n",
      "The 47237 th iteration gives loss of 0.13465134307946322\n",
      "The 47238 th iteration gives loss of 0.13465015620482076\n",
      "The 47239 th iteration gives loss of 0.13464896934797776\n",
      "The 47240 th iteration gives loss of 0.13464778250894074\n",
      "The 47241 th iteration gives loss of 0.13464659568770834\n",
      "The 47242 th iteration gives loss of 0.13464540888426957\n",
      "The 47243 th iteration gives loss of 0.1346442220986257\n",
      "The 47244 th iteration gives loss of 0.13464303533079652\n",
      "The 47245 th iteration gives loss of 0.13464184858076025\n",
      "The 47246 th iteration gives loss of 0.13464066184852236\n",
      "The 47247 th iteration gives loss of 0.13463947513407445\n",
      "The 47248 th iteration gives loss of 0.13463828843742065\n",
      "The 47249 th iteration gives loss of 0.13463710175856192\n",
      "The 47250 th iteration gives loss of 0.13463591509749798\n",
      "The 47251 th iteration gives loss of 0.13463472845421598\n",
      "The 47252 th iteration gives loss of 0.13463354182872952\n",
      "The 47253 th iteration gives loss of 0.13463235522102812\n",
      "The 47254 th iteration gives loss of 0.13463116863110833\n",
      "The 47255 th iteration gives loss of 0.1346299820589799\n",
      "The 47256 th iteration gives loss of 0.13462879550464138\n",
      "The 47257 th iteration gives loss of 0.13462760896807818\n",
      "The 47258 th iteration gives loss of 0.1346264224493036\n",
      "The 47259 th iteration gives loss of 0.1346252359483086\n",
      "The 47260 th iteration gives loss of 0.13462404946508685\n",
      "The 47261 th iteration gives loss of 0.13462286299965198\n",
      "The 47262 th iteration gives loss of 0.13462167655199453\n",
      "The 47263 th iteration gives loss of 0.13462049012211216\n",
      "The 47264 th iteration gives loss of 0.13461930371000094\n",
      "The 47265 th iteration gives loss of 0.13461811731567067\n",
      "The 47266 th iteration gives loss of 0.1346169309391015\n",
      "The 47267 th iteration gives loss of 0.13461574458031997\n",
      "The 47268 th iteration gives loss of 0.13461455823929255\n",
      "The 47269 th iteration gives loss of 0.13461337191604733\n",
      "The 47270 th iteration gives loss of 0.13461218561057234\n",
      "The 47271 th iteration gives loss of 0.13461099932285295\n",
      "The 47272 th iteration gives loss of 0.13460981305289876\n",
      "The 47273 th iteration gives loss of 0.13460862680072602\n",
      "The 47274 th iteration gives loss of 0.13460744056629648\n",
      "The 47275 th iteration gives loss of 0.13460625434964416\n",
      "The 47276 th iteration gives loss of 0.13460506815074882\n",
      "The 47277 th iteration gives loss of 0.13460388196961615\n",
      "The 47278 th iteration gives loss of 0.1346026958062373\n",
      "The 47279 th iteration gives loss of 0.13460150966062176\n",
      "The 47280 th iteration gives loss of 0.13460032353275725\n",
      "The 47281 th iteration gives loss of 0.13459913742265422\n",
      "The 47282 th iteration gives loss of 0.13459795133030109\n",
      "The 47283 th iteration gives loss of 0.13459676525570302\n",
      "The 47284 th iteration gives loss of 0.13459557919885762\n",
      "The 47285 th iteration gives loss of 0.1345943931597586\n",
      "The 47286 th iteration gives loss of 0.1345932071384151\n",
      "The 47287 th iteration gives loss of 0.13459202113481714\n",
      "The 47288 th iteration gives loss of 0.13459083514896622\n",
      "The 47289 th iteration gives loss of 0.1345896491808651\n",
      "The 47290 th iteration gives loss of 0.1345884632305076\n",
      "The 47291 th iteration gives loss of 0.13458727729788889\n",
      "The 47292 th iteration gives loss of 0.13458609138302277\n",
      "The 47293 th iteration gives loss of 0.1345849054858924\n",
      "The 47294 th iteration gives loss of 0.1345837196065006\n",
      "The 47295 th iteration gives loss of 0.1345825337448501\n",
      "The 47296 th iteration gives loss of 0.1345813479009406\n",
      "The 47297 th iteration gives loss of 0.13458016207476525\n",
      "The 47298 th iteration gives loss of 0.1345789762663182\n",
      "The 47299 th iteration gives loss of 0.1345777904756128\n",
      "The 47300 th iteration gives loss of 0.13457660470264693\n",
      "The 47301 th iteration gives loss of 0.1345754189474058\n",
      "The 47302 th iteration gives loss of 0.13457423320989867\n",
      "The 47303 th iteration gives loss of 0.13457304749012594\n",
      "The 47304 th iteration gives loss of 0.13457186178807182\n",
      "The 47305 th iteration gives loss of 0.13457067610374884\n",
      "The 47306 th iteration gives loss of 0.13456949043715963\n",
      "The 47307 th iteration gives loss of 0.13456830478828813\n",
      "The 47308 th iteration gives loss of 0.13456711915713856\n",
      "The 47309 th iteration gives loss of 0.13456593354372087\n",
      "The 47310 th iteration gives loss of 0.13456474794802123\n",
      "The 47311 th iteration gives loss of 0.13456356237004052\n",
      "The 47312 th iteration gives loss of 0.13456237680978003\n",
      "The 47313 th iteration gives loss of 0.13456119126724495\n",
      "The 47314 th iteration gives loss of 0.13456000574241764\n",
      "The 47315 th iteration gives loss of 0.13455882023530458\n",
      "The 47316 th iteration gives loss of 0.13455763474590807\n",
      "The 47317 th iteration gives loss of 0.1345564492742372\n",
      "The 47318 th iteration gives loss of 0.13455526382026584\n",
      "The 47319 th iteration gives loss of 0.13455407838401584\n",
      "The 47320 th iteration gives loss of 0.1345528929654689\n",
      "The 47321 th iteration gives loss of 0.13455170756463516\n",
      "The 47322 th iteration gives loss of 0.13455052218150307\n",
      "The 47323 th iteration gives loss of 0.13454933681608597\n",
      "The 47324 th iteration gives loss of 0.13454815146836765\n",
      "The 47325 th iteration gives loss of 0.13454696613835979\n",
      "The 47326 th iteration gives loss of 0.13454578082605875\n",
      "The 47327 th iteration gives loss of 0.13454459553144674\n",
      "The 47328 th iteration gives loss of 0.13454341025455052\n",
      "The 47329 th iteration gives loss of 0.1345422249953441\n",
      "The 47330 th iteration gives loss of 0.1345410397538434\n",
      "The 47331 th iteration gives loss of 0.13453985453003106\n",
      "The 47332 th iteration gives loss of 0.13453866932392014\n",
      "The 47333 th iteration gives loss of 0.13453748413550604\n",
      "The 47334 th iteration gives loss of 0.1345362989647863\n",
      "The 47335 th iteration gives loss of 0.13453511381175412\n",
      "The 47336 th iteration gives loss of 0.13453392867642125\n",
      "The 47337 th iteration gives loss of 0.1345327435587741\n",
      "The 47338 th iteration gives loss of 0.13453155845882464\n",
      "The 47339 th iteration gives loss of 0.13453037337655574\n",
      "The 47340 th iteration gives loss of 0.13452918831196778\n",
      "The 47341 th iteration gives loss of 0.1345280032650789\n",
      "The 47342 th iteration gives loss of 0.13452681823587426\n",
      "The 47343 th iteration gives loss of 0.13452563322434843\n",
      "The 47344 th iteration gives loss of 0.13452444823050602\n",
      "The 47345 th iteration gives loss of 0.13452326325434968\n",
      "The 47346 th iteration gives loss of 0.1345220782958672\n",
      "The 47347 th iteration gives loss of 0.1345208933550672\n",
      "The 47348 th iteration gives loss of 0.13451970843194516\n",
      "The 47349 th iteration gives loss of 0.13451852352650095\n",
      "The 47350 th iteration gives loss of 0.13451733863872598\n",
      "The 47351 th iteration gives loss of 0.13451615376863782\n",
      "The 47352 th iteration gives loss of 0.1345149689162194\n",
      "The 47353 th iteration gives loss of 0.13451378408146872\n",
      "The 47354 th iteration gives loss of 0.13451259926438597\n",
      "The 47355 th iteration gives loss of 0.1345114144649791\n",
      "The 47356 th iteration gives loss of 0.13451022968324247\n",
      "The 47357 th iteration gives loss of 0.13450904491916943\n",
      "The 47358 th iteration gives loss of 0.13450786017277028\n",
      "The 47359 th iteration gives loss of 0.13450667544403197\n",
      "The 47360 th iteration gives loss of 0.13450549073295578\n",
      "The 47361 th iteration gives loss of 0.1345043060395477\n",
      "The 47362 th iteration gives loss of 0.1345031213637971\n",
      "The 47363 th iteration gives loss of 0.1345019367057053\n",
      "The 47364 th iteration gives loss of 0.13450075206527232\n",
      "The 47365 th iteration gives loss of 0.13449956744250394\n",
      "The 47366 th iteration gives loss of 0.13449838283739152\n",
      "The 47367 th iteration gives loss of 0.1344971982499421\n",
      "The 47368 th iteration gives loss of 0.13449601368013453\n",
      "The 47369 th iteration gives loss of 0.1344948291279875\n",
      "The 47370 th iteration gives loss of 0.13449364459348978\n",
      "The 47371 th iteration gives loss of 0.13449246007665144\n",
      "The 47372 th iteration gives loss of 0.13449127557745916\n",
      "The 47373 th iteration gives loss of 0.13449009109591883\n",
      "The 47374 th iteration gives loss of 0.13448890663202137\n",
      "The 47375 th iteration gives loss of 0.13448772218577754\n",
      "The 47376 th iteration gives loss of 0.1344865377571697\n",
      "The 47377 th iteration gives loss of 0.13448535334621672\n",
      "The 47378 th iteration gives loss of 0.13448416895290108\n",
      "The 47379 th iteration gives loss of 0.13448298457723445\n",
      "The 47380 th iteration gives loss of 0.13448180021920875\n",
      "The 47381 th iteration gives loss of 0.13448061587881477\n",
      "The 47382 th iteration gives loss of 0.13447943155606829\n",
      "The 47383 th iteration gives loss of 0.1344782472509581\n",
      "The 47384 th iteration gives loss of 0.1344770629634832\n",
      "The 47385 th iteration gives loss of 0.13447587869364655\n",
      "The 47386 th iteration gives loss of 0.13447469444143828\n",
      "The 47387 th iteration gives loss of 0.13447351020687004\n",
      "The 47388 th iteration gives loss of 0.13447232598993097\n",
      "The 47389 th iteration gives loss of 0.13447114179061886\n",
      "The 47390 th iteration gives loss of 0.1344699576089488\n",
      "The 47391 th iteration gives loss of 0.13446877344489705\n",
      "The 47392 th iteration gives loss of 0.13446758929847605\n",
      "The 47393 th iteration gives loss of 0.13446640516968206\n",
      "The 47394 th iteration gives loss of 0.13446522105851863\n",
      "The 47395 th iteration gives loss of 0.13446403696497436\n",
      "The 47396 th iteration gives loss of 0.1344628528890565\n",
      "The 47397 th iteration gives loss of 0.13446166883074856\n",
      "The 47398 th iteration gives loss of 0.1344604847900768\n",
      "The 47399 th iteration gives loss of 0.13445930076701137\n",
      "The 47400 th iteration gives loss of 0.13445811676156935\n",
      "The 47401 th iteration gives loss of 0.13445693277374968\n",
      "The 47402 th iteration gives loss of 0.13445574880354152\n",
      "The 47403 th iteration gives loss of 0.13445456485095097\n",
      "The 47404 th iteration gives loss of 0.13445338091597175\n",
      "The 47405 th iteration gives loss of 0.13445219699861252\n",
      "The 47406 th iteration gives loss of 0.1344510130988523\n",
      "The 47407 th iteration gives loss of 0.13444982921671295\n",
      "The 47408 th iteration gives loss of 0.1344486453521818\n",
      "The 47409 th iteration gives loss of 0.13444746150525475\n",
      "The 47410 th iteration gives loss of 0.1344462776759403\n",
      "The 47411 th iteration gives loss of 0.13444509386423495\n",
      "The 47412 th iteration gives loss of 0.13444391007012593\n",
      "The 47413 th iteration gives loss of 0.13444272629362072\n",
      "The 47414 th iteration gives loss of 0.1344415425347239\n",
      "The 47415 th iteration gives loss of 0.13444035879342026\n",
      "The 47416 th iteration gives loss of 0.13443917506971947\n",
      "The 47417 th iteration gives loss of 0.13443799136362558\n",
      "The 47418 th iteration gives loss of 0.1344368076751248\n",
      "The 47419 th iteration gives loss of 0.13443562400421796\n",
      "The 47420 th iteration gives loss of 0.13443444035091368\n",
      "The 47421 th iteration gives loss of 0.13443325671520148\n",
      "The 47422 th iteration gives loss of 0.1344320730970853\n",
      "The 47423 th iteration gives loss of 0.13443088949655213\n",
      "The 47424 th iteration gives loss of 0.13442970591361714\n",
      "The 47425 th iteration gives loss of 0.13442852234826747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 47426 th iteration gives loss of 0.13442733880051916\n",
      "The 47427 th iteration gives loss of 0.13442615527034085\n",
      "The 47428 th iteration gives loss of 0.13442497175776472\n",
      "The 47429 th iteration gives loss of 0.134423788262763\n",
      "The 47430 th iteration gives loss of 0.13442260478534895\n",
      "The 47431 th iteration gives loss of 0.1344214213255167\n",
      "The 47432 th iteration gives loss of 0.13442023788327376\n",
      "The 47433 th iteration gives loss of 0.1344190544586062\n",
      "The 47434 th iteration gives loss of 0.13441787105152436\n",
      "The 47435 th iteration gives loss of 0.13441668766201295\n",
      "The 47436 th iteration gives loss of 0.1344155042900818\n",
      "The 47437 th iteration gives loss of 0.13441432093573924\n",
      "The 47438 th iteration gives loss of 0.13441313759896448\n",
      "The 47439 th iteration gives loss of 0.13441195427975997\n",
      "The 47440 th iteration gives loss of 0.13441077097812879\n",
      "The 47441 th iteration gives loss of 0.13440958769406855\n",
      "The 47442 th iteration gives loss of 0.13440840442757845\n",
      "The 47443 th iteration gives loss of 0.1344072211786629\n",
      "The 47444 th iteration gives loss of 0.1344060379473091\n",
      "The 47445 th iteration gives loss of 0.13440485473353575\n",
      "The 47446 th iteration gives loss of 0.1344036715373169\n",
      "The 47447 th iteration gives loss of 0.1344024883586676\n",
      "The 47448 th iteration gives loss of 0.13440130519757948\n",
      "The 47449 th iteration gives loss of 0.13440012205405527\n",
      "The 47450 th iteration gives loss of 0.1343989389280935\n",
      "The 47451 th iteration gives loss of 0.13439775581969318\n",
      "The 47452 th iteration gives loss of 0.13439657272885608\n",
      "The 47453 th iteration gives loss of 0.13439538965557177\n",
      "The 47454 th iteration gives loss of 0.134394206599848\n",
      "The 47455 th iteration gives loss of 0.1343930235616811\n",
      "The 47456 th iteration gives loss of 0.1343918405410658\n",
      "The 47457 th iteration gives loss of 0.13439065753799923\n",
      "The 47458 th iteration gives loss of 0.1343894745524953\n",
      "The 47459 th iteration gives loss of 0.13438829158453788\n",
      "The 47460 th iteration gives loss of 0.13438710863412973\n",
      "The 47461 th iteration gives loss of 0.13438592570127564\n",
      "The 47462 th iteration gives loss of 0.13438474278597018\n",
      "The 47463 th iteration gives loss of 0.1343835598882061\n",
      "The 47464 th iteration gives loss of 0.13438237700798908\n",
      "The 47465 th iteration gives loss of 0.13438119414531474\n",
      "The 47466 th iteration gives loss of 0.13438001130020025\n",
      "The 47467 th iteration gives loss of 0.13437882847260868\n",
      "The 47468 th iteration gives loss of 0.13437764566257102\n",
      "The 47469 th iteration gives loss of 0.13437646287006955\n",
      "The 47470 th iteration gives loss of 0.13437528009510655\n",
      "The 47471 th iteration gives loss of 0.13437409733768024\n",
      "The 47472 th iteration gives loss of 0.1343729145977902\n",
      "The 47473 th iteration gives loss of 0.13437173187543552\n",
      "The 47474 th iteration gives loss of 0.1343705491706183\n",
      "The 47475 th iteration gives loss of 0.13436936648334086\n",
      "The 47476 th iteration gives loss of 0.13436818381358803\n",
      "The 47477 th iteration gives loss of 0.1343670011613661\n",
      "The 47478 th iteration gives loss of 0.13436581852667356\n",
      "The 47479 th iteration gives loss of 0.13436463590951153\n",
      "The 47480 th iteration gives loss of 0.13436345330988214\n",
      "The 47481 th iteration gives loss of 0.13436227072777243\n",
      "The 47482 th iteration gives loss of 0.1343610881631961\n",
      "The 47483 th iteration gives loss of 0.13435990561613337\n",
      "The 47484 th iteration gives loss of 0.13435872308660024\n",
      "The 47485 th iteration gives loss of 0.1343575405745919\n",
      "The 47486 th iteration gives loss of 0.1343563580801065\n",
      "The 47487 th iteration gives loss of 0.13435517560313914\n",
      "The 47488 th iteration gives loss of 0.1343539931436862\n",
      "The 47489 th iteration gives loss of 0.13435281070174995\n",
      "The 47490 th iteration gives loss of 0.13435162827733851\n",
      "The 47491 th iteration gives loss of 0.13435044587043346\n",
      "The 47492 th iteration gives loss of 0.13434926348105586\n",
      "The 47493 th iteration gives loss of 0.13434808110917615\n",
      "The 47494 th iteration gives loss of 0.13434689875482103\n",
      "The 47495 th iteration gives loss of 0.13434571641797263\n",
      "The 47496 th iteration gives loss of 0.13434453409863636\n",
      "The 47497 th iteration gives loss of 0.13434335179680112\n",
      "The 47498 th iteration gives loss of 0.13434216951248104\n",
      "The 47499 th iteration gives loss of 0.13434098724566404\n",
      "The 47500 th iteration gives loss of 0.13433980499635625\n",
      "The 47501 th iteration gives loss of 0.13433862276454225\n",
      "The 47502 th iteration gives loss of 0.1343374405502446\n",
      "The 47503 th iteration gives loss of 0.13433625835343616\n",
      "The 47504 th iteration gives loss of 0.13433507617413315\n",
      "The 47505 th iteration gives loss of 0.1343338940123371\n",
      "The 47506 th iteration gives loss of 0.13433271186804166\n",
      "The 47507 th iteration gives loss of 0.13433152974122847\n",
      "The 47508 th iteration gives loss of 0.13433034763192034\n",
      "The 47509 th iteration gives loss of 0.1343291655401012\n",
      "The 47510 th iteration gives loss of 0.1343279834657898\n",
      "The 47511 th iteration gives loss of 0.13432680140896897\n",
      "The 47512 th iteration gives loss of 0.1343256193696366\n",
      "The 47513 th iteration gives loss of 0.1343244373477888\n",
      "The 47514 th iteration gives loss of 0.13432325534343884\n",
      "The 47515 th iteration gives loss of 0.13432207335657564\n",
      "The 47516 th iteration gives loss of 0.13432089138719663\n",
      "The 47517 th iteration gives loss of 0.13431970943531235\n",
      "The 47518 th iteration gives loss of 0.13431852750090442\n",
      "The 47519 th iteration gives loss of 0.1343173455839795\n",
      "The 47520 th iteration gives loss of 0.13431616368454452\n",
      "The 47521 th iteration gives loss of 0.13431498180258883\n",
      "The 47522 th iteration gives loss of 0.13431379993811637\n",
      "The 47523 th iteration gives loss of 0.13431261809111839\n",
      "The 47524 th iteration gives loss of 0.1343114362616087\n",
      "The 47525 th iteration gives loss of 0.13431025444956768\n",
      "The 47526 th iteration gives loss of 0.1343090726550009\n",
      "The 47527 th iteration gives loss of 0.1343078908779103\n",
      "The 47528 th iteration gives loss of 0.13430670911830014\n",
      "The 47529 th iteration gives loss of 0.13430552737614582\n",
      "The 47530 th iteration gives loss of 0.1343043456514901\n",
      "The 47531 th iteration gives loss of 0.13430316394428557\n",
      "The 47532 th iteration gives loss of 0.13430198225455806\n",
      "The 47533 th iteration gives loss of 0.13430080058229676\n",
      "The 47534 th iteration gives loss of 0.13429961892750228\n",
      "The 47535 th iteration gives loss of 0.13429843729017746\n",
      "The 47536 th iteration gives loss of 0.13429725567032297\n",
      "The 47537 th iteration gives loss of 0.13429607406792535\n",
      "The 47538 th iteration gives loss of 0.13429489248298912\n",
      "The 47539 th iteration gives loss of 0.13429371091551442\n",
      "The 47540 th iteration gives loss of 0.13429252936549735\n",
      "The 47541 th iteration gives loss of 0.13429134783294494\n",
      "The 47542 th iteration gives loss of 0.13429016631785398\n",
      "The 47543 th iteration gives loss of 0.13428898482021542\n",
      "The 47544 th iteration gives loss of 0.1342878033400346\n",
      "The 47545 th iteration gives loss of 0.13428662187730825\n",
      "The 47546 th iteration gives loss of 0.1342854404320409\n",
      "The 47547 th iteration gives loss of 0.13428425900422067\n",
      "The 47548 th iteration gives loss of 0.13428307759385127\n",
      "The 47549 th iteration gives loss of 0.13428189620094272\n",
      "The 47550 th iteration gives loss of 0.1342807148254691\n",
      "The 47551 th iteration gives loss of 0.13427953346745056\n",
      "The 47552 th iteration gives loss of 0.13427835212687908\n",
      "The 47553 th iteration gives loss of 0.1342771708037489\n",
      "The 47554 th iteration gives loss of 0.13427598949806704\n",
      "The 47555 th iteration gives loss of 0.134274808209835\n",
      "The 47556 th iteration gives loss of 0.1342736269390443\n",
      "The 47557 th iteration gives loss of 0.1342724456856948\n",
      "The 47558 th iteration gives loss of 0.13427126444977944\n",
      "The 47559 th iteration gives loss of 0.13427008323131237\n",
      "The 47560 th iteration gives loss of 0.1342689020302764\n",
      "The 47561 th iteration gives loss of 0.13426772084668234\n",
      "The 47562 th iteration gives loss of 0.13426653968052246\n",
      "The 47563 th iteration gives loss of 0.1342653585317925\n",
      "The 47564 th iteration gives loss of 0.13426417740050828\n",
      "The 47565 th iteration gives loss of 0.1342629962866515\n",
      "The 47566 th iteration gives loss of 0.13426181519022176\n",
      "The 47567 th iteration gives loss of 0.13426063411123287\n",
      "The 47568 th iteration gives loss of 0.13425945304966266\n",
      "The 47569 th iteration gives loss of 0.13425827200552362\n",
      "The 47570 th iteration gives loss of 0.1342570909788109\n",
      "The 47571 th iteration gives loss of 0.13425590996952597\n",
      "The 47572 th iteration gives loss of 0.1342547289776696\n",
      "The 47573 th iteration gives loss of 0.1342535480032387\n",
      "The 47574 th iteration gives loss of 0.13425236704622392\n",
      "The 47575 th iteration gives loss of 0.13425118610663653\n",
      "The 47576 th iteration gives loss of 0.13425000518446326\n",
      "The 47577 th iteration gives loss of 0.1342488242797161\n",
      "The 47578 th iteration gives loss of 0.13424764339238138\n",
      "The 47579 th iteration gives loss of 0.1342464625224693\n",
      "The 47580 th iteration gives loss of 0.13424528166997307\n",
      "The 47581 th iteration gives loss of 0.1342441008348868\n",
      "The 47582 th iteration gives loss of 0.13424292001721297\n",
      "The 47583 th iteration gives loss of 0.13424173921695776\n",
      "The 47584 th iteration gives loss of 0.13424055843411212\n",
      "The 47585 th iteration gives loss of 0.13423937766868452\n",
      "The 47586 th iteration gives loss of 0.13423819692065436\n",
      "The 47587 th iteration gives loss of 0.1342370161900362\n",
      "The 47588 th iteration gives loss of 0.13423583547683007\n",
      "The 47589 th iteration gives loss of 0.13423465478102642\n",
      "The 47590 th iteration gives loss of 0.1342334741026269\n",
      "The 47591 th iteration gives loss of 0.13423229344163257\n",
      "The 47592 th iteration gives loss of 0.13423111279803984\n",
      "The 47593 th iteration gives loss of 0.13422993217185897\n",
      "The 47594 th iteration gives loss of 0.13422875156306416\n",
      "The 47595 th iteration gives loss of 0.13422757097167315\n",
      "The 47596 th iteration gives loss of 0.13422639039768264\n",
      "The 47597 th iteration gives loss of 0.1342252098410876\n",
      "The 47598 th iteration gives loss of 0.13422402930189195\n",
      "The 47599 th iteration gives loss of 0.1342228487800942\n",
      "The 47600 th iteration gives loss of 0.1342216682756868\n",
      "The 47601 th iteration gives loss of 0.1342204877886705\n",
      "The 47602 th iteration gives loss of 0.13421930731905282\n",
      "The 47603 th iteration gives loss of 0.1342181268668196\n",
      "The 47604 th iteration gives loss of 0.13421694643197474\n",
      "The 47605 th iteration gives loss of 0.13421576601452262\n",
      "The 47606 th iteration gives loss of 0.13421458561445096\n",
      "The 47607 th iteration gives loss of 0.13421340523176795\n",
      "The 47608 th iteration gives loss of 0.13421222486647788\n",
      "The 47609 th iteration gives loss of 0.13421104451856336\n",
      "The 47610 th iteration gives loss of 0.13420986418803912\n",
      "The 47611 th iteration gives loss of 0.1342086838748919\n",
      "The 47612 th iteration gives loss of 0.13420750357912606\n",
      "The 47613 th iteration gives loss of 0.13420632330073548\n",
      "The 47614 th iteration gives loss of 0.13420514303973605\n",
      "The 47615 th iteration gives loss of 0.13420396279610203\n",
      "The 47616 th iteration gives loss of 0.13420278256984935\n",
      "The 47617 th iteration gives loss of 0.13420160236097384\n",
      "The 47618 th iteration gives loss of 0.1342004221694626\n",
      "The 47619 th iteration gives loss of 0.13419924199533573\n",
      "The 47620 th iteration gives loss of 0.13419806183857982\n",
      "The 47621 th iteration gives loss of 0.13419688169919042\n",
      "The 47622 th iteration gives loss of 0.13419570157716987\n",
      "The 47623 th iteration gives loss of 0.13419452147252137\n",
      "The 47624 th iteration gives loss of 0.1341933413852395\n",
      "The 47625 th iteration gives loss of 0.13419216131531683\n",
      "The 47626 th iteration gives loss of 0.13419098126276385\n",
      "The 47627 th iteration gives loss of 0.13418980122758084\n",
      "The 47628 th iteration gives loss of 0.13418862120977082\n",
      "The 47629 th iteration gives loss of 0.13418744120930423\n",
      "The 47630 th iteration gives loss of 0.13418626122620153\n",
      "The 47631 th iteration gives loss of 0.1341850812604715\n",
      "The 47632 th iteration gives loss of 0.134183901312084\n",
      "The 47633 th iteration gives loss of 0.13418272138106294\n",
      "The 47634 th iteration gives loss of 0.1341815414673932\n",
      "The 47635 th iteration gives loss of 0.13418036157107702\n",
      "The 47636 th iteration gives loss of 0.1341791816921236\n",
      "The 47637 th iteration gives loss of 0.13417800183052245\n",
      "The 47638 th iteration gives loss of 0.13417682198626982\n",
      "The 47639 th iteration gives loss of 0.13417564215937153\n",
      "The 47640 th iteration gives loss of 0.13417446234981772\n",
      "The 47641 th iteration gives loss of 0.13417328255761513\n",
      "The 47642 th iteration gives loss of 0.1341721027827649\n",
      "The 47643 th iteration gives loss of 0.13417092302525102\n",
      "The 47644 th iteration gives loss of 0.13416974328509182\n",
      "The 47645 th iteration gives loss of 0.1341685635622743\n",
      "The 47646 th iteration gives loss of 0.1341673838567984\n",
      "The 47647 th iteration gives loss of 0.13416620416866412\n",
      "The 47648 th iteration gives loss of 0.1341650244978785\n",
      "The 47649 th iteration gives loss of 0.13416384484443067\n",
      "The 47650 th iteration gives loss of 0.13416266520831405\n",
      "The 47651 th iteration gives loss of 0.13416148558953736\n",
      "The 47652 th iteration gives loss of 0.13416030598810116\n",
      "The 47653 th iteration gives loss of 0.13415912640399566\n",
      "The 47654 th iteration gives loss of 0.13415794683723434\n",
      "The 47655 th iteration gives loss of 0.13415676728780065\n",
      "The 47656 th iteration gives loss of 0.13415558775570227\n",
      "The 47657 th iteration gives loss of 0.13415440824092847\n",
      "The 47658 th iteration gives loss of 0.13415322874348462\n",
      "The 47659 th iteration gives loss of 0.13415204926337887\n",
      "The 47660 th iteration gives loss of 0.13415086980059954\n",
      "The 47661 th iteration gives loss of 0.1341496903551453\n",
      "The 47662 th iteration gives loss of 0.13414851092702612\n",
      "The 47663 th iteration gives loss of 0.1341473315162167\n",
      "The 47664 th iteration gives loss of 0.1341461521227322\n",
      "The 47665 th iteration gives loss of 0.13414497274657833\n",
      "The 47666 th iteration gives loss of 0.13414379338773852\n",
      "The 47667 th iteration gives loss of 0.13414261404622402\n",
      "The 47668 th iteration gives loss of 0.13414143472202292\n",
      "The 47669 th iteration gives loss of 0.13414025541514105\n",
      "The 47670 th iteration gives loss of 0.1341390761255793\n",
      "The 47671 th iteration gives loss of 0.13413789685333125\n",
      "The 47672 th iteration gives loss of 0.1341367175984032\n",
      "The 47673 th iteration gives loss of 0.13413553836078565\n",
      "The 47674 th iteration gives loss of 0.13413435914048066\n",
      "The 47675 th iteration gives loss of 0.13413317993748464\n",
      "The 47676 th iteration gives loss of 0.13413200075180098\n",
      "The 47677 th iteration gives loss of 0.13413082158342474\n",
      "The 47678 th iteration gives loss of 0.13412964243236183\n",
      "The 47679 th iteration gives loss of 0.13412846329860564\n",
      "The 47680 th iteration gives loss of 0.13412728418215403\n",
      "The 47681 th iteration gives loss of 0.1341261050830036\n",
      "The 47682 th iteration gives loss of 0.13412492600116196\n",
      "The 47683 th iteration gives loss of 0.13412374693662177\n",
      "The 47684 th iteration gives loss of 0.13412256788938523\n",
      "The 47685 th iteration gives loss of 0.13412138885945069\n",
      "The 47686 th iteration gives loss of 0.13412020984681325\n",
      "The 47687 th iteration gives loss of 0.13411903085147808\n",
      "The 47688 th iteration gives loss of 0.13411785187343409\n",
      "The 47689 th iteration gives loss of 0.13411667291268933\n",
      "The 47690 th iteration gives loss of 0.1341154939692376\n",
      "The 47691 th iteration gives loss of 0.13411431504308713\n",
      "The 47692 th iteration gives loss of 0.13411313613422207\n",
      "The 47693 th iteration gives loss of 0.13411195724265346\n",
      "The 47694 th iteration gives loss of 0.13411077836836696\n",
      "The 47695 th iteration gives loss of 0.1341095995113816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 47696 th iteration gives loss of 0.13410842067167958\n",
      "The 47697 th iteration gives loss of 0.13410724184926806\n",
      "The 47698 th iteration gives loss of 0.13410606304413933\n",
      "The 47699 th iteration gives loss of 0.13410488425629274\n",
      "The 47700 th iteration gives loss of 0.13410370548573783\n",
      "The 47701 th iteration gives loss of 0.134102526732463\n",
      "The 47702 th iteration gives loss of 0.13410134799647347\n",
      "The 47703 th iteration gives loss of 0.13410016927776086\n",
      "The 47704 th iteration gives loss of 0.13409899057633312\n",
      "The 47705 th iteration gives loss of 0.1340978118921813\n",
      "The 47706 th iteration gives loss of 0.13409663322530138\n",
      "The 47707 th iteration gives loss of 0.13409545457570937\n",
      "The 47708 th iteration gives loss of 0.1340942759433875\n",
      "The 47709 th iteration gives loss of 0.13409309732833866\n",
      "The 47710 th iteration gives loss of 0.13409191873056814\n",
      "The 47711 th iteration gives loss of 0.134090740150062\n",
      "The 47712 th iteration gives loss of 0.1340895615868305\n",
      "The 47713 th iteration gives loss of 0.1340883830408757\n",
      "The 47714 th iteration gives loss of 0.13408720451218065\n",
      "The 47715 th iteration gives loss of 0.13408602600075956\n",
      "The 47716 th iteration gives loss of 0.13408484750660063\n",
      "The 47717 th iteration gives loss of 0.13408366902971142\n",
      "The 47718 th iteration gives loss of 0.13408249057008215\n",
      "The 47719 th iteration gives loss of 0.13408131212771837\n",
      "The 47720 th iteration gives loss of 0.13408013370262079\n",
      "The 47721 th iteration gives loss of 0.1340789552947875\n",
      "The 47722 th iteration gives loss of 0.13407777690421122\n",
      "The 47723 th iteration gives loss of 0.13407659853089432\n",
      "The 47724 th iteration gives loss of 0.13407542017483398\n",
      "The 47725 th iteration gives loss of 0.13407424183603162\n",
      "The 47726 th iteration gives loss of 0.13407306351449014\n",
      "The 47727 th iteration gives loss of 0.13407188521019528\n",
      "The 47728 th iteration gives loss of 0.13407070692315803\n",
      "The 47729 th iteration gives loss of 0.13406952865337307\n",
      "The 47730 th iteration gives loss of 0.13406835040083953\n",
      "The 47731 th iteration gives loss of 0.13406717216555697\n",
      "The 47732 th iteration gives loss of 0.1340659939475301\n",
      "The 47733 th iteration gives loss of 0.13406481574674606\n",
      "The 47734 th iteration gives loss of 0.13406363756320666\n",
      "The 47735 th iteration gives loss of 0.13406245939691705\n",
      "The 47736 th iteration gives loss of 0.1340612812478794\n",
      "The 47737 th iteration gives loss of 0.1340601031160792\n",
      "The 47738 th iteration gives loss of 0.13405892500152167\n",
      "The 47739 th iteration gives loss of 0.13405774690420175\n",
      "The 47740 th iteration gives loss of 0.1340565688241237\n",
      "The 47741 th iteration gives loss of 0.13405539076129563\n",
      "The 47742 th iteration gives loss of 0.13405421271570273\n",
      "The 47743 th iteration gives loss of 0.13405303468734694\n",
      "The 47744 th iteration gives loss of 0.1340518566762219\n",
      "The 47745 th iteration gives loss of 0.13405067868233536\n",
      "The 47746 th iteration gives loss of 0.13404950070568675\n",
      "The 47747 th iteration gives loss of 0.13404832274626377\n",
      "The 47748 th iteration gives loss of 0.13404714480407864\n",
      "The 47749 th iteration gives loss of 0.13404596687912257\n",
      "The 47750 th iteration gives loss of 0.13404478897140185\n",
      "The 47751 th iteration gives loss of 0.13404361108091056\n",
      "The 47752 th iteration gives loss of 0.1340424332076401\n",
      "The 47753 th iteration gives loss of 0.13404125535160374\n",
      "The 47754 th iteration gives loss of 0.13404007751278965\n",
      "The 47755 th iteration gives loss of 0.13403889969120178\n",
      "The 47756 th iteration gives loss of 0.1340377218868349\n",
      "The 47757 th iteration gives loss of 0.1340365440996909\n",
      "The 47758 th iteration gives loss of 0.13403536632976315\n",
      "The 47759 th iteration gives loss of 0.13403418857706859\n",
      "The 47760 th iteration gives loss of 0.13403301084158795\n",
      "The 47761 th iteration gives loss of 0.13403183312332273\n",
      "The 47762 th iteration gives loss of 0.13403065542227724\n",
      "The 47763 th iteration gives loss of 0.13402947773844567\n",
      "The 47764 th iteration gives loss of 0.13402830007183064\n",
      "The 47765 th iteration gives loss of 0.13402712242242967\n",
      "The 47766 th iteration gives loss of 0.13402594479023525\n",
      "The 47767 th iteration gives loss of 0.13402476717526315\n",
      "The 47768 th iteration gives loss of 0.13402358957749308\n",
      "The 47769 th iteration gives loss of 0.13402241199693796\n",
      "The 47770 th iteration gives loss of 0.13402123443358832\n",
      "The 47771 th iteration gives loss of 0.13402005688744603\n",
      "The 47772 th iteration gives loss of 0.13401887935851942\n",
      "The 47773 th iteration gives loss of 0.13401770184678763\n",
      "The 47774 th iteration gives loss of 0.13401652435226213\n",
      "The 47775 th iteration gives loss of 0.13401534687494449\n",
      "The 47776 th iteration gives loss of 0.13401416941482236\n",
      "The 47777 th iteration gives loss of 0.13401299197191024\n",
      "The 47778 th iteration gives loss of 0.1340118145461917\n",
      "The 47779 th iteration gives loss of 0.1340106371376721\n",
      "The 47780 th iteration gives loss of 0.13400945974635173\n",
      "The 47781 th iteration gives loss of 0.13400828237222137\n",
      "The 47782 th iteration gives loss of 0.13400710501529653\n",
      "The 47783 th iteration gives loss of 0.13400592767556374\n",
      "The 47784 th iteration gives loss of 0.13400475035302375\n",
      "The 47785 th iteration gives loss of 0.13400357304766805\n",
      "The 47786 th iteration gives loss of 0.1340023957595101\n",
      "The 47787 th iteration gives loss of 0.13400121848854718\n",
      "The 47788 th iteration gives loss of 0.13400004123477374\n",
      "The 47789 th iteration gives loss of 0.1339988639981858\n",
      "The 47790 th iteration gives loss of 0.13399768677878193\n",
      "The 47791 th iteration gives loss of 0.13399650957656767\n",
      "The 47792 th iteration gives loss of 0.13399533239153402\n",
      "The 47793 th iteration gives loss of 0.13399415522368793\n",
      "The 47794 th iteration gives loss of 0.13399297807302032\n",
      "The 47795 th iteration gives loss of 0.13399180093953994\n",
      "The 47796 th iteration gives loss of 0.13399062382323615\n",
      "The 47797 th iteration gives loss of 0.13398944672411478\n",
      "The 47798 th iteration gives loss of 0.1339882696421698\n",
      "The 47799 th iteration gives loss of 0.13398709257741134\n",
      "The 47800 th iteration gives loss of 0.13398591552981687\n",
      "The 47801 th iteration gives loss of 0.13398473849940423\n",
      "The 47802 th iteration gives loss of 0.13398356148615956\n",
      "The 47803 th iteration gives loss of 0.13398238449009453\n",
      "The 47804 th iteration gives loss of 0.13398120751120124\n",
      "The 47805 th iteration gives loss of 0.13398003054947133\n",
      "The 47806 th iteration gives loss of 0.13397885360491757\n",
      "The 47807 th iteration gives loss of 0.13397767667752924\n",
      "The 47808 th iteration gives loss of 0.13397649976731574\n",
      "The 47809 th iteration gives loss of 0.13397532287426248\n",
      "The 47810 th iteration gives loss of 0.1339741459983777\n",
      "The 47811 th iteration gives loss of 0.13397296913965592\n",
      "The 47812 th iteration gives loss of 0.1339717922981047\n",
      "The 47813 th iteration gives loss of 0.133970615473704\n",
      "The 47814 th iteration gives loss of 0.1339694386664764\n",
      "The 47815 th iteration gives loss of 0.13396826187639127\n",
      "The 47816 th iteration gives loss of 0.13396708510348046\n",
      "The 47817 th iteration gives loss of 0.1339659083477231\n",
      "The 47818 th iteration gives loss of 0.13396473160912825\n",
      "The 47819 th iteration gives loss of 0.1339635548876888\n",
      "The 47820 th iteration gives loss of 0.13396237818339876\n",
      "The 47821 th iteration gives loss of 0.1339612014962672\n",
      "The 47822 th iteration gives loss of 0.13396002482628347\n",
      "The 47823 th iteration gives loss of 0.13395884817344816\n",
      "The 47824 th iteration gives loss of 0.1339576715377713\n",
      "The 47825 th iteration gives loss of 0.1339564949192428\n",
      "The 47826 th iteration gives loss of 0.13395531831786053\n",
      "The 47827 th iteration gives loss of 0.13395414173362974\n",
      "The 47828 th iteration gives loss of 0.13395296516654165\n",
      "The 47829 th iteration gives loss of 0.13395178861659804\n",
      "The 47830 th iteration gives loss of 0.1339506120838067\n",
      "The 47831 th iteration gives loss of 0.13394943556814948\n",
      "The 47832 th iteration gives loss of 0.13394825906963953\n",
      "The 47833 th iteration gives loss of 0.13394708258827223\n",
      "The 47834 th iteration gives loss of 0.13394590612403318\n",
      "The 47835 th iteration gives loss of 0.13394472967694815\n",
      "The 47836 th iteration gives loss of 0.13394355324699386\n",
      "The 47837 th iteration gives loss of 0.13394237683417312\n",
      "The 47838 th iteration gives loss of 0.13394120043848873\n",
      "The 47839 th iteration gives loss of 0.13394002405994088\n",
      "The 47840 th iteration gives loss of 0.13393884769853573\n",
      "The 47841 th iteration gives loss of 0.13393767135425297\n",
      "The 47842 th iteration gives loss of 0.13393649502710356\n",
      "The 47843 th iteration gives loss of 0.13393531871708647\n",
      "The 47844 th iteration gives loss of 0.13393414242419732\n",
      "The 47845 th iteration gives loss of 0.13393296614843603\n",
      "The 47846 th iteration gives loss of 0.13393178988980098\n",
      "The 47847 th iteration gives loss of 0.1339306136482962\n",
      "The 47848 th iteration gives loss of 0.13392943742391514\n",
      "The 47849 th iteration gives loss of 0.13392826121665471\n",
      "The 47850 th iteration gives loss of 0.1339270850265183\n",
      "The 47851 th iteration gives loss of 0.13392590885350875\n",
      "The 47852 th iteration gives loss of 0.13392473269761856\n",
      "The 47853 th iteration gives loss of 0.13392355655883964\n",
      "The 47854 th iteration gives loss of 0.13392238043718505\n",
      "The 47855 th iteration gives loss of 0.13392120433264876\n",
      "The 47856 th iteration gives loss of 0.1339200282452312\n",
      "The 47857 th iteration gives loss of 0.13391885217492944\n",
      "The 47858 th iteration gives loss of 0.13391767612174063\n",
      "The 47859 th iteration gives loss of 0.13391650008566397\n",
      "The 47860 th iteration gives loss of 0.13391532406670237\n",
      "The 47861 th iteration gives loss of 0.1339141480648475\n",
      "The 47862 th iteration gives loss of 0.1339129720800996\n",
      "The 47863 th iteration gives loss of 0.13391179611246795\n",
      "The 47864 th iteration gives loss of 0.1339106201619462\n",
      "The 47865 th iteration gives loss of 0.13390944422852646\n",
      "The 47866 th iteration gives loss of 0.13390826831221514\n",
      "The 47867 th iteration gives loss of 0.1339070924130053\n",
      "The 47868 th iteration gives loss of 0.13390591653089637\n",
      "The 47869 th iteration gives loss of 0.13390474066590932\n",
      "The 47870 th iteration gives loss of 0.13390356481800675\n",
      "The 47871 th iteration gives loss of 0.1339023889872091\n",
      "The 47872 th iteration gives loss of 0.13390121317350956\n",
      "The 47873 th iteration gives loss of 0.13390003737690845\n",
      "The 47874 th iteration gives loss of 0.13389886159740352\n",
      "The 47875 th iteration gives loss of 0.13389768583500336\n",
      "The 47876 th iteration gives loss of 0.13389651008968548\n",
      "The 47877 th iteration gives loss of 0.13389533436147455\n",
      "The 47878 th iteration gives loss of 0.13389415865035187\n",
      "The 47879 th iteration gives loss of 0.1338929829563192\n",
      "The 47880 th iteration gives loss of 0.13389180727938232\n",
      "The 47881 th iteration gives loss of 0.13389063161953174\n",
      "The 47882 th iteration gives loss of 0.13388945597677634\n",
      "The 47883 th iteration gives loss of 0.13388828035109815\n",
      "The 47884 th iteration gives loss of 0.13388710474251747\n",
      "The 47885 th iteration gives loss of 0.13388592915101835\n",
      "The 47886 th iteration gives loss of 0.1338847535766023\n",
      "The 47887 th iteration gives loss of 0.13388357801927656\n",
      "The 47888 th iteration gives loss of 0.13388240247902936\n",
      "The 47889 th iteration gives loss of 0.1338812269558576\n",
      "The 47890 th iteration gives loss of 0.13388005144976725\n",
      "The 47891 th iteration gives loss of 0.13387887596076475\n",
      "The 47892 th iteration gives loss of 0.1338777004888371\n",
      "The 47893 th iteration gives loss of 0.13387652503398298\n",
      "The 47894 th iteration gives loss of 0.13387534959621056\n",
      "The 47895 th iteration gives loss of 0.1338741741755078\n",
      "The 47896 th iteration gives loss of 0.13387299877188394\n",
      "The 47897 th iteration gives loss of 0.1338718233853273\n",
      "The 47898 th iteration gives loss of 0.13387064801585344\n",
      "The 47899 th iteration gives loss of 0.13386947266344684\n",
      "The 47900 th iteration gives loss of 0.1338682973281076\n",
      "The 47901 th iteration gives loss of 0.13386712200983478\n",
      "The 47902 th iteration gives loss of 0.13386594670863722\n",
      "The 47903 th iteration gives loss of 0.13386477142450498\n",
      "The 47904 th iteration gives loss of 0.13386359615743632\n",
      "The 47905 th iteration gives loss of 0.13386242090743158\n",
      "The 47906 th iteration gives loss of 0.1338612456744896\n",
      "The 47907 th iteration gives loss of 0.13386007045861661\n",
      "The 47908 th iteration gives loss of 0.13385889525979872\n",
      "The 47909 th iteration gives loss of 0.13385772007804206\n",
      "The 47910 th iteration gives loss of 0.13385654491335122\n",
      "The 47911 th iteration gives loss of 0.1338553697657084\n",
      "The 47912 th iteration gives loss of 0.1338541946351251\n",
      "The 47913 th iteration gives loss of 0.13385301952159778\n",
      "The 47914 th iteration gives loss of 0.13385184442512887\n",
      "The 47915 th iteration gives loss of 0.13385066934571854\n",
      "The 47916 th iteration gives loss of 0.13384949428335202\n",
      "The 47917 th iteration gives loss of 0.1338483192380458\n",
      "The 47918 th iteration gives loss of 0.13384714420978683\n",
      "The 47919 th iteration gives loss of 0.1338459691985756\n",
      "The 47920 th iteration gives loss of 0.13384479420441583\n",
      "The 47921 th iteration gives loss of 0.13384361922731203\n",
      "The 47922 th iteration gives loss of 0.13384244426724295\n",
      "The 47923 th iteration gives loss of 0.1338412693242273\n",
      "The 47924 th iteration gives loss of 0.13384009439825117\n",
      "The 47925 th iteration gives loss of 0.13383891948932208\n",
      "The 47926 th iteration gives loss of 0.13383774459743844\n",
      "The 47927 th iteration gives loss of 0.13383656972258764\n",
      "The 47928 th iteration gives loss of 0.13383539486478102\n",
      "The 47929 th iteration gives loss of 0.13383422002401668\n",
      "The 47930 th iteration gives loss of 0.13383304520029035\n",
      "The 47931 th iteration gives loss of 0.1338318703936012\n",
      "The 47932 th iteration gives loss of 0.1338306956039444\n",
      "The 47933 th iteration gives loss of 0.1338295208313188\n",
      "The 47934 th iteration gives loss of 0.13382834607574245\n",
      "The 47935 th iteration gives loss of 0.13382717133719044\n",
      "The 47936 th iteration gives loss of 0.1338259966156745\n",
      "The 47937 th iteration gives loss of 0.13382482191118397\n",
      "The 47938 th iteration gives loss of 0.1338236472237263\n",
      "The 47939 th iteration gives loss of 0.13382247255329383\n",
      "The 47940 th iteration gives loss of 0.13382129789989597\n",
      "The 47941 th iteration gives loss of 0.1338201232635223\n",
      "The 47942 th iteration gives loss of 0.13381894864417826\n",
      "The 47943 th iteration gives loss of 0.13381777404185055\n",
      "The 47944 th iteration gives loss of 0.13381659945655683\n",
      "The 47945 th iteration gives loss of 0.1338154248882723\n",
      "The 47946 th iteration gives loss of 0.13381425033702124\n",
      "The 47947 th iteration gives loss of 0.1338130758027824\n",
      "The 47948 th iteration gives loss of 0.13381190128556938\n",
      "The 47949 th iteration gives loss of 0.1338107267853708\n",
      "The 47950 th iteration gives loss of 0.1338095523021972\n",
      "The 47951 th iteration gives loss of 0.13380837783602914\n",
      "The 47952 th iteration gives loss of 0.13380720338688512\n",
      "The 47953 th iteration gives loss of 0.1338060289547427\n",
      "The 47954 th iteration gives loss of 0.13380485453962926\n",
      "The 47955 th iteration gives loss of 0.13380368014152483\n",
      "The 47956 th iteration gives loss of 0.13380250576043232\n",
      "The 47957 th iteration gives loss of 0.13380133139633993\n",
      "The 47958 th iteration gives loss of 0.13380015704926848\n",
      "The 47959 th iteration gives loss of 0.1337989827192017\n",
      "The 47960 th iteration gives loss of 0.13379780840613845\n",
      "The 47961 th iteration gives loss of 0.13379663411007559\n",
      "The 47962 th iteration gives loss of 0.13379545983103036\n",
      "The 47963 th iteration gives loss of 0.13379428556898068\n",
      "The 47964 th iteration gives loss of 0.1337931113239347\n",
      "The 47965 th iteration gives loss of 0.13379193709589318\n",
      "The 47966 th iteration gives loss of 0.1337907628848533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 47967 th iteration gives loss of 0.1337895886908087\n",
      "The 47968 th iteration gives loss of 0.13378841451376544\n",
      "The 47969 th iteration gives loss of 0.13378724035372339\n",
      "The 47970 th iteration gives loss of 0.1337860662106719\n",
      "The 47971 th iteration gives loss of 0.13378489208461639\n",
      "The 47972 th iteration gives loss of 0.13378371797556218\n",
      "The 47973 th iteration gives loss of 0.13378254388348562\n",
      "The 47974 th iteration gives loss of 0.1337813698084247\n",
      "The 47975 th iteration gives loss of 0.1337801957503418\n",
      "The 47976 th iteration gives loss of 0.1337790217092469\n",
      "The 47977 th iteration gives loss of 0.13377784768514359\n",
      "The 47978 th iteration gives loss of 0.1337766736780308\n",
      "The 47979 th iteration gives loss of 0.13377549968790184\n",
      "The 47980 th iteration gives loss of 0.13377432571476208\n",
      "The 47981 th iteration gives loss of 0.13377315175860205\n",
      "The 47982 th iteration gives loss of 0.133771977819433\n",
      "The 47983 th iteration gives loss of 0.13377080389723908\n",
      "The 47984 th iteration gives loss of 0.13376962999203343\n",
      "The 47985 th iteration gives loss of 0.13376845610380825\n",
      "The 47986 th iteration gives loss of 0.13376728223256032\n",
      "The 47987 th iteration gives loss of 0.13376610837829123\n",
      "The 47988 th iteration gives loss of 0.1337649345410106\n",
      "The 47989 th iteration gives loss of 0.13376376072069607\n",
      "The 47990 th iteration gives loss of 0.13376258691735993\n",
      "The 47991 th iteration gives loss of 0.13376141313099799\n",
      "The 47992 th iteration gives loss of 0.1337602393616093\n",
      "The 47993 th iteration gives loss of 0.13375906560919532\n",
      "The 47994 th iteration gives loss of 0.13375789187374915\n",
      "The 47995 th iteration gives loss of 0.1337567181552676\n",
      "The 47996 th iteration gives loss of 0.133755544453764\n",
      "The 47997 th iteration gives loss of 0.133754370769237\n",
      "The 47998 th iteration gives loss of 0.1337531971016667\n",
      "The 47999 th iteration gives loss of 0.13375202345105994\n",
      "The 48000 th iteration gives loss of 0.13375084981742275\n",
      "The 48001 th iteration gives loss of 0.13374967620074987\n",
      "The 48002 th iteration gives loss of 0.13374850260104465\n",
      "The 48003 th iteration gives loss of 0.1337473290182936\n",
      "The 48004 th iteration gives loss of 0.13374615545251062\n",
      "The 48005 th iteration gives loss of 0.13374498190368084\n",
      "The 48006 th iteration gives loss of 0.13374380837181957\n",
      "The 48007 th iteration gives loss of 0.13374263485690474\n",
      "The 48008 th iteration gives loss of 0.1337414613589578\n",
      "The 48009 th iteration gives loss of 0.133740287877956\n",
      "The 48010 th iteration gives loss of 0.13373911441391578\n",
      "The 48011 th iteration gives loss of 0.13373794096683705\n",
      "The 48012 th iteration gives loss of 0.13373676753670166\n",
      "The 48013 th iteration gives loss of 0.13373559412351366\n",
      "The 48014 th iteration gives loss of 0.13373442072728547\n",
      "The 48015 th iteration gives loss of 0.1337332473480005\n",
      "The 48016 th iteration gives loss of 0.13373207398566767\n",
      "The 48017 th iteration gives loss of 0.13373090064028126\n",
      "The 48018 th iteration gives loss of 0.13372972731185115\n",
      "The 48019 th iteration gives loss of 0.13372855400035244\n",
      "The 48020 th iteration gives loss of 0.13372738070580975\n",
      "The 48021 th iteration gives loss of 0.1337262074281982\n",
      "The 48022 th iteration gives loss of 0.13372503416753817\n",
      "The 48023 th iteration gives loss of 0.13372386092382157\n",
      "The 48024 th iteration gives loss of 0.13372268769703782\n",
      "The 48025 th iteration gives loss of 0.13372151448719444\n",
      "The 48026 th iteration gives loss of 0.13372034129429017\n",
      "The 48027 th iteration gives loss of 0.1337191681183204\n",
      "The 48028 th iteration gives loss of 0.1337179949592954\n",
      "The 48029 th iteration gives loss of 0.13371682181720065\n",
      "The 48030 th iteration gives loss of 0.13371564869204008\n",
      "The 48031 th iteration gives loss of 0.13371447558381627\n",
      "The 48032 th iteration gives loss of 0.13371330249252142\n",
      "The 48033 th iteration gives loss of 0.13371212941815652\n",
      "The 48034 th iteration gives loss of 0.1337109563607244\n",
      "The 48035 th iteration gives loss of 0.13370978332021569\n",
      "The 48036 th iteration gives loss of 0.13370861029664158\n",
      "The 48037 th iteration gives loss of 0.13370743728999046\n",
      "The 48038 th iteration gives loss of 0.13370626430026586\n",
      "The 48039 th iteration gives loss of 0.1337050913274723\n",
      "The 48040 th iteration gives loss of 0.13370391837159906\n",
      "The 48041 th iteration gives loss of 0.13370274543265168\n",
      "The 48042 th iteration gives loss of 0.13370157251062442\n",
      "The 48043 th iteration gives loss of 0.13370039960551086\n",
      "The 48044 th iteration gives loss of 0.13369922671731937\n",
      "The 48045 th iteration gives loss of 0.13369805384605102\n",
      "The 48046 th iteration gives loss of 0.13369688099169547\n",
      "The 48047 th iteration gives loss of 0.13369570815426265\n",
      "The 48048 th iteration gives loss of 0.1336945353337421\n",
      "The 48049 th iteration gives loss of 0.13369336253013728\n",
      "The 48050 th iteration gives loss of 0.13369218974344727\n",
      "The 48051 th iteration gives loss of 0.13369101697366173\n",
      "The 48052 th iteration gives loss of 0.13368984422079394\n",
      "The 48053 th iteration gives loss of 0.13368867148484087\n",
      "The 48054 th iteration gives loss of 0.13368749876579264\n",
      "The 48055 th iteration gives loss of 0.13368632606364997\n",
      "The 48056 th iteration gives loss of 0.13368515337841846\n",
      "The 48057 th iteration gives loss of 0.13368398071009152\n",
      "The 48058 th iteration gives loss of 0.133682808058666\n",
      "The 48059 th iteration gives loss of 0.13368163542415365\n",
      "The 48060 th iteration gives loss of 0.13368046280654605\n",
      "The 48061 th iteration gives loss of 0.13367929020583655\n",
      "The 48062 th iteration gives loss of 0.13367811762202028\n",
      "The 48063 th iteration gives loss of 0.13367694505511007\n",
      "The 48064 th iteration gives loss of 0.1336757725051034\n",
      "The 48065 th iteration gives loss of 0.13367459997198855\n",
      "The 48066 th iteration gives loss of 0.13367342745577754\n",
      "The 48067 th iteration gives loss of 0.13367225495645316\n",
      "The 48068 th iteration gives loss of 0.1336710824740384\n",
      "The 48069 th iteration gives loss of 0.13366991000850012\n",
      "The 48070 th iteration gives loss of 0.13366873755986552\n",
      "The 48071 th iteration gives loss of 0.13366756512812136\n",
      "The 48072 th iteration gives loss of 0.13366639271326838\n",
      "The 48073 th iteration gives loss of 0.1336652203153039\n",
      "The 48074 th iteration gives loss of 0.13366404793423103\n",
      "The 48075 th iteration gives loss of 0.13366287557004258\n",
      "The 48076 th iteration gives loss of 0.13366170322274304\n",
      "The 48077 th iteration gives loss of 0.1336605308923248\n",
      "The 48078 th iteration gives loss of 0.13365935857879593\n",
      "The 48079 th iteration gives loss of 0.1336581862821497\n",
      "The 48080 th iteration gives loss of 0.1336570140023889\n",
      "The 48081 th iteration gives loss of 0.13365584173950842\n",
      "The 48082 th iteration gives loss of 0.1336546694935048\n",
      "The 48083 th iteration gives loss of 0.13365349726438583\n",
      "The 48084 th iteration gives loss of 0.13365232505214214\n",
      "The 48085 th iteration gives loss of 0.13365115285678023\n",
      "The 48086 th iteration gives loss of 0.1336499806782869\n",
      "The 48087 th iteration gives loss of 0.1336488085166709\n",
      "The 48088 th iteration gives loss of 0.13364763637193725\n",
      "The 48089 th iteration gives loss of 0.13364646424407214\n",
      "The 48090 th iteration gives loss of 0.13364529213307633\n",
      "The 48091 th iteration gives loss of 0.13364412003895584\n",
      "The 48092 th iteration gives loss of 0.13364294796170204\n",
      "The 48093 th iteration gives loss of 0.13364177590132162\n",
      "The 48094 th iteration gives loss of 0.1336406038578109\n",
      "The 48095 th iteration gives loss of 0.1336394318311641\n",
      "The 48096 th iteration gives loss of 0.13363825982138902\n",
      "The 48097 th iteration gives loss of 0.13363708782847242\n",
      "The 48098 th iteration gives loss of 0.13363591585242532\n",
      "The 48099 th iteration gives loss of 0.13363474389323976\n",
      "The 48100 th iteration gives loss of 0.13363357195091818\n",
      "The 48101 th iteration gives loss of 0.13363240002544988\n",
      "The 48102 th iteration gives loss of 0.13363122811684622\n",
      "The 48103 th iteration gives loss of 0.13363005622510382\n",
      "The 48104 th iteration gives loss of 0.13362888435022002\n",
      "The 48105 th iteration gives loss of 0.1336277124921925\n",
      "The 48106 th iteration gives loss of 0.13362654065102295\n",
      "The 48107 th iteration gives loss of 0.13362536882670706\n",
      "The 48108 th iteration gives loss of 0.13362419701924355\n",
      "The 48109 th iteration gives loss of 0.13362302522863348\n",
      "The 48110 th iteration gives loss of 0.13362185345487626\n",
      "The 48111 th iteration gives loss of 0.13362068169796737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 48112 th iteration gives loss of 0.13361950995790506\n",
      "The 48113 th iteration gives loss of 0.13361833823469685\n",
      "The 48114 th iteration gives loss of 0.13361716652833705\n",
      "The 48115 th iteration gives loss of 0.1336159948388234\n",
      "The 48116 th iteration gives loss of 0.13361482316615736\n",
      "The 48117 th iteration gives loss of 0.13361365151033655\n",
      "The 48118 th iteration gives loss of 0.13361247987136\n",
      "The 48119 th iteration gives loss of 0.1336113082492227\n",
      "The 48120 th iteration gives loss of 0.13361013664392815\n",
      "The 48121 th iteration gives loss of 0.13360896505546552\n",
      "The 48122 th iteration gives loss of 0.1336077934838499\n",
      "The 48123 th iteration gives loss of 0.13360662192907805\n",
      "The 48124 th iteration gives loss of 0.1336054503911462\n",
      "The 48125 th iteration gives loss of 0.13360427887004675\n",
      "The 48126 th iteration gives loss of 0.13360310736578257\n",
      "The 48127 th iteration gives loss of 0.13360193587835398\n",
      "The 48128 th iteration gives loss of 0.13360076440775454\n",
      "The 48129 th iteration gives loss of 0.1335995929539869\n",
      "The 48130 th iteration gives loss of 0.13359842151705603\n",
      "The 48131 th iteration gives loss of 0.13359725009695894\n",
      "The 48132 th iteration gives loss of 0.13359607869368506\n",
      "The 48133 th iteration gives loss of 0.13359490730723378\n",
      "The 48134 th iteration gives loss of 0.13359373593762425\n",
      "The 48135 th iteration gives loss of 0.13359256458483565\n",
      "The 48136 th iteration gives loss of 0.13359139324886898\n",
      "The 48137 th iteration gives loss of 0.13359022192973352\n",
      "The 48138 th iteration gives loss of 0.13358905062741538\n",
      "The 48139 th iteration gives loss of 0.1335878793419241\n",
      "The 48140 th iteration gives loss of 0.13358670807325124\n",
      "The 48141 th iteration gives loss of 0.1335855368213967\n",
      "The 48142 th iteration gives loss of 0.13358436558636538\n",
      "The 48143 th iteration gives loss of 0.13358319436815208\n",
      "The 48144 th iteration gives loss of 0.13358202316675663\n",
      "The 48145 th iteration gives loss of 0.13358085198217265\n",
      "The 48146 th iteration gives loss of 0.13357968081441046\n",
      "The 48147 th iteration gives loss of 0.13357850966346257\n",
      "The 48148 th iteration gives loss of 0.1335773385293273\n",
      "The 48149 th iteration gives loss of 0.1335761674119959\n",
      "The 48150 th iteration gives loss of 0.13357499631148737\n",
      "The 48151 th iteration gives loss of 0.13357382522778585\n",
      "The 48152 th iteration gives loss of 0.13357265416088876\n",
      "The 48153 th iteration gives loss of 0.133571483110805\n",
      "The 48154 th iteration gives loss of 0.1335703120775249\n",
      "The 48155 th iteration gives loss of 0.13356914106105142\n",
      "The 48156 th iteration gives loss of 0.13356797006138765\n",
      "The 48157 th iteration gives loss of 0.13356679907852853\n",
      "The 48158 th iteration gives loss of 0.13356562811247025\n",
      "The 48159 th iteration gives loss of 0.13356445716320572\n",
      "The 48160 th iteration gives loss of 0.1335632862307519\n",
      "The 48161 th iteration gives loss of 0.13356211531509402\n",
      "The 48162 th iteration gives loss of 0.13356094441623487\n",
      "The 48163 th iteration gives loss of 0.13355977353417098\n",
      "The 48164 th iteration gives loss of 0.1335586026689112\n",
      "The 48165 th iteration gives loss of 0.1335574318204488\n",
      "The 48166 th iteration gives loss of 0.13355626098878262\n",
      "The 48167 th iteration gives loss of 0.13355509017390044\n",
      "The 48168 th iteration gives loss of 0.13355391937581668\n",
      "The 48169 th iteration gives loss of 0.13355274859452315\n",
      "The 48170 th iteration gives loss of 0.13355157783002683\n",
      "The 48171 th iteration gives loss of 0.1335504070823144\n",
      "The 48172 th iteration gives loss of 0.13354923635139454\n",
      "The 48173 th iteration gives loss of 0.13354806563725957\n",
      "The 48174 th iteration gives loss of 0.13354689493991545\n",
      "The 48175 th iteration gives loss of 0.13354572425935157\n",
      "The 48176 th iteration gives loss of 0.13354455359557846\n",
      "The 48177 th iteration gives loss of 0.13354338294858167\n",
      "The 48178 th iteration gives loss of 0.13354221231837424\n",
      "The 48179 th iteration gives loss of 0.13354104170494796\n",
      "The 48180 th iteration gives loss of 0.13353987110829854\n",
      "The 48181 th iteration gives loss of 0.1335387005284343\n",
      "The 48182 th iteration gives loss of 0.13353752996535412\n",
      "The 48183 th iteration gives loss of 0.1335363594190404\n",
      "The 48184 th iteration gives loss of 0.13353518888950405\n",
      "The 48185 th iteration gives loss of 0.1335340183767492\n",
      "The 48186 th iteration gives loss of 0.13353284788077285\n",
      "The 48187 th iteration gives loss of 0.13353167740155472\n",
      "The 48188 th iteration gives loss of 0.13353050693912152\n",
      "The 48189 th iteration gives loss of 0.13352933649346407\n",
      "The 48190 th iteration gives loss of 0.13352816606456813\n",
      "The 48191 th iteration gives loss of 0.13352699565244625\n",
      "The 48192 th iteration gives loss of 0.1335258252570894\n",
      "The 48193 th iteration gives loss of 0.13352465487850457\n",
      "The 48194 th iteration gives loss of 0.13352348451668242\n",
      "The 48195 th iteration gives loss of 0.13352231417162455\n",
      "The 48196 th iteration gives loss of 0.13352114384333594\n",
      "The 48197 th iteration gives loss of 0.13351997353181366\n",
      "The 48198 th iteration gives loss of 0.13351880323705306\n",
      "The 48199 th iteration gives loss of 0.13351763295905233\n",
      "The 48200 th iteration gives loss of 0.13351646269781112\n",
      "The 48201 th iteration gives loss of 0.1335152924533283\n",
      "The 48202 th iteration gives loss of 0.13351412222560652\n",
      "The 48203 th iteration gives loss of 0.13351295201464564\n",
      "The 48204 th iteration gives loss of 0.13351178182044043\n",
      "The 48205 th iteration gives loss of 0.13351061164298783\n",
      "The 48206 th iteration gives loss of 0.13350944148228672\n",
      "The 48207 th iteration gives loss of 0.13350827133834348\n",
      "The 48208 th iteration gives loss of 0.13350710121115547\n",
      "The 48209 th iteration gives loss of 0.13350593110071435\n",
      "The 48210 th iteration gives loss of 0.1335047610070268\n",
      "The 48211 th iteration gives loss of 0.13350359093008793\n",
      "The 48212 th iteration gives loss of 0.13350242086990494\n",
      "The 48213 th iteration gives loss of 0.1335012508264602\n",
      "The 48214 th iteration gives loss of 0.13350008079976883\n",
      "The 48215 th iteration gives loss of 0.13349891078981385\n",
      "The 48216 th iteration gives loss of 0.13349774079660215\n",
      "The 48217 th iteration gives loss of 0.1334965708201398\n",
      "The 48218 th iteration gives loss of 0.1334954008604272\n",
      "The 48219 th iteration gives loss of 0.13349423091744986\n",
      "The 48220 th iteration gives loss of 0.13349306099121142\n",
      "The 48221 th iteration gives loss of 0.13349189108171364\n",
      "The 48222 th iteration gives loss of 0.13349072118895425\n",
      "The 48223 th iteration gives loss of 0.13348955131293155\n",
      "The 48224 th iteration gives loss of 0.13348838145364633\n",
      "The 48225 th iteration gives loss of 0.13348721161109628\n",
      "The 48226 th iteration gives loss of 0.13348604178528403\n",
      "The 48227 th iteration gives loss of 0.1334848719762017\n",
      "The 48228 th iteration gives loss of 0.13348370218385594\n",
      "The 48229 th iteration gives loss of 0.13348253240823685\n",
      "The 48230 th iteration gives loss of 0.13348136264935112\n",
      "The 48231 th iteration gives loss of 0.13348019290719196\n",
      "The 48232 th iteration gives loss of 0.13347902318175833\n",
      "The 48233 th iteration gives loss of 0.13347785347305977\n",
      "The 48234 th iteration gives loss of 0.1334766837810884\n",
      "The 48235 th iteration gives loss of 0.13347551410584443\n",
      "The 48236 th iteration gives loss of 0.13347434444732062\n",
      "The 48237 th iteration gives loss of 0.13347317480552098\n",
      "The 48238 th iteration gives loss of 0.1334720051804429\n",
      "The 48239 th iteration gives loss of 0.13347083557208922\n",
      "The 48240 th iteration gives loss of 0.13346966598044546\n",
      "The 48241 th iteration gives loss of 0.13346849640553032\n",
      "The 48242 th iteration gives loss of 0.13346732684733864\n",
      "The 48243 th iteration gives loss of 0.13346615730585884\n",
      "The 48244 th iteration gives loss of 0.13346498778109658\n",
      "The 48245 th iteration gives loss of 0.13346381827304388\n",
      "The 48246 th iteration gives loss of 0.13346264878171246\n",
      "The 48247 th iteration gives loss of 0.13346147930709548\n",
      "The 48248 th iteration gives loss of 0.13346030984918253\n",
      "The 48249 th iteration gives loss of 0.13345914040799195\n",
      "The 48250 th iteration gives loss of 0.1334579709835049\n",
      "The 48251 th iteration gives loss of 0.13345680157572867\n",
      "The 48252 th iteration gives loss of 0.13345563218466758\n",
      "The 48253 th iteration gives loss of 0.1334544628103118\n",
      "The 48254 th iteration gives loss of 0.13345329345265253\n",
      "The 48255 th iteration gives loss of 0.13345212411170565\n",
      "The 48256 th iteration gives loss of 0.13345095478746233\n",
      "The 48257 th iteration gives loss of 0.1334497854799223\n",
      "The 48258 th iteration gives loss of 0.13344861618908338\n",
      "The 48259 th iteration gives loss of 0.13344744691494212\n",
      "The 48260 th iteration gives loss of 0.1334462776575199\n",
      "The 48261 th iteration gives loss of 0.13344510841678286\n",
      "The 48262 th iteration gives loss of 0.13344393919274392\n",
      "The 48263 th iteration gives loss of 0.13344276998540594\n",
      "The 48264 th iteration gives loss of 0.1334416007947654\n",
      "The 48265 th iteration gives loss of 0.13344043162082228\n",
      "The 48266 th iteration gives loss of 0.13343926246356821\n",
      "The 48267 th iteration gives loss of 0.13343809332300918\n",
      "The 48268 th iteration gives loss of 0.1334369241991393\n",
      "The 48269 th iteration gives loss of 0.1334357550919704\n",
      "The 48270 th iteration gives loss of 0.13343458600148378\n",
      "The 48271 th iteration gives loss of 0.13343341692769065\n",
      "The 48272 th iteration gives loss of 0.1334322478705886\n",
      "The 48273 th iteration gives loss of 0.13343107883017025\n",
      "The 48274 th iteration gives loss of 0.13342990980643143\n",
      "The 48275 th iteration gives loss of 0.13342874079938483\n",
      "The 48276 th iteration gives loss of 0.13342757180902162\n",
      "The 48277 th iteration gives loss of 0.13342640283534965\n",
      "The 48278 th iteration gives loss of 0.13342523387835406\n",
      "The 48279 th iteration gives loss of 0.13342406493804032\n",
      "The 48280 th iteration gives loss of 0.13342289601440938\n",
      "The 48281 th iteration gives loss of 0.1334217271074608\n",
      "The 48282 th iteration gives loss of 0.13342055821718252\n",
      "The 48283 th iteration gives loss of 0.1334193893435845\n",
      "The 48284 th iteration gives loss of 0.13341822048666577\n",
      "The 48285 th iteration gives loss of 0.13341705164642506\n",
      "The 48286 th iteration gives loss of 0.1334158828228543\n",
      "The 48287 th iteration gives loss of 0.1334147140159587\n",
      "The 48288 th iteration gives loss of 0.13341354522573395\n",
      "The 48289 th iteration gives loss of 0.1334123764521822\n",
      "The 48290 th iteration gives loss of 0.13341120769529666\n",
      "The 48291 th iteration gives loss of 0.13341003895509332\n",
      "The 48292 th iteration gives loss of 0.1334088702315489\n",
      "The 48293 th iteration gives loss of 0.13340770152467543\n",
      "The 48294 th iteration gives loss of 0.1334065328344698\n",
      "The 48295 th iteration gives loss of 0.1334053641609218\n",
      "The 48296 th iteration gives loss of 0.13340419550405108\n",
      "The 48297 th iteration gives loss of 0.13340302686383165\n",
      "The 48298 th iteration gives loss of 0.13340185824028322\n",
      "The 48299 th iteration gives loss of 0.13340068963339427\n",
      "The 48300 th iteration gives loss of 0.1333995210431701\n",
      "The 48301 th iteration gives loss of 0.13339835246959866\n",
      "The 48302 th iteration gives loss of 0.13339718391268682\n",
      "The 48303 th iteration gives loss of 0.13339601537243734\n",
      "The 48304 th iteration gives loss of 0.13339484684883773\n",
      "The 48305 th iteration gives loss of 0.13339367834190444\n",
      "The 48306 th iteration gives loss of 0.13339250985162168\n",
      "The 48307 th iteration gives loss of 0.13339134137799166\n",
      "The 48308 th iteration gives loss of 0.1333901729210077\n",
      "The 48309 th iteration gives loss of 0.13338900448068583\n",
      "The 48310 th iteration gives loss of 0.13338783605700913\n",
      "The 48311 th iteration gives loss of 0.13338666764998436\n",
      "The 48312 th iteration gives loss of 0.13338549925960724\n",
      "The 48313 th iteration gives loss of 0.13338433088587476\n",
      "The 48314 th iteration gives loss of 0.13338316252879354\n",
      "The 48315 th iteration gives loss of 0.13338199418836302\n",
      "The 48316 th iteration gives loss of 0.13338082586457373\n",
      "The 48317 th iteration gives loss of 0.1333796575574241\n",
      "The 48318 th iteration gives loss of 0.1333784892669269\n",
      "The 48319 th iteration gives loss of 0.13337732099306454\n",
      "The 48320 th iteration gives loss of 0.13337615273584058\n",
      "The 48321 th iteration gives loss of 0.1333749844952513\n",
      "The 48322 th iteration gives loss of 0.13337381627131134\n",
      "The 48323 th iteration gives loss of 0.13337264806401075\n",
      "The 48324 th iteration gives loss of 0.13337147987334017\n",
      "The 48325 th iteration gives loss of 0.13337031169931024\n",
      "The 48326 th iteration gives loss of 0.1333691435419152\n",
      "The 48327 th iteration gives loss of 0.13336797540114909\n",
      "The 48328 th iteration gives loss of 0.13336680727703148\n",
      "The 48329 th iteration gives loss of 0.13336563916952776\n",
      "The 48330 th iteration gives loss of 0.13336447107866273\n",
      "The 48331 th iteration gives loss of 0.13336330300442875\n",
      "The 48332 th iteration gives loss of 0.1333621349468241\n",
      "The 48333 th iteration gives loss of 0.13336096690584484\n",
      "The 48334 th iteration gives loss of 0.13335979888149957\n",
      "The 48335 th iteration gives loss of 0.13335863087377625\n",
      "The 48336 th iteration gives loss of 0.13335746288267636\n",
      "The 48337 th iteration gives loss of 0.133356294908201\n",
      "The 48338 th iteration gives loss of 0.13335512695034138\n",
      "The 48339 th iteration gives loss of 0.13335395900911112\n",
      "The 48340 th iteration gives loss of 0.1333527910845058\n",
      "The 48341 th iteration gives loss of 0.13335162317651714\n",
      "The 48342 th iteration gives loss of 0.1333504552851392\n",
      "The 48343 th iteration gives loss of 0.13334928741038837\n",
      "The 48344 th iteration gives loss of 0.13334811955225814\n",
      "The 48345 th iteration gives loss of 0.13334695171074043\n",
      "The 48346 th iteration gives loss of 0.13334578388583082\n",
      "The 48347 th iteration gives loss of 0.13334461607755163\n",
      "The 48348 th iteration gives loss of 0.13334344828587122\n",
      "The 48349 th iteration gives loss of 0.13334228051081387\n",
      "The 48350 th iteration gives loss of 0.13334111275236685\n",
      "The 48351 th iteration gives loss of 0.13333994501052435\n",
      "The 48352 th iteration gives loss of 0.13333877728529145\n",
      "The 48353 th iteration gives loss of 0.13333760957667434\n",
      "The 48354 th iteration gives loss of 0.13333644188465196\n",
      "The 48355 th iteration gives loss of 0.13333527420924557\n",
      "The 48356 th iteration gives loss of 0.13333410655044392\n",
      "The 48357 th iteration gives loss of 0.1333329389082469\n",
      "The 48358 th iteration gives loss of 0.13333177128264878\n",
      "The 48359 th iteration gives loss of 0.13333060367365948\n",
      "The 48360 th iteration gives loss of 0.13332943608126607\n",
      "The 48361 th iteration gives loss of 0.1333282685054696\n",
      "The 48362 th iteration gives loss of 0.13332710094628172\n",
      "The 48363 th iteration gives loss of 0.13332593340369042\n",
      "The 48364 th iteration gives loss of 0.13332476587769712\n",
      "The 48365 th iteration gives loss of 0.13332359836830018\n",
      "The 48366 th iteration gives loss of 0.1333224308754961\n",
      "The 48367 th iteration gives loss of 0.1333212633992869\n",
      "The 48368 th iteration gives loss of 0.13332009593966887\n",
      "The 48369 th iteration gives loss of 0.13331892849664093\n",
      "The 48370 th iteration gives loss of 0.13331776107021678\n",
      "The 48371 th iteration gives loss of 0.13331659366037568\n",
      "The 48372 th iteration gives loss of 0.13331542626713172\n",
      "The 48373 th iteration gives loss of 0.1333142588904671\n",
      "The 48374 th iteration gives loss of 0.13331309153039508\n",
      "The 48375 th iteration gives loss of 0.13331192418691\n",
      "The 48376 th iteration gives loss of 0.1333107568600085\n",
      "The 48377 th iteration gives loss of 0.1333095895496922\n",
      "The 48378 th iteration gives loss of 0.1333084222559624\n",
      "The 48379 th iteration gives loss of 0.13330725497881182\n",
      "The 48380 th iteration gives loss of 0.13330608771823937\n",
      "The 48381 th iteration gives loss of 0.13330492047425677\n",
      "The 48382 th iteration gives loss of 0.13330375324684787\n",
      "The 48383 th iteration gives loss of 0.1333025860360249\n",
      "The 48384 th iteration gives loss of 0.1333014188417774\n",
      "The 48385 th iteration gives loss of 0.13330025166410994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 48386 th iteration gives loss of 0.1332990845030114\n",
      "The 48387 th iteration gives loss of 0.13329791735849283\n",
      "The 48388 th iteration gives loss of 0.1332967502305433\n",
      "The 48389 th iteration gives loss of 0.13329558311917042\n",
      "The 48390 th iteration gives loss of 0.133294416024371\n",
      "The 48391 th iteration gives loss of 0.13329324894613773\n",
      "The 48392 th iteration gives loss of 0.1332920818844762\n",
      "The 48393 th iteration gives loss of 0.1332909148393855\n",
      "The 48394 th iteration gives loss of 0.1332897478108692\n",
      "The 48395 th iteration gives loss of 0.13328858079891737\n",
      "The 48396 th iteration gives loss of 0.1332874138035245\n",
      "The 48397 th iteration gives loss of 0.13328624682469525\n",
      "The 48398 th iteration gives loss of 0.1332850798624372\n",
      "The 48399 th iteration gives loss of 0.13328391291674374\n",
      "The 48400 th iteration gives loss of 0.13328274598760817\n",
      "The 48401 th iteration gives loss of 0.13328157907503535\n",
      "The 48402 th iteration gives loss of 0.13328041217902611\n",
      "The 48403 th iteration gives loss of 0.133279245299571\n",
      "The 48404 th iteration gives loss of 0.1332780784366743\n",
      "The 48405 th iteration gives loss of 0.1332769115903397\n",
      "The 48406 th iteration gives loss of 0.13327574476055548\n",
      "The 48407 th iteration gives loss of 0.133274577947329\n",
      "The 48408 th iteration gives loss of 0.1332734111506642\n",
      "The 48409 th iteration gives loss of 0.1332722443705494\n",
      "The 48410 th iteration gives loss of 0.13327107760697932\n",
      "The 48411 th iteration gives loss of 0.13326991085997303\n",
      "The 48412 th iteration gives loss of 0.1332687441295111\n",
      "The 48413 th iteration gives loss of 0.1332675774155981\n",
      "The 48414 th iteration gives loss of 0.13326641071823425\n",
      "The 48415 th iteration gives loss of 0.1332652440374139\n",
      "The 48416 th iteration gives loss of 0.1332640773731449\n",
      "The 48417 th iteration gives loss of 0.13326291072542773\n",
      "The 48418 th iteration gives loss of 0.13326174409425304\n",
      "The 48419 th iteration gives loss of 0.1332605774796186\n",
      "The 48420 th iteration gives loss of 0.1332594108815227\n",
      "The 48421 th iteration gives loss of 0.13325824429997096\n",
      "The 48422 th iteration gives loss of 0.133257077734954\n",
      "The 48423 th iteration gives loss of 0.13325591118648275\n",
      "The 48424 th iteration gives loss of 0.13325474465456077\n",
      "The 48425 th iteration gives loss of 0.13325357813916297\n",
      "The 48426 th iteration gives loss of 0.13325241164031706\n",
      "The 48427 th iteration gives loss of 0.13325124515800166\n",
      "The 48428 th iteration gives loss of 0.1332500786922122\n",
      "The 48429 th iteration gives loss of 0.13324891224296131\n",
      "The 48430 th iteration gives loss of 0.13324774581024598\n",
      "The 48431 th iteration gives loss of 0.1332465793940648\n",
      "The 48432 th iteration gives loss of 0.13324541299441164\n",
      "The 48433 th iteration gives loss of 0.1332442466112856\n",
      "The 48434 th iteration gives loss of 0.133243080244692\n",
      "The 48435 th iteration gives loss of 0.13324191389462967\n",
      "The 48436 th iteration gives loss of 0.13324074756108933\n",
      "The 48437 th iteration gives loss of 0.13323958124407898\n",
      "The 48438 th iteration gives loss of 0.13323841494359348\n",
      "The 48439 th iteration gives loss of 0.1332372486596326\n",
      "The 48440 th iteration gives loss of 0.13323608239218876\n",
      "The 48441 th iteration gives loss of 0.13323491614127167\n",
      "The 48442 th iteration gives loss of 0.13323374990687903\n",
      "The 48443 th iteration gives loss of 0.13323258368900015\n",
      "The 48444 th iteration gives loss of 0.13323141748764383\n",
      "The 48445 th iteration gives loss of 0.13323025130280883\n",
      "The 48446 th iteration gives loss of 0.13322908513449094\n",
      "The 48447 th iteration gives loss of 0.1332279189826874\n",
      "The 48448 th iteration gives loss of 0.13322675284739963\n",
      "The 48449 th iteration gives loss of 0.13322558672862753\n",
      "The 48450 th iteration gives loss of 0.1332244206263737\n",
      "The 48451 th iteration gives loss of 0.1332232545406254\n",
      "The 48452 th iteration gives loss of 0.1332220884713895\n",
      "The 48453 th iteration gives loss of 0.13322092241867425\n",
      "The 48454 th iteration gives loss of 0.13321975638245348\n",
      "The 48455 th iteration gives loss of 0.13321859036274697\n",
      "The 48456 th iteration gives loss of 0.1332174243595575\n",
      "The 48457 th iteration gives loss of 0.1332162583728636\n",
      "The 48458 th iteration gives loss of 0.13321509240268212\n",
      "The 48459 th iteration gives loss of 0.13321392644900526\n",
      "The 48460 th iteration gives loss of 0.13321276051182884\n",
      "The 48461 th iteration gives loss of 0.13321159459116522\n",
      "The 48462 th iteration gives loss of 0.13321042868699495\n",
      "The 48463 th iteration gives loss of 0.13320926279932935\n",
      "The 48464 th iteration gives loss of 0.133208096928148\n",
      "The 48465 th iteration gives loss of 0.13320693107348391\n",
      "The 48466 th iteration gives loss of 0.1332057652353113\n",
      "The 48467 th iteration gives loss of 0.13320459941364007\n",
      "The 48468 th iteration gives loss of 0.13320343360845668\n",
      "The 48469 th iteration gives loss of 0.13320226781977837\n",
      "The 48470 th iteration gives loss of 0.13320110204759822\n",
      "The 48471 th iteration gives loss of 0.13319993629189983\n",
      "The 48472 th iteration gives loss of 0.13319877055269966\n",
      "The 48473 th iteration gives loss of 0.1331976048299879\n",
      "The 48474 th iteration gives loss of 0.1331964391237679\n",
      "The 48475 th iteration gives loss of 0.13319527343403717\n",
      "The 48476 th iteration gives loss of 0.13319410776079574\n",
      "The 48477 th iteration gives loss of 0.133192942104047\n",
      "The 48478 th iteration gives loss of 0.13319177646378008\n",
      "The 48479 th iteration gives loss of 0.13319061083999406\n",
      "The 48480 th iteration gives loss of 0.13318944523270182\n",
      "The 48481 th iteration gives loss of 0.1331882796418895\n",
      "The 48482 th iteration gives loss of 0.1331871140675569\n",
      "The 48483 th iteration gives loss of 0.13318594850971022\n",
      "The 48484 th iteration gives loss of 0.13318478296834002\n",
      "The 48485 th iteration gives loss of 0.13318361744345472\n",
      "The 48486 th iteration gives loss of 0.1331824519350421\n",
      "The 48487 th iteration gives loss of 0.13318128644311042\n",
      "The 48488 th iteration gives loss of 0.13318012096765836\n",
      "The 48489 th iteration gives loss of 0.13317895550867664\n",
      "The 48490 th iteration gives loss of 0.13317779006617203\n",
      "The 48491 th iteration gives loss of 0.13317662464014426\n",
      "The 48492 th iteration gives loss of 0.13317545923058802\n",
      "The 48493 th iteration gives loss of 0.13317429383751297\n",
      "The 48494 th iteration gives loss of 0.13317312846089358\n",
      "The 48495 th iteration gives loss of 0.13317196310075263\n",
      "The 48496 th iteration gives loss of 0.13317079775707838\n",
      "The 48497 th iteration gives loss of 0.1331696324298741\n",
      "The 48498 th iteration gives loss of 0.13316846711913652\n",
      "The 48499 th iteration gives loss of 0.1331673018248684\n",
      "The 48500 th iteration gives loss of 0.13316613654706333\n",
      "The 48501 th iteration gives loss of 0.13316497128572188\n",
      "The 48502 th iteration gives loss of 0.13316380604084596\n",
      "The 48503 th iteration gives loss of 0.1331626408124282\n",
      "The 48504 th iteration gives loss of 0.13316147560048006\n",
      "The 48505 th iteration gives loss of 0.13316031040498283\n",
      "The 48506 th iteration gives loss of 0.1331591452259552\n",
      "The 48507 th iteration gives loss of 0.133157980063374\n",
      "The 48508 th iteration gives loss of 0.13315681491726167\n",
      "The 48509 th iteration gives loss of 0.1331556497875998\n",
      "The 48510 th iteration gives loss of 0.13315448467439644\n",
      "The 48511 th iteration gives loss of 0.13315331957764612\n",
      "The 48512 th iteration gives loss of 0.13315215449735868\n",
      "The 48513 th iteration gives loss of 0.1331509894335089\n",
      "The 48514 th iteration gives loss of 0.1331498243861196\n",
      "The 48515 th iteration gives loss of 0.13314865935518436\n",
      "The 48516 th iteration gives loss of 0.13314749434069684\n",
      "The 48517 th iteration gives loss of 0.13314632934265302\n",
      "The 48518 th iteration gives loss of 0.13314516436106252\n",
      "The 48519 th iteration gives loss of 0.13314399939591942\n",
      "The 48520 th iteration gives loss of 0.13314283444721833\n",
      "The 48521 th iteration gives loss of 0.13314166951497175\n",
      "The 48522 th iteration gives loss of 0.13314050459916635\n",
      "The 48523 th iteration gives loss of 0.13313933969979463\n",
      "The 48524 th iteration gives loss of 0.13313817481687668\n",
      "The 48525 th iteration gives loss of 0.13313700995039351\n",
      "The 48526 th iteration gives loss of 0.13313584510034895\n",
      "The 48527 th iteration gives loss of 0.13313468026675396\n",
      "The 48528 th iteration gives loss of 0.13313351544958738\n",
      "The 48529 th iteration gives loss of 0.13313235064886206\n",
      "The 48530 th iteration gives loss of 0.13313118586457462\n",
      "The 48531 th iteration gives loss of 0.1331300210967228\n",
      "The 48532 th iteration gives loss of 0.13312885634529736\n",
      "The 48533 th iteration gives loss of 0.13312769161032006\n",
      "The 48534 th iteration gives loss of 0.13312652689176632\n",
      "The 48535 th iteration gives loss of 0.13312536218964502\n",
      "The 48536 th iteration gives loss of 0.13312419750396107\n",
      "The 48537 th iteration gives loss of 0.13312303283469618\n",
      "The 48538 th iteration gives loss of 0.1331218681818705\n",
      "The 48539 th iteration gives loss of 0.1331207035454699\n",
      "The 48540 th iteration gives loss of 0.13311953892549608\n",
      "The 48541 th iteration gives loss of 0.13311837432194623\n",
      "The 48542 th iteration gives loss of 0.13311720973481658\n",
      "The 48543 th iteration gives loss of 0.13311604516411957\n",
      "The 48544 th iteration gives loss of 0.13311488060984417\n",
      "The 48545 th iteration gives loss of 0.13311371607199146\n",
      "The 48546 th iteration gives loss of 0.13311255155056245\n",
      "The 48547 th iteration gives loss of 0.13311138704555653\n",
      "The 48548 th iteration gives loss of 0.133110222556957\n",
      "The 48549 th iteration gives loss of 0.13310905808478515\n",
      "The 48550 th iteration gives loss of 0.13310789362902745\n",
      "The 48551 th iteration gives loss of 0.13310672918969071\n",
      "The 48552 th iteration gives loss of 0.1331055647667697\n",
      "The 48553 th iteration gives loss of 0.13310440036025487\n",
      "The 48554 th iteration gives loss of 0.1331032359701633\n",
      "The 48555 th iteration gives loss of 0.1331020715964817\n",
      "The 48556 th iteration gives loss of 0.13310090723920806\n",
      "The 48557 th iteration gives loss of 0.13309974289835239\n",
      "The 48558 th iteration gives loss of 0.13309857857389956\n",
      "The 48559 th iteration gives loss of 0.1330974142658532\n",
      "The 48560 th iteration gives loss of 0.13309624997422304\n",
      "The 48561 th iteration gives loss of 0.13309508569899303\n",
      "The 48562 th iteration gives loss of 0.13309392144017096\n",
      "The 48563 th iteration gives loss of 0.13309275719775784\n",
      "The 48564 th iteration gives loss of 0.13309159297174666\n",
      "The 48565 th iteration gives loss of 0.13309042876214075\n",
      "The 48566 th iteration gives loss of 0.13308926456894024\n",
      "The 48567 th iteration gives loss of 0.13308810039213556\n",
      "The 48568 th iteration gives loss of 0.1330869362317233\n",
      "The 48569 th iteration gives loss of 0.13308577208772862\n",
      "The 48570 th iteration gives loss of 0.13308460796012003\n",
      "The 48571 th iteration gives loss of 0.13308344384891455\n",
      "The 48572 th iteration gives loss of 0.13308227975410344\n",
      "The 48573 th iteration gives loss of 0.13308111567568687\n",
      "The 48574 th iteration gives loss of 0.13307995161366454\n",
      "The 48575 th iteration gives loss of 0.1330787875680407\n",
      "The 48576 th iteration gives loss of 0.13307762353880825\n",
      "The 48577 th iteration gives loss of 0.13307645952596028\n",
      "The 48578 th iteration gives loss of 0.13307529552950598\n",
      "The 48579 th iteration gives loss of 0.1330741315494463\n",
      "The 48580 th iteration gives loss of 0.13307296758577436\n",
      "The 48581 th iteration gives loss of 0.13307180363849613\n",
      "The 48582 th iteration gives loss of 0.1330706397075947\n",
      "The 48583 th iteration gives loss of 0.1330694757930762\n",
      "The 48584 th iteration gives loss of 0.13306831189494836\n",
      "The 48585 th iteration gives loss of 0.13306714801320854\n",
      "The 48586 th iteration gives loss of 0.13306598414785423\n",
      "The 48587 th iteration gives loss of 0.13306482029888206\n",
      "The 48588 th iteration gives loss of 0.13306365646628307\n",
      "The 48589 th iteration gives loss of 0.13306249265006784\n",
      "The 48590 th iteration gives loss of 0.13306132885023292\n",
      "The 48591 th iteration gives loss of 0.13306016506677623\n",
      "The 48592 th iteration gives loss of 0.1330590012996973\n",
      "The 48593 th iteration gives loss of 0.1330578375489917\n",
      "The 48594 th iteration gives loss of 0.13305667381466685\n",
      "The 48595 th iteration gives loss of 0.1330555100967133\n",
      "The 48596 th iteration gives loss of 0.13305434639513253\n",
      "The 48597 th iteration gives loss of 0.133053182709931\n",
      "The 48598 th iteration gives loss of 0.13305201904109654\n",
      "The 48599 th iteration gives loss of 0.13305085538863518\n",
      "The 48600 th iteration gives loss of 0.13304969175254228\n",
      "The 48601 th iteration gives loss of 0.13304852813281706\n",
      "The 48602 th iteration gives loss of 0.1330473645294593\n",
      "The 48603 th iteration gives loss of 0.13304620094247388\n",
      "The 48604 th iteration gives loss of 0.13304503737185114\n",
      "The 48605 th iteration gives loss of 0.13304387381759777\n",
      "The 48606 th iteration gives loss of 0.13304271027969486\n",
      "The 48607 th iteration gives loss of 0.13304154675817378\n",
      "The 48608 th iteration gives loss of 0.13304038325301054\n",
      "The 48609 th iteration gives loss of 0.13303921976419894\n",
      "The 48610 th iteration gives loss of 0.1330380562917573\n",
      "The 48611 th iteration gives loss of 0.13303689283567033\n",
      "The 48612 th iteration gives loss of 0.13303572939593925\n",
      "The 48613 th iteration gives loss of 0.1330345659725735\n",
      "The 48614 th iteration gives loss of 0.1330334025655622\n",
      "The 48615 th iteration gives loss of 0.13303223917491303\n",
      "The 48616 th iteration gives loss of 0.13303107580060877\n",
      "The 48617 th iteration gives loss of 0.13302991244266973\n",
      "The 48618 th iteration gives loss of 0.13302874910107487\n",
      "The 48619 th iteration gives loss of 0.13302758577583004\n",
      "The 48620 th iteration gives loss of 0.133026422466937\n",
      "The 48621 th iteration gives loss of 0.1330252591743953\n",
      "The 48622 th iteration gives loss of 0.13302409589820166\n",
      "The 48623 th iteration gives loss of 0.13302293263835602\n",
      "The 48624 th iteration gives loss of 0.13302176939486512\n",
      "The 48625 th iteration gives loss of 0.1330206061677066\n",
      "The 48626 th iteration gives loss of 0.1330194429569045\n",
      "The 48627 th iteration gives loss of 0.13301827976245365\n",
      "The 48628 th iteration gives loss of 0.13301711658433568\n",
      "The 48629 th iteration gives loss of 0.13301595342255645\n",
      "The 48630 th iteration gives loss of 0.13301479027712196\n",
      "The 48631 th iteration gives loss of 0.13301362714803072\n",
      "The 48632 th iteration gives loss of 0.13301246403528583\n",
      "The 48633 th iteration gives loss of 0.13301130093887653\n",
      "The 48634 th iteration gives loss of 0.13301013785880275\n",
      "The 48635 th iteration gives loss of 0.13300897479505627\n",
      "The 48636 th iteration gives loss of 0.13300781174766158\n",
      "The 48637 th iteration gives loss of 0.13300664871658868\n",
      "The 48638 th iteration gives loss of 0.13300548570185683\n",
      "The 48639 th iteration gives loss of 0.1330043227034626\n",
      "The 48640 th iteration gives loss of 0.1330031597213904\n",
      "The 48641 th iteration gives loss of 0.13300199675565064\n",
      "The 48642 th iteration gives loss of 0.1330008338062477\n",
      "The 48643 th iteration gives loss of 0.13299967087317158\n",
      "The 48644 th iteration gives loss of 0.13299850795642412\n",
      "The 48645 th iteration gives loss of 0.13299734505600178\n",
      "The 48646 th iteration gives loss of 0.1329961821719081\n",
      "The 48647 th iteration gives loss of 0.13299501930414112\n",
      "The 48648 th iteration gives loss of 0.1329938564526966\n",
      "The 48649 th iteration gives loss of 0.13299269361757524\n",
      "The 48650 th iteration gives loss of 0.1329915307987854\n",
      "The 48651 th iteration gives loss of 0.13299036799631547\n",
      "The 48652 th iteration gives loss of 0.13298920521015759\n",
      "The 48653 th iteration gives loss of 0.13298804244032258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 48654 th iteration gives loss of 0.13298687968681314\n",
      "The 48655 th iteration gives loss of 0.13298571694961286\n",
      "The 48656 th iteration gives loss of 0.1329845542287386\n",
      "The 48657 th iteration gives loss of 0.13298339152417632\n",
      "The 48658 th iteration gives loss of 0.1329822288359217\n",
      "The 48659 th iteration gives loss of 0.13298106616398933\n",
      "The 48660 th iteration gives loss of 0.13297990350837105\n",
      "The 48661 th iteration gives loss of 0.132978740869064\n",
      "The 48662 th iteration gives loss of 0.1329775782460654\n",
      "The 48663 th iteration gives loss of 0.1329764156393799\n",
      "The 48664 th iteration gives loss of 0.13297525304901223\n",
      "The 48665 th iteration gives loss of 0.13297409047494968\n",
      "The 48666 th iteration gives loss of 0.1329729279171845\n",
      "The 48667 th iteration gives loss of 0.13297176537572925\n",
      "The 48668 th iteration gives loss of 0.1329706028505864\n",
      "The 48669 th iteration gives loss of 0.13296944034174502\n",
      "The 48670 th iteration gives loss of 0.13296827784920653\n",
      "The 48671 th iteration gives loss of 0.13296711537297415\n",
      "The 48672 th iteration gives loss of 0.13296595291304789\n",
      "The 48673 th iteration gives loss of 0.13296479046941367\n",
      "The 48674 th iteration gives loss of 0.13296362804208547\n",
      "The 48675 th iteration gives loss of 0.13296246563105477\n",
      "The 48676 th iteration gives loss of 0.13296130323632158\n",
      "The 48677 th iteration gives loss of 0.13296014085789087\n",
      "The 48678 th iteration gives loss of 0.1329589784957486\n",
      "The 48679 th iteration gives loss of 0.1329578161499081\n",
      "The 48680 th iteration gives loss of 0.13295665382036406\n",
      "The 48681 th iteration gives loss of 0.13295549150711214\n",
      "The 48682 th iteration gives loss of 0.13295432921014988\n",
      "The 48683 th iteration gives loss of 0.13295316692947848\n",
      "The 48684 th iteration gives loss of 0.13295200466510257\n",
      "The 48685 th iteration gives loss of 0.13295084241700916\n",
      "The 48686 th iteration gives loss of 0.13294968018521305\n",
      "The 48687 th iteration gives loss of 0.1329485179697077\n",
      "The 48688 th iteration gives loss of 0.13294735577047598\n",
      "The 48689 th iteration gives loss of 0.13294619358753987\n",
      "The 48690 th iteration gives loss of 0.1329450314208927\n",
      "The 48691 th iteration gives loss of 0.13294386927051527\n",
      "The 48692 th iteration gives loss of 0.13294270713643716\n",
      "The 48693 th iteration gives loss of 0.13294154501864294\n",
      "The 48694 th iteration gives loss of 0.13294038291711932\n",
      "The 48695 th iteration gives loss of 0.1329392208318804\n",
      "The 48696 th iteration gives loss of 0.13293805876292006\n",
      "The 48697 th iteration gives loss of 0.1329368967102419\n",
      "The 48698 th iteration gives loss of 0.13293573467384082\n",
      "The 48699 th iteration gives loss of 0.13293457265372438\n",
      "The 48700 th iteration gives loss of 0.1329334106498689\n",
      "The 48701 th iteration gives loss of 0.13293224866229042\n",
      "The 48702 th iteration gives loss of 0.13293108669099826\n",
      "The 48703 th iteration gives loss of 0.13292992473596632\n",
      "The 48704 th iteration gives loss of 0.13292876279720414\n",
      "The 48705 th iteration gives loss of 0.13292760087471842\n",
      "The 48706 th iteration gives loss of 0.13292643896851145\n",
      "The 48707 th iteration gives loss of 0.13292527707857058\n",
      "The 48708 th iteration gives loss of 0.13292411520489697\n",
      "The 48709 th iteration gives loss of 0.13292295334748935\n",
      "The 48710 th iteration gives loss of 0.1329217915063516\n",
      "The 48711 th iteration gives loss of 0.13292062968147575\n",
      "The 48712 th iteration gives loss of 0.1329194678728757\n",
      "The 48713 th iteration gives loss of 0.1329183060805266\n",
      "The 48714 th iteration gives loss of 0.13291714430443408\n",
      "The 48715 th iteration gives loss of 0.13291598254461665\n",
      "The 48716 th iteration gives loss of 0.1329148208010635\n",
      "The 48717 th iteration gives loss of 0.13291365907376\n",
      "The 48718 th iteration gives loss of 0.13291249736271998\n",
      "The 48719 th iteration gives loss of 0.13291133566794214\n",
      "The 48720 th iteration gives loss of 0.13291017398941857\n",
      "The 48721 th iteration gives loss of 0.13290901232714922\n",
      "The 48722 th iteration gives loss of 0.13290785068113853\n",
      "The 48723 th iteration gives loss of 0.1329066890513793\n",
      "The 48724 th iteration gives loss of 0.13290552743787443\n",
      "The 48725 th iteration gives loss of 0.13290436584061902\n",
      "The 48726 th iteration gives loss of 0.13290320425962146\n",
      "The 48727 th iteration gives loss of 0.13290204269488087\n",
      "The 48728 th iteration gives loss of 0.1329008811463843\n",
      "The 48729 th iteration gives loss of 0.1328997196141311\n",
      "The 48730 th iteration gives loss of 0.13289855809812867\n",
      "The 48731 th iteration gives loss of 0.13289739659838049\n",
      "The 48732 th iteration gives loss of 0.13289623511486995\n",
      "The 48733 th iteration gives loss of 0.13289507364761183\n",
      "The 48734 th iteration gives loss of 0.1328939121965847\n",
      "The 48735 th iteration gives loss of 0.1328927507618189\n",
      "The 48736 th iteration gives loss of 0.13289158934328688\n",
      "The 48737 th iteration gives loss of 0.13289042794099346\n",
      "The 48738 th iteration gives loss of 0.1328892665549495\n",
      "The 48739 th iteration gives loss of 0.13288810518513403\n",
      "The 48740 th iteration gives loss of 0.1328869438315674\n",
      "The 48741 th iteration gives loss of 0.1328857824942275\n",
      "The 48742 th iteration gives loss of 0.13288462117313254\n",
      "The 48743 th iteration gives loss of 0.13288345986826855\n",
      "The 48744 th iteration gives loss of 0.1328822985796482\n",
      "The 48745 th iteration gives loss of 0.1328811373072597\n",
      "The 48746 th iteration gives loss of 0.13287997605111068\n",
      "The 48747 th iteration gives loss of 0.13287881481118347\n",
      "The 48748 th iteration gives loss of 0.1328776535874879\n",
      "The 48749 th iteration gives loss of 0.1328764923800283\n",
      "The 48750 th iteration gives loss of 0.13287533118879202\n",
      "The 48751 th iteration gives loss of 0.13287417001378196\n",
      "The 48752 th iteration gives loss of 0.1328730088550066\n",
      "The 48753 th iteration gives loss of 0.1328718477124582\n",
      "The 48754 th iteration gives loss of 0.13287068658613455\n",
      "The 48755 th iteration gives loss of 0.13286952547603728\n",
      "The 48756 th iteration gives loss of 0.13286836438216265\n",
      "The 48757 th iteration gives loss of 0.1328672033045131\n",
      "The 48758 th iteration gives loss of 0.1328660422430847\n",
      "The 48759 th iteration gives loss of 0.13286488119787257\n",
      "The 48760 th iteration gives loss of 0.13286372016888962\n",
      "The 48761 th iteration gives loss of 0.13286255915611397\n",
      "The 48762 th iteration gives loss of 0.13286139815956804\n",
      "The 48763 th iteration gives loss of 0.13286023717923806\n",
      "The 48764 th iteration gives loss of 0.13285907621511892\n",
      "The 48765 th iteration gives loss of 0.13285791526722052\n",
      "The 48766 th iteration gives loss of 0.13285675433553532\n",
      "The 48767 th iteration gives loss of 0.13285559342006556\n",
      "The 48768 th iteration gives loss of 0.13285443252080775\n",
      "The 48769 th iteration gives loss of 0.1328532716377565\n",
      "The 48770 th iteration gives loss of 0.1328521107709181\n",
      "The 48771 th iteration gives loss of 0.13285094992029722\n",
      "The 48772 th iteration gives loss of 0.13284978908587594\n",
      "The 48773 th iteration gives loss of 0.1328486282676733\n",
      "The 48774 th iteration gives loss of 0.13284746746567314\n",
      "The 48775 th iteration gives loss of 0.1328463066798785\n",
      "The 48776 th iteration gives loss of 0.13284514591028823\n",
      "The 48777 th iteration gives loss of 0.1328439851569058\n",
      "The 48778 th iteration gives loss of 0.13284282441972595\n",
      "The 48779 th iteration gives loss of 0.13284166369875308\n",
      "The 48780 th iteration gives loss of 0.1328405029939796\n",
      "The 48781 th iteration gives loss of 0.13283934230540176\n",
      "The 48782 th iteration gives loss of 0.13283818163303096\n",
      "The 48783 th iteration gives loss of 0.13283702097685487\n",
      "The 48784 th iteration gives loss of 0.1328358603368779\n",
      "The 48785 th iteration gives loss of 0.13283469971309728\n",
      "The 48786 th iteration gives loss of 0.1328335391055114\n",
      "The 48787 th iteration gives loss of 0.13283237851412652\n",
      "The 48788 th iteration gives loss of 0.13283121793893032\n",
      "The 48789 th iteration gives loss of 0.13283005737993936\n",
      "The 48790 th iteration gives loss of 0.1328288968371314\n",
      "The 48791 th iteration gives loss of 0.13282773631051506\n",
      "The 48792 th iteration gives loss of 0.1328265758000939\n",
      "The 48793 th iteration gives loss of 0.13282541530585926\n",
      "The 48794 th iteration gives loss of 0.13282425482781957\n",
      "The 48795 th iteration gives loss of 0.132823094365953\n",
      "The 48796 th iteration gives loss of 0.13282193392028102\n",
      "The 48797 th iteration gives loss of 0.13282077349080576\n",
      "The 48798 th iteration gives loss of 0.13281961307750734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 48799 th iteration gives loss of 0.13281845268038864\n",
      "The 48800 th iteration gives loss of 0.13281729229945635\n",
      "The 48801 th iteration gives loss of 0.13281613193471287\n",
      "The 48802 th iteration gives loss of 0.13281497158614888\n",
      "The 48803 th iteration gives loss of 0.13281381125375954\n",
      "The 48804 th iteration gives loss of 0.13281265093755568\n",
      "The 48805 th iteration gives loss of 0.1328114906375273\n",
      "The 48806 th iteration gives loss of 0.1328103303536795\n",
      "The 48807 th iteration gives loss of 0.13280917008600832\n",
      "The 48808 th iteration gives loss of 0.1328080098345145\n",
      "The 48809 th iteration gives loss of 0.13280684959919434\n",
      "The 48810 th iteration gives loss of 0.13280568938004378\n",
      "The 48811 th iteration gives loss of 0.13280452917707433\n",
      "The 48812 th iteration gives loss of 0.13280336899027265\n",
      "The 48813 th iteration gives loss of 0.1328022088196524\n",
      "The 48814 th iteration gives loss of 0.13280104866519254\n",
      "The 48815 th iteration gives loss of 0.1327998885269042\n",
      "The 48816 th iteration gives loss of 0.1327987284047876\n",
      "The 48817 th iteration gives loss of 0.13279756829883685\n",
      "The 48818 th iteration gives loss of 0.13279640820905683\n",
      "The 48819 th iteration gives loss of 0.13279524813543522\n",
      "The 48820 th iteration gives loss of 0.13279408807797827\n",
      "The 48821 th iteration gives loss of 0.1327929280366923\n",
      "The 48822 th iteration gives loss of 0.1327917680115598\n",
      "The 48823 th iteration gives loss of 0.1327906080026099\n",
      "The 48824 th iteration gives loss of 0.13278944800980788\n",
      "The 48825 th iteration gives loss of 0.13278828803316192\n",
      "The 48826 th iteration gives loss of 0.13278712807268578\n",
      "The 48827 th iteration gives loss of 0.1327859681283656\n",
      "The 48828 th iteration gives loss of 0.1327848082001955\n",
      "The 48829 th iteration gives loss of 0.13278364828818978\n",
      "The 48830 th iteration gives loss of 0.13278248839234313\n",
      "The 48831 th iteration gives loss of 0.13278132851264143\n",
      "The 48832 th iteration gives loss of 0.13278016864910194\n",
      "The 48833 th iteration gives loss of 0.13277900880171206\n",
      "The 48834 th iteration gives loss of 0.13277784897047865\n",
      "The 48835 th iteration gives loss of 0.1327766891553897\n",
      "The 48836 th iteration gives loss of 0.13277552935645956\n",
      "The 48837 th iteration gives loss of 0.13277436957367103\n",
      "The 48838 th iteration gives loss of 0.13277320980703783\n",
      "The 48839 th iteration gives loss of 0.13277205005654838\n",
      "The 48840 th iteration gives loss of 0.13277089032221004\n",
      "The 48841 th iteration gives loss of 0.13276973060401626\n",
      "The 48842 th iteration gives loss of 0.1327685709019565\n",
      "The 48843 th iteration gives loss of 0.13276741121605576\n",
      "The 48844 th iteration gives loss of 0.13276625154629024\n",
      "The 48845 th iteration gives loss of 0.13276509189266944\n",
      "The 48846 th iteration gives loss of 0.1327639322551863\n",
      "The 48847 th iteration gives loss of 0.13276277263384972\n",
      "The 48848 th iteration gives loss of 0.13276161302865178\n",
      "The 48849 th iteration gives loss of 0.13276045343959358\n",
      "The 48850 th iteration gives loss of 0.13275929386667082\n",
      "The 48851 th iteration gives loss of 0.13275813430989056\n",
      "The 48852 th iteration gives loss of 0.13275697476923826\n",
      "The 48853 th iteration gives loss of 0.13275581524472163\n",
      "The 48854 th iteration gives loss of 0.1327546557363401\n",
      "The 48855 th iteration gives loss of 0.13275349624409755\n",
      "The 48856 th iteration gives loss of 0.13275233676797987\n",
      "The 48857 th iteration gives loss of 0.13275117730799582\n",
      "The 48858 th iteration gives loss of 0.1327500178641468\n",
      "The 48859 th iteration gives loss of 0.1327488584364123\n",
      "The 48860 th iteration gives loss of 0.13274769902481787\n",
      "The 48861 th iteration gives loss of 0.13274653962934974\n",
      "The 48862 th iteration gives loss of 0.13274538025001237\n",
      "The 48863 th iteration gives loss of 0.13274422088679788\n",
      "The 48864 th iteration gives loss of 0.13274306153970744\n",
      "The 48865 th iteration gives loss of 0.13274190220874646\n",
      "The 48866 th iteration gives loss of 0.13274074289389934\n",
      "The 48867 th iteration gives loss of 0.1327395835951779\n",
      "The 48868 th iteration gives loss of 0.13273842431258503\n",
      "The 48869 th iteration gives loss of 0.1327372650461102\n",
      "The 48870 th iteration gives loss of 0.13273610579574854\n",
      "The 48871 th iteration gives loss of 0.1327349465615047\n",
      "The 48872 th iteration gives loss of 0.13273378734338037\n",
      "The 48873 th iteration gives loss of 0.13273262814138048\n",
      "The 48874 th iteration gives loss of 0.13273146895548962\n",
      "The 48875 th iteration gives loss of 0.13273030978572006\n",
      "The 48876 th iteration gives loss of 0.13272915063205618\n",
      "The 48877 th iteration gives loss of 0.13272799149451367\n",
      "The 48878 th iteration gives loss of 0.13272683237307525\n",
      "The 48879 th iteration gives loss of 0.1327256732677542\n",
      "The 48880 th iteration gives loss of 0.13272451417854095\n",
      "The 48881 th iteration gives loss of 0.13272335510543698\n",
      "The 48882 th iteration gives loss of 0.13272219604843982\n",
      "The 48883 th iteration gives loss of 0.13272103700755541\n",
      "The 48884 th iteration gives loss of 0.13271987798277068\n",
      "The 48885 th iteration gives loss of 0.1327187189741017\n",
      "The 48886 th iteration gives loss of 0.13271755998153578\n",
      "The 48887 th iteration gives loss of 0.13271640100506896\n",
      "The 48888 th iteration gives loss of 0.1327152420447068\n",
      "The 48889 th iteration gives loss of 0.13271408310044558\n",
      "The 48890 th iteration gives loss of 0.1327129241722875\n",
      "The 48891 th iteration gives loss of 0.13271176526023074\n",
      "The 48892 th iteration gives loss of 0.1327106063642786\n",
      "The 48893 th iteration gives loss of 0.13270944748441788\n",
      "The 48894 th iteration gives loss of 0.13270828862066075\n",
      "The 48895 th iteration gives loss of 0.13270712977299362\n",
      "The 48896 th iteration gives loss of 0.13270597094142889\n",
      "The 48897 th iteration gives loss of 0.1327048121259542\n",
      "The 48898 th iteration gives loss of 0.1327036533265711\n",
      "The 48899 th iteration gives loss of 0.13270249454328437\n",
      "The 48900 th iteration gives loss of 0.1327013357760896\n",
      "The 48901 th iteration gives loss of 0.1327001770249856\n",
      "The 48902 th iteration gives loss of 0.13269901828997918\n",
      "The 48903 th iteration gives loss of 0.13269785957105493\n",
      "The 48904 th iteration gives loss of 0.1326967008682253\n",
      "The 48905 th iteration gives loss of 0.13269554218148175\n",
      "The 48906 th iteration gives loss of 0.13269438351082447\n",
      "The 48907 th iteration gives loss of 0.13269322485625853\n",
      "The 48908 th iteration gives loss of 0.13269206621776697\n",
      "The 48909 th iteration gives loss of 0.13269090759536747\n",
      "The 48910 th iteration gives loss of 0.13268974898904495\n",
      "The 48911 th iteration gives loss of 0.13268859039881142\n",
      "The 48912 th iteration gives loss of 0.132687431824653\n",
      "The 48913 th iteration gives loss of 0.1326862732665788\n",
      "The 48914 th iteration gives loss of 0.1326851147245852\n",
      "The 48915 th iteration gives loss of 0.13268395619866996\n",
      "The 48916 th iteration gives loss of 0.1326827976888309\n",
      "The 48917 th iteration gives loss of 0.13268163919507162\n",
      "The 48918 th iteration gives loss of 0.13268048071738295\n",
      "The 48919 th iteration gives loss of 0.13267932225577503\n",
      "The 48920 th iteration gives loss of 0.13267816381023942\n",
      "The 48921 th iteration gives loss of 0.13267700538078236\n",
      "The 48922 th iteration gives loss of 0.13267584696739285\n",
      "The 48923 th iteration gives loss of 0.13267468857006867\n",
      "The 48924 th iteration gives loss of 0.13267353018882178\n",
      "The 48925 th iteration gives loss of 0.13267237182364156\n",
      "The 48926 th iteration gives loss of 0.1326712134745433\n",
      "The 48927 th iteration gives loss of 0.1326700551415006\n",
      "The 48928 th iteration gives loss of 0.13266889682452723\n",
      "The 48929 th iteration gives loss of 0.1326677385236234\n",
      "The 48930 th iteration gives loss of 0.13266658023877814\n",
      "The 48931 th iteration gives loss of 0.1326654219700049\n",
      "The 48932 th iteration gives loss of 0.13266426371729567\n",
      "The 48933 th iteration gives loss of 0.1326631054806421\n",
      "The 48934 th iteration gives loss of 0.13266194726005015\n",
      "The 48935 th iteration gives loss of 0.13266078905551842\n",
      "The 48936 th iteration gives loss of 0.132659630867049\n",
      "The 48937 th iteration gives loss of 0.1326584726946374\n",
      "The 48938 th iteration gives loss of 0.13265731453828342\n",
      "The 48939 th iteration gives loss of 0.13265615639799666\n",
      "The 48940 th iteration gives loss of 0.13265499827376054\n",
      "The 48941 th iteration gives loss of 0.13265384016557535\n",
      "The 48942 th iteration gives loss of 0.13265268207344627\n",
      "The 48943 th iteration gives loss of 0.1326515239973711\n",
      "The 48944 th iteration gives loss of 0.13265036593734744\n",
      "The 48945 th iteration gives loss of 0.1326492078933775\n",
      "The 48946 th iteration gives loss of 0.1326480498654613\n",
      "The 48947 th iteration gives loss of 0.13264689185359305\n",
      "The 48948 th iteration gives loss of 0.13264573385776945\n",
      "The 48949 th iteration gives loss of 0.13264457587800543\n",
      "The 48950 th iteration gives loss of 0.13264341791428372\n",
      "The 48951 th iteration gives loss of 0.13264225996660206\n",
      "The 48952 th iteration gives loss of 0.13264110203496965\n",
      "The 48953 th iteration gives loss of 0.1326399441193905\n",
      "The 48954 th iteration gives loss of 0.13263878621984032\n",
      "The 48955 th iteration gives loss of 0.13263762833633924\n",
      "The 48956 th iteration gives loss of 0.13263647046887228\n",
      "The 48957 th iteration gives loss of 0.1326353126174697\n",
      "The 48958 th iteration gives loss of 0.13263415478209103\n",
      "The 48959 th iteration gives loss of 0.13263299696275396\n",
      "The 48960 th iteration gives loss of 0.13263183915945306\n",
      "The 48961 th iteration gives loss of 0.13263068137218942\n",
      "The 48962 th iteration gives loss of 0.13262952360096367\n",
      "The 48963 th iteration gives loss of 0.13262836584576998\n",
      "The 48964 th iteration gives loss of 0.13262720810661816\n",
      "The 48965 th iteration gives loss of 0.13262605038349182\n",
      "The 48966 th iteration gives loss of 0.1326248926764059\n",
      "The 48967 th iteration gives loss of 0.1326237349853519\n",
      "The 48968 th iteration gives loss of 0.13262257731032165\n",
      "The 48969 th iteration gives loss of 0.1326214196513297\n",
      "The 48970 th iteration gives loss of 0.13262026200836502\n",
      "The 48971 th iteration gives loss of 0.13261910438142782\n",
      "The 48972 th iteration gives loss of 0.13261794677051758\n",
      "The 48973 th iteration gives loss of 0.13261678917563624\n",
      "The 48974 th iteration gives loss of 0.13261563159677894\n",
      "The 48975 th iteration gives loss of 0.13261447403394347\n",
      "The 48976 th iteration gives loss of 0.13261331648713603\n",
      "The 48977 th iteration gives loss of 0.13261215895635908\n",
      "The 48978 th iteration gives loss of 0.13261100144158866\n",
      "The 48979 th iteration gives loss of 0.1326098439428514\n",
      "The 48980 th iteration gives loss of 0.13260868646013424\n",
      "The 48981 th iteration gives loss of 0.1326075289934278\n",
      "The 48982 th iteration gives loss of 0.13260637154275104\n",
      "The 48983 th iteration gives loss of 0.13260521410808201\n",
      "The 48984 th iteration gives loss of 0.13260405668943503\n",
      "The 48985 th iteration gives loss of 0.13260289928680258\n",
      "The 48986 th iteration gives loss of 0.13260174190018226\n",
      "The 48987 th iteration gives loss of 0.13260058452958132\n",
      "The 48988 th iteration gives loss of 0.13259942717499718\n",
      "The 48989 th iteration gives loss of 0.13259826983641945\n",
      "The 48990 th iteration gives loss of 0.13259711251385456\n",
      "The 48991 th iteration gives loss of 0.13259595520730472\n",
      "The 48992 th iteration gives loss of 0.13259479791675746\n",
      "The 48993 th iteration gives loss of 0.1325936406422185\n",
      "The 48994 th iteration gives loss of 0.13259248338369062\n",
      "The 48995 th iteration gives loss of 0.13259132614117225\n",
      "The 48996 th iteration gives loss of 0.13259016891465775\n",
      "The 48997 th iteration gives loss of 0.13258901170415177\n",
      "The 48998 th iteration gives loss of 0.13258785450964983\n",
      "The 48999 th iteration gives loss of 0.13258669733114242\n",
      "The 49000 th iteration gives loss of 0.1325855401686458\n",
      "The 49001 th iteration gives loss of 0.13258438302215272\n",
      "The 49002 th iteration gives loss of 0.13258322589165347\n",
      "The 49003 th iteration gives loss of 0.1325820687771538\n",
      "The 49004 th iteration gives loss of 0.1325809116786659\n",
      "The 49005 th iteration gives loss of 0.1325797545961603\n",
      "The 49006 th iteration gives loss of 0.13257859752966566\n",
      "The 49007 th iteration gives loss of 0.13257744047915618\n",
      "The 49008 th iteration gives loss of 0.13257628344464817\n",
      "The 49009 th iteration gives loss of 0.13257512642614175\n",
      "The 49010 th iteration gives loss of 0.13257396942361946\n",
      "The 49011 th iteration gives loss of 0.13257281243708904\n",
      "The 49012 th iteration gives loss of 0.1325716554665483\n",
      "The 49013 th iteration gives loss of 0.13257049851200453\n",
      "The 49014 th iteration gives loss of 0.13256934157344982\n",
      "The 49015 th iteration gives loss of 0.13256818465088455\n",
      "The 49016 th iteration gives loss of 0.132567027744303\n",
      "The 49017 th iteration gives loss of 0.13256587085371407\n",
      "The 49018 th iteration gives loss of 0.1325647139791167\n",
      "The 49019 th iteration gives loss of 0.13256355712050352\n",
      "The 49020 th iteration gives loss of 0.13256240027786734\n",
      "The 49021 th iteration gives loss of 0.13256124345121906\n",
      "The 49022 th iteration gives loss of 0.13256008664055793\n",
      "The 49023 th iteration gives loss of 0.13255892984587705\n",
      "The 49024 th iteration gives loss of 0.13255777306717706\n",
      "The 49025 th iteration gives loss of 0.13255661630445292\n",
      "The 49026 th iteration gives loss of 0.13255545955770992\n",
      "The 49027 th iteration gives loss of 0.13255430282694813\n",
      "The 49028 th iteration gives loss of 0.13255314611216226\n",
      "The 49029 th iteration gives loss of 0.132551989413356\n",
      "The 49030 th iteration gives loss of 0.13255083273051718\n",
      "The 49031 th iteration gives loss of 0.13254967606366455\n",
      "The 49032 th iteration gives loss of 0.13254851941277773\n",
      "The 49033 th iteration gives loss of 0.1325473627778695\n",
      "The 49034 th iteration gives loss of 0.13254620615893903\n",
      "The 49035 th iteration gives loss of 0.13254504955597132\n",
      "The 49036 th iteration gives loss of 0.13254389296897331\n",
      "The 49037 th iteration gives loss of 0.13254273639794717\n",
      "The 49038 th iteration gives loss of 0.13254157984289688\n",
      "The 49039 th iteration gives loss of 0.13254042330380408\n",
      "The 49040 th iteration gives loss of 0.13253926678068578\n",
      "The 49041 th iteration gives loss of 0.13253811027353288\n",
      "The 49042 th iteration gives loss of 0.13253695378234245\n",
      "The 49043 th iteration gives loss of 0.1325357973071194\n",
      "The 49044 th iteration gives loss of 0.13253464084785657\n",
      "The 49045 th iteration gives loss of 0.1325334844045572\n",
      "The 49046 th iteration gives loss of 0.13253232797722198\n",
      "The 49047 th iteration gives loss of 0.13253117156584196\n",
      "The 49048 th iteration gives loss of 0.1325300151704352\n",
      "The 49049 th iteration gives loss of 0.1325288587909715\n",
      "The 49050 th iteration gives loss of 0.132527702427472\n",
      "The 49051 th iteration gives loss of 0.13252654607993267\n",
      "The 49052 th iteration gives loss of 0.13252538974834874\n",
      "The 49053 th iteration gives loss of 0.1325242334327184\n",
      "The 49054 th iteration gives loss of 0.13252307713304298\n",
      "The 49055 th iteration gives loss of 0.13252192084932252\n",
      "The 49056 th iteration gives loss of 0.13252076458156517\n",
      "The 49057 th iteration gives loss of 0.13251960832974216\n",
      "The 49058 th iteration gives loss of 0.13251845209387855\n",
      "The 49059 th iteration gives loss of 0.13251729587396857\n",
      "The 49060 th iteration gives loss of 0.1325161396700033\n",
      "The 49061 th iteration gives loss of 0.1325149834819883\n",
      "The 49062 th iteration gives loss of 0.13251382730992134\n",
      "The 49063 th iteration gives loss of 0.13251267115380014\n",
      "The 49064 th iteration gives loss of 0.13251151501362635\n",
      "The 49065 th iteration gives loss of 0.13251035888939552\n",
      "The 49066 th iteration gives loss of 0.13250920278111006\n",
      "The 49067 th iteration gives loss of 0.13250804668876787\n",
      "The 49068 th iteration gives loss of 0.13250689061236892\n",
      "The 49069 th iteration gives loss of 0.1325057345519122\n",
      "The 49070 th iteration gives loss of 0.1325045785073894\n",
      "The 49071 th iteration gives loss of 0.13250342247881416\n",
      "The 49072 th iteration gives loss of 0.13250226646617616\n",
      "The 49073 th iteration gives loss of 0.13250111046947116\n",
      "The 49074 th iteration gives loss of 0.1324999544887109\n",
      "The 49075 th iteration gives loss of 0.1324987985238819\n",
      "The 49076 th iteration gives loss of 0.1324976425749878\n",
      "The 49077 th iteration gives loss of 0.1324964866420267\n",
      "The 49078 th iteration gives loss of 0.13249533072501088\n",
      "The 49079 th iteration gives loss of 0.13249417482391687\n",
      "The 49080 th iteration gives loss of 0.1324930189387597\n",
      "The 49081 th iteration gives loss of 0.13249186306953128\n",
      "The 49082 th iteration gives loss of 0.132490707216232\n",
      "The 49083 th iteration gives loss of 0.13248955137886173\n",
      "The 49084 th iteration gives loss of 0.1324883955574244\n",
      "The 49085 th iteration gives loss of 0.13248723975190266\n",
      "The 49086 th iteration gives loss of 0.13248608396231873\n",
      "The 49087 th iteration gives loss of 0.13248492818866084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 49088 th iteration gives loss of 0.1324837724309191\n",
      "The 49089 th iteration gives loss of 0.13248261668910385\n",
      "The 49090 th iteration gives loss of 0.13248146096322092\n",
      "The 49091 th iteration gives loss of 0.13248030525324728\n",
      "The 49092 th iteration gives loss of 0.1324791495592039\n",
      "The 49093 th iteration gives loss of 0.1324779938810826\n",
      "The 49094 th iteration gives loss of 0.13247683821887316\n",
      "The 49095 th iteration gives loss of 0.132475682572586\n",
      "The 49096 th iteration gives loss of 0.132474526942215\n",
      "The 49097 th iteration gives loss of 0.13247337132776402\n",
      "The 49098 th iteration gives loss of 0.13247221572922427\n",
      "The 49099 th iteration gives loss of 0.13247106014660356\n",
      "The 49100 th iteration gives loss of 0.1324699045798952\n",
      "The 49101 th iteration gives loss of 0.13246874902910566\n",
      "The 49102 th iteration gives loss of 0.1324675934942209\n",
      "The 49103 th iteration gives loss of 0.1324664379752493\n",
      "The 49104 th iteration gives loss of 0.1324652824721869\n",
      "The 49105 th iteration gives loss of 0.1324641269850427\n",
      "The 49106 th iteration gives loss of 0.13246297151380812\n",
      "The 49107 th iteration gives loss of 0.13246181605847235\n",
      "The 49108 th iteration gives loss of 0.13246066061904854\n",
      "The 49109 th iteration gives loss of 0.13245950519552907\n",
      "The 49110 th iteration gives loss of 0.13245834978791507\n",
      "The 49111 th iteration gives loss of 0.13245719439620396\n",
      "The 49112 th iteration gives loss of 0.13245603902039715\n",
      "The 49113 th iteration gives loss of 0.13245488366049965\n",
      "The 49114 th iteration gives loss of 0.13245372831649846\n",
      "The 49115 th iteration gives loss of 0.13245257298840193\n",
      "The 49116 th iteration gives loss of 0.1324514176762071\n",
      "The 49117 th iteration gives loss of 0.13245026237991125\n",
      "The 49118 th iteration gives loss of 0.13244910709950394\n",
      "The 49119 th iteration gives loss of 0.13244795183500066\n",
      "The 49120 th iteration gives loss of 0.1324467965863996\n",
      "The 49121 th iteration gives loss of 0.1324456413536902\n",
      "The 49122 th iteration gives loss of 0.13244448613687182\n",
      "The 49123 th iteration gives loss of 0.1324433309359497\n",
      "The 49124 th iteration gives loss of 0.13244217575092443\n",
      "The 49125 th iteration gives loss of 0.13244102058178978\n",
      "The 49126 th iteration gives loss of 0.13243986542854863\n",
      "The 49127 th iteration gives loss of 0.13243871029119522\n",
      "The 49128 th iteration gives loss of 0.1324375551697331\n",
      "The 49129 th iteration gives loss of 0.13243640006415452\n",
      "The 49130 th iteration gives loss of 0.1324352449744748\n",
      "The 49131 th iteration gives loss of 0.13243408990067684\n",
      "The 49132 th iteration gives loss of 0.13243293484275737\n",
      "The 49133 th iteration gives loss of 0.13243177980072962\n",
      "The 49134 th iteration gives loss of 0.13243062477459036\n",
      "The 49135 th iteration gives loss of 0.13242946976432582\n",
      "The 49136 th iteration gives loss of 0.13242831476994987\n",
      "The 49137 th iteration gives loss of 0.13242715979145966\n",
      "The 49138 th iteration gives loss of 0.13242600482884048\n",
      "The 49139 th iteration gives loss of 0.13242484988210876\n",
      "The 49140 th iteration gives loss of 0.1324236949512628\n",
      "The 49141 th iteration gives loss of 0.13242254003628837\n",
      "The 49142 th iteration gives loss of 0.13242138513718596\n",
      "The 49143 th iteration gives loss of 0.13242023025396693\n",
      "The 49144 th iteration gives loss of 0.1324190753866178\n",
      "The 49145 th iteration gives loss of 0.13241792053514911\n",
      "The 49146 th iteration gives loss of 0.13241676569955266\n",
      "The 49147 th iteration gives loss of 0.1324156108798286\n",
      "The 49148 th iteration gives loss of 0.1324144560759731\n",
      "The 49149 th iteration gives loss of 0.13241330128799786\n",
      "The 49150 th iteration gives loss of 0.13241214651589484\n",
      "The 49151 th iteration gives loss of 0.13241099175965299\n",
      "The 49152 th iteration gives loss of 0.13240983701928444\n",
      "The 49153 th iteration gives loss of 0.1324086822947815\n",
      "The 49154 th iteration gives loss of 0.1324075275861459\n",
      "The 49155 th iteration gives loss of 0.1324063728933712\n",
      "The 49156 th iteration gives loss of 0.13240521821647203\n",
      "The 49157 th iteration gives loss of 0.13240406355543904\n",
      "The 49158 th iteration gives loss of 0.1324029089102589\n",
      "The 49159 th iteration gives loss of 0.1324017542809489\n",
      "The 49160 th iteration gives loss of 0.13240059966749756\n",
      "The 49161 th iteration gives loss of 0.13239944506991236\n",
      "The 49162 th iteration gives loss of 0.13239829048817628\n",
      "The 49163 th iteration gives loss of 0.13239713592230776\n",
      "The 49164 th iteration gives loss of 0.1323959813722941\n",
      "The 49165 th iteration gives loss of 0.1323948268381459\n",
      "The 49166 th iteration gives loss of 0.1323936723198483\n",
      "The 49167 th iteration gives loss of 0.132392517817401\n",
      "The 49168 th iteration gives loss of 0.1323913633308168\n",
      "The 49169 th iteration gives loss of 0.13239020886008976\n",
      "The 49170 th iteration gives loss of 0.13238905440520843\n",
      "The 49171 th iteration gives loss of 0.13238789996618552\n",
      "The 49172 th iteration gives loss of 0.13238674554300908\n",
      "The 49173 th iteration gives loss of 0.13238559113567977\n",
      "The 49174 th iteration gives loss of 0.13238443674420902\n",
      "The 49175 th iteration gives loss of 0.13238328236857877\n",
      "The 49176 th iteration gives loss of 0.13238212800880328\n",
      "The 49177 th iteration gives loss of 0.1323809736648752\n",
      "The 49178 th iteration gives loss of 0.1323798193367872\n",
      "The 49179 th iteration gives loss of 0.13237866502454948\n",
      "The 49180 th iteration gives loss of 0.13237751072815826\n",
      "The 49181 th iteration gives loss of 0.1323763564476055\n",
      "The 49182 th iteration gives loss of 0.13237520218289714\n",
      "The 49183 th iteration gives loss of 0.1323740479340329\n",
      "The 49184 th iteration gives loss of 0.13237289370099983\n",
      "The 49185 th iteration gives loss of 0.13237173948381772\n",
      "The 49186 th iteration gives loss of 0.13237058528247403\n",
      "The 49187 th iteration gives loss of 0.13236943109696792\n",
      "The 49188 th iteration gives loss of 0.13236827692729872\n",
      "The 49189 th iteration gives loss of 0.1323671227734652\n",
      "The 49190 th iteration gives loss of 0.132365968635471\n",
      "The 49191 th iteration gives loss of 0.13236481451331145\n",
      "The 49192 th iteration gives loss of 0.1323636604069847\n",
      "The 49193 th iteration gives loss of 0.13236250631648092\n",
      "The 49194 th iteration gives loss of 0.1323613522418239\n",
      "The 49195 th iteration gives loss of 0.13236019818299355\n",
      "The 49196 th iteration gives loss of 0.13235904413999114\n",
      "The 49197 th iteration gives loss of 0.1323578901128233\n",
      "The 49198 th iteration gives loss of 0.1323567361014827\n",
      "The 49199 th iteration gives loss of 0.1323555821059678\n",
      "The 49200 th iteration gives loss of 0.1323544281262871\n",
      "The 49201 th iteration gives loss of 0.13235327416242676\n",
      "The 49202 th iteration gives loss of 0.13235212021438764\n",
      "The 49203 th iteration gives loss of 0.13235096628218754\n",
      "The 49204 th iteration gives loss of 0.1323498123657971\n",
      "The 49205 th iteration gives loss of 0.1323486584652372\n",
      "The 49206 th iteration gives loss of 0.13234750458049685\n",
      "The 49207 th iteration gives loss of 0.13234635071157705\n",
      "The 49208 th iteration gives loss of 0.13234519685848056\n",
      "The 49209 th iteration gives loss of 0.13234404302120256\n",
      "The 49210 th iteration gives loss of 0.1323428891997328\n",
      "The 49211 th iteration gives loss of 0.13234173539409413\n",
      "The 49212 th iteration gives loss of 0.13234058160426898\n",
      "The 49213 th iteration gives loss of 0.13233942783026217\n",
      "The 49214 th iteration gives loss of 0.13233827407206736\n",
      "The 49215 th iteration gives loss of 0.13233712032968964\n",
      "The 49216 th iteration gives loss of 0.1323359666031222\n",
      "The 49217 th iteration gives loss of 0.13233481289237195\n",
      "The 49218 th iteration gives loss of 0.13233365919742446\n",
      "The 49219 th iteration gives loss of 0.13233250551830011\n",
      "The 49220 th iteration gives loss of 0.13233135185497294\n",
      "The 49221 th iteration gives loss of 0.1323301982074549\n",
      "The 49222 th iteration gives loss of 0.13232904457575054\n",
      "The 49223 th iteration gives loss of 0.132327890959859\n",
      "The 49224 th iteration gives loss of 0.13232673735977135\n",
      "The 49225 th iteration gives loss of 0.13232558377548428\n",
      "The 49226 th iteration gives loss of 0.1323244302070087\n",
      "The 49227 th iteration gives loss of 0.13232327665432736\n",
      "The 49228 th iteration gives loss of 0.13232212311746477\n",
      "The 49229 th iteration gives loss of 0.1323209695963948\n",
      "The 49230 th iteration gives loss of 0.13231981609112217\n",
      "The 49231 th iteration gives loss of 0.13231866260165331\n",
      "The 49232 th iteration gives loss of 0.1323175091279859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 49233 th iteration gives loss of 0.13231635567011815\n",
      "The 49234 th iteration gives loss of 0.13231520222804047\n",
      "The 49235 th iteration gives loss of 0.13231404880177344\n",
      "The 49236 th iteration gives loss of 0.13231289539129423\n",
      "The 49237 th iteration gives loss of 0.13231174199661108\n",
      "The 49238 th iteration gives loss of 0.1323105886177203\n",
      "The 49239 th iteration gives loss of 0.13230943525462566\n",
      "The 49240 th iteration gives loss of 0.13230828190732347\n",
      "The 49241 th iteration gives loss of 0.1323071285758163\n",
      "The 49242 th iteration gives loss of 0.13230597526009738\n",
      "The 49243 th iteration gives loss of 0.13230482196017546\n",
      "The 49244 th iteration gives loss of 0.1323036686760339\n",
      "The 49245 th iteration gives loss of 0.13230251540768337\n",
      "The 49246 th iteration gives loss of 0.13230136215512228\n",
      "The 49247 th iteration gives loss of 0.1323002089183422\n",
      "The 49248 th iteration gives loss of 0.13229905569735134\n",
      "The 49249 th iteration gives loss of 0.13229790249214454\n",
      "The 49250 th iteration gives loss of 0.13229674930272267\n",
      "The 49251 th iteration gives loss of 0.13229559612908662\n",
      "The 49252 th iteration gives loss of 0.13229444297123358\n",
      "The 49253 th iteration gives loss of 0.13229328982916486\n",
      "The 49254 th iteration gives loss of 0.13229213670287276\n",
      "The 49255 th iteration gives loss of 0.1322909835923562\n",
      "The 49256 th iteration gives loss of 0.13228983049762097\n",
      "The 49257 th iteration gives loss of 0.13228867741866754\n",
      "The 49258 th iteration gives loss of 0.13228752435549188\n",
      "The 49259 th iteration gives loss of 0.13228637130809281\n",
      "The 49260 th iteration gives loss of 0.13228521827646847\n",
      "The 49261 th iteration gives loss of 0.13228406526061984\n",
      "The 49262 th iteration gives loss of 0.13228291226054376\n",
      "The 49263 th iteration gives loss of 0.13228175927623154\n",
      "The 49264 th iteration gives loss of 0.13228060630770244\n",
      "The 49265 th iteration gives loss of 0.13227945335493865\n",
      "The 49266 th iteration gives loss of 0.1322783004179541\n",
      "The 49267 th iteration gives loss of 0.13227714749673067\n",
      "The 49268 th iteration gives loss of 0.1322759945912791\n",
      "The 49269 th iteration gives loss of 0.13227484170159973\n",
      "The 49270 th iteration gives loss of 0.13227368882768445\n",
      "The 49271 th iteration gives loss of 0.13227253596953356\n",
      "The 49272 th iteration gives loss of 0.13227138312715384\n",
      "The 49273 th iteration gives loss of 0.13227023030053936\n",
      "The 49274 th iteration gives loss of 0.13226907748967887\n",
      "The 49275 th iteration gives loss of 0.1322679246945905\n",
      "The 49276 th iteration gives loss of 0.1322667719152521\n",
      "The 49277 th iteration gives loss of 0.13226561915167903\n",
      "The 49278 th iteration gives loss of 0.13226446640387282\n",
      "The 49279 th iteration gives loss of 0.13226331367182484\n",
      "The 49280 th iteration gives loss of 0.1322621609555322\n",
      "The 49281 th iteration gives loss of 0.13226100825498754\n",
      "The 49282 th iteration gives loss of 0.1322598555702206\n",
      "The 49283 th iteration gives loss of 0.13225870290120562\n",
      "The 49284 th iteration gives loss of 0.13225755024793379\n",
      "The 49285 th iteration gives loss of 0.13225639761042274\n",
      "The 49286 th iteration gives loss of 0.13225524498866578\n",
      "The 49287 th iteration gives loss of 0.13225409238266292\n",
      "The 49288 th iteration gives loss of 0.13225293979240382\n",
      "The 49289 th iteration gives loss of 0.1322517872179065\n",
      "The 49290 th iteration gives loss of 0.13225063465915593\n",
      "The 49291 th iteration gives loss of 0.13224948211614967\n",
      "The 49292 th iteration gives loss of 0.13224832958889737\n",
      "The 49293 th iteration gives loss of 0.13224717707738992\n",
      "The 49294 th iteration gives loss of 0.13224602458163234\n",
      "The 49295 th iteration gives loss of 0.13224487210161273\n",
      "The 49296 th iteration gives loss of 0.132243719637344\n",
      "The 49297 th iteration gives loss of 0.132242567188818\n",
      "The 49298 th iteration gives loss of 0.13224141475603826\n",
      "The 49299 th iteration gives loss of 0.1322402623389976\n",
      "The 49300 th iteration gives loss of 0.13223910993769994\n",
      "The 49301 th iteration gives loss of 0.13223795755213785\n",
      "The 49302 th iteration gives loss of 0.13223680518231923\n",
      "The 49303 th iteration gives loss of 0.1322356528282451\n",
      "The 49304 th iteration gives loss of 0.1322345004899005\n",
      "The 49305 th iteration gives loss of 0.1322333481673017\n",
      "The 49306 th iteration gives loss of 0.13223219586043009\n",
      "The 49307 th iteration gives loss of 0.13223104356929954\n",
      "The 49308 th iteration gives loss of 0.1322298912939078\n",
      "The 49309 th iteration gives loss of 0.13222873903423943\n",
      "The 49310 th iteration gives loss of 0.1322275867903164\n",
      "The 49311 th iteration gives loss of 0.13222643456211747\n",
      "The 49312 th iteration gives loss of 0.13222528234964528\n",
      "The 49313 th iteration gives loss of 0.13222413015291115\n",
      "The 49314 th iteration gives loss of 0.13222297797190558\n",
      "The 49315 th iteration gives loss of 0.13222182580663186\n",
      "The 49316 th iteration gives loss of 0.1322206736570826\n",
      "The 49317 th iteration gives loss of 0.13221952152325547\n",
      "The 49318 th iteration gives loss of 0.13221836940516274\n",
      "The 49319 th iteration gives loss of 0.13221721730279146\n",
      "The 49320 th iteration gives loss of 0.13221606521614301\n",
      "The 49321 th iteration gives loss of 0.1322149131452233\n",
      "The 49322 th iteration gives loss of 0.1322137610900262\n",
      "The 49323 th iteration gives loss of 0.13221260905054422\n",
      "The 49324 th iteration gives loss of 0.13221145702679166\n",
      "The 49325 th iteration gives loss of 0.13221030501875086\n",
      "The 49326 th iteration gives loss of 0.1322091530264409\n",
      "The 49327 th iteration gives loss of 0.13220800104984562\n",
      "The 49328 th iteration gives loss of 0.13220684908896288\n",
      "The 49329 th iteration gives loss of 0.13220569714380243\n",
      "The 49330 th iteration gives loss of 0.132204545214359\n",
      "The 49331 th iteration gives loss of 0.13220339330062036\n",
      "The 49332 th iteration gives loss of 0.13220224140259973\n",
      "The 49333 th iteration gives loss of 0.13220108952029624\n",
      "The 49334 th iteration gives loss of 0.1321999376537157\n",
      "The 49335 th iteration gives loss of 0.13219878580282954\n",
      "The 49336 th iteration gives loss of 0.1321976339676628\n",
      "The 49337 th iteration gives loss of 0.13219648214820426\n",
      "The 49338 th iteration gives loss of 0.13219533034445802\n",
      "The 49339 th iteration gives loss of 0.13219417855641813\n",
      "The 49340 th iteration gives loss of 0.1321930267840804\n",
      "The 49341 th iteration gives loss of 0.13219187502746002\n",
      "The 49342 th iteration gives loss of 0.13219072328653572\n",
      "The 49343 th iteration gives loss of 0.1321895715613332\n",
      "The 49344 th iteration gives loss of 0.1321884198518199\n",
      "The 49345 th iteration gives loss of 0.13218726815801954\n",
      "The 49346 th iteration gives loss of 0.13218611647990638\n",
      "The 49347 th iteration gives loss of 0.13218496481750483\n",
      "The 49348 th iteration gives loss of 0.1321838131708057\n",
      "The 49349 th iteration gives loss of 0.1321826615398088\n",
      "The 49350 th iteration gives loss of 0.13218150992450411\n",
      "The 49351 th iteration gives loss of 0.13218035832489758\n",
      "The 49352 th iteration gives loss of 0.13217920674098865\n",
      "The 49353 th iteration gives loss of 0.13217805517277728\n",
      "The 49354 th iteration gives loss of 0.13217690362025897\n",
      "The 49355 th iteration gives loss of 0.13217575208344948\n",
      "The 49356 th iteration gives loss of 0.13217460056232072\n",
      "The 49357 th iteration gives loss of 0.1321734490568955\n",
      "The 49358 th iteration gives loss of 0.13217229756715032\n",
      "The 49359 th iteration gives loss of 0.1321711460931031\n",
      "The 49360 th iteration gives loss of 0.13216999463474688\n",
      "The 49361 th iteration gives loss of 0.1321688431920854\n",
      "The 49362 th iteration gives loss of 0.13216769176510745\n",
      "The 49363 th iteration gives loss of 0.1321665403538209\n",
      "The 49364 th iteration gives loss of 0.13216538895821361\n",
      "The 49365 th iteration gives loss of 0.13216423757829582\n",
      "The 49366 th iteration gives loss of 0.13216308621406503\n",
      "The 49367 th iteration gives loss of 0.1321619348655242\n",
      "The 49368 th iteration gives loss of 0.13216078353266128\n",
      "The 49369 th iteration gives loss of 0.1321596322154926\n",
      "The 49370 th iteration gives loss of 0.1321584809139975\n",
      "The 49371 th iteration gives loss of 0.13215732962817864\n",
      "The 49372 th iteration gives loss of 0.13215617835804375\n",
      "The 49373 th iteration gives loss of 0.13215502710359048\n",
      "The 49374 th iteration gives loss of 0.13215387586482033\n",
      "The 49375 th iteration gives loss of 0.13215272464172237\n",
      "The 49376 th iteration gives loss of 0.13215157343429768\n",
      "The 49377 th iteration gives loss of 0.13215042224256182\n",
      "The 49378 th iteration gives loss of 0.13214927106649163\n",
      "The 49379 th iteration gives loss of 0.13214811990609657\n",
      "The 49380 th iteration gives loss of 0.13214696876138216\n",
      "The 49381 th iteration gives loss of 0.1321458176323385\n",
      "The 49382 th iteration gives loss of 0.13214466651896067\n",
      "The 49383 th iteration gives loss of 0.1321435154212556\n",
      "The 49384 th iteration gives loss of 0.13214236433923385\n",
      "The 49385 th iteration gives loss of 0.1321412132728652\n",
      "The 49386 th iteration gives loss of 0.1321400622221757\n",
      "The 49387 th iteration gives loss of 0.13213891118715104\n",
      "The 49388 th iteration gives loss of 0.13213776016779227\n",
      "The 49389 th iteration gives loss of 0.13213660916410253\n",
      "The 49390 th iteration gives loss of 0.13213545817607733\n",
      "The 49391 th iteration gives loss of 0.13213430720371902\n",
      "The 49392 th iteration gives loss of 0.1321331562470245\n",
      "The 49393 th iteration gives loss of 0.1321320053059921\n",
      "The 49394 th iteration gives loss of 0.1321308543806248\n",
      "The 49395 th iteration gives loss of 0.13212970347090913\n",
      "The 49396 th iteration gives loss of 0.1321285525768632\n",
      "The 49397 th iteration gives loss of 0.13212740169847784\n",
      "The 49398 th iteration gives loss of 0.13212625083574436\n",
      "The 49399 th iteration gives loss of 0.13212509998866936\n",
      "The 49400 th iteration gives loss of 0.13212394915724984\n",
      "The 49401 th iteration gives loss of 0.13212279834148882\n",
      "The 49402 th iteration gives loss of 0.13212164754138675\n",
      "The 49403 th iteration gives loss of 0.13212049675693782\n",
      "The 49404 th iteration gives loss of 0.13211934598814024\n",
      "The 49405 th iteration gives loss of 0.1321181952349976\n",
      "The 49406 th iteration gives loss of 0.1321170444975122\n",
      "The 49407 th iteration gives loss of 0.1321158937756747\n",
      "The 49408 th iteration gives loss of 0.1321147430694875\n",
      "The 49409 th iteration gives loss of 0.13211359237894438\n",
      "The 49410 th iteration gives loss of 0.13211244170404726\n",
      "The 49411 th iteration gives loss of 0.13211129104481523\n",
      "The 49412 th iteration gives loss of 0.13211014040121605\n",
      "The 49413 th iteration gives loss of 0.1321089897732664\n",
      "The 49414 th iteration gives loss of 0.1321078391609646\n",
      "The 49415 th iteration gives loss of 0.13210668856430116\n",
      "The 49416 th iteration gives loss of 0.13210553798328506\n",
      "The 49417 th iteration gives loss of 0.13210438741791317\n",
      "The 49418 th iteration gives loss of 0.1321032368681822\n",
      "The 49419 th iteration gives loss of 0.13210208633409856\n",
      "The 49420 th iteration gives loss of 0.1321009358156472\n",
      "The 49421 th iteration gives loss of 0.13209978531283711\n",
      "The 49422 th iteration gives loss of 0.1320986348256707\n",
      "The 49423 th iteration gives loss of 0.13209748435413382\n",
      "The 49424 th iteration gives loss of 0.13209633389824235\n",
      "The 49425 th iteration gives loss of 0.13209518345799146\n",
      "The 49426 th iteration gives loss of 0.13209403303336734\n",
      "The 49427 th iteration gives loss of 0.1320928826243727\n",
      "The 49428 th iteration gives loss of 0.13209173223102408\n",
      "The 49429 th iteration gives loss of 0.13209058185329872\n",
      "The 49430 th iteration gives loss of 0.13208943149120253\n",
      "The 49431 th iteration gives loss of 0.132088281144745\n",
      "The 49432 th iteration gives loss of 0.13208713081392368\n",
      "The 49433 th iteration gives loss of 0.13208598049871964\n",
      "The 49434 th iteration gives loss of 0.1320848301991485\n",
      "The 49435 th iteration gives loss of 0.13208367991520628\n",
      "The 49436 th iteration gives loss of 0.13208252964689066\n",
      "The 49437 th iteration gives loss of 0.13208137939420525\n",
      "The 49438 th iteration gives loss of 0.13208022915714568\n",
      "The 49439 th iteration gives loss of 0.1320790789357023\n",
      "The 49440 th iteration gives loss of 0.13207792872989052\n",
      "The 49441 th iteration gives loss of 0.13207677853969613\n",
      "The 49442 th iteration gives loss of 0.13207562836512632\n",
      "The 49443 th iteration gives loss of 0.13207447820617524\n",
      "The 49444 th iteration gives loss of 0.1320733280628477\n",
      "The 49445 th iteration gives loss of 0.13207217793514736\n",
      "The 49446 th iteration gives loss of 0.1320710278230548\n",
      "The 49447 th iteration gives loss of 0.1320698777265847\n",
      "The 49448 th iteration gives loss of 0.13206872764572375\n",
      "The 49449 th iteration gives loss of 0.1320675775804878\n",
      "The 49450 th iteration gives loss of 0.13206642753086678\n",
      "The 49451 th iteration gives loss of 0.13206527749686184\n",
      "The 49452 th iteration gives loss of 0.13206412747846605\n",
      "The 49453 th iteration gives loss of 0.13206297747568807\n",
      "The 49454 th iteration gives loss of 0.1320618274885126\n",
      "The 49455 th iteration gives loss of 0.13206067751694847\n",
      "The 49456 th iteration gives loss of 0.13205952756100173\n",
      "The 49457 th iteration gives loss of 0.1320583776206667\n",
      "The 49458 th iteration gives loss of 0.13205722769594044\n",
      "The 49459 th iteration gives loss of 0.13205607778682213\n",
      "The 49460 th iteration gives loss of 0.13205492789330447\n",
      "The 49461 th iteration gives loss of 0.13205377801539742\n",
      "The 49462 th iteration gives loss of 0.1320526281530962\n",
      "The 49463 th iteration gives loss of 0.13205147830639857\n",
      "The 49464 th iteration gives loss of 0.1320503284753058\n",
      "The 49465 th iteration gives loss of 0.13204917865981503\n",
      "The 49466 th iteration gives loss of 0.1320480288599266\n",
      "The 49467 th iteration gives loss of 0.13204687907563648\n",
      "The 49468 th iteration gives loss of 0.13204572930694733\n",
      "The 49469 th iteration gives loss of 0.13204457955385937\n",
      "The 49470 th iteration gives loss of 0.1320434298163728\n",
      "The 49471 th iteration gives loss of 0.13204228009448074\n",
      "The 49472 th iteration gives loss of 0.13204113038818702\n",
      "The 49473 th iteration gives loss of 0.1320399806974951\n",
      "The 49474 th iteration gives loss of 0.1320388310223913\n",
      "The 49475 th iteration gives loss of 0.13203768136288588\n",
      "The 49476 th iteration gives loss of 0.13203653171897198\n",
      "The 49477 th iteration gives loss of 0.13203538209065394\n",
      "The 49478 th iteration gives loss of 0.1320342324779239\n",
      "The 49479 th iteration gives loss of 0.13203308288078786\n",
      "The 49480 th iteration gives loss of 0.13203193329924387\n",
      "The 49481 th iteration gives loss of 0.13203078373328964\n",
      "The 49482 th iteration gives loss of 0.13202963418291494\n",
      "The 49483 th iteration gives loss of 0.1320284846481412\n",
      "The 49484 th iteration gives loss of 0.1320273351289526\n",
      "The 49485 th iteration gives loss of 0.1320261856253447\n",
      "The 49486 th iteration gives loss of 0.13202503613732752\n",
      "The 49487 th iteration gives loss of 0.1320238866648918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 49488 th iteration gives loss of 0.13202273720803762\n",
      "The 49489 th iteration gives loss of 0.13202158776676692\n",
      "The 49490 th iteration gives loss of 0.1320204383410829\n",
      "The 49491 th iteration gives loss of 0.13201928893098155\n",
      "The 49492 th iteration gives loss of 0.13201813953646424\n",
      "The 49493 th iteration gives loss of 0.1320169901575101\n",
      "The 49494 th iteration gives loss of 0.13201584079414783\n",
      "The 49495 th iteration gives loss of 0.1320146914463571\n",
      "The 49496 th iteration gives loss of 0.1320135421141526\n",
      "The 49497 th iteration gives loss of 0.13201239279752314\n",
      "The 49498 th iteration gives loss of 0.13201124349646667\n",
      "The 49499 th iteration gives loss of 0.13201009421098064\n",
      "The 49500 th iteration gives loss of 0.1320089449410738\n",
      "The 49501 th iteration gives loss of 0.13200779568673826\n",
      "The 49502 th iteration gives loss of 0.13200664644798218\n",
      "The 49503 th iteration gives loss of 0.1320054972247869\n",
      "The 49504 th iteration gives loss of 0.1320043480171643\n",
      "The 49505 th iteration gives loss of 0.13200319882511743\n",
      "The 49506 th iteration gives loss of 0.13200204964863432\n",
      "The 49507 th iteration gives loss of 0.13200090048772434\n",
      "The 49508 th iteration gives loss of 0.13199975134237962\n",
      "The 49509 th iteration gives loss of 0.13199860221259652\n",
      "The 49510 th iteration gives loss of 0.13199745309838729\n",
      "The 49511 th iteration gives loss of 0.13199630399973677\n",
      "The 49512 th iteration gives loss of 0.13199515491665792\n",
      "The 49513 th iteration gives loss of 0.1319940058491295\n",
      "The 49514 th iteration gives loss of 0.1319928567971719\n",
      "The 49515 th iteration gives loss of 0.1319917077607803\n",
      "The 49516 th iteration gives loss of 0.13199055873994633\n",
      "The 49517 th iteration gives loss of 0.1319894097346685\n",
      "The 49518 th iteration gives loss of 0.13198826074495124\n",
      "The 49519 th iteration gives loss of 0.13198711177079267\n",
      "The 49520 th iteration gives loss of 0.13198596281219435\n",
      "The 49521 th iteration gives loss of 0.13198481386914931\n",
      "The 49522 th iteration gives loss of 0.13198366494167094\n",
      "The 49523 th iteration gives loss of 0.13198251602973443\n",
      "The 49524 th iteration gives loss of 0.13198136713335346\n",
      "The 49525 th iteration gives loss of 0.13198021825253126\n",
      "The 49526 th iteration gives loss of 0.13197906938725346\n",
      "The 49527 th iteration gives loss of 0.1319779205375375\n",
      "The 49528 th iteration gives loss of 0.13197677170336428\n",
      "The 49529 th iteration gives loss of 0.1319756228847485\n",
      "The 49530 th iteration gives loss of 0.131974474081681\n",
      "The 49531 th iteration gives loss of 0.1319733252941559\n",
      "The 49532 th iteration gives loss of 0.1319721765221855\n",
      "The 49533 th iteration gives loss of 0.13197102776576297\n",
      "The 49534 th iteration gives loss of 0.13196987902487436\n",
      "The 49535 th iteration gives loss of 0.13196873029954218\n",
      "The 49536 th iteration gives loss of 0.1319675815897468\n",
      "The 49537 th iteration gives loss of 0.13196643289550242\n",
      "The 49538 th iteration gives loss of 0.13196528421679873\n",
      "The 49539 th iteration gives loss of 0.13196413555363495\n",
      "The 49540 th iteration gives loss of 0.13196298690601854\n",
      "The 49541 th iteration gives loss of 0.13196183827393243\n",
      "The 49542 th iteration gives loss of 0.1319606896573909\n",
      "The 49543 th iteration gives loss of 0.1319595410563905\n",
      "The 49544 th iteration gives loss of 0.13195839247092767\n",
      "The 49545 th iteration gives loss of 0.13195724390100413\n",
      "The 49546 th iteration gives loss of 0.13195609534660185\n",
      "The 49547 th iteration gives loss of 0.13195494680774883\n",
      "The 49548 th iteration gives loss of 0.13195379828443077\n",
      "The 49549 th iteration gives loss of 0.13195264977664137\n",
      "The 49550 th iteration gives loss of 0.1319515012843821\n",
      "The 49551 th iteration gives loss of 0.1319503528076603\n",
      "The 49552 th iteration gives loss of 0.13194920434647409\n",
      "The 49553 th iteration gives loss of 0.1319480559008094\n",
      "The 49554 th iteration gives loss of 0.1319469074706735\n",
      "The 49555 th iteration gives loss of 0.13194575905607547\n",
      "The 49556 th iteration gives loss of 0.1319446106570006\n",
      "The 49557 th iteration gives loss of 0.13194346227345366\n",
      "The 49558 th iteration gives loss of 0.13194231390543376\n",
      "The 49559 th iteration gives loss of 0.1319411655529346\n",
      "The 49560 th iteration gives loss of 0.1319400172159692\n",
      "The 49561 th iteration gives loss of 0.13193886889451797\n",
      "The 49562 th iteration gives loss of 0.131937720588595\n",
      "The 49563 th iteration gives loss of 0.13193657229820335\n",
      "The 49564 th iteration gives loss of 0.13193542402331718\n",
      "The 49565 th iteration gives loss of 0.1319342757639602\n",
      "The 49566 th iteration gives loss of 0.1319331275201267\n",
      "The 49567 th iteration gives loss of 0.1319319792918025\n",
      "The 49568 th iteration gives loss of 0.1319308310789974\n",
      "The 49569 th iteration gives loss of 0.1319296828817162\n",
      "The 49570 th iteration gives loss of 0.13192853469994745\n",
      "The 49571 th iteration gives loss of 0.13192738653369715\n",
      "The 49572 th iteration gives loss of 0.1319262383829539\n",
      "The 49573 th iteration gives loss of 0.131925090247735\n",
      "The 49574 th iteration gives loss of 0.1319239421280264\n",
      "The 49575 th iteration gives loss of 0.13192279402382384\n",
      "The 49576 th iteration gives loss of 0.1319216459351446\n",
      "The 49577 th iteration gives loss of 0.13192049786197138\n",
      "The 49578 th iteration gives loss of 0.13191934980430936\n",
      "The 49579 th iteration gives loss of 0.13191820176214922\n",
      "The 49580 th iteration gives loss of 0.1319170537355019\n",
      "The 49581 th iteration gives loss of 0.131915905724365\n",
      "The 49582 th iteration gives loss of 0.13191475772874045\n",
      "The 49583 th iteration gives loss of 0.13191360974861496\n",
      "The 49584 th iteration gives loss of 0.1319124617839943\n",
      "The 49585 th iteration gives loss of 0.131911313834869\n",
      "The 49586 th iteration gives loss of 0.1319101659012602\n",
      "The 49587 th iteration gives loss of 0.1319090179831538\n",
      "The 49588 th iteration gives loss of 0.13190787008054297\n",
      "The 49589 th iteration gives loss of 0.1319067221934341\n",
      "The 49590 th iteration gives loss of 0.13190557432183048\n",
      "The 49591 th iteration gives loss of 0.13190442646572295\n",
      "The 49592 th iteration gives loss of 0.13190327862511594\n",
      "The 49593 th iteration gives loss of 0.13190213080000218\n",
      "The 49594 th iteration gives loss of 0.13190098299039302\n",
      "The 49595 th iteration gives loss of 0.1318998351962729\n",
      "The 49596 th iteration gives loss of 0.13189868741764807\n",
      "The 49597 th iteration gives loss of 0.13189753965453097\n",
      "The 49598 th iteration gives loss of 0.13189639190689106\n",
      "The 49599 th iteration gives loss of 0.13189524417475013\n",
      "The 49600 th iteration gives loss of 0.1318940964581009\n",
      "The 49601 th iteration gives loss of 0.13189294875694638\n",
      "The 49602 th iteration gives loss of 0.13189180107127954\n",
      "The 49603 th iteration gives loss of 0.13189065340110148\n",
      "The 49604 th iteration gives loss of 0.1318895057464168\n",
      "The 49605 th iteration gives loss of 0.13188835810722147\n",
      "The 49606 th iteration gives loss of 0.13188721048350505\n",
      "The 49607 th iteration gives loss of 0.13188606287527227\n",
      "The 49608 th iteration gives loss of 0.13188491528253835\n",
      "The 49609 th iteration gives loss of 0.1318837677052768\n",
      "The 49610 th iteration gives loss of 0.13188262014351484\n",
      "The 49611 th iteration gives loss of 0.13188147259722666\n",
      "The 49612 th iteration gives loss of 0.13188032506641928\n",
      "The 49613 th iteration gives loss of 0.13187917755110243\n",
      "The 49614 th iteration gives loss of 0.13187803005125293\n",
      "The 49615 th iteration gives loss of 0.13187688256689178\n",
      "The 49616 th iteration gives loss of 0.13187573509800568\n",
      "The 49617 th iteration gives loss of 0.13187458764459983\n",
      "The 49618 th iteration gives loss of 0.13187344020667338\n",
      "The 49619 th iteration gives loss of 0.13187229278422447\n",
      "The 49620 th iteration gives loss of 0.13187114537724656\n",
      "The 49621 th iteration gives loss of 0.1318699979857512\n",
      "The 49622 th iteration gives loss of 0.13186885060972306\n",
      "The 49623 th iteration gives loss of 0.1318677032491723\n",
      "The 49624 th iteration gives loss of 0.13186655590408858\n",
      "The 49625 th iteration gives loss of 0.1318654085744793\n",
      "The 49626 th iteration gives loss of 0.1318642612603437\n",
      "The 49627 th iteration gives loss of 0.13186311396168243\n",
      "The 49628 th iteration gives loss of 0.13186196667848532\n",
      "The 49629 th iteration gives loss of 0.13186081941075728\n",
      "The 49630 th iteration gives loss of 0.1318596721584922\n",
      "The 49631 th iteration gives loss of 0.13185852492170744\n",
      "The 49632 th iteration gives loss of 0.13185737770037798\n",
      "The 49633 th iteration gives loss of 0.13185623049451378\n",
      "The 49634 th iteration gives loss of 0.13185508330411752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 49635 th iteration gives loss of 0.13185393612918667\n",
      "The 49636 th iteration gives loss of 0.1318527889697221\n",
      "The 49637 th iteration gives loss of 0.13185164182571693\n",
      "The 49638 th iteration gives loss of 0.13185049469717372\n",
      "The 49639 th iteration gives loss of 0.13184934758408706\n",
      "The 49640 th iteration gives loss of 0.1318482004864558\n",
      "The 49641 th iteration gives loss of 0.13184705340429415\n",
      "The 49642 th iteration gives loss of 0.1318459063375806\n",
      "The 49643 th iteration gives loss of 0.13184475928632627\n",
      "The 49644 th iteration gives loss of 0.13184361225052982\n",
      "The 49645 th iteration gives loss of 0.13184246523019214\n",
      "The 49646 th iteration gives loss of 0.13184131822531003\n",
      "The 49647 th iteration gives loss of 0.13184017123588415\n",
      "The 49648 th iteration gives loss of 0.1318390242619032\n",
      "The 49649 th iteration gives loss of 0.13183787730338453\n",
      "The 49650 th iteration gives loss of 0.13183673036031118\n",
      "The 49651 th iteration gives loss of 0.1318355834326876\n",
      "The 49652 th iteration gives loss of 0.13183443652051424\n",
      "The 49653 th iteration gives loss of 0.13183328962379073\n",
      "The 49654 th iteration gives loss of 0.13183214274252178\n",
      "The 49655 th iteration gives loss of 0.13183099587669325\n",
      "The 49656 th iteration gives loss of 0.13182984902631503\n",
      "The 49657 th iteration gives loss of 0.13182870219138298\n",
      "The 49658 th iteration gives loss of 0.13182755537189364\n",
      "The 49659 th iteration gives loss of 0.13182640856785316\n",
      "The 49660 th iteration gives loss of 0.13182526177925435\n",
      "The 49661 th iteration gives loss of 0.13182411500609773\n",
      "The 49662 th iteration gives loss of 0.13182296824838433\n",
      "The 49663 th iteration gives loss of 0.13182182150610905\n",
      "The 49664 th iteration gives loss of 0.13182067477928383\n",
      "The 49665 th iteration gives loss of 0.13181952806788416\n",
      "The 49666 th iteration gives loss of 0.13181838137192994\n",
      "The 49667 th iteration gives loss of 0.13181723469141468\n",
      "The 49668 th iteration gives loss of 0.1318160880263352\n",
      "The 49669 th iteration gives loss of 0.13181494137669297\n",
      "The 49670 th iteration gives loss of 0.1318137947424892\n",
      "The 49671 th iteration gives loss of 0.13181264812371724\n",
      "The 49672 th iteration gives loss of 0.1318115015203753\n",
      "The 49673 th iteration gives loss of 0.13181035493246634\n",
      "The 49674 th iteration gives loss of 0.1318092083599936\n",
      "The 49675 th iteration gives loss of 0.1318080618029574\n",
      "The 49676 th iteration gives loss of 0.13180691526134905\n",
      "The 49677 th iteration gives loss of 0.13180576873516706\n",
      "The 49678 th iteration gives loss of 0.13180462222441008\n",
      "The 49679 th iteration gives loss of 0.13180347572908957\n",
      "The 49680 th iteration gives loss of 0.1318023292492004\n",
      "The 49681 th iteration gives loss of 0.13180118278472355\n",
      "The 49682 th iteration gives loss of 0.131800036335688\n",
      "The 49683 th iteration gives loss of 0.13179888990206684\n",
      "The 49684 th iteration gives loss of 0.13179774348387685\n",
      "The 49685 th iteration gives loss of 0.1317965970811091\n",
      "The 49686 th iteration gives loss of 0.13179545069376583\n",
      "The 49687 th iteration gives loss of 0.13179430432184122\n",
      "The 49688 th iteration gives loss of 0.13179315796534008\n",
      "The 49689 th iteration gives loss of 0.13179201162425255\n",
      "The 49690 th iteration gives loss of 0.13179086529858564\n",
      "The 49691 th iteration gives loss of 0.13178971898834357\n",
      "The 49692 th iteration gives loss of 0.13178857269351626\n",
      "The 49693 th iteration gives loss of 0.13178742641410668\n",
      "The 49694 th iteration gives loss of 0.13178628015011692\n",
      "The 49695 th iteration gives loss of 0.13178513390153987\n",
      "The 49696 th iteration gives loss of 0.13178398766837757\n",
      "The 49697 th iteration gives loss of 0.13178284145062694\n",
      "The 49698 th iteration gives loss of 0.1317816952482837\n",
      "The 49699 th iteration gives loss of 0.13178054906136943\n",
      "The 49700 th iteration gives loss of 0.13177940288985465\n",
      "The 49701 th iteration gives loss of 0.13177825673375443\n",
      "The 49702 th iteration gives loss of 0.13177711059306582\n",
      "The 49703 th iteration gives loss of 0.1317759644677836\n",
      "The 49704 th iteration gives loss of 0.13177481835791494\n",
      "The 49705 th iteration gives loss of 0.13177367226344552\n",
      "The 49706 th iteration gives loss of 0.13177252618438295\n",
      "The 49707 th iteration gives loss of 0.13177138012073208\n",
      "The 49708 th iteration gives loss of 0.13177023407248672\n",
      "The 49709 th iteration gives loss of 0.131769088039641\n",
      "The 49710 th iteration gives loss of 0.1317679420221952\n",
      "The 49711 th iteration gives loss of 0.1317667960201719\n",
      "The 49712 th iteration gives loss of 0.13176565003353422\n",
      "The 49713 th iteration gives loss of 0.1317645040622942\n",
      "The 49714 th iteration gives loss of 0.13176335810645912\n",
      "The 49715 th iteration gives loss of 0.13176221216603487\n",
      "The 49716 th iteration gives loss of 0.13176106624099815\n",
      "The 49717 th iteration gives loss of 0.13175992033135833\n",
      "The 49718 th iteration gives loss of 0.13175877443711942\n",
      "The 49719 th iteration gives loss of 0.13175762855827808\n",
      "The 49720 th iteration gives loss of 0.131756482694826\n",
      "The 49721 th iteration gives loss of 0.13175533684677496\n",
      "The 49722 th iteration gives loss of 0.13175419101411448\n",
      "The 49723 th iteration gives loss of 0.131753045196849\n",
      "The 49724 th iteration gives loss of 0.13175189939497614\n",
      "The 49725 th iteration gives loss of 0.13175075360849497\n",
      "The 49726 th iteration gives loss of 0.13174960783740108\n",
      "The 49727 th iteration gives loss of 0.13174846208170218\n",
      "The 49728 th iteration gives loss of 0.13174731634139097\n",
      "The 49729 th iteration gives loss of 0.13174617061646182\n",
      "The 49730 th iteration gives loss of 0.1317450249069266\n",
      "The 49731 th iteration gives loss of 0.1317438792127814\n",
      "The 49732 th iteration gives loss of 0.13174273353401902\n",
      "The 49733 th iteration gives loss of 0.13174158787063978\n",
      "The 49734 th iteration gives loss of 0.13174044222263975\n",
      "The 49735 th iteration gives loss of 0.13173929659003503\n",
      "The 49736 th iteration gives loss of 0.1317381509728086\n",
      "The 49737 th iteration gives loss of 0.13173700537096217\n",
      "The 49738 th iteration gives loss of 0.13173585978449734\n",
      "The 49739 th iteration gives loss of 0.13173471421342062\n",
      "The 49740 th iteration gives loss of 0.13173356865771443\n",
      "The 49741 th iteration gives loss of 0.13173242311738698\n",
      "The 49742 th iteration gives loss of 0.13173127759243836\n",
      "The 49743 th iteration gives loss of 0.13173013208286322\n",
      "The 49744 th iteration gives loss of 0.13172898658867635\n",
      "The 49745 th iteration gives loss of 0.13172784110985866\n",
      "The 49746 th iteration gives loss of 0.13172669564641762\n",
      "The 49747 th iteration gives loss of 0.1317255501983514\n",
      "The 49748 th iteration gives loss of 0.13172440476566066\n",
      "The 49749 th iteration gives loss of 0.1317232593483324\n",
      "The 49750 th iteration gives loss of 0.131722113946378\n",
      "The 49751 th iteration gives loss of 0.13172096855979867\n",
      "The 49752 th iteration gives loss of 0.13171982318858869\n",
      "The 49753 th iteration gives loss of 0.13171867783275898\n",
      "The 49754 th iteration gives loss of 0.13171753249228205\n",
      "The 49755 th iteration gives loss of 0.13171638716718154\n",
      "The 49756 th iteration gives loss of 0.1317152418574367\n",
      "The 49757 th iteration gives loss of 0.1317140965630654\n",
      "The 49758 th iteration gives loss of 0.13171295128406021\n",
      "The 49759 th iteration gives loss of 0.13171180602042099\n",
      "The 49760 th iteration gives loss of 0.13171066077214297\n",
      "The 49761 th iteration gives loss of 0.13170951553922638\n",
      "The 49762 th iteration gives loss of 0.13170837032166952\n",
      "The 49763 th iteration gives loss of 0.13170722511948388\n",
      "The 49764 th iteration gives loss of 0.13170607993265682\n",
      "The 49765 th iteration gives loss of 0.1317049347611812\n",
      "The 49766 th iteration gives loss of 0.13170378960507176\n",
      "The 49767 th iteration gives loss of 0.13170264446431965\n",
      "The 49768 th iteration gives loss of 0.13170149933892372\n",
      "The 49769 th iteration gives loss of 0.13170035422888418\n",
      "The 49770 th iteration gives loss of 0.1316992091342\n",
      "The 49771 th iteration gives loss of 0.1316980640548742\n",
      "The 49772 th iteration gives loss of 0.13169691899089678\n",
      "The 49773 th iteration gives loss of 0.13169577394227697\n",
      "The 49774 th iteration gives loss of 0.1316946289090117\n",
      "The 49775 th iteration gives loss of 0.1316934838910935\n",
      "The 49776 th iteration gives loss of 0.1316923388885297\n",
      "The 49777 th iteration gives loss of 0.13169119390132242\n",
      "The 49778 th iteration gives loss of 0.13169004892945063\n",
      "The 49779 th iteration gives loss of 0.1316889039729378\n",
      "The 49780 th iteration gives loss of 0.13168775903176458\n",
      "The 49781 th iteration gives loss of 0.13168661410595128\n",
      "The 49782 th iteration gives loss of 0.13168546919547708\n",
      "The 49783 th iteration gives loss of 0.13168432430034663\n",
      "The 49784 th iteration gives loss of 0.1316831794205607\n",
      "The 49785 th iteration gives loss of 0.1316820345561193\n",
      "The 49786 th iteration gives loss of 0.1316808897070206\n",
      "The 49787 th iteration gives loss of 0.13167974487326317\n",
      "The 49788 th iteration gives loss of 0.13167860005485096\n",
      "The 49789 th iteration gives loss of 0.1316774552517822\n",
      "The 49790 th iteration gives loss of 0.13167631046405479\n",
      "The 49791 th iteration gives loss of 0.13167516569165583\n",
      "The 49792 th iteration gives loss of 0.13167402093460165\n",
      "The 49793 th iteration gives loss of 0.13167287619288176\n",
      "The 49794 th iteration gives loss of 0.13167173146650243\n",
      "The 49795 th iteration gives loss of 0.13167058675546464\n",
      "The 49796 th iteration gives loss of 0.13166944205975148\n",
      "The 49797 th iteration gives loss of 0.13166829737938152\n",
      "The 49798 th iteration gives loss of 0.13166715271433949\n",
      "The 49799 th iteration gives loss of 0.13166600806463616\n",
      "The 49800 th iteration gives loss of 0.13166486343025816\n",
      "The 49801 th iteration gives loss of 0.13166371881121403\n",
      "The 49802 th iteration gives loss of 0.13166257420749933\n",
      "The 49803 th iteration gives loss of 0.1316614296191171\n",
      "The 49804 th iteration gives loss of 0.1316602850460628\n",
      "The 49805 th iteration gives loss of 0.13165914048833555\n",
      "The 49806 th iteration gives loss of 0.13165799594593963\n",
      "The 49807 th iteration gives loss of 0.13165685141886685\n",
      "The 49808 th iteration gives loss of 0.13165570690712045\n",
      "The 49809 th iteration gives loss of 0.1316545624107016\n",
      "The 49810 th iteration gives loss of 0.13165341792960833\n",
      "The 49811 th iteration gives loss of 0.1316522734638345\n",
      "The 49812 th iteration gives loss of 0.13165112901337966\n",
      "The 49813 th iteration gives loss of 0.13164998457825466\n",
      "The 49814 th iteration gives loss of 0.13164884015844847\n",
      "The 49815 th iteration gives loss of 0.13164769575396146\n",
      "The 49816 th iteration gives loss of 0.13164655136479259\n",
      "The 49817 th iteration gives loss of 0.1316454069909447\n",
      "The 49818 th iteration gives loss of 0.13164426263241208\n",
      "The 49819 th iteration gives loss of 0.13164311828919947\n",
      "The 49820 th iteration gives loss of 0.13164197396130745\n",
      "The 49821 th iteration gives loss of 0.1316408296487201\n",
      "The 49822 th iteration gives loss of 0.13163968535145562\n",
      "The 49823 th iteration gives loss of 0.1316385410695019\n",
      "The 49824 th iteration gives loss of 0.1316373968028656\n",
      "The 49825 th iteration gives loss of 0.13163625255153688\n",
      "The 49826 th iteration gives loss of 0.13163510831552674\n",
      "The 49827 th iteration gives loss of 0.13163396409482153\n",
      "The 49828 th iteration gives loss of 0.13163281988942688\n",
      "The 49829 th iteration gives loss of 0.13163167569934398\n",
      "The 49830 th iteration gives loss of 0.1316305315245695\n",
      "The 49831 th iteration gives loss of 0.13162938736509808\n",
      "The 49832 th iteration gives loss of 0.1316282432209393\n",
      "The 49833 th iteration gives loss of 0.13162709909208273\n",
      "The 49834 th iteration gives loss of 0.13162595497853338\n",
      "The 49835 th iteration gives loss of 0.13162481088028882\n",
      "The 49836 th iteration gives loss of 0.13162366679734766\n",
      "The 49837 th iteration gives loss of 0.1316225227297085\n",
      "The 49838 th iteration gives loss of 0.13162137867737458\n",
      "The 49839 th iteration gives loss of 0.13162023464033826\n",
      "The 49840 th iteration gives loss of 0.13161909061861\n",
      "The 49841 th iteration gives loss of 0.13161794661217152\n",
      "The 49842 th iteration gives loss of 0.131616802621036\n",
      "The 49843 th iteration gives loss of 0.1316156586452001\n",
      "The 49844 th iteration gives loss of 0.13161451468465954\n",
      "The 49845 th iteration gives loss of 0.13161337073942445\n",
      "The 49846 th iteration gives loss of 0.13161222680947668\n",
      "The 49847 th iteration gives loss of 0.13161108289482915\n",
      "The 49848 th iteration gives loss of 0.13160993899546877\n",
      "The 49849 th iteration gives loss of 0.13160879511141152\n",
      "The 49850 th iteration gives loss of 0.13160765124264479\n",
      "The 49851 th iteration gives loss of 0.13160650738917085\n",
      "The 49852 th iteration gives loss of 0.13160536355098004\n",
      "The 49853 th iteration gives loss of 0.1316042197280868\n",
      "The 49854 th iteration gives loss of 0.13160307592047818\n",
      "The 49855 th iteration gives loss of 0.1316019321281633\n",
      "The 49856 th iteration gives loss of 0.1316007883511393\n",
      "The 49857 th iteration gives loss of 0.1315996445893954\n",
      "The 49858 th iteration gives loss of 0.1315985008429387\n",
      "The 49859 th iteration gives loss of 0.13159735711177487\n",
      "The 49860 th iteration gives loss of 0.13159621339589325\n",
      "The 49861 th iteration gives loss of 0.13159506969529197\n",
      "The 49862 th iteration gives loss of 0.13159392600997646\n",
      "The 49863 th iteration gives loss of 0.13159278233994726\n",
      "The 49864 th iteration gives loss of 0.13159163868519785\n",
      "The 49865 th iteration gives loss of 0.13159049504572612\n",
      "The 49866 th iteration gives loss of 0.13158935142153574\n",
      "The 49867 th iteration gives loss of 0.13158820781262806\n",
      "The 49868 th iteration gives loss of 0.13158706421899855\n",
      "The 49869 th iteration gives loss of 0.13158592064065042\n",
      "The 49870 th iteration gives loss of 0.13158477707756908\n",
      "The 49871 th iteration gives loss of 0.13158363352977243\n",
      "The 49872 th iteration gives loss of 0.13158248999725541\n",
      "The 49873 th iteration gives loss of 0.13158134648000136\n",
      "The 49874 th iteration gives loss of 0.1315802029780275\n",
      "The 49875 th iteration gives loss of 0.13157905949133278\n",
      "The 49876 th iteration gives loss of 0.13157791601990512\n",
      "The 49877 th iteration gives loss of 0.1315767725637487\n",
      "The 49878 th iteration gives loss of 0.13157562912287002\n",
      "The 49879 th iteration gives loss of 0.1315744856972537\n",
      "The 49880 th iteration gives loss of 0.13157334228690662\n",
      "The 49881 th iteration gives loss of 0.13157219889182561\n",
      "The 49882 th iteration gives loss of 0.13157105551202547\n",
      "The 49883 th iteration gives loss of 0.13156991214748634\n",
      "The 49884 th iteration gives loss of 0.13156876879821222\n",
      "The 49885 th iteration gives loss of 0.13156762546419984\n",
      "The 49886 th iteration gives loss of 0.1315664821454554\n",
      "The 49887 th iteration gives loss of 0.13156533884197483\n",
      "The 49888 th iteration gives loss of 0.1315641955537611\n",
      "The 49889 th iteration gives loss of 0.1315630522808069\n",
      "The 49890 th iteration gives loss of 0.1315619090231157\n",
      "The 49891 th iteration gives loss of 0.1315607657806816\n",
      "The 49892 th iteration gives loss of 0.13155962255351317\n",
      "The 49893 th iteration gives loss of 0.13155847934160392\n",
      "The 49894 th iteration gives loss of 0.13155733614494705\n",
      "The 49895 th iteration gives loss of 0.1315561929635515\n",
      "The 49896 th iteration gives loss of 0.13155504979741214\n",
      "The 49897 th iteration gives loss of 0.131553906646536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 49898 th iteration gives loss of 0.1315527635109074\n",
      "The 49899 th iteration gives loss of 0.13155162039053586\n",
      "The 49900 th iteration gives loss of 0.1315504772854209\n",
      "The 49901 th iteration gives loss of 0.13154933419555623\n",
      "The 49902 th iteration gives loss of 0.13154819112094157\n",
      "The 49903 th iteration gives loss of 0.13154704806157733\n",
      "The 49904 th iteration gives loss of 0.1315459050174733\n",
      "The 49905 th iteration gives loss of 0.13154476198861087\n",
      "The 49906 th iteration gives loss of 0.13154361897500394\n",
      "The 49907 th iteration gives loss of 0.13154247597664503\n",
      "The 49908 th iteration gives loss of 0.13154133299352813\n",
      "The 49909 th iteration gives loss of 0.131540190025656\n",
      "The 49910 th iteration gives loss of 0.13153904707304442\n",
      "The 49911 th iteration gives loss of 0.13153790413566685\n",
      "The 49912 th iteration gives loss of 0.13153676121353416\n",
      "The 49913 th iteration gives loss of 0.13153561830664917\n",
      "The 49914 th iteration gives loss of 0.13153447541500551\n",
      "The 49915 th iteration gives loss of 0.13153333253860577\n",
      "The 49916 th iteration gives loss of 0.1315321896774439\n",
      "The 49917 th iteration gives loss of 0.13153104683152347\n",
      "The 49918 th iteration gives loss of 0.13152990400084263\n",
      "The 49919 th iteration gives loss of 0.1315287611854047\n",
      "The 49920 th iteration gives loss of 0.13152761838520471\n",
      "The 49921 th iteration gives loss of 0.1315264756002354\n",
      "The 49922 th iteration gives loss of 0.13152533283051088\n",
      "The 49923 th iteration gives loss of 0.13152419007602148\n",
      "The 49924 th iteration gives loss of 0.1315230473367698\n",
      "The 49925 th iteration gives loss of 0.13152190461275406\n",
      "The 49926 th iteration gives loss of 0.13152076190395964\n",
      "The 49927 th iteration gives loss of 0.13151961921040536\n",
      "The 49928 th iteration gives loss of 0.13151847653208343\n",
      "The 49929 th iteration gives loss of 0.13151733386899397\n",
      "The 49930 th iteration gives loss of 0.13151619122113153\n",
      "The 49931 th iteration gives loss of 0.1315150485885047\n",
      "The 49932 th iteration gives loss of 0.13151390597110502\n",
      "The 49933 th iteration gives loss of 0.13151276336894024\n",
      "The 49934 th iteration gives loss of 0.1315116207819987\n",
      "The 49935 th iteration gives loss of 0.13151047821027365\n",
      "The 49936 th iteration gives loss of 0.13150933565378128\n",
      "The 49937 th iteration gives loss of 0.13150819311251266\n",
      "The 49938 th iteration gives loss of 0.1315070505864789\n",
      "The 49939 th iteration gives loss of 0.13150590807565962\n",
      "The 49940 th iteration gives loss of 0.13150476558006935\n",
      "The 49941 th iteration gives loss of 0.13150362309969563\n",
      "The 49942 th iteration gives loss of 0.13150248063454792\n",
      "The 49943 th iteration gives loss of 0.13150133818461868\n",
      "The 49944 th iteration gives loss of 0.13150019574990385\n",
      "The 49945 th iteration gives loss of 0.13149905333041612\n",
      "The 49946 th iteration gives loss of 0.13149791092613822\n",
      "The 49947 th iteration gives loss of 0.1314967685370898\n",
      "The 49948 th iteration gives loss of 0.1314956261632441\n",
      "The 49949 th iteration gives loss of 0.1314944838046229\n",
      "The 49950 th iteration gives loss of 0.13149334146121924\n",
      "The 49951 th iteration gives loss of 0.13149219913302646\n",
      "The 49952 th iteration gives loss of 0.13149105682004839\n",
      "The 49953 th iteration gives loss of 0.13148991452228262\n",
      "The 49954 th iteration gives loss of 0.13148877223972932\n",
      "The 49955 th iteration gives loss of 0.131487629972395\n",
      "The 49956 th iteration gives loss of 0.1314864877202606\n",
      "The 49957 th iteration gives loss of 0.13148534548334792\n",
      "The 49958 th iteration gives loss of 0.13148420326162766\n",
      "The 49959 th iteration gives loss of 0.13148306105512242\n",
      "The 49960 th iteration gives loss of 0.13148191886383215\n",
      "The 49961 th iteration gives loss of 0.13148077668773736\n",
      "The 49962 th iteration gives loss of 0.1314796345268586\n",
      "The 49963 th iteration gives loss of 0.1314784923811799\n",
      "The 49964 th iteration gives loss of 0.13147735025070084\n",
      "The 49965 th iteration gives loss of 0.13147620813543312\n",
      "The 49966 th iteration gives loss of 0.13147506603537285\n",
      "The 49967 th iteration gives loss of 0.13147392395050475\n",
      "The 49968 th iteration gives loss of 0.13147278188084396\n",
      "The 49969 th iteration gives loss of 0.1314716398263842\n",
      "The 49970 th iteration gives loss of 0.1314704977871195\n",
      "The 49971 th iteration gives loss of 0.13146935576306146\n",
      "The 49972 th iteration gives loss of 0.13146821375419646\n",
      "The 49973 th iteration gives loss of 0.13146707176053055\n",
      "The 49974 th iteration gives loss of 0.1314659297820617\n",
      "The 49975 th iteration gives loss of 0.13146478781878582\n",
      "The 49976 th iteration gives loss of 0.13146364587070752\n",
      "The 49977 th iteration gives loss of 0.13146250393782152\n",
      "The 49978 th iteration gives loss of 0.1314613620201308\n",
      "The 49979 th iteration gives loss of 0.13146022011763228\n",
      "The 49980 th iteration gives loss of 0.1314590782303299\n",
      "The 49981 th iteration gives loss of 0.13145793635821842\n",
      "The 49982 th iteration gives loss of 0.13145679450129535\n",
      "The 49983 th iteration gives loss of 0.13145565265956105\n",
      "The 49984 th iteration gives loss of 0.13145451083301463\n",
      "The 49985 th iteration gives loss of 0.13145336902166654\n",
      "The 49986 th iteration gives loss of 0.1314522272254944\n",
      "The 49987 th iteration gives loss of 0.13145108544451384\n",
      "The 49988 th iteration gives loss of 0.13144994367872329\n",
      "The 49989 th iteration gives loss of 0.13144880192811434\n",
      "The 49990 th iteration gives loss of 0.13144766019269155\n",
      "The 49991 th iteration gives loss of 0.13144651847244326\n",
      "The 49992 th iteration gives loss of 0.1314453767673962\n",
      "The 49993 th iteration gives loss of 0.13144423507752095\n",
      "The 49994 th iteration gives loss of 0.13144309340282728\n",
      "The 49995 th iteration gives loss of 0.13144195174331166\n",
      "The 49996 th iteration gives loss of 0.13144081009897984\n",
      "The 49997 th iteration gives loss of 0.13143966846982755\n",
      "The 49998 th iteration gives loss of 0.1314385268558535\n",
      "The 49999 th iteration gives loss of 0.1314373852570537\n"
     ]
    }
   ],
   "source": [
    "#start gradient decent\n",
    "#calculate prediction\n",
    "#calculate loss\n",
    "#diravative, gradeient decent\n",
    "#loop again\n",
    "learning_rate=0.0000000001\n",
    "for i in range(50000):\n",
    "    prediction = numpy.dot(train_x,weight)\n",
    "    #print(prediction,numpy.shape(prediction))\n",
    "    loss=prediction-train_y\n",
    "    #for j in range(len(loss)):\n",
    "     #   loss[j]*=std_y\n",
    "     #   loss[j]+=mean_y\n",
    "    #print(loss,numpy.shape(loss))\n",
    "    gradient = 2*numpy.dot(train_x.T,loss)\n",
    "    #print(gradient,numpy.shape(gradient))\n",
    "    weight=weight-learning_rate*gradient\n",
    "    #print(f'Shape of gradient:{numpy.shape(gradient)},shape of prediction{numpy.shape(gradient)}, shape of loss {numpy.shape(loss)}')\n",
    "    print(f\"The {i} th iteration gives loss of \",end=\"\")\n",
    "    print(numpy.mean(loss))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.save('weight.npy',weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction=numpy.dot(test_x,weight)\n",
    "test_loss=test_prediction-test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1314373852570537\n"
     ]
    }
   ],
   "source": [
    "print(numpy.mean(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['21' '21' '20' ... '19' '18' '17']\n",
      " ['1.7' '1.7' '1.7' ... '1.7' '1.7' '1.8']\n",
      " ['0.39' '0.36' '0.36' ... '0.34' '0.31' '0.23']\n",
      " ...\n",
      " ['76' '99' '93' ... '98' '97' '65']\n",
      " ['2.2' '3.2' '2.5' ... '5.7' '4.9' '3.6']\n",
      " ['1.7' '2.8' '2.6' ... '4.9' '5.2' '3.6']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Neo\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "validData=pandas.read_csv('./data_ml1/test.csv',header=None,encoding='utf8')\n",
    "valid_data=validData.iloc[:,2:]\n",
    "valid_data[valid_data=='NR']=0\n",
    "valid_data=valid_data.to_numpy()\n",
    "test_x=numpy.empty((240,162),dtype=float)\n",
    "for i in range (240):\n",
    "    test_x[i,:]=valid_data[18*i:18*i+18,:].reshape(1,-1)\n",
    "print(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 1)\n"
     ]
    }
   ],
   "source": [
    "#valid_prediction=numpy.dot(test_x,weight)\n",
    "#print(valid_prediction)\n",
    "print(numpy.shape(valid_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'value']\n",
      "['id_0', 2.6514136665095798]\n",
      "['id_1', 16.88421132557367]\n",
      "['id_2', 18.644232575464624]\n",
      "['id_3', 16.230511038885957]\n",
      "['id_4', 20.31557065289793]\n",
      "['id_5', 23.681100041536695]\n",
      "['id_6', 21.866408694433208]\n",
      "['id_7', 31.607957987856555]\n",
      "['id_8', 21.34537828252421]\n",
      "['id_9', 56.08241957889004]\n",
      "['id_10', 8.759110973288987]\n",
      "['id_11', 27.645116284768164]\n",
      "['id_12', 59.84807031350698]\n",
      "['id_13', 41.934265240504075]\n",
      "['id_14', 17.430619596480458]\n",
      "['id_15', 0.6500988683938196]\n",
      "['id_16', 29.33311370206604]\n",
      "['id_17', 55.915778056895974]\n",
      "['id_18', 5.663325919030884]\n",
      "['id_19', 12.062924891241753]\n",
      "['id_20', 39.925916811099846]\n",
      "['id_21', 67.17758658324335]\n",
      "['id_22', 10.418910744599943]\n",
      "['id_23', 19.05542693311931]\n",
      "['id_24', 14.242135050515323]\n",
      "['id_25', 37.06032967512538]\n",
      "['id_26', 10.410016801984296]\n",
      "['id_27', 70.70460953692535]\n",
      "['id_28', 5.958285675904983]\n",
      "['id_29', 59.01538249823989]\n",
      "['id_30', 35.28269896632034]\n",
      "['id_31', 0.5753336708941127]\n",
      "['id_32', 10.62212524037024]\n",
      "['id_33', 27.42899819281601]\n",
      "['id_34', 31.32104035772427]\n",
      "['id_35', 28.41291974362955]\n",
      "['id_36', 36.30391670133982]\n",
      "['id_37', 27.51256335077806]\n",
      "['id_38', 63.04404934574981]\n",
      "['id_39', 45.20102761187518]\n",
      "['id_40', 16.631038275133502]\n",
      "['id_41', 42.482686746372366]\n",
      "['id_42', 23.91649289866192]\n",
      "['id_43', 43.70411652394508]\n",
      "['id_44', 9.654696040273647]\n",
      "['id_45', 31.787700892221014]\n",
      "['id_46', 16.85421607233531]\n",
      "['id_47', 9.439802181381655]\n",
      "['id_48', 26.388889355499995]\n",
      "['id_49', 40.03006501757129]\n",
      "['id_50', 29.559218050540984]\n",
      "['id_51', 15.169792571788642]\n",
      "['id_52', 15.649748728778611]\n",
      "['id_53', 56.11272325660938]\n",
      "['id_54', 22.087064057713103]\n",
      "['id_55', 28.93976664025854]\n",
      "['id_56', 23.160717954627216]\n",
      "['id_57', 17.66203317164771]\n",
      "['id_58', 57.64517695201191]\n",
      "['id_59', 18.983884856831505]\n",
      "['id_60', 27.934757786696007]\n",
      "['id_61', 34.794101519163824]\n",
      "['id_62', 8.812312651320578]\n",
      "['id_63', 38.405491465178805]\n",
      "['id_64', 10.142141031412852]\n",
      "['id_65', 11.274045850831904]\n",
      "['id_66', 12.221882894451022]\n",
      "['id_67', 15.030892575762648]\n",
      "['id_68', 48.78066026985886]\n",
      "['id_69', 27.367893892310995]\n",
      "['id_70', 18.636239646297927]\n",
      "['id_71', 39.348246492521106]\n",
      "['id_72', 48.46398107786508]\n",
      "['id_73', 1.313698768878396]\n",
      "['id_74', 20.13856726498597]\n",
      "['id_75', 6.191321530248585]\n",
      "['id_76', 36.35082706867125]\n",
      "['id_77', 8.717490304180297]\n",
      "['id_78', 21.2906483619731]\n",
      "['id_79', 21.199286823505485]\n",
      "['id_80', 22.092919422681202]\n",
      "['id_81', 46.512177328928836]\n",
      "['id_82', 29.91914452968788]\n",
      "['id_83', 89.69967109101692]\n",
      "['id_84', 35.08740950823184]\n",
      "['id_85', 28.044377501268837]\n",
      "['id_86', 31.373853211392788]\n",
      "['id_87', 28.783128840782382]\n",
      "['id_88', 24.79804430262041]\n",
      "['id_89', 14.253401355678541]\n",
      "['id_90', 27.41044921301387]\n",
      "['id_91', 38.63514120100684]\n",
      "['id_92', 8.077984889020708]\n",
      "['id_93', 32.248911626566574]\n",
      "['id_94', 45.77914940010902]\n",
      "['id_95', 11.301945067584256]\n",
      "['id_96', 32.16574421321508]\n",
      "['id_97', 10.079167616441634]\n",
      "['id_98', 24.11623756562508]\n",
      "['id_99', 8.1554998330033]\n",
      "['id_100', 14.234052582825813]\n",
      "['id_101', 29.3390455814125]\n",
      "['id_102', 29.8744966858156]\n",
      "['id_103', 12.038099907796807]\n",
      "['id_104', 28.09115579486861]\n",
      "['id_105', 35.702156860625166]\n",
      "['id_106', 34.160598454749874]\n",
      "['id_107', 6.25149674910671]\n",
      "['id_108', 22.964613966646436]\n",
      "['id_109', 69.46500692421952]\n",
      "['id_110', 42.236638511855794]\n",
      "['id_111', 11.610493987886858]\n",
      "['id_112', 25.86701687215571]\n",
      "['id_113', 16.530707852480706]\n",
      "['id_114', 18.285013933035906]\n",
      "['id_115', 17.964577068171348]\n",
      "['id_116', 26.415691819128792]\n",
      "['id_117', 15.579776230214854]\n",
      "['id_118', 6.808965954037317]\n",
      "['id_119', 11.686417036616206]\n",
      "['id_120', 85.48275326406949]\n",
      "['id_121', 30.0224155465405]\n",
      "['id_122', 27.517737335977305]\n",
      "['id_123', 20.84062687814322]\n",
      "['id_124', 13.796722979268628]\n",
      "['id_125', 36.64538175679311]\n",
      "['id_126', 11.724109775040073]\n",
      "['id_127', 22.645534791832247]\n",
      "['id_128', 31.446254566071268]\n",
      "['id_129', 58.57511982720946]\n",
      "['id_130', 26.605442906918626]\n",
      "['id_131', 7.634372686509692]\n",
      "['id_132', 50.097651381624644]\n",
      "['id_133', 14.785299194747147]\n",
      "['id_134', 13.700554783122247]\n",
      "['id_135', -5.589719789684363]\n",
      "['id_136', 26.334010798842396]\n",
      "['id_137', 55.40282785415801]\n",
      "['id_138', 32.40292060059739]\n",
      "['id_139', 14.288076090941091]\n",
      "['id_140', 22.380896631930096]\n",
      "['id_141', 26.785445381967595]\n",
      "['id_142', 32.75331454957316]\n",
      "['id_143', 33.76391211691277]\n",
      "['id_144', 16.300697101242985]\n",
      "['id_145', 15.648621683808623]\n",
      "['id_146', 6.8065635437985765]\n",
      "['id_147', 51.94022505828216]\n",
      "['id_148', 19.00074027526901]\n",
      "['id_149', 30.038251853330603]\n",
      "['id_150', 5.172578204940095]\n",
      "['id_151', 8.389660715887715]\n",
      "['id_152', 19.507341649195922]\n",
      "['id_153', 7.635294468200276]\n",
      "['id_154', 8.999501989724019]\n",
      "['id_155', 45.95893721088755]\n",
      "['id_156', 5.605083447048299]\n",
      "['id_157', 38.47206415365969]\n",
      "['id_158', 8.30120910836978]\n",
      "['id_159', 18.869253960021492]\n",
      "['id_160', 44.82937259475632]\n",
      "['id_161', 34.32384806501766]\n",
      "['id_162', 7.071452996670155]\n",
      "['id_163', 6.511812300688995]\n",
      "['id_164', 61.97988427459187]\n",
      "['id_165', 38.88660801226238]\n",
      "['id_166', 22.17942142269581]\n",
      "['id_167', 15.391031068207202]\n",
      "['id_168', 53.21663661667724]\n",
      "['id_169', 7.111303398998724]\n",
      "['id_170', 77.68333998136454]\n",
      "['id_171', 37.76903268019388]\n",
      "['id_172', 25.94058665130594]\n",
      "['id_173', 17.156899304381263]\n",
      "['id_174', 67.9676753489078]\n",
      "['id_175', 23.65317381975316]\n",
      "['id_176', 22.954278005696246]\n",
      "['id_177', 23.725598455440615]\n",
      "['id_178', 14.130685840442757]\n",
      "['id_179', 18.740836788874642]\n",
      "['id_180', 25.330687361020438]\n",
      "['id_181', 6.749103585847628]\n",
      "['id_182', 57.601572297089625]\n",
      "['id_183', 34.17452994055497]\n",
      "['id_184', 17.548022474416882]\n",
      "['id_185', 25.93822191670172]\n",
      "['id_186', 22.97449409312168]\n",
      "['id_187', 64.68196092614878]\n",
      "['id_188', 4.908932346520799]\n",
      "['id_189', 43.00667569008062]\n",
      "['id_190', 27.91526335054189]\n",
      "['id_191', 23.519707703899197]\n",
      "['id_192', 21.47363186349439]\n",
      "['id_193', 3.391243554384138]\n",
      "['id_194', 17.015944883894292]\n",
      "['id_195', 3.378216659773212]\n",
      "['id_196', 28.435383317188123]\n",
      "['id_197', 3.5557161450550403]\n",
      "['id_198', 16.999501394714727]\n",
      "['id_199', 52.6049602090356]\n",
      "['id_200', 23.84620824426019]\n",
      "['id_201', 30.51864274189236]\n",
      "['id_202', 60.79576449524026]\n",
      "['id_203', 13.54932278075695]\n",
      "['id_204', 13.51749273942902]\n",
      "['id_205', 12.299370659771713]\n",
      "['id_206', 11.64619061729492]\n",
      "['id_207', 2.79268730361275]\n",
      "['id_208', 112.87356305466125]\n",
      "['id_209', 12.024142364818912]\n",
      "['id_210', 13.934846764913312]\n",
      "['id_211', 5.1006821445118025]\n",
      "['id_212', 35.182862324700906]\n",
      "['id_213', 28.918535440750997]\n",
      "['id_214', 22.87021832507602]\n",
      "['id_215', 31.198723402396674]\n",
      "['id_216', 70.05772650812553]\n",
      "['id_217', 2.5232171855370975]\n",
      "['id_218', 8.222695628012394]\n",
      "['id_219', 27.85180336460219]\n",
      "['id_220', 12.995393987849146]\n",
      "['id_221', 10.480085591096529]\n",
      "['id_222', 116.6880544677119]\n",
      "['id_223', 19.8850994087448]\n",
      "['id_224', 11.186285316365584]\n",
      "['id_225', 56.07631587332965]\n",
      "['id_226', 17.387774958437458]\n",
      "['id_227', 15.755264545972578]\n",
      "['id_228', 10.575925284470106]\n",
      "['id_229', 9.11853723205414]\n",
      "['id_230', 44.6703297373901]\n",
      "['id_231', 11.700126771733817]\n",
      "['id_232', 33.92288197619192]\n",
      "['id_233', 44.456478824217584]\n",
      "['id_234', 24.202044102918475]\n",
      "['id_235', 30.895163551777177]\n",
      "['id_236', 58.61442453784626]\n",
      "['id_237', 32.061009488421355]\n",
      "['id_238', 22.52120501986657]\n",
      "['id_239', 31.088665252566937]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open('submit.csv',mode='w',newline='') as submit_file:\n",
    "    csv_writer=csv.writer(submit_file)\n",
    "    header=['id','value']\n",
    "    csv_writer.writerow(header)\n",
    "    print(header)\n",
    "    for i in range (240):\n",
    "        row = ['id_'+str(i),valid_prediction[i][0]]\n",
    "        csv_writer.writerow(row)\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
